{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define default variables\n",
    "prefix = 'large'\n",
    "cut = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get stop words\n",
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './data/large-padded'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e146b1f985ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/{}-padded'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mpaddedreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaddedreader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/{}-responses'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: './data/large-padded'"
     ]
    }
   ],
   "source": [
    "with open('./data/{}-padded'.format(prefix), 'rb') as f:\n",
    "    paddedreader = csv.reader(f)\n",
    "\n",
    "    padded = [row for row in paddedreader]\n",
    "with open('./data/{}-responses'.format(prefix), 'r') as f2:\n",
    "    responses = json.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_docs = map(lambda x: \" \".join(x).replace(\" <PAD/>\", \"\"),padded)\n",
    "y_raw = map(lambda x: x.index(1) ,responses)\n",
    "df =  pd.DataFrame({\"Doc\" : raw_docs, \"Category\" : y_raw})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Split training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create training/testing\n",
    "doc_train, doc_test = train_test_split(df, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Build classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "[[132   2   0   4   1   0   0   2   3   1   1]\n",
      " [  5 102   1   0  11   1   1   8   3   7  12]\n",
      " [  0   2 119  10   2   0   1   2   0   1   0]\n",
      " [  1   0   7 143   2   0   1   0   0   3   1]\n",
      " [  1  22   3   2  68   1   7  17   9   1   8]\n",
      " [  0   1   0   0   0 148   6   0   1   3   3]\n",
      " [  6   2   5   2   1  14 107   0   2   5  14]\n",
      " [  1   9   2   2   5   1   1 116   1   1   2]\n",
      " [  9   9   0   1   6   4   1  10  91  18   2]\n",
      " [  2   9   2   2   4   3   1   3  14  76  11]\n",
      " [  0   9   3   2   7   3   8   0   1   9 115]]\n"
     ]
    }
   ],
   "source": [
    "# Build linear classifier using square_hinge loss. Can use SVM by switching to hinge loss.\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words = en_stop)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='squared_hinge', penalty='l2',\n",
    "                                           alpha=1e-3, n_iter=5, random_state=42)),\n",
    "])\n",
    "text_clf.fit(doc_train[\"Doc\"], doc_train[\"Category\"])\n",
    "predicted_category = text_clf.predict(doc_test[\"Doc\"])\n",
    "print('Accuracy: %.2f' % accuracy_score(doc_test[\"Category\"], predicted_category))\n",
    "print(confusion_matrix(y_true=doc_test[\"Category\"], y_pred=predicted_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.84      0.90      0.87       146\n",
      "    class 1       0.61      0.68      0.64       151\n",
      "    class 2       0.84      0.87      0.85       137\n",
      "    class 3       0.85      0.91      0.88       158\n",
      "    class 4       0.64      0.49      0.55       139\n",
      "    class 5       0.85      0.91      0.88       162\n",
      "    class 6       0.80      0.68      0.73       158\n",
      "    class 7       0.73      0.82      0.78       141\n",
      "    class 8       0.73      0.60      0.66       151\n",
      "    class 9       0.61      0.60      0.60       127\n",
      "   class 10       0.68      0.73      0.71       157\n",
      "\n",
      "avg / total       0.75      0.75      0.74      1627\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1', 'class 2', 'class 3', 'class 4', 'class 5', 'class 6', 'class 7', 'class 8', 'class 9', 'class 10']\n",
    "print(classification_report(y_true=doc_test[\"Category\"], y_pred=predicted_category, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.80829493  0.73548387  0.76476015  0.70940959  0.59409594]\n",
      "Accuracy: 0.72 (+/- 0.14)\n"
     ]
    }
   ],
   "source": [
    "# run cross validation\n",
    "scores = cross_val_score(text_clf, df[\"Doc\"], df[\"Category\"], cv=5)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Accuracy on training set. Detect potential overfitting\n",
    "predicted_category = text_clf.predict(doc_train[\"Doc\"])\n",
    "print('Accuracy: %.2f' % accuracy_score(doc_train[\"Category\"], predicted_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Futher work\n",
    "\n",
    "- More features, especially adding word vectors\n",
    "- include topic modeling as features?\n",
    "- Training on large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
