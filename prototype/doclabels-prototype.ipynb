{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, Text, ForeignKey\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "# Data injection\n",
    "engine = create_engine('postgresql://jliu3230@localhost/enshance')\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = session.execute('''SELECT * from labelling_titledescriptionsubjects order by id''')\n",
    "# Preprocessing\n",
    "\n",
    "results = results.fetchall()\n",
    "subjects = [result[4] for result in results]\n",
    "doc_ids = [result[0] for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subjects = list(map(lambda x: x.strip('[').strip(']').strip('u\\'').strip('\\''), subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "subjects_list = list(map(lambda x: re.findall('u\\'(.*?)\\'', x), subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ex14001 = subjects[14000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ex14001 = clean_doc(ex14001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer(min_df=1)\n",
    "vectorizer2.fit_transform([ex14001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ex_vector = np.asarray([np.asarray(model[x]) for x in vectorizer2.get_feature_names()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ex_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(ex_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"exvector_noscience\", 'wb') as f:\n",
    "    np.save(f, np.delete(ex_vector, (1), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get taxonomy terms\n",
    "engine2 = create_engine('postgresql://jliu3230@localhost/sharetaxonomy')\n",
    "Session2 = sessionmaker(bind=engine2)\n",
    "session2 = Session2()\n",
    "results2 = session2.execute('''SELECT term from terms''')\n",
    "terms = [result for result in results2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = subjects_list[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = [subject.split(' ') for subject in example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep training\n",
    "total = 0\n",
    "missing = []\n",
    "for tv in terms:\n",
    "    for t in tv[0].split(\" \"):\n",
    "        total += 1\n",
    "        try: \n",
    "            model[t]\n",
    "        except:\n",
    "            missing.append(tv)\n",
    "            break\n",
    "print(total)\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total words: 20140; missing words: 3003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.similarity('mathematics', 'biological')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "norm(model['computer'] - model['physical'])+norm(model['science'] - model['science'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove stop words from terms\n",
    "- Remove punctuations\n",
    "- Split subjects by \", \" and \" - \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove stop words from taxonomy terms\n",
    "\n",
    "# get stop words\n",
    "import requests\n",
    "import bs4\n",
    "\n",
    "html = requests.get(\"http://www.ranks.nl/stopwords\").content\n",
    "soup = bs4.BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str(soup.find_all('td')[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = str(soup.find_all('td')[0])[17:-5].split(\"<br/>\") + str(soup.find_all('td')[1])[17:-5].split(\"<br/>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the collections of terms for each root\n",
    "# Computer science\n",
    "collections = {}\n",
    "roots = (991, 2092, 2911, 2948, 3208, 5930, 7398, 7626, 8600, 8912, 9268)\n",
    "for root in roots:\n",
    "    tree = session2.execute('select * from taxonomytree where array_length(path, 1) < 10 and path[1] = {};'.format(root)).fetchall()\n",
    "    child_terms = [row[3] for row in tree]\n",
    "    child_ids = [row[2] for row in tree]\n",
    "    collections[root] = [(tree[0][0], tree[0][1])] + [x for x in zip(child_ids, child_terms)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collections[2092]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# join all terms for each collection\n",
    "collection_docs = {}\n",
    "for key in collections.keys():\n",
    "    collection_docs[key] = \" \".join([termtuple[1] for termtuple in collections[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection_docs[2092]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build corpus\n",
    "collection_corpus = list(collection_docs.values())\n",
    "collection_corpus = list(map(clean_doc, collection_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection_corpus[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# analyze docs using scikit learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "analyze = vectorizer.build_analyzer()\n",
    "analyze(collection_corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(collection_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_of_words = np.sum(X.toarray(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection_weights = X.toarray() / num_of_words[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make sure weights add up to 1\n",
    "np.sum(collection_weights, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection_vectors = [model[x] for x in vectorizer.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"collection_vectors.pickle\", 'wb') as f:\n",
    "    np.save(f, collection_vectors)\n",
    "with open(\"collection_weights.pickle\", 'wb') as f:\n",
    "    np.save(f, collection_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now deal with subjects from the data\n",
    "subjects_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First just try a few example\n",
    "subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(collection_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection_docs.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# see if the word science is dominant\n",
    "\n",
    "vectorizer.get_feature_names()[1588]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection_weights[:,1588]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"collection_vectors_noscience\", 'wb') as f:\n",
    "    np.save(f, np.delete(collection_vectors, (1588), 0))\n",
    "with open(\"collection_weights_noscience\", 'wb') as f:\n",
    "    np.save(f, np.delete(collection_weights, 1588, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.delete(collection_vectors, (1588),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_dist(ex_vector, np.asarray(collection_vectors)[np.where(collection_weights[0]>0)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_dist(ex_vector, np.asarray(collection_vectors)[np.where(collection_weights[8]>0)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_dist(ex_vector, np.asarray(collection_vectors)[np.where(collection_weights[9]>0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(11):\n",
    "    print(doc_dist(np.delete(ex_vector,(1),0), np.asarray(collection_vectors)[np.where(collection_weights[i]>0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.repeat(ex_vector, len(collection_vectors), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_dist(ex_vector, np.asarray(collection_vectors)[np.where(collection_weights[0]>0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(collection_weights[0]>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.asarray(collection_vectors)[np.where(collection_weights[0]>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### try title and description\n",
    "\n",
    "titles = [result[1] for result in results]\n",
    "descriptions = [result[2] for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "td14001 = clean_doc(clean_doc(titles[14000] + descriptions[14000]))\n",
    "td14001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer3 = CountVectorizer(min_df=1, stop_words=stopwords)\n",
    "vectorizer3.fit_transform([td14001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "td_vector = np.asarray([np.asarray(model[x]) for x in vectorizer3.get_feature_names()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(11):\n",
    "    print(doc_dist(td_vector, np.asarray(collection_vectors)[np.where(collection_weights[i]>0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_sample = titles[14002:14021]\n",
    "description_sample = descriptions[14002:14021]\n",
    "td_sample = list(map(lambda x: \" \".join(x), zip(title_sample, description_sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "td_sample = list(map(clean_doc, td_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "td_sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer3 = CountVectorizer(min_df=1, stop_words=stopwords)\n",
    "X3 = vectorizer3.fit_transform(td_sample)\n",
    "min_dist = []\n",
    "for i in range(19):\n",
    "    td_sample_vector = np.asarray([np.asarray(model[x]) for x in np.asarray(vectorizer3.get_feature_names())[np.where(X3.toarray()[i,:]>0)]])\n",
    "    distances = [doc_dist(td_sample_vector, np.asarray(collection_vectors)[np.where(collection_weights[j]>0)]) for j in range(11)]\n",
    "    val, idx = min((val, idx) for (idx, val) in enumerate(distances))\n",
    "    min_dist += (val,idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(min_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.asarray(vectorizer3.get_feature_names())[np.where(X3[1,:]>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X3.toarray()[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try basic word count method\n",
    "sample_vocab = np.asarray(vectorizer3.get_feature_names())\n",
    "collection_vocab = np.asarray(vectorizer.get_feature_names())\n",
    "sample_X = X3.toarray()\n",
    "collection_X = X.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sample_dist in sample_X:\n",
    "    sample_words = list(sample_vocab[np.where(sample_dist>0)])\n",
    "    print(sample_words)\n",
    "    count = []\n",
    "    min_dist = []\n",
    "    for collection_dist in collection_X:\n",
    "        collection_words = list(collection_vocab[np.where(collection_dist>0)])\n",
    "        # print(len([x for x in sample_words if x in collection_words]))\n",
    "        print(len([x for x in sample_words if x in collection_words]))\n",
    "        print([x for x in sample_words if x in collection_words])\n",
    "    #min_dist += (min((val, idx) for (idx, val) in enumerate(count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(min_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare data: subjects, title and description, title description subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test functions here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list((map(clean_doc, td_sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_docs(td_sample, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_docs(td_sample, vectorizer, method='wmd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_docs(td_sample, vectorizer, method='count_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare documents\n",
    "# read labels\n",
    "truelabelscsv = np.genfromtxt ('truelabels.csv', delimiter=\",\").astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_labels = list(enumerate(collection_docs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subjects only, title_description\n",
    "# sample ids\n",
    "sample_ids = truelabelscsv[:,0]\n",
    "truelabels = truelabelscsv[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get all the rows from result\n",
    "sample = np.asarray(results)[sample_ids-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subjects\n",
    "sample_subjects = sample[:,4]\n",
    "# title\n",
    "sample_titles = sample[:,1]\n",
    "# description\n",
    "sample_descriptions = sample[:,2]\n",
    "# title_descriptions\n",
    "sample_title_descriptions = sample[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all\n",
    "sample_all = list(map(lambda x: \" \".join(x), zip(sample_descriptions, sample_subjects)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# labelling for each set of documents using all methods\n",
    "methods = ['count', 'count_weight', 'wmd']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_s = np.array([label_docs(sample_subjects,vectorizer,method=x) for x in methods])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_t = np.array([label_docs(sample_titles,vectorizer,method=x) for x in methods])\n",
    "model_d = np.array([label_docs(sample_descriptions,vectorizer,method=x) for x in methods])\n",
    "model_td = np.array([label_docs(sample_title_descriptions,vectorizer,method=x) for x in methods])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_all = np.array([label_docs(sample_all,vectorizer,method=x) for x in methods])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pull predicted labels\n",
    "pred_s = pull_labels(model_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_t = pull_labels(model_t)\n",
    "pred_d = pull_labels(model_d)\n",
    "pred_td = pull_labels(model_td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_all = pull_labels(model_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy_s = [get_accuracy(pred_s[i], truelabels) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_t = [get_accuracy(pred_t[i], truelabels) for i in range(3)]\n",
    "accuracy_d = [get_accuracy(pred_d[i], truelabels) for i in range(3)]\n",
    "accuracy_td = [get_accuracy(pred_td[i], truelabels) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_all = [get_accuracy(pred_all[i], truelabels) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(accuracy_s)\n",
    "print(accuracy_td)\n",
    "print(accuracy_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_all[2] == truelabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.asarray([accuracy_s[0],accuracy_td[0],accuracy_all[0]])[np.newaxis].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(pred_s[0],pred_td[0],pred_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add true labels to the results\n",
    "table = np.column_stack((sample, truelabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put labels together. Only use subjects, title_descriptions\n",
    "# order by method, then by type of documents\n",
    "for i in range(3):\n",
    "    table = np.column_stack((table,pred_s[i]))\n",
    "    table = np.column_stack((table,pred_td[i]))\n",
    "    table = np.column_stack((table,pred_all[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('sample_table.csv', 'w') as f:\n",
    "    fieldnames = ['id','title','description','title_description','subjects','manual_label', 'by_subjects_count', 'by_title_desc_count', 'by_all_count', 'by_subjects_count_weight', 'by_title_desc_count_weight', 'by_all_count_weight', 'by_subjects_wmd', 'by_title_desc_wmd', 'by_all_wmd']\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(fieldnames)\n",
    "    writer.writerows(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(1974)\n",
    "\n",
    "# Generate Data\n",
    "num = 20\n",
    "x, y = np.random.random((2, num))\n",
    "labels = np.random.choice(['a', 'b', 'c'], num)\n",
    "df = pd.DataFrame(dict(x=x, y=y, label=labels))\n",
    "\n",
    "groups = df.groupby('label')\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=name)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define all functions here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean docs\n",
    "def clean_doc(doc, stop_words):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", doc)\n",
    "    letters_only = re.sub(\"  \", \" \", letters_only)\n",
    "    letters_list = letters_only.lower().split(\" \")\n",
    "    no_stopwords = [w for w in letters_list if w not in stopwords and w in model]\n",
    "    return \" \".join(no_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def word_dist(v1, v2):\n",
    "    return norm(v1-v2)\n",
    "def doc_dist(d1, d2):\n",
    "    dist = 0\n",
    "    for u in d1:\n",
    "        dist += min(list(map(word_dist, np.repeat(u,len(d2),0), d2)))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write a function that does all the above jobs given any doc list\n",
    "def label_docs(docs, tax_vectorizer, method='count'):\n",
    "    \"\"\"given a method (count, count_weight, wmd), label the docs using information in the taxonomy.\n",
    "    docs: a list of documents to be labelled.\n",
    "    tax_vectorizer: sk-learn vectorizer for the taxonomy\n",
    "    method: can be count, count_weight, wmd.\n",
    "    return the index of the label in collection_corpus and corresponding metrics\"\"\"\n",
    "    docs = list((map(clean_doc, docs)))\n",
    "    docs_vectorizer = CountVectorizer(min_df=1, stop_words=stopwords)\n",
    "    docs_X = docs_vectorizer.fit_transform(docs).toarray()\n",
    "    docs_vocab = np.asarray(docs_vectorizer.get_feature_names())\n",
    "    tax_vocab = np.asarray(tax_vectorizer.get_feature_names())\n",
    "    tax_X = X.toarray()\n",
    "    if method == 'count':\n",
    "        max_count = []\n",
    "        for doc_distribution in docs_X:\n",
    "            doc_words = list(docs_vocab[np.where(doc_distribution>0)])\n",
    "            count = []\n",
    "            for collection_distribution in tax_X:\n",
    "                collection_words = list(tax_vocab[np.where(collection_distribution>0)])\n",
    "                # print(len([x for x in sample_words if x in collection_words]))\n",
    "                count.append(len([x for x in doc_words if x in collection_words]))\n",
    "            max_count.append((max((val, idx) for (idx, val) in enumerate(count))))\n",
    "        return max_count\n",
    "    if method == 'count_weight':\n",
    "        max_count = []\n",
    "        for doc_distribution in docs_X:\n",
    "            doc_words = list(docs_vocab[np.where(doc_distribution>0)])\n",
    "            count = []\n",
    "            for i, collection_distribution in enumerate(tax_X):\n",
    "                collection_words = list(tax_vocab[np.where(collection_distribution>0)])\n",
    "                # print(len([x for x in sample_words if x in collection_words]))\n",
    "                cocurrence = [x for x in doc_words if x in collection_words]\n",
    "                count.append(sum([collection_weights[i, idx] for idx, val in enumerate(tax_vocab) if val in cocurrence]))\n",
    "            max_count.append((max((val, idx) for (idx, val) in enumerate(count))))\n",
    "        return max_count\n",
    "    if method == 'wmd':\n",
    "        min_dist = []\n",
    "        for i in range(len(docs)):\n",
    "            doc_vector = np.asarray([np.asarray(model[x]) for x in np.asarray(docs_vectorizer.get_feature_names())[np.where(docs_X[i,:]>0)]])\n",
    "            distances = [doc_dist(doc_vector, np.asarray(collection_vectors)[np.where(collection_weights[j]>0)]) for j in range(11)]\n",
    "            val, idx = min((val, idx) for (idx, val) in enumerate(distances))\n",
    "            min_dist.append((val,idx))\n",
    "        return min_dist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get labels from the results\n",
    "def pull_labels(ob):\n",
    "    return np.asarray(list(map(lambda o: np.asarray([all_labels[int(x[1])][1] for x in o]), ob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(pred, test):\n",
    "    \"\"\"Percentage of correct labels\"\"\"\n",
    "    return np.sum(pred==test)/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[x for x in collection_corpus]\n",
    "labels = [()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(pred_s[2]==truelabels)/len(truelabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
