id,title,description,title_description,subjects,manual_label,by_subjects_count,by_title_desc_count,by_all_count,by_subjects_count_weight,by_title_desc_count_weight,by_all_count_weight,by_subjects_wmd,by_title_desc_wmd,by_all_wmd
14001,Code Injection Attacks on HTML5-based Mobile Apps,"  HTML5-based mobile apps become more and more popular, mostly because they are
much easier to be ported across different mobile platforms than native apps.
HTML5-based apps are implemented using the standard web technologies, including
HTML5, JavaScript and CSS; they depend on some middlewares, such as PhoneGap,
to interact with the underlying OS.
  Knowing that JavaScript is subject to code injection attacks, we have
conducted a systematic study on HTML5-based mobile apps, trying to evaluate
whether it is safe to rely on the web technologies for mobile app development.
Our discoveries are quite surprising. We found out that if HTML5-based mobile
apps become popular--it seems to go that direction based on the current
projection--many of the things that we normally do today may become dangerous,
including reading from 2D barcodes, scanning Wi-Fi access points, playing MP4
videos, pairing with Bluetooth devices, etc. This paper describes how
HTML5-based apps can become vulnerable, how attackers can exploit their
vulnerabilities through a variety of channels, and what damage can be achieved
by the attackers. In addition to demonstrating the attacks through example
apps, we have studied 186 PhoneGap plugins, used by apps to achieve a variety
of functionalities, and we found that 11 are vulnerable. We also found two real
HTML5-based apps that are vulnerable to the attacks.
","Code Injection Attacks on HTML5-based Mobile Apps   HTML5-based mobile apps become more and more popular, mostly because they are
much easier to be ported across different mobile platforms than native apps.
HTML5-based apps are implemented using the standard web technologies, including
HTML5, JavaScript and CSS; they depend on some middlewares, such as PhoneGap,
to interact with the underlying OS.
  Knowing that JavaScript is subject to code injection attacks, we have
conducted a systematic study on HTML5-based mobile apps, trying to evaluate
whether it is safe to rely on the web technologies for mobile app development.
Our discoveries are quite surprising. We found out that if HTML5-based mobile
apps become popular--it seems to go that direction based on the current
projection--many of the things that we normally do today may become dangerous,
including reading from 2D barcodes, scanning Wi-Fi access points, playing MP4
videos, pairing with Bluetooth devices, etc. This paper describes how
HTML5-based apps can become vulnerable, how attackers can exploit their
vulnerabilities through a variety of channels, and what damage can be achieved
by the attackers. In addition to demonstrating the attacks through example
apps, we have studied 186 PhoneGap plugins, used by apps to achieve a variety
of functionalities, and we found that 11 are vulnerable. We also found two real
HTML5-based apps that are vulnerable to the attacks.
",[u'computer science - cryptography and security'],2092,2092,991,991,8912,8912,8912,8600,7626,7626
14003,"On the origin of the narrow peak and the isospin symmetry breaking of
  the $X$(3872)","  The $X$(3872) formation and decay processes in the $B$-decay are investigated
by a $c\bar c$-two-meson hybrid model. The two-meson state consists of the
$D^0{\bar D}^*{}^0$, $D^+D^{*-}$, $J/\psi\rho$, and $J/\psi\omega$ channels.
The energy-dependent decay widths of the $\rho$ and $\omega$ mesons are
introduced. The $D$-${\bar D}^*$ interaction is taken to be consistent with a
lack of the $B{\bar B}^*$ bound state. The coupling between the $D{\bar D}^*$
and $J/\psi\rho$ or the $D{\bar D}^*$ and $J/\psi\omega$ channels is obtained
from a quark model. The $c{\bar c}$-$D{\bar D}^*$ coupling is taken as a
parameter to fit the $X$(3872) mass. The spectrum is calculated up to 4 GeV.
  It is found that very narrow $J/\psi\rho$ and $J/\psi\omega$ peaks appear
around the $D^0{\bar D}^*{}^0$ threshold. The size of the $J/\psi\pi^3$ peak we
calculated is 1.29-2.38 times as large as that of the $J/\psi\pi^2$. The
isospin symmetry breaking in the present model comes from the mass difference
of the charged and neutral $D$ and $D^*$ mesons, which gives a sufficiently
large isospin mixing to explain the experiments. It is also found that values
of the ratios of the transfer strengths can give the information on the
$X$(3872) mass or the size of the $c{\bar c}$-$D{\bar D}^*$ coupling.
","On the origin of the narrow peak and the isospin symmetry breaking of
  the $X$(3872)   The $X$(3872) formation and decay processes in the $B$-decay are investigated
by a $c\bar c$-two-meson hybrid model. The two-meson state consists of the
$D^0{\bar D}^*{}^0$, $D^+D^{*-}$, $J/\psi\rho$, and $J/\psi\omega$ channels.
The energy-dependent decay widths of the $\rho$ and $\omega$ mesons are
introduced. The $D$-${\bar D}^*$ interaction is taken to be consistent with a
lack of the $B{\bar B}^*$ bound state. The coupling between the $D{\bar D}^*$
and $J/\psi\rho$ or the $D{\bar D}^*$ and $J/\psi\omega$ channels is obtained
from a quark model. The $c{\bar c}$-$D{\bar D}^*$ coupling is taken as a
parameter to fit the $X$(3872) mass. The spectrum is calculated up to 4 GeV.
  It is found that very narrow $J/\psi\rho$ and $J/\psi\omega$ peaks appear
around the $D^0{\bar D}^*{}^0$ threshold. The size of the $J/\psi\pi^3$ peak we
calculated is 1.29-2.38 times as large as that of the $J/\psi\pi^2$. The
isospin symmetry breaking in the present model comes from the mass difference
of the charged and neutral $D$ and $D^*$ mesons, which gives a sufficiently
large isospin mixing to explain the experiments. It is also found that values
of the ratios of the transfer strengths can give the information on the
$X$(3872) mass or the size of the $c{\bar c}$-$D{\bar D}^*$ coupling.
",[u'high energy physics - phenomenology'],7626,7626,7626,7626,7626,7626,7626,7626,7626,7626
14004,Tolerances induced by irredundant coverings,"  In this paper, we consider tolerances induced by irredundant coverings. Each
tolerance $R$ on $U$ determines a quasiorder $\lesssim_R$ by setting $x
\lesssim_R y$ if and only if $R(x) \subseteq R(y)$. We prove that for a
tolerance $R$ induced by a covering $\mathcal{H}$ of $U$, the covering
$\mathcal{H}$ is irredundant if and only if the quasiordered set $(U,
\lesssim_R)$ is bounded by minimal elements and the tolerance $R$ coincides
with the product ${\gtrsim_R} \circ {\lesssim_R}$. We also show that in such a
case $\mathcal{H} = \{ {\uparrow}m \mid \text{$m$ is minimal in
$(U,\lesssim_R)$} \}$, and for each minimal $m$, we have $R(m) = {\uparrow} m$.
Additionally, this irredundant covering $\mathcal{H}$ inducing $R$ consists of
some blocks of the tolerance $R$. We give necessary and sufficient conditions
under which $\mathcal{H}$ and the set of $R$-blocks coincide. These results are
established by applying the notion of Helly numbers of quasiordered sets.
","Tolerances induced by irredundant coverings   In this paper, we consider tolerances induced by irredundant coverings. Each
tolerance $R$ on $U$ determines a quasiorder $\lesssim_R$ by setting $x
\lesssim_R y$ if and only if $R(x) \subseteq R(y)$. We prove that for a
tolerance $R$ induced by a covering $\mathcal{H}$ of $U$, the covering
$\mathcal{H}$ is irredundant if and only if the quasiordered set $(U,
\lesssim_R)$ is bounded by minimal elements and the tolerance $R$ coincides
with the product ${\gtrsim_R} \circ {\lesssim_R}$. We also show that in such a
case $\mathcal{H} = \{ {\uparrow}m \mid \text{$m$ is minimal in
$(U,\lesssim_R)$} \}$, and for each minimal $m$, we have $R(m) = {\uparrow} m$.
Additionally, this irredundant covering $\mathcal{H}$ inducing $R$ consists of
some blocks of the tolerance $R$. We give necessary and sufficient conditions
under which $\mathcal{H}$ and the set of $R$-blocks coincide. These results are
established by applying the notion of Helly numbers of quasiordered sets.
","[u'mathematics - rings and algebras', u'computer science - discrete mathematics', u'68t37', u'05c69 (primary)', u'06b23 (secondary)', u'03e20']",7626,7626,5930,5930,8912,2092,8912,5930,5930,5930
14005,It\^{o} isomorphisms for $L^{p}$-valued Poisson stochastic integrals,"  Motivated by the study of existence, uniqueness and regularity of solutions
to stochastic partial differential equations driven by jump noise, we prove
It\^{o} isomorphisms for $L^p$-valued stochastic integrals with respect to a
compensated Poisson random measure. The principal ingredients for the proof are
novel Rosenthal type inequalities for independent random variables taking
values in a (noncommutative) $L^p$-space, which may be of independent interest.
As a by-product of our proof, we observe some moment estimates for the operator
norm of a sum of independent random matrices.
","It\^{o} isomorphisms for $L^{p}$-valued Poisson stochastic integrals   Motivated by the study of existence, uniqueness and regularity of solutions
to stochastic partial differential equations driven by jump noise, we prove
It\^{o} isomorphisms for $L^p$-valued stochastic integrals with respect to a
compensated Poisson random measure. The principal ingredients for the proof are
novel Rosenthal type inequalities for independent random variables taking
values in a (noncommutative) $L^p$-space, which may be of independent interest.
As a by-product of our proof, we observe some moment estimates for the operator
norm of a sum of independent random matrices.
","[u'mathematics - functional analysis', u'mathematics - probability']",7626,7626,7626,7626,8600,7626,8600,7626,5930,5930
14006,XMM-Newton RGS observations of the Cat's Eye Nebula,"  We present an analysis of XMM-Newton Reflection Grating Spectrometer (RGS)
observations of the planetary nebula (PN) NGC 6543, rendering it the second PN
with high resolution X-ray spectroscopic observations besides BD+30 3639. The
observations consist of 26 pointings, of which 14 included RGS observations for
a total integration time of 435 ks. Many of these observations, however, were
severely affected by high-background levels, and the net useful exposure time
is drastically reduced to 25 ks. Only the O VII triplet at 22 \AA\ is
unambiguously detected in the RGS spectrum of NGC 6543. We find this spectrum
consistent with an optically thin plasma at 0.147 keV (1.7 MK) and nebular
abundances. Unlike the case of BD+30 3639, the X-ray emission from NGC 6543
does not reveal overabundances of C and Ne. The results suggest the N/O ratio
of the hot plasma is consistent with that of the stellar wind, i.e., lower than
the nebular N/O ratio, but this result is not conclusive.
","XMM-Newton RGS observations of the Cat's Eye Nebula   We present an analysis of XMM-Newton Reflection Grating Spectrometer (RGS)
observations of the planetary nebula (PN) NGC 6543, rendering it the second PN
with high resolution X-ray spectroscopic observations besides BD+30 3639. The
observations consist of 26 pointings, of which 14 included RGS observations for
a total integration time of 435 ks. Many of these observations, however, were
severely affected by high-background levels, and the net useful exposure time
is drastically reduced to 25 ks. Only the O VII triplet at 22 \AA\ is
unambiguously detected in the RGS spectrum of NGC 6543. We find this spectrum
consistent with an optically thin plasma at 0.147 keV (1.7 MK) and nebular
abundances. Unlike the case of BD+30 3639, the X-ray emission from NGC 6543
does not reveal overabundances of C and Ne. The results suggest the N/O ratio
of the hot plasma is consistent with that of the stellar wind, i.e., lower than
the nebular N/O ratio, but this result is not conclusive.
",[u'astrophysics - solar and stellar astrophysics'],7626,7626,7626,7626,7626,8600,8600,2092,2092,2092
14007,Resonant Landau-Zener transitions in helical magnetic fields,"  The spin-dependent electron transport has been studied in magnetic
semiconductor waveguides (nanowires) in the helical magnetic field. We have
shown that -- apart from the known conductance dip located at the magnetic
field equal to the helical-field amplitude $B_h$ -- the additional conductance
dips (with zero conductance) appear at magnetic field different from $B_h$.
This effect occuring in the non-adiabatic regime is explained as resulting from
the resonant Landau-Zener transitions between the spin-splitted subbands.
","Resonant Landau-Zener transitions in helical magnetic fields   The spin-dependent electron transport has been studied in magnetic
semiconductor waveguides (nanowires) in the helical magnetic field. We have
shown that -- apart from the known conductance dip located at the magnetic
field equal to the helical-field amplitude $B_h$ -- the additional conductance
dips (with zero conductance) appear at magnetic field different from $B_h$.
This effect occuring in the non-adiabatic regime is explained as resulting from
the resonant Landau-Zener transitions between the spin-splitted subbands.
",[u'condensed matter - mesoscale and nanoscale physics'],7626,7626,7626,7626,7626,8600,7626,7626,2092,2092
14008,WFIRST Ultra-Precise Astrometry II: Asteroseismology,"  WFIRST microlensing observations will return high-precision parallaxes,
sigma(pi) < 0.3 microarcsec, for the roughly 1 million stars with H<14 in its
2.8 deg^2 field toward the Galactic bulge. Combined with its 40,000 epochs of
high precision photometry (~0.7 mmag at H_vega=14 and ~0.1 mmag at H=8), this
will yield a wealth of asteroseismic data of giant stars, primarily in the
Galactic bulge but including a substantial fraction of disk stars at all
Galactocentric radii interior to the Sun. For brighter stars, the astrometric
data will yield an external check on the radii derived from the two
asteroseismic parameters, <Delta nu> and nu_max, while for the fainter ones, it
will enable a mass measurement from the single measurable asteroseismic
parameter nu_max. Simulations based on Kepler data indicate that WFIRST will be
capable of detecting oscillations in stars from slightly less luminous than the
red clump to the tip of the red giant branch, yielding roughly 1 million
detections.
","WFIRST Ultra-Precise Astrometry II: Asteroseismology   WFIRST microlensing observations will return high-precision parallaxes,
sigma(pi) < 0.3 microarcsec, for the roughly 1 million stars with H<14 in its
2.8 deg^2 field toward the Galactic bulge. Combined with its 40,000 epochs of
high precision photometry (~0.7 mmag at H_vega=14 and ~0.1 mmag at H=8), this
will yield a wealth of asteroseismic data of giant stars, primarily in the
Galactic bulge but including a substantial fraction of disk stars at all
Galactocentric radii interior to the Sun. For brighter stars, the astrometric
data will yield an external check on the radii derived from the two
asteroseismic parameters, <Delta nu> and nu_max, while for the fainter ones, it
will enable a mass measurement from the single measurable asteroseismic
parameter nu_max. Simulations based on Kepler data indicate that WFIRST will be
capable of detecting oscillations in stars from slightly less luminous than the
red clump to the tip of the red giant branch, yielding roughly 1 million
detections.
",[u'astrophysics - solar and stellar astrophysics'],7626,7626,991,991,7626,2092,2092,2092,7626,7626
14009,"Super-resolution method using sparse regularization for point-spread
  function recovery","  In large-scale spatial surveys, such as the forthcoming ESA Euclid mission,
images may be undersampled due to the optical sensors sizes. Therefore, one may
consider using a super-resolution (SR) method to recover aliased frequencies,
prior to further analysis. This is particularly relevant for point-source
images, which provide direct measurements of the instrument point-spread
function (PSF). We introduce SPRITE, SParse Recovery of InsTrumental rEsponse,
which is an SR algorithm using a sparse analysis prior. We show that such a
prior provides significant improvements over existing methods, especially on
low SNR PSFs.
","Super-resolution method using sparse regularization for point-spread
  function recovery   In large-scale spatial surveys, such as the forthcoming ESA Euclid mission,
images may be undersampled due to the optical sensors sizes. Therefore, one may
consider using a super-resolution (SR) method to recover aliased frequencies,
prior to further analysis. This is particularly relevant for point-source
images, which provide direct measurements of the instrument point-spread
function (PSF). We introduce SPRITE, SParse Recovery of InsTrumental rEsponse,
which is an SR algorithm using a sparse analysis prior. We show that such a
prior provides significant improvements over existing methods, especially on
low SNR PSFs.
","[u'astrophysics - instrumentation and methods for astrophysics', u'computer science - computer vision and pattern recognition']",7626,991,8600,8600,8912,8600,8912,5930,7626,7626
14010,$Z_b$ and $Z_c$ Exotic States as Coupled Channel Cusps,"  It is demonstrated that the candidate tetraquark states $Z_b(10610)$,
$Z_b(10650)$, $Z_c(3900)$, and $Z_c(4025)$ are coupled channel cusp effects.
The model explains in a natural way the masses and quantum numbers of the
putative states and the near equality of the widths of the $Z_b$ states. It is
argued that the $Z_c(3900)$ and $Z_c(4025)$ should be visible in $\bar B^0 \to
J/\psi \pi^0\pi^0$ or $B^- \to J/\psi \pi^- \pi^0$, but should {\it not} appear
in $\bar B^0 \to J/\psi \pi^+\pi^-$, in agreement with recent LHCb results.
Additional tests for cusp effects are suggested.
","$Z_b$ and $Z_c$ Exotic States as Coupled Channel Cusps   It is demonstrated that the candidate tetraquark states $Z_b(10610)$,
$Z_b(10650)$, $Z_c(3900)$, and $Z_c(4025)$ are coupled channel cusp effects.
The model explains in a natural way the masses and quantum numbers of the
putative states and the near equality of the widths of the $Z_b$ states. It is
argued that the $Z_c(3900)$ and $Z_c(4025)$ should be visible in $\bar B^0 \to
J/\psi \pi^0\pi^0$ or $B^- \to J/\psi \pi^- \pi^0$, but should {\it not} appear
in $\bar B^0 \to J/\psi \pi^+\pi^-$, in agreement with recent LHCb results.
Additional tests for cusp effects are suggested.
",[u'high energy physics - phenomenology'],7626,7626,7626,7626,7626,9268,7626,7626,7626,7626
14011,"Wind mass transfer in S-type symbiotic binaries I. Focusing by the wind
  compression model","  Context: Luminosities of hot components in symbiotic binaries require
accretion rates that are higher than those that can be achieved via a standard
Bondi-Hoyle accretion. This implies that the wind mass transfer in symbiotic
binaries has to be more efficient.
  Aims: We suggest that the accretion rate onto the white dwarfs (WDs) in
S-type symbiotic binaries can be enhanced sufficiently by focusing the wind
from their slowly rotating normal giants towards the binary orbital plane.
  Methods: We applied the wind compression model to the stellar wind of slowly
rotating red giants in S-type symbiotic binaries.
  Results: Our analysis reveals that for typical terminal velocities of the
giant wind, 20 to 50 km/s, and measured rotational velocities between 6 and 10
km/s, the densities of the compressed wind at a typical distance of the
accretor from its donor correspond to the mass-loss rate, which can be a factor
of $\sim$10 higher than for the spherically symmetric wind. This allows the WD
to accrete at rates of $10^{-8} - 10^{-7}$ M(Sun)/year, and thus to power its
luminosity.
  Conclusions: We show that the high wind-mass-transfer efficiency in S-type
symbiotic stars can be caused by compression of the wind from their slowly
rotating normal giants, whereas in D-type symbiotic stars, the high mass
transfer ratio can be achieved via the gravitational focusing, which has
recently been suggested for very slow winds in Mira-type binaries.
","Wind mass transfer in S-type symbiotic binaries I. Focusing by the wind
  compression model   Context: Luminosities of hot components in symbiotic binaries require
accretion rates that are higher than those that can be achieved via a standard
Bondi-Hoyle accretion. This implies that the wind mass transfer in symbiotic
binaries has to be more efficient.
  Aims: We suggest that the accretion rate onto the white dwarfs (WDs) in
S-type symbiotic binaries can be enhanced sufficiently by focusing the wind
from their slowly rotating normal giants towards the binary orbital plane.
  Methods: We applied the wind compression model to the stellar wind of slowly
rotating red giants in S-type symbiotic binaries.
  Results: Our analysis reveals that for typical terminal velocities of the
giant wind, 20 to 50 km/s, and measured rotational velocities between 6 and 10
km/s, the densities of the compressed wind at a typical distance of the
accretor from its donor correspond to the mass-loss rate, which can be a factor
of $\sim$10 higher than for the spherically symmetric wind. This allows the WD
to accrete at rates of $10^{-8} - 10^{-7}$ M(Sun)/year, and thus to power its
luminosity.
  Conclusions: We show that the high wind-mass-transfer efficiency in S-type
symbiotic stars can be caused by compression of the wind from their slowly
rotating normal giants, whereas in D-type symbiotic stars, the high mass
transfer ratio can be achieved via the gravitational focusing, which has
recently been suggested for very slow winds in Mira-type binaries.
",[u'astrophysics - solar and stellar astrophysics'],7626,7626,7626,7626,7626,8600,8600,2092,7626,5930
14012,Recent results of the ANTARES Neutrino Telescope,"  The latest results from the ANTARES Neutrino Telescope are reported. Limits
on a high energy neutrino diffuse flux have been set using for the first time
both muon-track and showering events. The results for point sources obtained by
ANTARES are also shown. These are the most stringent limits for the southern
sky for neutrino energies below 100 TeV. Constraints on the nature of the
cluster of neutrino events near the Galactic Centre observed by IceCube are
also reported. In particular, ANTARES data excludes a single point-like
neutrino source as the origin of this cluster. Looking for neutrinos coming
from the Sun or the centre of the Galaxy, very competitive limits are set by
the ANTARES data to the flux of neutrinos produced by self-annihilation of
weakly interacting massive particles.
","Recent results of the ANTARES Neutrino Telescope   The latest results from the ANTARES Neutrino Telescope are reported. Limits
on a high energy neutrino diffuse flux have been set using for the first time
both muon-track and showering events. The results for point sources obtained by
ANTARES are also shown. These are the most stringent limits for the southern
sky for neutrino energies below 100 TeV. Constraints on the nature of the
cluster of neutrino events near the Galactic Centre observed by IceCube are
also reported. In particular, ANTARES data excludes a single point-like
neutrino source as the origin of this cluster. Looking for neutrinos coming
from the Sun or the centre of the Galaxy, very competitive limits are set by
the ANTARES data to the flux of neutrinos produced by self-annihilation of
weakly interacting massive particles.
",[u'astrophysics - high energy astrophysical phenomena'],7626,7626,991,7626,7626,2092,2092,5930,7626,7626
14013,"Universal Knowledge Discovery from Big Data: Towards a Paradigm Shift
  from 'Knowledge Discovery' to 'Wisdom Discovery'","  Many people hold a vision that big data will provide big insights and have a
big impact in the future, and big-data-assisted scientific discovery is seen as
an emerging and promising scientific paradigm. However, how to turn big data
into deep insights with tremendous value still remains obscure. To meet the
challenge, universal knowledge discovery from big data (UKD) is proposed. The
new concept focuses on discovering universal knowledge, which exists in the
statistical analyses of big data and provides valuable insights into big data.
Universal knowledge comes in different forms, e.g., universal patterns, rules,
correlations, models and mechanisms. To accelerate big data assisted universal
knowledge discovery, a unified research paradigm should be built based on
techniques and paradigms from related research domains, especially big data
mining and complex systems science. Therefore, I propose an iBEST@SEE
methodology. This study lays a solid foundation for the future development of
universal knowledge discovery, and offers a pathway to the discovery of
""treasure-trove"" hidden in big data.
","Universal Knowledge Discovery from Big Data: Towards a Paradigm Shift
  from 'Knowledge Discovery' to 'Wisdom Discovery'   Many people hold a vision that big data will provide big insights and have a
big impact in the future, and big-data-assisted scientific discovery is seen as
an emerging and promising scientific paradigm. However, how to turn big data
into deep insights with tremendous value still remains obscure. To meet the
challenge, universal knowledge discovery from big data (UKD) is proposed. The
new concept focuses on discovering universal knowledge, which exists in the
statistical analyses of big data and provides valuable insights into big data.
Universal knowledge comes in different forms, e.g., universal patterns, rules,
correlations, models and mechanisms. To accelerate big data assisted universal
knowledge discovery, a unified research paradigm should be built based on
techniques and paradigms from related research domains, especially big data
mining and complex systems science. Therefore, I propose an iBEST@SEE
methodology. This study lays a solid foundation for the future development of
universal knowledge discovery, and offers a pathway to the discovery of
""treasure-trove"" hidden in big data.
","[u'computer science - computers and society', u'68t01']",2092,2092,991,991,8912,8912,8912,7626,7626,7626
14014,"Arrival Modeling and Error Analysis for Molecular Communication via
  Diffusion with Drift","  The arrival of molecules in molecular communication via diffusion (MCvD) is a
counting process, exhibiting by its nature binomial distribution. Even if the
binomial process describes well the arrival of molecules, when considering
consecutively sent symbols, the process struggles to work with the binomial
cumulative distribution function (CDF). Therefore, in the literature, Poisson
and Gaussian approximations of the binomial distribution are used. In this
paper, we analyze these two approximations of the binomial model of the arrival
process in MCvD with drift. Considering the distance, drift velocity, and the
number of emitted molecules, we investigate the regions in which either Poisson
or Gaussian model is better in terms of root mean squared error (RMSE) of the
CDFs; we confirm the boundaries of the region via numerical simulations.
Moreover, we derive the error probabilities for continuous communication and
analyze which model approximates it more accurately.
","Arrival Modeling and Error Analysis for Molecular Communication via
  Diffusion with Drift   The arrival of molecules in molecular communication via diffusion (MCvD) is a
counting process, exhibiting by its nature binomial distribution. Even if the
binomial process describes well the arrival of molecules, when considering
consecutively sent symbols, the process struggles to work with the binomial
cumulative distribution function (CDF). Therefore, in the literature, Poisson
and Gaussian approximations of the binomial distribution are used. In this
paper, we analyze these two approximations of the binomial model of the arrival
process in MCvD with drift. Considering the distance, drift velocity, and the
number of emitted molecules, we investigate the regions in which either Poisson
or Gaussian model is better in terms of root mean squared error (RMSE) of the
CDFs; we confirm the boundaries of the region via numerical simulations.
Moreover, we derive the error probabilities for continuous communication and
analyze which model approximates it more accurately.
",[u'computer science - emerging technologies'],2092,991,991,991,8912,8600,8912,7626,7626,7626
14015,"Thermodynamically self-consistent non-stochastic micromagnetic model for
  the ferromagnetic state","  In this work, a self-consistent thermodynamic approach to micromagnetism is
presented. The magnetic degrees of freedom are modeled using the
Landau-Lifshitz-Baryakhtar theory, that separates the different contributions
to the magnetic damping, and thereby allows them to be coupled to the electron
and phonon systems in a self-consistent way. We show that this model can
quantitatively reproduce ultrafast magnetization dynamics in Nickel.
","Thermodynamically self-consistent non-stochastic micromagnetic model for
  the ferromagnetic state   In this work, a self-consistent thermodynamic approach to micromagnetism is
presented. The magnetic degrees of freedom are modeled using the
Landau-Lifshitz-Baryakhtar theory, that separates the different contributions
to the magnetic damping, and thereby allows them to be coupled to the electron
and phonon systems in a self-consistent way. We show that this model can
quantitatively reproduce ultrafast magnetization dynamics in Nickel.
",[u'condensed matter - materials science'],7626,7626,7626,7626,8912,2092,8912,7626,7626,7626
14016,ORAC-DR: A generic data reduction pipeline infrastructure,"  ORAC-DR is a general purpose data reduction pipeline system designed to be
instrument and observatory agnostic. The pipeline works with instruments as
varied as infrared integral field units, imaging arrays and spectrographs, and
sub-millimeter heterodyne arrays & continuum cameras. This paper describes the
architecture of the pipeline system and the implementation of the core
infrastructure. We finish by discussing the lessons learned since the initial
deployment of the pipeline system in the late 1990s.
","ORAC-DR: A generic data reduction pipeline infrastructure   ORAC-DR is a general purpose data reduction pipeline system designed to be
instrument and observatory agnostic. The pipeline works with instruments as
varied as infrared integral field units, imaging arrays and spectrographs, and
sub-millimeter heterodyne arrays & continuum cameras. This paper describes the
architecture of the pipeline system and the implementation of the core
infrastructure. We finish by discussing the lessons learned since the initial
deployment of the pipeline system in the late 1990s.
",[u'astrophysics - instrumentation and methods for astrophysics'],7626,7626,8600,7626,2092,2092,2092,2092,2092,2092
14017,"The Gindikin-Karpelevich Formula and Constant Terms of Eisenstein Series
  for Brylinski-Deligne Extensions","  We firstly discuss properties of the L-group for Brylinski-Deligne (BD)
extensions constructed by M. Weissman. Secondly, the Gindikin-Karpelevich (GK)
formula for arbitrary BD extensions is computed and expressed in terms of
naturally defined elements of the group. We show that the GK formula can be
interpreted as Langlands-Shahidi type L-functions associated with the ajoint
action of the L-group of the Levi on certain Lie algebras. As a consequence,
the constant term of Eisenstein series for BD extensions could be expressed in
terms of global (partial) Langlands-Shahidi type L-functions. In the end, we
use this to determine the residual spectrum of BD covers of some semisimple
rank one groups.
","The Gindikin-Karpelevich Formula and Constant Terms of Eisenstein Series
  for Brylinski-Deligne Extensions   We firstly discuss properties of the L-group for Brylinski-Deligne (BD)
extensions constructed by M. Weissman. Secondly, the Gindikin-Karpelevich (GK)
formula for arbitrary BD extensions is computed and expressed in terms of
naturally defined elements of the group. We show that the GK formula can be
interpreted as Langlands-Shahidi type L-functions associated with the ajoint
action of the L-group of the Levi on certain Lie algebras. As a consequence,
the constant term of Eisenstein series for BD extensions could be expressed in
terms of global (partial) Langlands-Shahidi type L-functions. In the end, we
use this to determine the residual spectrum of BD covers of some semisimple
rank one groups.
","[u'mathematics - number theory', u'mathematics - representation theory']",7626,7626,5930,991,2092,7626,7626,7626,7626,7626
14018,"Finite groups of automorphisms of Enriques surfaces and the Mathieu
  group $M_{12}$","  An action of a group $G$ on an Enriques surface $S$ is called Mathieu if it
acts on $H^0(2K_S)$ trivially and every element of order 2, 4 has Lefschetz
number 4. A finite group $G$ has a Mathieu action on some Enriques surface if
and only if it is isomorphic to a subgroup of the symmetric group
$\mathfrak{S}_6$ of degree 6 and the order $|G|$ is not divisible by $2^4$.
Explicit Mathieu actions of the three groups $\mathfrak S_5, N_{72}$ and
$\mathfrak A_6$, together with non-Mathieu one of $H_{192}$, on polarized
Enriques surfaces of degree 30, 18, 10 and 6, respectively, are constructed
without Torelli type theorem to prove the if part.
","Finite groups of automorphisms of Enriques surfaces and the Mathieu
  group $M_{12}$   An action of a group $G$ on an Enriques surface $S$ is called Mathieu if it
acts on $H^0(2K_S)$ trivially and every element of order 2, 4 has Lefschetz
number 4. A finite group $G$ has a Mathieu action on some Enriques surface if
and only if it is isomorphic to a subgroup of the symmetric group
$\mathfrak{S}_6$ of degree 6 and the order $|G|$ is not divisible by $2^4$.
Explicit Mathieu actions of the three groups $\mathfrak S_5, N_{72}$ and
$\mathfrak A_6$, together with non-Mathieu one of $H_{192}$, on polarized
Enriques surfaces of degree 30, 18, 10 and 6, respectively, are constructed
without Torelli type theorem to prove the if part.
","[u'14j28', u'mathematics - algebraic geometry', u'mathematics - group theory', u'20d08']",7626,7626,5930,7626,7626,7626,7626,8600,7626,7626
14019,"Lattice Hamiltonian approach to the Schwinger model: further results
  from the strong coupling expansion","  We employ exact diagonalization with strong coupling expansion to the
massless and massive Schwinger model. New results are presented for the ground
state energy and scalar mass gap in the massless model, which improve the
precision to nearly $10^{-9} %$. We also investigate the chiral condensate and
compare our calculations to previous results available in the literature.
Oscillations of the chiral condensate which are present while increasing the
expansion order are also studied and are shown to be directly linked to the
presence of flux loops in the system.
","Lattice Hamiltonian approach to the Schwinger model: further results
  from the strong coupling expansion   We employ exact diagonalization with strong coupling expansion to the
massless and massive Schwinger model. New results are presented for the ground
state energy and scalar mass gap in the massless model, which improve the
precision to nearly $10^{-9} %$. We also investigate the chiral condensate and
compare our calculations to previous results available in the literature.
Oscillations of the chiral condensate which are present while increasing the
expansion order are also studied and are shown to be directly linked to the
presence of flux loops in the system.
",[u'high energy physics - lattice'],7626,7626,5930,5930,7626,7626,7626,7626,7626,7626
14020,Exact Recovery in the Stochastic Block Model,"  The stochastic block model (SBM) with two communities, or equivalently the
planted bisection model, is a popular model of random graph exhibiting a
cluster behaviour. In the symmetric case, the graph has two equally sized
clusters and vertices connect with probability $p$ within clusters and $q$
across clusters. In the past two decades, a large body of literature in
statistics and computer science has focused on providing lower-bounds on the
scaling of $|p-q|$ to ensure exact recovery. In this paper, we identify a sharp
threshold phenomenon for exact recovery: if $\alpha=pn/\log(n)$ and
$\beta=qn/\log(n)$ are constant (with $\alpha>\beta$), recovering the
communities with high probability is possible if $\frac{\alpha+\beta}{2} -
\sqrt{\alpha \beta}>1$ and impossible if $\frac{\alpha+\beta}{2} - \sqrt{\alpha
\beta}<1$. In particular, this improves the existing bounds. This also sets a
new line of sight for efficient clustering algorithms. While maximum likelihood
(ML) achieves the optimal threshold (by definition), it is in the worst-case
NP-hard. This paper proposes an efficient algorithm based on a semidefinite
programming relaxation of ML, which is proved to succeed in recovering the
communities close to the threshold, while numerical experiments suggest it may
achieve the threshold. An efficient algorithm which succeeds all the way down
to the threshold is also obtained using a partial recovery algorithm combined
with a local improvement procedure.
","Exact Recovery in the Stochastic Block Model   The stochastic block model (SBM) with two communities, or equivalently the
planted bisection model, is a popular model of random graph exhibiting a
cluster behaviour. In the symmetric case, the graph has two equally sized
clusters and vertices connect with probability $p$ within clusters and $q$
across clusters. In the past two decades, a large body of literature in
statistics and computer science has focused on providing lower-bounds on the
scaling of $|p-q|$ to ensure exact recovery. In this paper, we identify a sharp
threshold phenomenon for exact recovery: if $\alpha=pn/\log(n)$ and
$\beta=qn/\log(n)$ are constant (with $\alpha>\beta$), recovering the
communities with high probability is possible if $\frac{\alpha+\beta}{2} -
\sqrt{\alpha \beta}>1$ and impossible if $\frac{\alpha+\beta}{2} - \sqrt{\alpha
\beta}<1$. In particular, this improves the existing bounds. This also sets a
new line of sight for efficient clustering algorithms. While maximum likelihood
(ML) achieves the optimal threshold (by definition), it is in the worst-case
NP-hard. This paper proposes an efficient algorithm based on a semidefinite
programming relaxation of ML, which is proved to succeed in recovering the
communities close to the threshold, while numerical experiments suggest it may
achieve the threshold. An efficient algorithm which succeeds all the way down
to the threshold is also obtained using a partial recovery algorithm combined
with a local improvement procedure.
","[u'physics - physics and society', u'mathematics - probability', u'computer science - social and information networks']",7626,2092,8600,8600,8912,8912,8912,7626,7626,7626
14021,Dynamics of Chemical Equilibrium of Hadronic Matter Close to $T_c$,"  Quick chemical equilibration times of hadrons (specifically, $p\bar{p}$,
$K\bar{K}$, $\Lambda\bar{\Lambda}$, and $\Omega\bar{\Omega}$ pairs) within a
hadron gas are explained dynamically using Hagedorn states, which drive
particles into equilibrium close to the critical temperature. Within this
scheme, we use master equations and derive various analytical estimates for the
chemical equilibration times. We compare our model to recent lattice results
and find that for both $T_c=176$ MeV and $T_c=196$ MeV, the hadrons can reach
chemical equilibrium almost immediately, well before the chemical freeze-out
temperatures found in thermal fits for a hadron gas without Hagedorn states.
Furthermore the ratios $p/\pi$, $K/\pi$, $\Lambda/\pi$, and $\Omega / \pi $
match experimental values well in our dynamical scenario.
","Dynamics of Chemical Equilibrium of Hadronic Matter Close to $T_c$   Quick chemical equilibration times of hadrons (specifically, $p\bar{p}$,
$K\bar{K}$, $\Lambda\bar{\Lambda}$, and $\Omega\bar{\Omega}$ pairs) within a
hadron gas are explained dynamically using Hagedorn states, which drive
particles into equilibrium close to the critical temperature. Within this
scheme, we use master equations and derive various analytical estimates for the
chemical equilibration times. We compare our model to recent lattice results
and find that for both $T_c=176$ MeV and $T_c=196$ MeV, the hadrons can reach
chemical equilibrium almost immediately, well before the chemical freeze-out
temperatures found in thermal fits for a hadron gas without Hagedorn states.
Furthermore the ratios $p/\pi$, $K/\pi$, $\Lambda/\pi$, and $\Omega / \pi $
match experimental values well in our dynamical scenario.
",[u'nuclear theory'],7626,991,7626,7626,2092,7626,7626,5930,7626,7626
9043,Bounds on Distance Estimation via Diffusive Molecular Communication,"  This paper studies distance estimation for diffusive molecular communication.
The Cramer-Rao lower bound on the variance of the distance estimation error is
derived. The lower bound is derived for a physically unbounded environment with
molecule degradation and steady uniform flow. The maximum likelihood distance
estimator is derived and its accuracy is shown via simulation to perform very
close to the Cramer-Rao lower bound. An existing protocol is shown to be
equivalent to the maximum likelihood distance estimator if only one observation
is made. Simulation results also show the accuracy of existing protocols with
respect to the Cramer-Rao lower bound.
","Bounds on Distance Estimation via Diffusive Molecular Communication   This paper studies distance estimation for diffusive molecular communication.
The Cramer-Rao lower bound on the variance of the distance estimation error is
derived. The lower bound is derived for a physically unbounded environment with
molecule degradation and steady uniform flow. The maximum likelihood distance
estimator is derived and its accuracy is shown via simulation to perform very
close to the Cramer-Rao lower bound. An existing protocol is shown to be
equivalent to the maximum likelihood distance estimator if only one observation
is made. Simulation results also show the accuracy of existing protocols with
respect to the Cramer-Rao lower bound.
",[u'computer science - information theory'],2092,2092,8600,8600,8912,2948,8912,7626,2092,5930
9045,"Factors Influencing Quality of Mobile Apps:Role of Mobile App
  Development Life Cycle","  In this paper, The mobile application field has been receiving astronomical
attention from the past few years due to the growing number of mobile app
downloads and withal due to the revenues being engendered .With the surge in
the number of apps, the number of lamentable apps/failing apps has withal been
growing.Interesting mobile app statistics are included in this paper which
might avail the developers understand the concerns and merits of mobile
apps.The authors have made an effort to integrate all the crucial factors that
cause apps to fail which include negligence by the developers, technical
issues, inadequate marketing efforts, and high prospects of the
users/consumers.The paper provides suggestions to eschew failure of apps. As
per the various surveys, the number of lamentable/failing apps is growing
enormously, primarily because mobile app developers are not adopting a standard
development life cycle for the development of apps. In this paper, we have
developed a mobile application with the aid of traditional software development
life cycle phases (Requirements, Design, Develop, Test, and, Maintenance) and
we have used UML, M-UML, and mobile application development technologies.
","Factors Influencing Quality of Mobile Apps:Role of Mobile App
  Development Life Cycle   In this paper, The mobile application field has been receiving astronomical
attention from the past few years due to the growing number of mobile app
downloads and withal due to the revenues being engendered .With the surge in
the number of apps, the number of lamentable apps/failing apps has withal been
growing.Interesting mobile app statistics are included in this paper which
might avail the developers understand the concerns and merits of mobile
apps.The authors have made an effort to integrate all the crucial factors that
cause apps to fail which include negligence by the developers, technical
issues, inadequate marketing efforts, and high prospects of the
users/consumers.The paper provides suggestions to eschew failure of apps. As
per the various surveys, the number of lamentable/failing apps is growing
enormously, primarily because mobile app developers are not adopting a standard
development life cycle for the development of apps. In this paper, we have
developed a mobile application with the aid of traditional software development
life cycle phases (Requirements, Design, Develop, Test, and, Maintenance) and
we have used UML, M-UML, and mobile application development technologies.
",[u'computer science - software engineering'],2092,2092,991,991,8912,2092,8912,7626,7626,7626
9046,Memristor models for machine learning,"  In the quest for alternatives to traditional CMOS, it is being suggested that
digital computing efficiency and power can be improved by matching the
precision to the application. Many applications do not need the high precision
that is being used today. In particular, large gains in area- and power
efficiency could be achieved by dedicated analog realizations of approximate
computing engines. In this work, we explore the use of memristor networks for
analog approximate computation, based on a machine learning framework called
reservoir computing. Most experimental investigations on the dynamics of
memristors focus on their nonvolatile behavior. Hence, the volatility that is
present in the developed technologies is usually unwanted and it is not
included in simulation models. In contrast, in reservoir computing, volatility
is not only desirable but necessary. Therefore, in this work, we propose two
different ways to incorporate it into memristor simulation models. The first is
an extension of Strukov's model and the second is an equivalent Wiener model
approximation. We analyze and compare the dynamical properties of these models
and discuss their implications for the memory and the nonlinear processing
capacity of memristor networks. Our results indicate that device variability,
increasingly causing problems in traditional computer design, is an asset in
the context of reservoir computing. We conclude that, although both models
could lead to useful memristor based reservoir computing systems, their
computational performance will differ. Therefore, experimental modeling
research is required for the development of accurate volatile memristor models.
","Memristor models for machine learning   In the quest for alternatives to traditional CMOS, it is being suggested that
digital computing efficiency and power can be improved by matching the
precision to the application. Many applications do not need the high precision
that is being used today. In particular, large gains in area- and power
efficiency could be achieved by dedicated analog realizations of approximate
computing engines. In this work, we explore the use of memristor networks for
analog approximate computation, based on a machine learning framework called
reservoir computing. Most experimental investigations on the dynamics of
memristors focus on their nonvolatile behavior. Hence, the volatility that is
present in the developed technologies is usually unwanted and it is not
included in simulation models. In contrast, in reservoir computing, volatility
is not only desirable but necessary. Therefore, in this work, we propose two
different ways to incorporate it into memristor simulation models. The first is
an extension of Strukov's model and the second is an equivalent Wiener model
approximation. We analyze and compare the dynamical properties of these models
and discuss their implications for the memory and the nonlinear processing
capacity of memristor networks. Our results indicate that device variability,
increasingly causing problems in traditional computer design, is an asset in
the context of reservoir computing. We conclude that, although both models
could lead to useful memristor based reservoir computing systems, their
computational performance will differ. Therefore, experimental modeling
research is required for the development of accurate volatile memristor models.
","[u'computer science - learning', u'condensed matter - materials science']",2092,7626,991,7626,8912,2092,2092,7626,7626,7626
9052,"Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of
  Convex Sets","  Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique
for simultaneously discovering group and within-group sparse patterns by using
a combination of the $\ell_1$ and $\ell_2$ norms. However, in large-scale
applications, the complexity of the regularizers entails great computational
challenges. In this paper, we propose a novel Two-Layer Feature REduction
method (TLFre) for SGL via a decomposition of its dual feasible set. The
two-layer reduction is able to quickly identify the inactive groups and the
inactive features, respectively, which are guaranteed to be absent from the
sparse representation and can be removed from the optimization. Existing
feature reduction methods are only applicable for sparse models with one
sparsity-inducing regularizer. To our best knowledge, TLFre is the first one
that is capable of dealing with multiple sparsity-inducing regularizers.
Moreover, TLFre has a very low computational cost and can be integrated with
any existing solvers. We also develop a screening method---called DPC
(DecomPosition of Convex set)---for the nonnegative Lasso problem. Experiments
on both synthetic and real data sets show that TLFre and DPC improve the
efficiency of SGL and nonnegative Lasso by several orders of magnitude.
","Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of
  Convex Sets   Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique
for simultaneously discovering group and within-group sparse patterns by using
a combination of the $\ell_1$ and $\ell_2$ norms. However, in large-scale
applications, the complexity of the regularizers entails great computational
challenges. In this paper, we propose a novel Two-Layer Feature REduction
method (TLFre) for SGL via a decomposition of its dual feasible set. The
two-layer reduction is able to quickly identify the inactive groups and the
inactive features, respectively, which are guaranteed to be absent from the
sparse representation and can be removed from the optimization. Existing
feature reduction methods are only applicable for sparse models with one
sparsity-inducing regularizer. To our best knowledge, TLFre is the first one
that is capable of dealing with multiple sparsity-inducing regularizers.
Moreover, TLFre has a very low computational cost and can be integrated with
any existing solvers. We also develop a screening method---called DPC
(DecomPosition of Convex set)---for the nonnegative Lasso problem. Experiments
on both synthetic and real data sets show that TLFre and DPC improve the
efficiency of SGL and nonnegative Lasso by several orders of magnitude.
",[u'computer science - learning'],2092,991,991,991,8912,2092,8912,7626,7626,7626
9060,Can SDN Mitigate Disasters?,"  Datacenter networks and services are at risk in the face of disasters.
Existing fault-tolerant storage services cannot even achieve a nil recovery
point objective (RPO) as client-generated data may get lost before the
termination of their migration across geo-replicated datacenters. SDN has
proved instrumental in exploiting application-level information to optimise the
routing of information. In this paper, we propose Software Defined Edge (SDE)
or the implementation of SDN at the network edge to achieve nil RPO. We
illustrate our proposal with a fault-tolerant key-value store that
experimentally recovers from disaster within 30s. Although SDE is inherently
fault-tolerant and scalable, its deployment raises new challenges on the
partnership between ISPs and CDN providers. We conclude that failure detection
information at the SDN-level can effectively benefit applications to recover
from disaster.
","Can SDN Mitigate Disasters?   Datacenter networks and services are at risk in the face of disasters.
Existing fault-tolerant storage services cannot even achieve a nil recovery
point objective (RPO) as client-generated data may get lost before the
termination of their migration across geo-replicated datacenters. SDN has
proved instrumental in exploiting application-level information to optimise the
routing of information. In this paper, we propose Software Defined Edge (SDE)
or the implementation of SDN at the network edge to achieve nil RPO. We
illustrate our proposal with a fault-tolerant key-value store that
experimentally recovers from disaster within 30s. Although SDE is inherently
fault-tolerant and scalable, its deployment raises new challenges on the
partnership between ISPs and CDN providers. We conclude that failure detection
information at the SDN-level can effectively benefit applications to recover
from disaster.
",[u'computer science - networking and internet architecture'],2092,2092,5930,2092,8912,2092,8912,7626,2092,5930
9065,Classical automata on promise problems,"  Promise problems were mainly studied in quantum automata theory. Here we
focus on state complexity of classical automata for promise problems. First, it
was known that there is a family of unary promise problems solvable by quantum
automata by using a single qubit, but the number of states required by
corresponding one-way deterministic automata cannot be bounded by a constant.
For this family, we show that even two-way nondeterminism does not help to save
a single state. By comparing this with the corresponding state complexity of
alternating machines, we then get a tight exponential gap between two-way
nondeterministic and one-way alternating automata solving unary promise
problems. Second, despite of the existing quadratic gap between Las Vegas
realtime probabilistic automata and one-way deterministic automata for language
recognition, we show that, by turning to promise problems, the tight gap
becomes exponential. Last, we show that the situation is different for one-way
probabilistic automata with two-sided bounded-error. We present a family of
unary promise problems that is very easy for these machines; solvable with only
two states, but the number of states in two-way alternating or any simpler
automata is not limited by a constant. Moreover, we show that one-way
bounded-error probabilistic automata can solve promise problems not solvable at
all by any other classical model.
","Classical automata on promise problems   Promise problems were mainly studied in quantum automata theory. Here we
focus on state complexity of classical automata for promise problems. First, it
was known that there is a family of unary promise problems solvable by quantum
automata by using a single qubit, but the number of states required by
corresponding one-way deterministic automata cannot be bounded by a constant.
For this family, we show that even two-way nondeterminism does not help to save
a single state. By comparing this with the corresponding state complexity of
alternating machines, we then get a tight exponential gap between two-way
nondeterministic and one-way alternating automata solving unary promise
problems. Second, despite of the existing quadratic gap between Las Vegas
realtime probabilistic automata and one-way deterministic automata for language
recognition, we show that, by turning to promise problems, the tight gap
becomes exponential. Last, we show that the situation is different for one-way
probabilistic automata with two-sided bounded-error. We present a family of
unary promise problems that is very easy for these machines; solvable with only
two states, but the number of states in two-way alternating or any simpler
automata is not limited by a constant. Moreover, we show that one-way
bounded-error probabilistic automata can solve promise problems not solvable at
all by any other classical model.
","[u'computer science - formal languages and automata theory', u'computer science - computational complexity']",2092,991,991,991,8912,9268,8912,7626,7626,7626
9066,kLog: A Language for Logical and Relational Learning with Kernels,"  We introduce kLog, a novel approach to statistical relational learning.
Unlike standard approaches, kLog does not represent a probability distribution
directly. It is rather a language to perform kernel-based learning on
expressive logical and relational representations. kLog allows users to specify
learning problems declaratively. It builds on simple but powerful concepts:
learning from interpretations, entity/relationship data modeling, logic
programming, and deductive databases. Access by the kernel to the rich
representation is mediated by a technique we call graphicalization: the
relational representation is first transformed into a graph --- in particular,
a grounded entity/relationship diagram. Subsequently, a choice of graph kernel
defines the feature space. kLog supports mixed numerical and symbolic data, as
well as background knowledge in the form of Prolog or Datalog programs as in
inductive logic programming systems. The kLog framework can be applied to
tackle the same range of tasks that has made statistical relational learning so
popular, including classification, regression, multitask learning, and
collective classification. We also report about empirical comparisons, showing
that kLog can be either more accurate, or much faster at the same level of
accuracy, than Tilde and Alchemy. kLog is GPLv3 licensed and is available at
http://klog.dinfo.unifi.it along with tutorials.
","kLog: A Language for Logical and Relational Learning with Kernels   We introduce kLog, a novel approach to statistical relational learning.
Unlike standard approaches, kLog does not represent a probability distribution
directly. It is rather a language to perform kernel-based learning on
expressive logical and relational representations. kLog allows users to specify
learning problems declaratively. It builds on simple but powerful concepts:
learning from interpretations, entity/relationship data modeling, logic
programming, and deductive databases. Access by the kernel to the rich
representation is mediated by a technique we call graphicalization: the
relational representation is first transformed into a graph --- in particular,
a grounded entity/relationship diagram. Subsequently, a choice of graph kernel
defines the feature space. kLog supports mixed numerical and symbolic data, as
well as background knowledge in the form of Prolog or Datalog programs as in
inductive logic programming systems. The kLog framework can be applied to
tackle the same range of tasks that has made statistical relational learning so
popular, including classification, regression, multitask learning, and
collective classification. We also report about empirical comparisons, showing
that kLog can be either more accurate, or much faster at the same level of
accuracy, than Tilde and Alchemy. kLog is GPLv3 licensed and is available at
http://klog.dinfo.unifi.it along with tutorials.
","[u'computer science - artificial intelligence', u'd.3.2', u'computer science - learning', u'i.2.6', u'i.2.3', u'computer science - programming languages']",2092,2092,991,991,8912,2092,8912,7626,7626,7626
9067,On terminating improvement in two-player games,"  A real-valued game has the finite improvement property (FIP), if starting
from an arbitrary strategy profile and letting the players change strategies to
increase their individual payoffs in a sequential but non-deterministic order
always reaches a Nash equilibrium. E.g., potential games have the FIP. Many of
them have the FIP by chance nonetheless, since modifying even a single payoff
may ruin the property. This article characterises (in quadratic time) the class
of the finite games where FIP not only holds but is also preserved when
modifying all the occurrences of an arbitrary payoff. The characterisation
relies on a pattern-matching sufficient condition for games (finite or
infinite) to enjoy the FIP, and is followed by an inductive description of this
class.
  A real-valued game is weakly acyclic if the improvement described above can
reach a Nash equilibrium. This article characterises the finite such games
using Markov chains and almost sure convergence to equilibrium. It also gives
an inductive description of the two-player such games.
","On terminating improvement in two-player games   A real-valued game has the finite improvement property (FIP), if starting
from an arbitrary strategy profile and letting the players change strategies to
increase their individual payoffs in a sequential but non-deterministic order
always reaches a Nash equilibrium. E.g., potential games have the FIP. Many of
them have the FIP by chance nonetheless, since modifying even a single payoff
may ruin the property. This article characterises (in quadratic time) the class
of the finite games where FIP not only holds but is also preserved when
modifying all the occurrences of an arbitrary payoff. The characterisation
relies on a pattern-matching sufficient condition for games (finite or
infinite) to enjoy the FIP, and is followed by an inductive description of this
class.
  A real-valued game is weakly acyclic if the improvement described above can
reach a Nash equilibrium. This article characterises the finite such games
using Markov chains and almost sure convergence to equilibrium. It also gives
an inductive description of the two-player such games.
","[u'computer science - computer science and game theory', u'91a']",2092,991,991,991,8912,2092,8912,8600,7626,7626
9068,USBcat - Towards an Intrusion Surveillance Toolset,"  This paper identifies an intrusion surveillance framework which provides an
analyst with the ability to investigate and monitor cyber-attacks in a covert
manner. Where cyber-attacks are perpetrated for the purposes of espionage the
ability to understand an adversary's techniques and objectives are an important
element in network and computer security. With the appropriate toolset,
security investigators would be permitted to perform both live and stealthy
counter-intelligence operations by observing the behaviour and communications
of the intruder. Subsequently a more complete picture of the attacker's
identity, objectives, capabilities, and infiltration could be formulated than
is possible with present technologies. This research focused on developing an
extensible framework to permit the covert investigation of malware.
Additionally, a Universal Serial Bus (USB) Mass Storage Device (MSD) based
covert channel was designed to enable remote command and control of the
framework. The work was validated through the design, implementation and
testing of a toolset.
","USBcat - Towards an Intrusion Surveillance Toolset   This paper identifies an intrusion surveillance framework which provides an
analyst with the ability to investigate and monitor cyber-attacks in a covert
manner. Where cyber-attacks are perpetrated for the purposes of espionage the
ability to understand an adversary's techniques and objectives are an important
element in network and computer security. With the appropriate toolset,
security investigators would be permitted to perform both live and stealthy
counter-intelligence operations by observing the behaviour and communications
of the intruder. Subsequently a more complete picture of the attacker's
identity, objectives, capabilities, and infiltration could be formulated than
is possible with present technologies. This research focused on developing an
extensible framework to permit the covert investigation of malware.
Additionally, a Universal Serial Bus (USB) Mass Storage Device (MSD) based
covert channel was designed to enable remote command and control of the
framework. The work was validated through the design, implementation and
testing of a toolset.
",[u'computer science - cryptography and security'],2092,2092,5930,5930,8912,2092,8912,8600,5930,7626
9082,Ultra-Reliable Communication in 5G Wireless Systems,"  Wireless 5G systems will not only be ""4G, but faster"". One of the novel
features discussed in relation to 5G is Ultra-Reliable Communication (URC), an
operation mode not present in today's wireless systems. URC refers to provision
of certain level of communication service almost 100 % of the time. Example URC
applications include reliable cloud connectivity, critical connections for
industrial automation and reliable wireless coordination among vehicles. This
paper puts forward a systematic view on URC in 5G wireless systems. It starts
by analyzing the fundamental mechanisms that constitute a wireless connection
and concludes that one of the key steps towards enabling URC is revision of the
methods for encoding control information (metadata) and data. It introduces the
key concept of Reliable Service Composition, where a service is designed to
adapt its requirements to the level of reliability that can be attained. The
problem of URC is analyzed across two different dimensions. The first dimension
is the type of URC problem that is defined based on the time frame used to
measure the reliability of the packet transmission. Two types of URC problems
are identified: long-term URC (URC-L) and short-term URC (URC-S). The second
dimension is represented by the type of reliability impairment that can affect
the communication reliability in a given scenario. The main objective of this
paper is to create the context for defining and solving the new engineering
problems posed by URC in 5G.
","Ultra-Reliable Communication in 5G Wireless Systems   Wireless 5G systems will not only be ""4G, but faster"". One of the novel
features discussed in relation to 5G is Ultra-Reliable Communication (URC), an
operation mode not present in today's wireless systems. URC refers to provision
of certain level of communication service almost 100 % of the time. Example URC
applications include reliable cloud connectivity, critical connections for
industrial automation and reliable wireless coordination among vehicles. This
paper puts forward a systematic view on URC in 5G wireless systems. It starts
by analyzing the fundamental mechanisms that constitute a wireless connection
and concludes that one of the key steps towards enabling URC is revision of the
methods for encoding control information (metadata) and data. It introduces the
key concept of Reliable Service Composition, where a service is designed to
adapt its requirements to the level of reliability that can be attained. The
problem of URC is analyzed across two different dimensions. The first dimension
is the type of URC problem that is defined based on the time frame used to
measure the reliability of the packet transmission. Two types of URC problems
are identified: long-term URC (URC-L) and short-term URC (URC-S). The second
dimension is represented by the type of reliability impairment that can affect
the communication reliability in a given scenario. The main objective of this
paper is to create the context for defining and solving the new engineering
problems posed by URC in 5G.
","[u'computer science - networking and internet architecture', u'computer science - information theory']",2092,2092,991,991,8912,2092,2092,7626,7626,7626
9086,"Maintaining a Spanning Forest in Highly Dynamic Networks: The
  Synchronous Case","  Highly dynamic networks are characterized by frequent changes in the
availability of communication links. Many of these networks are in general
partitioned into several components that keep splitting and merging
continuously and unpredictably. We present an algorithm that strives to
maintain a forest of spanning trees in such networks, without any kind of
assumption on the rate of changes. Our algorithm is the adaptation of a
coarse-grain interaction algorithm (Casteigts et al., 2013) to the synchronous
message passing model (for dynamic networks). While the high-level principles
of the coarse-grain variant are preserved, the new algorithm turns out to be
significantly more complex. In particular, it involves a new technique that
consists of maintaining a distributed permutation of the set of all nodes IDs
throughout the execution. The algorithm also inherits the properties of its
original variant: It relies on purely localized decisions, for which no global
information is ever collected at the nodes, and yet it maintains a number of
critical properties whatever the frequency and scale of the changes. In
particular, the network remains always covered by a spanning forest in which 1)
no cycle can ever appear, 2) every node belongs to a tree, and 3) after an
arbitrary number of edge disappearance, all maximal subtrees immediately
restore exactly one token (at their root). These properties are ensured
whatever the dynamics, even if it keeps going for an arbitrary long period of
time. Optimality is not the focus here, however the number of tree per
components -- the metric of interest here -- eventually converges to one if the
network stops changing (which is never expected to happen, though). The
algorithm correctness is proven and its behavior is tested through
experimentation.
","Maintaining a Spanning Forest in Highly Dynamic Networks: The
  Synchronous Case   Highly dynamic networks are characterized by frequent changes in the
availability of communication links. Many of these networks are in general
partitioned into several components that keep splitting and merging
continuously and unpredictably. We present an algorithm that strives to
maintain a forest of spanning trees in such networks, without any kind of
assumption on the rate of changes. Our algorithm is the adaptation of a
coarse-grain interaction algorithm (Casteigts et al., 2013) to the synchronous
message passing model (for dynamic networks). While the high-level principles
of the coarse-grain variant are preserved, the new algorithm turns out to be
significantly more complex. In particular, it involves a new technique that
consists of maintaining a distributed permutation of the set of all nodes IDs
throughout the execution. The algorithm also inherits the properties of its
original variant: It relies on purely localized decisions, for which no global
information is ever collected at the nodes, and yet it maintains a number of
critical properties whatever the frequency and scale of the changes. In
particular, the network remains always covered by a spanning forest in which 1)
no cycle can ever appear, 2) every node belongs to a tree, and 3) after an
arbitrary number of edge disappearance, all maximal subtrees immediately
restore exactly one token (at their root). These properties are ensured
whatever the dynamics, even if it keeps going for an arbitrary long period of
time. Optimality is not the focus here, however the number of tree per
components -- the metric of interest here -- eventually converges to one if the
network stops changing (which is never expected to happen, though). The
algorithm correctness is proven and its behavior is tested through
experimentation.
","[u'computer science - distributed', u'parallel', u'and cluster computing']",2092,991,991,991,8912,2092,2092,8600,7626,7626
9088,LP/SDP Hierarchy Lower Bounds for Decoding Random LDPC Codes,"  Random (dv,dc)-regular LDPC codes are well-known to achieve the Shannon
capacity of the binary symmetric channel (for sufficiently large dv and dc)
under exponential time decoding. However, polynomial time algorithms are only
known to correct a much smaller fraction of errors. One of the most powerful
polynomial-time algorithms with a formal analysis is the LP decoding algorithm
of Feldman et al. which is known to correct an Omega(1/dc) fraction of errors.
In this work, we show that fairly powerful extensions of LP decoding, based on
the Sherali-Adams and Lasserre hierarchies, fail to correct much more errors
than the basic LP-decoder. In particular, we show that:
  1) For any values of dv and dc, a linear number of rounds of the
Sherali-Adams LP hierarchy cannot correct more than an O(1/dc) fraction of
errors on a random (dv,dc)-regular LDPC code.
  2) For any value of dv and infinitely many values of dc, a linear number of
rounds of the Lasserre SDP hierarchy cannot correct more than an O(1/dc)
fraction of errors on a random (dv,dc)-regular LDPC code.
  Our proofs use a new stretching and collapsing technique that allows us to
leverage recent progress in the study of the limitations of LP/SDP hierarchies
for Maximum Constraint Satisfaction Problems (Max-CSPs). The problem then
reduces to the construction of special balanced pairwise independent
distributions for Sherali-Adams and special cosets of balanced pairwise
independent subgroups for Lasserre.
  Some of our techniques are more generally applicable to a large class of
Boolean CSPs called Min-Ones. In particular, for k-Hypergraph Vertex Cover, we
obtain an improved integrality gap of $k-1-\epsilon$ that holds after a
\emph{linear} number of rounds of the Lasserre hierarchy, for any k = q+1 with
q an arbitrary prime power. The best previous gap for a linear number of rounds
was equal to $2-\epsilon$ and due to Schoenebeck.
","LP/SDP Hierarchy Lower Bounds for Decoding Random LDPC Codes   Random (dv,dc)-regular LDPC codes are well-known to achieve the Shannon
capacity of the binary symmetric channel (for sufficiently large dv and dc)
under exponential time decoding. However, polynomial time algorithms are only
known to correct a much smaller fraction of errors. One of the most powerful
polynomial-time algorithms with a formal analysis is the LP decoding algorithm
of Feldman et al. which is known to correct an Omega(1/dc) fraction of errors.
In this work, we show that fairly powerful extensions of LP decoding, based on
the Sherali-Adams and Lasserre hierarchies, fail to correct much more errors
than the basic LP-decoder. In particular, we show that:
  1) For any values of dv and dc, a linear number of rounds of the
Sherali-Adams LP hierarchy cannot correct more than an O(1/dc) fraction of
errors on a random (dv,dc)-regular LDPC code.
  2) For any value of dv and infinitely many values of dc, a linear number of
rounds of the Lasserre SDP hierarchy cannot correct more than an O(1/dc)
fraction of errors on a random (dv,dc)-regular LDPC code.
  Our proofs use a new stretching and collapsing technique that allows us to
leverage recent progress in the study of the limitations of LP/SDP hierarchies
for Maximum Constraint Satisfaction Problems (Max-CSPs). The problem then
reduces to the construction of special balanced pairwise independent
distributions for Sherali-Adams and special cosets of balanced pairwise
independent subgroups for Lasserre.
  Some of our techniques are more generally applicable to a large class of
Boolean CSPs called Min-Ones. In particular, for k-Hypergraph Vertex Cover, we
obtain an improved integrality gap of $k-1-\epsilon$ that holds after a
\emph{linear} number of rounds of the Lasserre hierarchy, for any k = q+1 with
q an arbitrary prime power. The best previous gap for a linear number of rounds
was equal to $2-\epsilon$ and due to Schoenebeck.
","[u'computer science - computational complexity', u'computer science - information theory']",2092,991,991,991,8912,8600,8912,7626,7626,7626
9093,"Numerical Fitting-based Likelihood Calculation to Speed up the Particle
  Filter","  The likelihood calculation of a vast number of particles is the computational
bottleneck for the particle filter in applications where the observation
information is rich. For fast computing the likelihood of particles, a
numerical fitting approach is proposed to construct the Likelihood Probability
Density Function (Li-PDF) by using a comparably small number of so-called
fulcrums. The likelihood of particles is thereby analytically inferred,
explicitly or implicitly, based on the Li-PDF instead of directly computed by
utilizing the observation, which can significantly reduce the computation and
enables real time filtering. The proposed approach guarantees the estimation
quality when an appropriate fitting function and properly distributed fulcrums
are used. The details for construction of the fitting function and fulcrums are
addressed respectively in detail. In particular, to deal with multivariate
fitting, the nonparametric kernel density estimator is presented which is
flexible and convenient for implicit Li-PDF implementation. Simulation
comparison with a variety of existing approaches on a benchmark 1-dimensional
model and multi-dimensional robot localization and visual tracking demonstrate
the validity of our approach.
","Numerical Fitting-based Likelihood Calculation to Speed up the Particle
  Filter   The likelihood calculation of a vast number of particles is the computational
bottleneck for the particle filter in applications where the observation
information is rich. For fast computing the likelihood of particles, a
numerical fitting approach is proposed to construct the Likelihood Probability
Density Function (Li-PDF) by using a comparably small number of so-called
fulcrums. The likelihood of particles is thereby analytically inferred,
explicitly or implicitly, based on the Li-PDF instead of directly computed by
utilizing the observation, which can significantly reduce the computation and
enables real time filtering. The proposed approach guarantees the estimation
quality when an appropriate fitting function and properly distributed fulcrums
are used. The details for construction of the fitting function and fulcrums are
addressed respectively in detail. In particular, to deal with multivariate
fitting, the nonparametric kernel density estimator is presented which is
flexible and convenient for implicit Li-PDF implementation. Simulation
comparison with a variety of existing approaches on a benchmark 1-dimensional
model and multi-dimensional robot localization and visual tracking demonstrate
the validity of our approach.
","[u'computer science - information theory', u'computer science - numerical analysis']",2092,2092,8600,8600,8912,2092,8912,7626,7626,7626
9094,"Capacity of Diffusion based Molecular Communication Networks over
  LTI-Poisson Channels","  In this paper, the capacity of a diffusion based molecular communication
network under the model of a Linear Time Invarient-Poisson (LTI-Poisson)
channel is studied. Introduced in the context of molecular communication, the
LTI-Poisson model is a natural extension of the conventional memoryless Poisson
channel to include memory. Exploiting prior art on linear ISI channels, a
computable finite-letter characterization of the capacity of single-hop
LTI-Poisson networks is provided. Then, the problem of finding more explicit
bounds on the capacity is examined, where lower and upper bounds for the point
to point case are provided. Furthermore, an approach for bounding mutual
information in the low SNR regime using the symmetrized KL divergence is
introduced and its applicability to Poisson channels is shown. To best of our
knowledge, the first non-trivial upper bound on the capacity of Poisson channel
with a maximum transmission constraint in the low SNR regime is found.
Numerical results show that the proposed upper bound is of the same order as
the capacity in the low SNR regime.
","Capacity of Diffusion based Molecular Communication Networks over
  LTI-Poisson Channels   In this paper, the capacity of a diffusion based molecular communication
network under the model of a Linear Time Invarient-Poisson (LTI-Poisson)
channel is studied. Introduced in the context of molecular communication, the
LTI-Poisson model is a natural extension of the conventional memoryless Poisson
channel to include memory. Exploiting prior art on linear ISI channels, a
computable finite-letter characterization of the capacity of single-hop
LTI-Poisson networks is provided. Then, the problem of finding more explicit
bounds on the capacity is examined, where lower and upper bounds for the point
to point case are provided. Furthermore, an approach for bounding mutual
information in the low SNR regime using the symmetrized KL divergence is
introduced and its applicability to Poisson channels is shown. To best of our
knowledge, the first non-trivial upper bound on the capacity of Poisson channel
with a maximum transmission constraint in the low SNR regime is found.
Numerical results show that the proposed upper bound is of the same order as
the capacity in the low SNR regime.
",[u'computer science - information theory'],2092,2092,8600,991,8912,2092,8912,7626,7626,7626
9102,Weighted sampling of outer products,"  This note gives a simple analysis of the randomized approximation scheme for
matrix multiplication of Drineas et al (2006) with a particular sampling
distribution over outer products. The result follows from a matrix version of
Bernstein's inequality. To approximate the matrix product $AB^\top$ to spectral
norm error $\varepsilon\|A\|\|B\|$, it suffices to sample on the order of
$(\mathrm{sr}(A) \vee \mathrm{sr}(B)) \log(\mathrm{sr}(A) \wedge
\mathrm{sr}(B)) / \varepsilon^2$ outer products, where $\mathrm{sr}(M)$ is the
stable rank of a matrix $M$.
","Weighted sampling of outer products   This note gives a simple analysis of the randomized approximation scheme for
matrix multiplication of Drineas et al (2006) with a particular sampling
distribution over outer products. The result follows from a matrix version of
Bernstein's inequality. To approximate the matrix product $AB^\top$ to spectral
norm error $\varepsilon\|A\|\|B\|$, it suffices to sample on the order of
$(\mathrm{sr}(A) \vee \mathrm{sr}(B)) \log(\mathrm{sr}(A) \wedge
\mathrm{sr}(B)) / \varepsilon^2$ outer products, where $\mathrm{sr}(M)$ is the
stable rank of a matrix $M$.
",[u'computer science - data structures and algorithms'],2092,991,5930,991,8912,8600,8912,7626,7626,7626
9123,A Comprehensive Survey of Recent Advancements in Molecular Communication,"  In molecular communication, information is conveyed through chemical
messages. With significant advances in the fields of nanotechnology,
bioengineering, and synthetic biology over the past decade, microscale and
nanoscale devices are becoming a reality. Yet the problem of engineering a
reliable communication system between tiny devices is still an open problem. At
the same time, despite the prevalence of radio communication, there are still
areas where traditional electromagnetic waves find it inefficient or expensive
to reach. Points of interest in industry, cities, medicine, and military
applications often lie in embedded and entrenched areas, accessible only by
ventricles at scales too small for conventional wave based communications, or
they are structured in such a way that directional high frequency systems are
ineffective. Molecular communication is a biologically inspired communication
scheme that could be employed for solving these problems. Although biologists
have studied molecular communication, it is poorly understood from a
telecommunication perspective. In this paper, we highlight the recent
advancements in the field of molecular communication engineering.
","A Comprehensive Survey of Recent Advancements in Molecular Communication   In molecular communication, information is conveyed through chemical
messages. With significant advances in the fields of nanotechnology,
bioengineering, and synthetic biology over the past decade, microscale and
nanoscale devices are becoming a reality. Yet the problem of engineering a
reliable communication system between tiny devices is still an open problem. At
the same time, despite the prevalence of radio communication, there are still
areas where traditional electromagnetic waves find it inefficient or expensive
to reach. Points of interest in industry, cities, medicine, and military
applications often lie in embedded and entrenched areas, accessible only by
ventricles at scales too small for conventional wave based communications, or
they are structured in such a way that directional high frequency systems are
ineffective. Molecular communication is a biologically inspired communication
scheme that could be employed for solving these problems. Although biologists
have studied molecular communication, it is poorly understood from a
telecommunication perspective. In this paper, we highlight the recent
advancements in the field of molecular communication engineering.
",[u'computer science - emerging technologies'],2092,991,991,991,8912,3208,8912,7626,5930,5930
9125,Online Network Design Algorithms via Hierarchical Decompositions,"  We develop a new approach for online network design and obtain improved
competitive ratios for several problems. Our approach gives natural
deterministic algorithms and simple analyses. At the heart of our work is a
novel application of embeddings into hierarchically well-separated trees (HSTs)
to the analysis of online network design algorithms --- we charge the cost of
the algorithm to the cost of the optimal solution on any HST embedding of the
terminals. This analysis technique is widely applicable to many problems and
gives a unified framework for online network design.
  In a sense, our work brings together two of the main approaches to online
network design. The first uses greedy-like algorithms and analyzes them using
dual-fitting. The second uses tree embeddings and results in randomized $O(\log
n)$-competitive algorithms, where $n$ is the total number of vertices in the
graph. Our approach uses deterministic greedy-like algorithms but analyzes them
via HST embeddings of the terminals. Our proofs are simpler as we do not need
to carefully construct dual solutions and we get $O(\log k)$ competitive
ratios, where $k$ is the number of terminals.
  In this paper, we apply our approach to obtain deterministic $O(\log
k)$-competitive online algorithms for the following problems.
  - Steiner network with edge duplication. Previously, only a randomized
$O(\log n)$-competitive algorithm was known.
  - Rent-or-buy. Previously, only deterministic $O(\log^2 k)$-competitive and
randomized $O(\log k)$-competitive algorithms by Awerbuch, Azar and Bartal
(2004) were known.
  - Connected facility location. Previously, only a randomized $O(\log^2
k)$-competitive algorithm by San Felice, Williamson and Lee (2014) was known.
  - Prize-collecting Steiner forest. We match the competitive ratio first
achieved by Qian and Williamson (2011) and give a simpler analysis.
","Online Network Design Algorithms via Hierarchical Decompositions   We develop a new approach for online network design and obtain improved
competitive ratios for several problems. Our approach gives natural
deterministic algorithms and simple analyses. At the heart of our work is a
novel application of embeddings into hierarchically well-separated trees (HSTs)
to the analysis of online network design algorithms --- we charge the cost of
the algorithm to the cost of the optimal solution on any HST embedding of the
terminals. This analysis technique is widely applicable to many problems and
gives a unified framework for online network design.
  In a sense, our work brings together two of the main approaches to online
network design. The first uses greedy-like algorithms and analyzes them using
dual-fitting. The second uses tree embeddings and results in randomized $O(\log
n)$-competitive algorithms, where $n$ is the total number of vertices in the
graph. Our approach uses deterministic greedy-like algorithms but analyzes them
via HST embeddings of the terminals. Our proofs are simpler as we do not need
to carefully construct dual solutions and we get $O(\log k)$ competitive
ratios, where $k$ is the number of terminals.
  In this paper, we apply our approach to obtain deterministic $O(\log
k)$-competitive online algorithms for the following problems.
  - Steiner network with edge duplication. Previously, only a randomized
$O(\log n)$-competitive algorithm was known.
  - Rent-or-buy. Previously, only deterministic $O(\log^2 k)$-competitive and
randomized $O(\log k)$-competitive algorithms by Awerbuch, Azar and Bartal
(2004) were known.
  - Connected facility location. Previously, only a randomized $O(\log^2
k)$-competitive algorithm by San Felice, Williamson and Lee (2014) was known.
  - Prize-collecting Steiner forest. We match the competitive ratio first
achieved by Qian and Williamson (2011) and give a simpler analysis.
",[u'computer science - data structures and algorithms'],2092,991,991,991,8912,8600,8912,7626,7626,7626
9130,Diffusive Molecular Communication with Disruptive Flows,"  In this paper, we study the performance of detectors in a diffusive molecular
communication environment where steady uniform flow is present. We derive the
expected number of information molecules to be observed in a passive spherical
receiver, and determine the impact of flow on the assumption that the
concentration of molecules throughout the receiver is uniform. Simulation
results show the impact of advection on detector performance as a function of
the flow's magnitude and direction. We highlight that there are disruptive
flows, i.e., flows that are not in the direction of information transmission,
that lead to an improvement in detector performance as long as the disruptive
flow does not dominate diffusion and sufficient samples are taken.
","Diffusive Molecular Communication with Disruptive Flows   In this paper, we study the performance of detectors in a diffusive molecular
communication environment where steady uniform flow is present. We derive the
expected number of information molecules to be observed in a passive spherical
receiver, and determine the impact of flow on the assumption that the
concentration of molecules throughout the receiver is uniform. Simulation
results show the impact of advection on detector performance as a function of
the flow's magnitude and direction. We highlight that there are disruptive
flows, i.e., flows that are not in the direction of information transmission,
that lead to an improvement in detector performance as long as the disruptive
flow does not dominate diffusion and sufficient samples are taken.
",[u'computer science - information theory'],2092,2092,991,991,8912,2092,8912,7626,5930,7626
9145,"MIMO Broadcasting for Simultaneous Wireless Information and Power
  Transfer: Weighted MMSE Approaches","  We consider simultaneous wireless information and power transfer (SWIPT) in
MIMO Broadcast networks where one energy harvesting (EH) user and one
information decoding (ID) user share the same time and frequency resource. In
contrast to previous SWIPT systems based on the information rate, this paper
addresses the problem in terms of the weighted minimum mean squared error
(WMMSE) criterion. First, we formulate the WMMSE-SWIPT problem which minimizes
the weighted sum-MSE of the message signal arrived at the ID user, while
satisfying the requirement on the energy that can be harvested from the signal
at the EH user. Then, we propose the optimal precoder structure of the problem
and identify the best possible MSE-energy tradeoff region through the
alternative update of the linear precoder at the transmitter with the linear
receiver at the ID user. From the derived solution, several interesting
observations are made compared to the conventional SWIPT designs.
","MIMO Broadcasting for Simultaneous Wireless Information and Power
  Transfer: Weighted MMSE Approaches   We consider simultaneous wireless information and power transfer (SWIPT) in
MIMO Broadcast networks where one energy harvesting (EH) user and one
information decoding (ID) user share the same time and frequency resource. In
contrast to previous SWIPT systems based on the information rate, this paper
addresses the problem in terms of the weighted minimum mean squared error
(WMMSE) criterion. First, we formulate the WMMSE-SWIPT problem which minimizes
the weighted sum-MSE of the message signal arrived at the ID user, while
satisfying the requirement on the energy that can be harvested from the signal
at the EH user. Then, we propose the optimal precoder structure of the problem
and identify the best possible MSE-energy tradeoff region through the
alternative update of the linear precoder at the transmitter with the linear
receiver at the ID user. From the derived solution, several interesting
observations are made compared to the conventional SWIPT designs.
",[u'computer science - information theory'],2092,2092,8600,8600,8912,2092,2092,7626,7626,7626
9148,"Competitive Comparison of Optimal Designs of Experiments for
  Sampling-based Sensitivity Analysis","  Nowadays, the numerical models of real-world structures are more precise,
more complex and, of course, more time-consuming. Despite the growth of a
computational effort, the exploration of model behaviour remains a complex
task. The sensitivity analysis is a basic tool for investigating the
sensitivity of the model to its inputs. One widely used strategy to assess the
sensitivity is based on a finite set of simulations for a given sets of input
parameters, i.e. points in the design space. An estimate of the sensitivity can
be then obtained by computing correlations between the input parameters and the
chosen response of the model. The accuracy of the sensitivity prediction
depends on the choice of design points called the design of experiments. The
aim of the presented paper is to review and compare available criteria
determining the quality of the design of experiments suitable for
sampling-based sensitivity analysis.
","Competitive Comparison of Optimal Designs of Experiments for
  Sampling-based Sensitivity Analysis   Nowadays, the numerical models of real-world structures are more precise,
more complex and, of course, more time-consuming. Despite the growth of a
computational effort, the exploration of model behaviour remains a complex
task. The sensitivity analysis is a basic tool for investigating the
sensitivity of the model to its inputs. One widely used strategy to assess the
sensitivity is based on a finite set of simulations for a given sets of input
parameters, i.e. points in the design space. An estimate of the sensitivity can
be then obtained by computing correlations between the input parameters and the
chosen response of the model. The accuracy of the sensitivity prediction
depends on the choice of design points called the design of experiments. The
aim of the presented paper is to review and compare available criteria
determining the quality of the design of experiments suitable for
sampling-based sensitivity analysis.
","[u'computer science - computational engineering', u'statistics - methodology', u'finance', u'and science', u'computer science - numerical analysis']",2092,991,991,7626,8912,2092,8912,7626,7626,7626
9150,"XSS Peeker: A Systematic Analysis of Cross-site Scripting Vulnerability
  Scanners","  Since the first publication of the ""OWASP Top 10"" (2004), cross-site
scripting (XSS) vulnerabilities have always been among the top 5 web
application security bugs. Black-box vulnerability scanners are widely used in
the industry to reproduce (XSS) attacks automatically. In spite of the
technical sophistication and advancement, previous work showed that black-box
scanners miss a non-negligible portion of vulnerabilities, and report
non-existing, non-exploitable or uninteresting vulnerabilities. Unfortunately,
these results hold true even for XSS vulnerabilities, which are relatively
simple to trigger if compared, for instance, to logic flaws.
  Black-box scanners have not been studied in depth on this vertical: knowing
precisely how scanners try to detect XSS can provide useful insights to
understand their limitations, to design better detection methods. In this
paper, we present and discuss the results of a detailed and systematic study on
6 black-box web scanners (both proprietary and open source) that we conducted
in coordination with the respective vendors. To this end, we developed an
automated tool to (1) extract the payloads used by each scanner, (2) distill
the ""templates"" that have originated each payload, (3) evaluate them according
to quality indicators, and (4) perform a cross-scanner analysis. Unlike
previous work, our testbed application, which contains a large set of XSS
vulnerabilities, including DOM XSS, was gradually retrofitted to accomodate for
the payloads that triggered no vulnerabilities.
  Our analysis reveals a highly fragmented scenario. Scanners exhibit a wide
variety of distinct payloads, a non-uniform approach to fuzzing and mutating
the payloads, and a very diverse detection effectiveness.
","XSS Peeker: A Systematic Analysis of Cross-site Scripting Vulnerability
  Scanners   Since the first publication of the ""OWASP Top 10"" (2004), cross-site
scripting (XSS) vulnerabilities have always been among the top 5 web
application security bugs. Black-box vulnerability scanners are widely used in
the industry to reproduce (XSS) attacks automatically. In spite of the
technical sophistication and advancement, previous work showed that black-box
scanners miss a non-negligible portion of vulnerabilities, and report
non-existing, non-exploitable or uninteresting vulnerabilities. Unfortunately,
these results hold true even for XSS vulnerabilities, which are relatively
simple to trigger if compared, for instance, to logic flaws.
  Black-box scanners have not been studied in depth on this vertical: knowing
precisely how scanners try to detect XSS can provide useful insights to
understand their limitations, to design better detection methods. In this
paper, we present and discuss the results of a detailed and systematic study on
6 black-box web scanners (both proprietary and open source) that we conducted
in coordination with the respective vendors. To this end, we developed an
automated tool to (1) extract the payloads used by each scanner, (2) distill
the ""templates"" that have originated each payload, (3) evaluate them according
to quality indicators, and (4) perform a cross-scanner analysis. Unlike
previous work, our testbed application, which contains a large set of XSS
vulnerabilities, including DOM XSS, was gradually retrofitted to accomodate for
the payloads that triggered no vulnerabilities.
  Our analysis reveals a highly fragmented scenario. Scanners exhibit a wide
variety of distinct payloads, a non-uniform approach to fuzzing and mutating
the payloads, and a very diverse detection effectiveness.
",[u'computer science - cryptography and security'],2092,2092,8600,8600,8912,8912,8912,8600,7626,7626
9162,"Multi-Gigabits Millimetre Wave Wireless Communications for 5G: From
  Fixed Access to Cellular Networks","  With the formidable growth of various booming wireless communication services
that require ever-increasing data throughputs, the conventional microwave band
below 10 GHz, which is currently used by almost all mobile communication
systems, is going to reach its saturation point within just a few years.
Therefore, the attention of radio system designers has been pushed towards
ever-higher segments of the frequency spectrum in a quest for capacity
increase. In this article, we investigate the feasibility, advantages and
challenges of future wireless communications over the E-band frequencies. We
start from a brief review of the history of E-band spectrum and its light
licensing policy as well as benefits/challenges. Then we introduce the
propagation characteristics of E-band signals, based on which some potential
fixed and mobile applications at the E-band are investigated. In particular, we
analyze the achievability of non-trivial multiplexing gain in fixed
point-to-point E-band links and propose an E-band mobile broadband (EMB) system
as a candidate for the next generation mobile communication networks. The
channelization and frame structure of the EMB system are discussed in details.
","Multi-Gigabits Millimetre Wave Wireless Communications for 5G: From
  Fixed Access to Cellular Networks   With the formidable growth of various booming wireless communication services
that require ever-increasing data throughputs, the conventional microwave band
below 10 GHz, which is currently used by almost all mobile communication
systems, is going to reach its saturation point within just a few years.
Therefore, the attention of radio system designers has been pushed towards
ever-higher segments of the frequency spectrum in a quest for capacity
increase. In this article, we investigate the feasibility, advantages and
challenges of future wireless communications over the E-band frequencies. We
start from a brief review of the history of E-band spectrum and its light
licensing policy as well as benefits/challenges. Then we introduce the
propagation characteristics of E-band signals, based on which some potential
fixed and mobile applications at the E-band are investigated. In particular, we
analyze the achievability of non-trivial multiplexing gain in fixed
point-to-point E-band links and propose an E-band mobile broadband (EMB) system
as a candidate for the next generation mobile communication networks. The
channelization and frame structure of the EMB system are discussed in details.
",[u'computer science - networking and internet architecture'],2092,2092,991,991,8912,2092,8912,7626,2092,5930
