{"docs": [{"journal": "PLoS ONE", "abstract": ["Background: Research in the predictors of all-cause mortality in HIV-infected people has widely been reported in literature. Making an informed decision requires understanding the methods used. Objectives: We present a review on study designs, statistical methods and their appropriateness in original articles reporting on predictors of all-cause mortality in HIV-infected people between January 2002 and December 2011. Statistical methods were compared between 2002\u20132006 and 2007\u20132011. Time-to-event analysis techniques were considered appropriate. Data Sources: Pubmed/Medline. Study Eligibility Criteria: Original English-language articles were abstracted. Letters to the editor, editorials, reviews, systematic reviews, meta-analysis, case reports and any other ineligible articles were excluded. Results: A total of 189 studies were identified (n\u200a=\u200a91 in 2002\u20132006 and n\u200a=\u200a98 in 2007\u20132011) out of which 130 (69%) were prospective and 56 (30%) were retrospective. One hundred and eighty-two (96%) studies described their sample using descriptive statistics while 32 (17%) made comparisons using t-tests. Kaplan-Meier methods for time-to-event analysis were commonly used in the earlier period (n\u200a=\u200a69, 76% vs. n\u200a=\u200a53, 54%, p\u200a=\u200a0.002). Predictors of mortality in the two periods were commonly determined using Cox regression analysis (n\u200a=\u200a67, 75% vs. n\u200a=\u200a63, 64%, p\u200a=\u200a0.12). Only 7 (4%) used advanced survival analysis methods of Cox regression analysis with frailty in which 6 (3%) were used in the later period. Thirty-two (17%) used logistic regression while 8 (4%) used other methods. There were significantly more articles from the first period using appropriate methods compared to the second (n\u200a=\u200a80, 88% vs. n\u200a=\u200a69, 70%, p-value\u200a=\u200a0.003). Conclusion: Descriptive statistics and survival analysis techniques remain the most common methods of analysis in publications on predictors of all-cause mortality in HIV-infected cohorts while prospective research designs are favoured. Sophisticated techniques of time-dependent Cox regression and Cox regression with frailty are scarce. This motivates for more training in the use of advanced time-to-event methods. "], "author_display": ["Kennedy N. Otwombe", "Max Petzold", "Neil Martinson", "Tobias Chirwa"], "article_type": "Research Article", "score": 0.64278555, "title_display": "A Review of the Study Designs and Statistical Methods Used in the Determination of Predictors of All-Cause Mortality in HIV-Infected Cohorts: 2002\u20132011", "publication_date": "2014-02-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0087356"}, {"journal": "PLoS ONE", "abstract": [": In computational science literature including, e.g., bioinformatics, computational statistics or machine learning, most published articles are devoted to the development of \u201cnew methods\u201d, while comparison studies are generally appreciated by readers but surprisingly given poor consideration by many journals. This paper stresses the importance of neutral comparison studies for the objective evaluation of existing methods and the establishment of standards by drawing parallels with clinical research. The goal of the paper is twofold. Firstly, we present a survey of recent computational papers on supervised classification published in seven high-ranking computational science journals. The aim is to provide an up-to-date picture of current scientific practice with respect to the comparison of methods in both articles presenting new methods and articles focusing on the comparison study itself. Secondly, based on the results of our survey we critically discuss the necessity, impact and limitations of neutral comparison studies in computational sciences. We define three reasonable criteria a comparison study has to fulfill in order to be considered as neutral, and explicate general considerations on the individual components of a \u201ctidy neutral comparison study\u201d. R codes for completely replicating our statistical analyses and figures are available from the companion website http://www.ibe.med.uni-muenchen.de/organisation/mitarbeiter/020_professuren/boulesteix/plea2013. "], "author_display": ["Anne-Laure Boulesteix", "Sabine Lauer", "Manuel J. A. Eugster"], "article_type": "Research Article", "score": 0.64208776, "title_display": "A Plea for Neutral Comparison Studies in Computational Sciences", "publication_date": "2013-04-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0061562"}, {"journal": "PLoS ONE", "abstract": ["Background: The inherent complexity of statistical methods and clinical phenomena compel researchers with diverse domains of expertise to work in interdisciplinary teams, where none of them have a complete knowledge in their counterpart's field. As a result, knowledge exchange may often be characterized by miscommunication leading to misinterpretation, ultimately resulting in errors in research and even clinical practice. Though communication has a central role in interdisciplinary collaboration and since miscommunication can have a negative impact on research processes, to the best of our knowledge, no study has yet explored how data analysis specialists and clinical researchers communicate over time. Methods/Principal Findings: We conducted qualitative analysis of encounters between clinical researchers and data analysis specialists (epidemiologist, clinical epidemiologist, and data mining specialist). These encounters were recorded and systematically analyzed using a grounded theory methodology for extraction of emerging themes, followed by data triangulation and analysis of negative cases for validation. A policy analysis was then performed using a system dynamics methodology looking for potential interventions to improve this process. Four major emerging themes were found. Definitions using lay language were frequently employed as a way to bridge the language gap between the specialties. Thought experiments presented a series of \u201cwhat if\u201d situations that helped clarify how the method or information from the other field would behave, if exposed to alternative situations, ultimately aiding in explaining their main objective. Metaphors and analogies were used to translate concepts across fields, from the unfamiliar to the familiar. Prolepsis was used to anticipate study outcomes, thus helping specialists understand the current context based on an understanding of their final goal. Conclusion/Significance: The communication between clinical researchers and data analysis specialists presents multiple challenges that can lead to errors. "], "author_display": ["Guilherme Roberto Zammar", "Jatin Shah", "Ana Paula Bonilauri Ferreira", "Luciana Cofiel", "Kenneth W. Lyles", "Ricardo Pietrobon"], "article_type": "Research Article", "score": 0.6020323, "title_display": "Qualitative Analysis of the Interdisciplinary Interaction between Data Analysis Specialists and Novice Clinical Researchers", "publication_date": "2010-02-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0009400"}, {"journal": "PLoS ONE", "abstract": ["\nIdentification of functional sets of genes associated with conditions of interest from omics data was first reported in 1999, and since, a plethora of enrichment methods were published for systematic analysis of gene sets collections including Gene Ontology and biological pathways. Despite their widespread usage in reducing the complexity of omics experiment results, their performance is poorly understood. Leveraging the existence of disease specific gene sets in KEGG and Metacore\u00ae databases, we compared the performance of sixteen methods under relaxed assumptions while using 42 real datasets (over 1,400 samples). Most of the methods ranked high the gene sets designed for specific diseases whenever samples from affected individuals were compared against controls via microarrays. The top methods for gene set prioritization were different from the top ones in terms of sensitivity, and four of the sixteen methods had large false positives rates assessed by permuting the phenotype of the samples. The best overall methods among those that generated reasonably low false positive rates, when permuting phenotypes, were PLAGE, GLOBALTEST, and PADOG. The best method in the category that generated higher than expected false positives was MRGSE.\n"], "author_display": ["Adi L. Tarca", "Gaurav Bhatti", "Roberto Romero"], "article_type": "Research Article", "score": 0.59679836, "title_display": "A Comparison of Gene Set Analysis Methods in Terms of Sensitivity, Prioritization and Specificity", "publication_date": "2013-11-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0079217"}, {"journal": "PLoS ONE", "abstract": ["Background: Commentators have expressed concern regarding the existence of proper ethics review systems in developing countries. Our aim is to explore the extent with which investigators from countries in the Eastern Mediterranean (EM) Region consider several ethical practices in the conduct of their research. Methodology/Principal Findings: Investigators from 12 countries in the EM region submitted 143 proposals involving Public Health and Biotechnology & Genomics to a grant scheme funded by the Eastern Mediterranean Regional Office of the WHO and the Organization of Islamic Conference Standing Committee for Science and Technological Cooperation in 2006. The grant application included a 1-page questionnaire that asked investigators 1) whether ethical clearance was obtained, 2) whether they plan to obtain informed consent, and 3) whether confidentiality of human subject data would be ensured. The methodologies of the submitted researches were categorized as to whether it involved 1) human subject research (e.g., the prospective collection of biological specimens or the performance of qualitative research), 2) research that could be exempt from ongoing ethics review, and 3) research not involving human subjects. A descriptive analysis was used to analyze the investigators' responses and a chi-square analysis was used to analyze categorical variables. Of the 79 submitted proposals determined to involve \u201chuman subjects\u201d, ethical clearance was not obtained in 29%; investigators thought that informed consent was not needed in 29%; and investigators did not mention that they would ensure confidentiality of the obtained data in 8% of the studies. The magnitude of these deficiencies was similar regardless of study design type, i.e., prospective collection of biological samples and qualitative research methods. Conclusion/Significance: These results suggest that attention to ethical safeguards is not optimal among investigators in the EM Region. Further guidelines for strengthening ethical review systems, as well as enhanced educational training in concepts of research ethics for investigators are warranted in this region. "], "author_display": ["Mohammad Abdur Rab", "Mohammad Afzal", "Alaa Abou-Zeid", "Henry Silverman"], "article_type": "Research Article", "score": 0.58397925, "title_display": "Ethical Practices for Health Research in the Eastern Mediterranean Region of the World Health Organization: A Retrospective Data Analysis", "publication_date": "2008-05-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0002094"}, {"journal": "PLoS ONE", "abstract": ["\n        Doubt about the relevance, appropriateness and transparency of peer review has promoted the use of citation metrics as a viable adjunct or alternative in the assessment of research impact. It is also commonly acknowledged that research metrics will not replace peer review unless they are shown to correspond with the assessment of peers. This paper evaluates the relationship between researchers' influence as evaluated by their peers and various citation metrics representing different aspects of research output in 6 fields of public health in Australia. For four fields, the results showed a modest positive correlation between different research metrics and peer assessments of research influence. However, for two fields, tobacco and injury, negative or no correlations were found. This suggests a peer understanding of research influence within these fields differed from visibility in the mainstream, peer-reviewed scientific literature. This research therefore recommends the use of both peer review and metrics in a combined approach in assessing research influence. Future research evaluation frameworks intent on incorporating metrics should first analyse each field closely to determine what measures of research influence are valued highly by members of that research community. This will aid the development of comprehensive and relevant frameworks with which to fairly and transparently distribute research funds or approve promotion applications.\n      "], "author_display": ["Gemma Elizabeth Derrick", "Abby Haynes", "Simon Chapman", "Wayne D. Hall"], "article_type": "Research Article", "score": 0.58388126, "title_display": "The Association between Four Citation Metrics and Peer Rankings of Research Influence of Australian Researchers in Six Fields of Public Health", "publication_date": "2011-04-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0018521"}, {"journal": "PLoS ONE", "abstract": ["Background: Systematic reviews (SRs) can provide accurate and reliable evidence, typically about the effectiveness of health interventions. Evidence is dynamic, and if SRs are out-of-date this information may not be useful; it may even be harmful. This study aimed to compare five statistical methods to identify out-of-date SRs. Methods: A retrospective cohort of SRs registered in the Cochrane Pregnancy and Childbirth Group (CPCG), published between 2008 and 2010, were considered for inclusion. For each eligible CPCG review, data were extracted and \u201c3-years previous\u201d meta-analyses were assessed for the need to update, given the data from the most recent 3 years. Each of the five statistical methods was used, with random effects analyses throughout the study. Results: Eighty reviews were included in this study; most were in the area of induction of labour. The numbers of reviews identified as being out-of-date using the Ottawa, recursive cumulative meta-analysis (CMA), and Barrowman methods were 34, 7, and 7 respectively. No reviews were identified as being out-of-date using the simulation-based power method, or the CMA for sufficiency and stability method. The overall agreement among the three discriminating statistical methods was slight (Kappa\u200a=\u200a0.14; 95% CI 0.05 to 0.23). The recursive cumulative meta-analysis, Ottawa, and Barrowman methods were practical according to the study criteria. Conclusion: Our study shows that three practical statistical methods could be applied to examine the need to update SRs. "], "author_display": ["Porjai Pattanittum", "Malinee Laopaiboon", "David Moher", "Pisake Lumbiganon", "Chetta Ngamjarus"], "article_type": "Research Article", "score": 0.58019716, "title_display": "A Comparison of Statistical Methods for Identifying Out-of-Date Systematic Reviews", "publication_date": "2012-11-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0048894"}, {"journal": "PLoS ONE", "abstract": ["\n        Assessing an individual's research impact on the basis of a transparent algorithm is an important task for evaluation and comparison purposes. Besides simple but also inaccurate indices such as counting the mere number of publications or the accumulation of overall citations, and highly complex but also overwhelming full-range publication lists in their raw format, Hirsch (2005) introduced a single figure cleverly combining different approaches. The so-called h-index has undoubtedly become the standard in scientometrics of individuals' research impact (note: in the present paper I will always use the term \u201cresearch impact\u201d to describe the research performance as the logic of the paper is based on the h-index, which quantifies the specific \u201cimpact\u201d of, e.g., researchers, but also because the genuine meaning of impact refers to quality as well). As the h-index reflects the number h of papers a researcher has published with at least h citations, the index is inherently positively biased towards senior level researchers. This might sometimes be problematic when predictive tools are needed for assessing young scientists' potential, especially when recruiting early career positions or equipping young scientists' labs. To be compatible with the standard h-index, the proposed index integrates the scientist's research age (Carbon_h-factor) into the h-index, thus reporting the average gain of h-index per year. Comprehensive calculations of the Carbon_h-factor were made for a broad variety of four research-disciplines (economics, neuroscience, physics and psychology) and for researchers performing on three high levels of research impact (substantial, outstanding and epochal) with ten researchers per category. For all research areas and output levels we obtained linear developments of the h-index demonstrating the validity of predicting one's later impact in terms of research impact already at an early stage of their career with the Carbon_h-factor being approx. 0.4, 0.8, and 1.5 for substantial, outstanding and epochal researchers, respectively.\n      "], "author_display": ["Claus-Christian Carbon"], "article_type": "Research Article", "score": 0.57764184, "title_display": "The <i>Carbon_h-</i>Factor: Predicting Individuals' Research Impact at Early Stages of Their Career", "publication_date": "2011-12-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0028770"}, {"journal": "PLoS ONE", "abstract": ["\nOptimal nutrition is critical for human development and economic growth. Sub-Saharan Africa is facing high levels of food insecurity and only few sub-Saharan African countries are on track to eradicate extreme poverty and hunger by 2015. Effective research capacity is crucial for addressing emerging challenges and designing appropriate mitigation strategies in sub-Saharan Africa. A clear understanding of the operating environment for nutrition research in sub-Saharan Africa is a much needed prerequisite. We collected data on the barriers and requirements for conducting nutrition research in sub-Saharan Africa through semi-structured interviews with 144 participants involved in nutrition research in 35 countries in sub-Saharan Africa. A total of 133 interviews were retained for coding. The main barriers identified for effective nutrition research were the lack of funding due to poor recognition by policymakers of the importance of nutrition research and under-utilisation of research findings for developing policy, as well as an absence of research priority setting from within Africa. Current research topics were perceived to be mainly determined by funding bodies from outside Africa. Nutrition researchers argued for more commitment from policymakers at national level. The low capacity for nutrition research was mainly seen as a consequence of insufficient numbers of nutrition researchers, limited skills and a poor research infrastructure. In conclusion, African nutrition researchers argued how research priorities need to be identified by African stakeholders, accompanied by consensus building to enable creating a problem-driven national research agenda. In addition, it was considered necessary to promote interactions among researchers, and between researchers and policymakers. Multidisciplinary research and international and cross-African collaboration were seen as crucial to build capacity in sub-Saharan nutrition research.\n"], "author_display": ["Kathleen Van Royen", "Carl Lachat", "Michelle Holdsworth", "Karlien Smit", "Joyce Kinabo", "Dominique Roberfroid", "Eunice Nago", "Christopher Garimoi Orach", "Patrick Kolsteren"], "article_type": "Research Article", "score": 0.57582897, "title_display": "How Can the Operating Environment for Nutrition Research Be Improved in Sub-Saharan Africa? The Views of African Researchers", "publication_date": "2013-06-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0066355"}, {"journal": "PLoS ONE", "abstract": ["\n        Researchers' networks have been subject to active modeling and analysis. Earlier literature mostly focused on citation or co-authorship networks reconstructed from annotated scientific publication databases, which have several limitations. Recently, general-purpose web search engines have also been utilized to collect information about social networks. Here we reconstructed, using web search engines, a network representing the relatedness of researchers to their peers as well as to various research topics. Relatedness between researchers and research topics was characterized by visibility boost\u2014increase of a researcher's visibility by focusing on a particular topic. It was observed that researchers who had high visibility boosts by the same research topic tended to be close to each other in their network. We calculated correlations between visibility boosts by research topics and researchers' interdisciplinarity at the individual level (diversity of topics related to the researcher) and at the social level (his/her centrality in the researchers' network). We found that visibility boosts by certain research topics were positively correlated with researchers' individual-level interdisciplinarity despite their negative correlations with the general popularity of researchers. It was also found that visibility boosts by network-related topics had positive correlations with researchers' social-level interdisciplinarity. Research topics' correlations with researchers' individual- and social-level interdisciplinarities were found to be nearly independent from each other. These findings suggest that the notion of \u201cinterdisciplinarity\" of a researcher should be understood as a multi-dimensional concept that should be evaluated using multiple assessment means.\n      "], "author_display": ["Hiroki Sayama", "Jin Akaishi"], "article_type": "Research Article", "score": 0.5740088, "title_display": "Characterizing Interdisciplinarity of Researchers and Research Topics Using Web Search Engines", "publication_date": "2012-06-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0038747"}, {"journal": "PLoS ONE", "abstract": ["Background: Accurate values are a must in medicine. An important parameter in determining the quality of a medical instrument is agreement with a gold standard. Various statistical methods have been used to test for agreement. Some of these methods have been shown to be inappropriate. This can result in misleading conclusions about the validity of an instrument. The Bland-Altman method is the most popular method judging by the many citations of the article proposing this method. However, the number of citations does not necessarily mean that this method has been applied in agreement research. No previous study has been conducted to look into this. This is the first systematic review to identify statistical methods used to test for agreement of medical instruments. The proportion of various statistical methods found in this review will also reflect the proportion of medical instruments that have been validated using those particular methods in current clinical practice. Methodology/Findings: Five electronic databases were searched between 2007 and 2009 to look for agreement studies. A total of 3,260 titles were initially identified. Only 412 titles were potentially related, and finally 210 fitted the inclusion criteria. The Bland-Altman method is the most popular method with 178 (85%) studies having used this method, followed by the correlation coefficient (27%) and means comparison (18%). Some of the inappropriate methods highlighted by Altman and Bland since the 1980s are still in use. Conclusions: This study finds that the Bland-Altman method is the most popular method used in agreement research. There are still inappropriate applications of statistical methods in some studies. It is important for a clinician or medical researcher to be aware of this issue because misleading conclusions from inappropriate analyses will jeopardize the quality of the evidence, which in turn will influence quality of care given to patients in the future. "], "author_display": ["Rafdzah Zaki", "Awang Bulgiba", "Roshidi Ismail", "Noor Azina Ismail"], "article_type": "Research Article", "score": 0.56699145, "title_display": "Statistical Methods Used to Test for Agreement of Medical Instruments Measuring Continuous Variables in Method Comparison Studies: A Systematic Review", "publication_date": "2012-05-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0037908"}, {"journal": "PLOS ONE", "abstract": ["\nThe analysis of heart rate variability (HRV) has been performed on long-term electrocardiography (ECG) recordings (12~24 hours) and short-term recordings (2~5 minutes), which may not capture momentary change of HRV. In this study, we present a new method to analyze the momentary HRV (mHRV). The ECG recordings were segmented into a series of overlapped HRV analysis windows with a window length of 5 minutes and different time increments. The performance of the proposed method in delineating the dynamics of momentary HRV measurement was evaluated with four commonly used time courses of HRV measures on both synthetic time series and real ECG recordings from human subjects and dogs. Our results showed that a smaller time increment could capture more dynamical information on transient changes. Considering a too short increment such as 10 s would cause the indented time courses of the four measures, a 1-min time increment (4-min overlapping) was suggested in the analysis of mHRV in the study. ECG recordings from human subjects and dogs were used to further assess the effectiveness of the proposed method. The pilot study demonstrated that the proposed analysis of mHRV could provide more accurate assessment of the dynamical changes in cardiac activity than the conventional measures of HRV (without time overlapping). The proposed method may provide an efficient means in delineating the dynamics of momentary HRV and it would be worthy performing more investigations.\n"], "author_display": ["Haoshi Zhang", "Mingxing Zhu", "Yue Zheng", "Guanglin Li"], "article_type": "Research Article", "score": 0.5662838, "title_display": "Toward Capturing Momentary Changes of Heart Rate Variability by a Dynamic Analysis Method", "publication_date": "2015-07-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0133148"}, {"journal": "PLoS ONE", "abstract": ["Background: The source of funding is one of many possible causes of bias in scientific research. One method of detecting potential for bias is to evaluate the quality of research reports. Research exploring the relationship between funding source and nutrition-related research report quality is limited and in other disciplines the findings are mixed. Objective: The purpose of this study is to determine whether types of funding sources of nutrition research are associated with differences in research report quality. Design: A retrospective study of research reporting quality, research design and funding source was conducted on 2539 peer reviewed research articles from the American Dietetic Association's Evidence Analysis Library\u00ae database. Results: Quality rating frequency distributions indicate 43.3% of research reports were rated as positive, 50.1% neutral, and 6.6% as negative. Multinomial logistic regression results showed that while both funding source and type of research design are significant predictors of quality ratings (\u03c72\u200a=\u200a118.99, p<0.001), the model's usefulness in predicting overall research report quality is little better than chance. Compared to research reports with government funding, those not acknowledging any funding sources, followed by studies with University/hospital funding were more likely to receive neutral vs positive quality ratings, OR\u200a=\u200a1.85, P <0.001 and OR\u200a=\u200a1.54, P<0.001, respectively and those that did not report funding were more likely to receive negative quality ratings (OR\u200a=\u200a4.97, P<0.001). After controlling for research design, industry funded research reports were no more likely to receive a neutral or negative quality rating than those funded by government sources. Conclusion: Research report quality cannot be accurately predicted from the funding source after controlling for research design. Continued vigilance to evaluate the quality of all research regardless of the funding source and to further understand other factors that affect quality ratings are warranted. "], "author_display": ["Esther F. Myers", "J. Scott Parrott", "Deborah S. Cummins", "Patricia Splett"], "article_type": "Research Article", "score": 0.56271935, "title_display": "Funding Source and Research Report Quality in Nutrition Practice-Related Research", "publication_date": "2011-12-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0028437"}, {"journal": "PLOS ONE", "abstract": ["Background: Although a substantial number of studies focus on the teaching and application of medical statistics in China, few studies comprehensively evaluate the recognition of and demand for medical statistics. In addition, the results of these various studies differ and are insufficiently comprehensive and systematic. Objectives: This investigation aimed to evaluate the general cognition of and demand for medical statistics by undergraduates, graduates, and medical staff in China. Methods: We performed a comprehensive database search related to the cognition of and demand for medical statistics from January 2007 to July 2014 and conducted a meta-analysis of non-controlled studies with sub-group analysis for undergraduates, graduates, and medical staff. Results: There are substantial differences with respect to the cognition of theory in medical statistics among undergraduates (73.5%), graduates (60.7%), and medical staff (39.6%). The demand for theory in medical statistics is high among graduates (94.6%), undergraduates (86.1%), and medical staff (88.3%). Regarding specific statistical methods, the cognition of basic statistical methods is higher than of advanced statistical methods. The demand for certain advanced statistical methods, including (but not limited to) multiple analysis of variance (ANOVA), multiple linear regression, and logistic regression, is higher than that for basic statistical methods. The use rates of the Statistical Package for the Social Sciences (SPSS) software and statistical analysis software (SAS) are only 55% and 15%, respectively. Conclusion: The overall statistical competence of undergraduates, graduates, and medical staff is insufficient, and their ability to practically apply their statistical knowledge is limited, which constitutes an unsatisfactory state of affairs for medical statistics education. Because the demand for skills in this area is increasing, the need to reform medical statistics education in China has become urgent. "], "author_display": ["Yazhou Wu", "Liang Zhou", "Gaoming Li", "Dali Yi", "Xiaojiao Wu", "Xiaoyu Liu", "Yanqi Zhang", "Ling Liu", "Dong Yi"], "article_type": "Research Article", "score": 0.55984926, "title_display": "Cognition of and Demand for Education and Teaching in Medical Statistics in China: A Systematic Review and Meta-Analysis", "publication_date": "2015-06-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0128721"}, {"journal": "PLoS Medicine", "abstract": ["\n        Carol Bennett and colleagues review the evidence and find that there is limited guidance and no consensus on the optimal reporting of survey research.\n      Background: Research needs to be reported transparently so readers can critically assess the strengths and weaknesses of the design, conduct, and analysis of studies. Reporting guidelines have been developed to inform reporting for a variety of study designs. The objective of this study was to identify whether there is a need to develop a reporting guideline for survey research. Methods and Findings: We conducted a three-part project: (1) a systematic review of the literature (including \u201cInstructions to Authors\u201d from the top five journals of 33 medical specialties and top 15 general and internal medicine journals) to identify guidance for reporting survey research; (2) a systematic review of evidence on the quality of reporting of surveys; and (3) a review of reporting of key quality criteria for survey research in 117 recently published reports of self-administered surveys. Fewer than 7% of medical journals (n\u200a=\u200a165) provided guidance to authors on survey research despite a majority having published survey-based studies in recent years. We identified four published checklists for conducting or reporting survey research, none of which were validated. We identified eight previous reviews of survey reporting quality, which focused on issues of non-response and accessibility of questionnaires. Our own review of 117 published survey studies revealed that many items were poorly reported: few studies provided the survey or core questions (35%), reported the validity or reliability of the instrument (19%), defined the response rate (25%), discussed the representativeness of the sample (11%), or identified how missing data were handled (11%). Conclusions: There is limited guidance and no consensus regarding the optimal reporting of survey research. The majority of key reporting criteria are poorly reported in peer-reviewed survey research articles. Our findings highlight the need for clear and consistent reporting guidelines specific to survey research. : \n            Please see later in the article for the Editors' Summary\n           Background: Surveys, or questionnaires, are an essential component of many types of research, including health, and usually gather information by asking a sample of people questions on a specific topic and then generalizing the results to a larger population. Surveys are especially important when addressing topics that are difficult to assess using other approaches and usually rely on self reporting, for example self-reported behaviors, such as eating habits, satisfaction, beliefs, knowledge, attitudes, opinions. However, the methods used in conducting survey research can significantly affect the reliability, validity, and generalizability of study results, and without clear reporting of the methods used in surveys, it is difficult or impossible to assess these characteristics and therefore to have confidence in the findings. Why Was This Study Done?: This uncertainty in other forms of research has given rise to Reporting Guidelines\u2014evidence-based, validated tools that aim to improve the reporting quality of health research. The STROBE (STrengthening the Reporting of OBservational studies in Epidemiology) Statement includes cross-sectional studies, which often involve surveys. But not all surveys are epidemiological, and STROBE does not include methods' and results' reporting characteristics that are unique to surveys. Therefore, the researchers conducted this study to help determine whether there is a need for a reporting guideline for health survey research. What Did the Researchers Do and Find?: The researchers identified any previous relevant guidance for survey research, and any evidence on the quality of reporting of survey research, by: reviewing current guidance for reporting survey research in the \u201cInstructions to Authors\u201d of leading medical journals and in published literature; conducting a systematic review of evidence on the quality of reporting of surveys; identifying key quality criteria for the conduct of survey research; and finally, reviewing how these criteria are currently reported by conducting a review of recently published reports of self-administered surveys. What Do These Findings Mean?: Overall, these results show that guidance is limited and consensus lacking about the optimal reporting of survey research, and they highlight the need for a well-developed reporting guideline specifically for survey research\u2014possibly an extension of the guideline for observational studies in epidemiology (STROBE)\u2014that will provide the structure to ensure more complete reporting and allow clearer review and interpretation of the results from surveys. Additional Information: Please access these web sites via the online version of this summary at http://dx.doi.org/10.1371/journal.pmed.1001069. "], "author_display": ["Carol Bennett", "Sara Khangura", "Jamie C. Brehaut", "Ian D. Graham", "David Moher", "Beth K. Potter", "Jeremy M. Grimshaw"], "article_type": "Research Article", "score": 0.5550274, "title_display": "Reporting Guidelines for Survey Research: An Analysis of Published Guidance and Reporting Practices", "publication_date": "2011-08-02T00:00:00Z", "eissn": "1549-1676", "id": "10.1371/journal.pmed.1001069"}, {"journal": "PLOS ONE", "abstract": ["Motivation: When we were asked for help with high-level microarray data analysis (on Affymetrix HGU-133A microarray), we faced the problem of selecting an appropriate method. We wanted to select a method that would yield \"the best result\" (detected as many \"really\" differentially expressed genes (DEGs) as possible, without false positives and false negatives). However, life scientists could not help us \u2013 they use their \"favorite\" method without special argumentation. We also did not find any norm or recommendation. Therefore, we decided to examine it for our own purpose. We considered whether the results obtained using different methods of high-level microarray data analyses \u2013 Significant Analysis of Microarrays, Rank Products, Bland-Altman, Mann-Whitney test, T test and the Linear Models for Microarray Data \u2013 would be in agreement. Initially, we conducted a comparative analysis of the results on eight real data sets from microarray experiments (from the Array Express database). The results were surprising. On the same array set, the set of DEGs by different methods were significantly different. We also applied the methods to artificial data sets and determined some measures that allow the preparation of the overall scoring of tested methods for future recommendation. Results: We found a very low level concordance of results from tested methods on real array sets. The number of common DEGs (detected by all six methods on fixed array sets, checked on eight array sets) ranged from 6 to 433 (22,283 total array readings). Results on artificial data sets were better than those on the real data. However, they were not fully satisfying. We scored tested methods on accuracy, recall, precision, f-measure and Matthews correlation coefficient. Based on the overall scoring, the best methods were SAM and LIMMA. We also found TT to be acceptable. The worst scoring was MW. Based on our study, we recommend: 1. Carefully taking into account the need for study when choosing a method, 2. Making high-level analysis with more than one method and then only taking the genes that are common to all methods (which seems to be reasonable) and 3. Being very careful (while summarizing facts) about sets of differentially expressed genes: different methods discover different sets of DEGs. "], "author_display": ["Kornel Chrominski", "Magdalena Tkacz"], "article_type": "Research Article", "score": 0.5549057, "title_display": "Comparison of High-Level Microarray Analysis Methods in the Context of Result Consistency", "publication_date": "2015-06-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0128845"}, {"journal": "PLoS ONE", "abstract": ["Background: Priority setting is increasingly recognised as essential for directing finite resources to support research that maximizes public health benefits and drives health equity. Priority setting processes have been undertaken in a number of low- and middle-income country (LMIC) settings, using a variety of methods. We undertook a critical review of reports of these processes. Methods and Findings: We searched electronic databases and online for peer reviewed and non-peer reviewed literature. We found 91 initiatives that met inclusion criteria. The majority took place at the global level (46%). For regional or national initiatives, most focused on Sub Saharan Africa (49%), followed by East Asia and Pacific (20%) and Latin America and the Caribbean (18%). A quarter of initiatives aimed to cover all areas of health research, with a further 20% covering communicable diseases. The most frequently used process was a conference or workshop to determine priorities (24%), followed by the Child Health and Nutrition Initiative (CHNRI) method (18%). The majority were initiated by an international organization or collaboration (46%). Researchers and government were the most frequently represented stakeholders. There was limited evidence of any implementation or follow-up strategies. Challenges in priority setting included engagement with stakeholders, data availability, and capacity constraints. Conclusions: Health research priority setting (HRPS) has been undertaken in a variety of LMIC settings. While not consistently used, the application of established methods provides a means of identifying health research priorities in a repeatable and transparent manner. In the absence of published information on implementation or evaluation, it is not possible to assess what the impact and effectiveness of health research priority setting may have been. "], "author_display": ["Skye McGregor", "Klara J. Henderson", "John M. Kaldor"], "article_type": "Research Article", "score": 0.553247, "title_display": "How Are Health Research Priorities Set in Low and Middle Income Countries? A Systematic Review of Published Reports", "publication_date": "2014-10-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0108787"}, {"journal": "PLOS ONE", "abstract": ["Objective: In this study, Argentine health researchers were surveyed regarding their perceptions of facilitators and barriers to evidence-based policymaking in Argentina, as well as their publication activities, and research environment satisfaction. Methods: A self-administered online survey was sent to health researchers in Argentina. The survey questions were based on a preceding qualitative study of Argentine health researchers, as well as the scientific literature. Results: Of the 647 researchers that were reached, 226 accessed the survey, for a response rate of 34.9%. Over 80% of researchers surveyed had never been involved in or contributed to decision-making, while over 90% of researchers indicated they would like to be involved in the decision-making process. Decision-maker self-interest was perceived to be the driving factor in the development of health and healthcare policies. Research conducted by a research leader was seen to be the most influential factor in influencing health policy, followed by policy relevance of the research. With respect to their occupational environment, researchers rated highest and most favourably the opportunities available to present, discuss and publish research results and their ability to further their education and training. Argentine researchers surveyed demonstrated a strong interest and willingness to contribute their work and expertise to inform Argentine health policy development. Conclusion: Despite Argentina\u2019s long scientific tradition, there are relatively few institutionalized linkages between health research results and health policymaking. Based on the results of this study, the disconnect between political decision-making and the health research system, coupled with fewer opportunities for formalized or informal researcher/decision-maker interaction, contribute to the challenges in evidence informing health policymaking in Argentina. Improving personal contact and the building of relationships between researchers and policymakers in Argentina will require taking into account researcher perceptions of policymakers, as highlighted in this study. "], "author_display": ["Adrijana Corluka", "Adnan A. Hyder", "Elsa Segura", "Peter Winch", "Robert K. D. McLean"], "article_type": "Research Article", "score": 0.55078423, "title_display": "Survey of Argentine Health Researchers on the Use of Evidence in Policymaking", "publication_date": "2015-04-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0125711"}, {"journal": "PLoS ONE", "abstract": ["\nWe investigate the extent to which advances in the health and life sciences (HLS) are dependent on research in the engineering and physical sciences (EPS), particularly physics, chemistry, mathematics, and engineering. The analysis combines two different bibliometric approaches. The first approach to analyze the \u2018EPS-HLS interface\u2019 is based on term map visualizations of HLS research fields. We consider 16 clinical fields and five life science fields. On the basis of expert judgment, EPS research in these fields is studied by identifying EPS-related terms in the term maps. In the second approach, a large-scale citation-based network analysis is applied to publications from all fields of science. We work with about 22,000 clusters of publications, each representing a topic in the scientific literature. Citation relations are used to identify topics at the EPS-HLS interface. The two approaches complement each other. The advantages of working with textual data compensate for the limitations of working with citation relations and the other way around. An important advantage of working with textual data is in the in-depth qualitative insights it provides. Working with citation relations, on the other hand, yields many relevant quantitative statistics. We find that EPS research contributes to HLS developments mainly in the following five ways: new materials and their properties; chemical methods for analysis and molecular synthesis; imaging of parts of the body as well as of biomaterial surfaces; medical engineering mainly related to imaging, radiation therapy, signal processing technology, and other medical instrumentation; mathematical and statistical methods for data analysis. In our analysis, about 10% of all EPS and HLS publications are classified as being at the EPS-HLS interface. This percentage has remained more or less constant during the past decade.\n"], "author_display": ["Ludo Waltman", "Anthony F. J. van Raan", "Sue Smart"], "article_type": "Research Article", "score": 0.5465155, "title_display": "Exploring the Relationship between the Engineering and Physical Sciences and the Health and Life Sciences by Advanced Bibliometric Methods", "publication_date": "2014-10-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0111530"}, {"journal": "PLoS ONE", "abstract": ["Background: A recent report from the British Nuffield Council on Bioethics associated \u2018emerging biotechnologies\u2019 with a threefold challenge: 1) uncertainty about outcomes, 2) diverse public views on the values and implications attached to biotechnologies and 3) the possibility of creating radical changes regarding societal relations and practices. To address these challenges, leading international institutions stress the need for public involvement activities (PIAs). The objective of this study was to assess the state of PIA reports in the field of biomedical research. Methods: PIA reports were identified via a systematic literature search. Thematic text analysis was employed for data extraction. Results: After filtering, 35 public consultation and 11 public participation studies were included in this review. Analysis and synthesis of all 46 PIA studies resulted in 6 distinguishable PIA objectives and 37 corresponding PIA methods. Reports of outcome translation and PIA evaluation were found in 9 and 10 studies respectively (20% and 22%). The paper presents qualitative details. Discussion: The state of PIAs on biomedical research and innovation is characterized by a broad range of methods and awkward variation in the wording of objectives. Better comparability of PIAs might improve the translation of PIA findings into further policy development. PIA-specific reporting guidelines would help in this regard. The modest level of translation efforts is another pointer to the \u201cdeliberation to policy gap\u201d. The results of this review could inform the design of new PIAs and future efforts to improve PIA comparability and outcome translation. "], "author_display": ["Jonas Lander", "Tobias Hainz", "Irene Hirschberg", "Daniel Strech"], "article_type": "Research Article", "score": 0.54134303, "title_display": "Current Practice of Public Involvement Activities in Biomedical Research and Innovation: A Systematic Qualitative Review", "publication_date": "2014-12-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0113274"}, {"journal": "PLOS ONE", "abstract": ["Background: microRNAs (miRNAs) are short regulatory RNAs that are involved in several diseases, including cancers. Identifying miRNA functions is very important in understanding disease mechanisms and determining the efficacy of drugs. An increasing number of computational methods have been developed to explore miRNA functions by inferring the miRNA-mRNA regulatory relationships from data. Each of the methods is developed based on some assumptions and constraints, for instance, assuming linear relationships between variables. For such reasons, computational methods are often subject to the problem of inconsistent performance across different datasets. On the other hand, ensemble methods integrate the results from individual methods and have been proved to outperform each of their individual component methods in theory. Results: In this paper, we investigate the performance of some ensemble methods over the commonly used miRNA target prediction methods. We apply eight different popular miRNA target prediction methods to three cancer datasets, and compare their performance with the ensemble methods which integrate the results from each combination of the individual methods. The validation results using experimentally confirmed databases show that the results of the ensemble methods complement those obtained by the individual methods and the ensemble methods perform better than the individual methods across different datasets. The ensemble method, Pearson+IDA+Lasso, which combines methods in different approaches, including a correlation method, a causal inference method, and a regression method, is the best performed ensemble method in this study. Further analysis of the results of this ensemble method shows that the ensemble method can obtain more targets which could not be found by any of the single methods, and the discovered targets are more statistically significant and functionally enriched. The source codes, datasets, miRNA target predictions by all methods, and the ground truth for validation are available in the Supplementary materials. "], "author_display": ["Thuc Duy Le", "Junpeng Zhang", "Lin Liu", "Jiuyong Li"], "article_type": "Research Article", "score": 0.5403942, "title_display": "Ensemble Methods for MiRNA Target Prediction from Expression Data", "publication_date": "2015-06-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0131627"}, {"journal": "PLoS ONE", "abstract": ["\nInteraction among the scientific disciplines is of vital importance in modern science. Focusing on the case of Slovenia, we study the dynamics of interdisciplinary sciences from  to . Our approach relies on quantifying the interdisciplinarity of research communities detected in the coauthorship network of Slovenian scientists over time. Examining the evolution of the community structure, we find that the frequency of interdisciplinary research is only proportional with the overall growth of the network. Although marginal improvements in favor of interdisciplinarity are inferable during the 70s and 80s, the overall trends during the past 20 years are constant and indicative of stalemate. We conclude that the flow of knowledge between different fields of research in Slovenia is in need of further stimulation.\n"], "author_display": ["Borut Lu\u017ear", "Zoran Levnaji\u0107", "Janez Povh", "Matja\u017e Perc"], "article_type": "Research Article", "score": 0.5394088, "title_display": "Community Structure and the Evolution of Interdisciplinarity in Slovenia's Scientific Collaboration Network", "publication_date": "2014-04-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0094429"}, {"journal": "PLoS Medicine", "abstract": ["\n        Using documents obtained through litigation, S. Swaroop Vedula and colleagues compared internal company documents regarding industry-sponsored trials of off-label uses of gabapentin with the published trial reports and find discrepancies in reporting of analyses.\n      Background: Details about the type of analysis (e.g., intent to treat [ITT]) and definitions (i.e., criteria for including participants in the analysis) are necessary for interpreting a clinical trial's findings. Our objective was to compare the description of types of analyses and criteria for including participants in the publication (i.e., what was reported) with descriptions in the corresponding internal company documents (i.e., what was planned and what was done). Trials were for off-label uses of gabapentin sponsored by Pfizer and Parke-Davis, and documents were obtained through litigation. Methods and Findings: For each trial, we compared internal company documents (protocols, statistical analysis plans, and research reports, all unpublished), with publications. One author extracted data and another verified, with a third person verifying discordant items and a sample of the rest. Extracted data included the number of participants randomized and analyzed for efficacy, and types of analyses for efficacy and safety and their definitions (i.e., criteria for including participants in each type of analysis). We identified 21 trials, 11 of which were published randomized controlled trials, and that provided the documents needed for planned comparisons. For three trials, there was disagreement on the number of randomized participants between the research report and publication. Seven types of efficacy analyses were described in the protocols, statistical analysis plans, and publications, including ITT and six others. The protocol or publication described ITT using six different definitions, resulting in frequent disagreements between the two documents (i.e., different numbers of participants were included in the analyses). Conclusions: Descriptions of analyses conducted did not agree between internal company documents and what was publicly reported. Internal company documents provide extensive documentation of methods planned and used, and trial findings, and should be publicly accessible. Reporting standards for randomized controlled trials should recommend transparent descriptions and definitions of analyses performed and which study participants are excluded. Background: To be credible, published research must present an unbiased, transparent, and accurate description of the study methods and findings so that readers can assess all relevant information to make informed decisions about the impact of any conclusions. Therefore, research publications should conform to universally adopted guidelines and checklists. Studies to establish whether a treatment is effective, termed randomized controlled trials (RCTs), are checked against a comprehensive set of guidelines: The robustness of trial protocols are measured through the Standard Protocol Items for Randomized Trials (SPIRIT), and the Consolidated Standards of Reporting Trials (CONSORT) statement (which was constructed and agreed by a meeting of journal editors in 1996, and has been updated over the years) includes a 25-point checklist that covers all of the key points in reporting RCTs. Why Was This Study Done?: Although the CONSORT statement has helped improve transparency in the reporting of the methods and findings from RCTs, the statement does not define how certain types of analyses should be conducted and which patients should be included in the analyses, for example, in an intention-to-treat analysis (in which all participants are included in the data analysis of the group to which they were assigned, whether or not they completed the intervention given to the group). So in this study, the researchers used internal company documents released in the course of litigation against the pharmaceutical company Pfizer regarding the drug gabapentin, to compare between the internal and published reports the reporting of the numbers of participants, the description of the types of analyses, and the definitions of each type of analysis. The reports involved studies of gabapentin used for medical reasons not approved for marketing by the US Food and Drug Administration, known as \u201coff-label\u201d uses. What Did the Researchers Do and Find?: The researchers identified trials sponsored by Pfizer relating to four off-label uses of gabapentin and examined the internal company protocols, statistical analysis plans, research reports, and the main publications related to each trial. The researchers then compared the numbers of participants randomized and analyzed for the main (primary) outcome and the type of analysis for efficacy and safety in both the internal research report and the trial publication. The researchers identified 21 trials, 11 of which were published RCTs that had the associated documents necessary for comparison. What Do These Findings Mean?: These findings from a sample of industry-sponsored trials on the off-label use of gabapentin suggest that when compared to the internal research reports, the trial publications did not always accurately reflect what was actually done in the trial. Therefore, the trial publication could not be considered to be an accurate and transparent record of the numbers of participants randomized and analyzed for efficacy. These findings support the need for further revisions of the CONSORT statement, such as including explicit statements about the criteria used to define each type of analysis and the numbers of participants excluded from each type of analysis. Further guidance is also needed to ensure consistent terminology for types of analysis. Of course, these revisions will improve reporting only if authors and journals adhere to them. These findings also highlight the need for all individual patient data to be made accessible to readers of the published article. Additional Information: Please access these Web sites via the online version of this summary at http://dx.doi.org/10.1371/journal.pmed.1001378. "], "author_display": ["S. Swaroop Vedula", "Tianjing Li", "Kay Dickersin"], "article_type": "Research Article", "score": 0.53933275, "title_display": "Differences in Reporting of Analyses in Internal Company Documents Versus Published Trial Reports: Comparisons in Industry-Sponsored Trials in Off-Label Uses of Gabapentin", "publication_date": "2013-01-29T00:00:00Z", "eissn": "1549-1676", "id": "10.1371/journal.pmed.1001378"}, {"journal": "PLoS ONE", "abstract": ["Background: Qualitative research appears to be gaining acceptability in medical journals. Yet, little is actually known about the proportion of qualitative research and factors affecting its publication. This study describes the proportion of qualitative research over a 10 year period and correlates associated with its publication. Design: A quantitative longitudinal examination of the proportion of original qualitative research in 67 journals of general medicine during a 10 year period (1998\u20132007). The proportion of qualitative research was determined by dividing original qualitative studies published (numerator) by all original research articles published (denominator). We used a generalized estimating equations approach to assess the longitudinal association between the proportion of qualitative studies and independent variables (i.e. journals' country of publication and impact factor; editorial/methodological papers discussing qualitative research; and specific journal guidelines pertaining to qualitative research). Findings: A 2.9% absolute increase and 3.4-fold relative increase in qualitative research publications occurred over a 10 year period (1.2% in 1998 vs. 4.1% in 2007). The proportion of original qualitative research was independently and significantly associated with the publication of editorial/methodological papers in the journal (b\u200a=\u200a3.688, P\u200a=\u200a0.012); and with qualitative research specifically mentioned in guidelines for authors (b\u200a=\u200a6.847, P<0.001). Additionally, a higher proportion of qualitative research was associated only with journals published in the UK in comparison to other countries, yet with borderline statistical significance (b\u200a=\u200a1.776, P\u200a=\u200a0.075). The journals' impact factor was not associated with the publication of qualitative research. Conclusions: Despite an increase in the proportion of qualitative research in medical journals over a 10 year period, the proportion remains low. Journals' policies pertaining to qualitative research, as expressed by the appearance of specific guidelines and editorials/methodological papers on the subject, are independently associated with the publication of original qualitative research; irrespective of the journals' impact factor. "], "author_display": ["Kerem Shuval", "Karen Harker", "Bahman Roudsari", "Nora E. Groce", "Britain Mills", "Zoveen Siddiqi", "Aviv Shachak"], "article_type": "Research Article", "score": 0.53754926, "title_display": "Is Qualitative Research Second Class Science? A Quantitative Longitudinal Examination of Qualitative Research in Medical Journals", "publication_date": "2011-02-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0016937"}, {"journal": "PLOS ONE", "abstract": ["\nThe lack of reproducibility with animal phenotyping experiments is a growing concern among the biomedical community. One contributing factor is the inadequate description of statistical analysis methods that prevents researchers from replicating results even when the original data are provided. Here we present PhenStat \u2013 a freely available R package that provides a variety of statistical methods for the identification of phenotypic associations. The methods have been developed for high throughput phenotyping pipelines implemented across various experimental designs with an emphasis on managing temporal variation. PhenStat is targeted to two user groups: small-scale users who wish to interact and test data from large resources and large-scale users who require an automated statistical analysis pipeline. The software provides guidance to the user for selecting appropriate analysis methods based on the dataset and is designed to allow for additions and modifications as needed. The package was tested on mouse and rat data and is used by the International Mouse Phenotyping Consortium (IMPC). By providing raw data and the version of PhenStat used, resources like the IMPC give users the ability to replicate and explore results within their own computing environment.\n"], "author_display": ["Natalja Kurbatova", "Jeremy C. Mason", "Hugh Morgan", "Terrence F. Meehan", "Natasha A. Karp"], "article_type": "Research Article", "score": 0.53738546, "title_display": "PhenStat: A Tool Kit for Standardized Analysis of High Throughput Phenotypic Data", "publication_date": "2015-07-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0131274"}, {"journal": "PLoS Genetics", "abstract": ["In case-control studies, genetic associations for complex diseases may be probed either with single-locus tests or with haplotype-based tests. Although there are different views on the relative merits and preferences of the two test strategies, haplotype-based analyses are generally believed to be more powerful to detect genes with modest effects. However, a main drawback of haplotype-based association tests is the large number of distinct haplotypes, which increases the degrees of freedom for corresponding test statistics and thus reduces the statistical power. To decrease the degrees of freedom and enhance the efficiency and power of haplotype analysis, we propose an improved haplotype clustering method that is based on the haplotype cladistic analysis developed by Durrant et al. In our method, we attempt to combine the strengths of single-locus analysis and haplotype-based analysis into one single test framework. Novel in our method is that we develop a more informative haplotype similarity measurement by using p-values obtained from single-locus association tests to construct a measure of weight, which to some extent incorporates the information of disease outcomes. The weights are then used in computation of similarity measures to construct distance metrics between haplotype pairs in haplotype cladistic analysis. To assess our proposed new method, we performed simulation analyses to compare the relative performances of (1) conventional haplotype-based analysis using original haplotype, (2) single-locus allele-based analysis, (3) original haplotype cladistic analysis (CLADHC) by Durrant et al., and (4) our weighted haplotype cladistic analysis method, under different scenarios. Our weighted cladistic analysis method shows an increased statistical power and robustness, compared with the methods of haplotype cladistic analysis, single-locus test, and the traditional haplotype-based analyses. The real data analyses also show that our proposed method has practical significance in the human genetics field.: Methods of haplotype-based analysis and single-locus analysis are widely used in genetic association studies. There is no consensus as to the best strategy for the performance of the two methods. Although haplotype-based analysis is a powerful tool, the large number of distinct haplotypes may reduce its efficiency. Haplotype clustering analysis is a promising way of decreasing haplotype dimensionality. A potential limitation of many existing clustering methods is that they do not allow the clustering to adapt to the position of the underlying trait locus. In this study, we proposed a weighted haplotype cladistic analysis method by incorporating a single-locus test into haplotype clustering. Under this framework, relationships between single loci and the disease outcomes can be considered when creating the hierarchical tree of haplotypes. The extensive simulations show that our method is robust against varied simulation conditions and is more powerful than either the original unweighted cladistic analysis method or single-locus analysis methods in case-control studies. Our hybrid method combining haplotype-based and single-locus analyses can be readily extended to whole genome association studies. "], "author_display": ["Jianfeng Liu", "Chris Papasian", "Hong-Wen Deng"], "article_type": "Research Article", "score": 0.5334537, "title_display": "Incorporating Single-Locus Tests into Haplotype Cladistic Analysis in Case-Control Studies", "publication_date": "2007-03-23T00:00:00Z", "eissn": "1553-7404", "id": "10.1371/journal.pgen.0030046"}, {"journal": "PLOS ONE", "abstract": ["Background: Proliferative activity (Ki-67 Labelling Index) in breast cancer increasingly serves as an additional tool in the decision for or against adjuvant chemotherapy in midrange hormone receptor positive breast cancer. Ki-67 Index has been previously shown to suffer from high inter-observer variability especially in midrange (G2) breast carcinomas. In this study we conducted a systematic approach using different Ki-67 assessments on large tissue sections in order to identify the method with the highest reliability and the lowest variability. Materials and Methods: Five breast pathologists retrospectively analyzed proliferative activity of 50 G2 invasive breast carcinomas using large tissue sections by assessing Ki-67 immunohistochemistry. Ki-67-assessments were done on light microscopy and on digital images following these methods: 1) assessing five regions, 2) assessing only darkly stained nuclei and 3) considering only condensed proliferative areas (\u2018hotspots\u2019). An individual review (the first described assessment from 2008) was also performed. The assessments on light microscopy were done by estimating. All measurements were performed three times. Inter-observer and intra-observer reliabilities were calculated using the approach proposed by Eliasziw et al. Clinical cutoffs (14% and 20%) were tested using Fleiss\u2019 Kappa. Results: There was a good intra-observer reliability in 5 of 7 methods (ICC: 0.76\u20130.89). The two highest inter-observer reliability was fair to moderate (ICC: 0.71 and 0.74) in 2 methods (region-analysis and individual-review) on light microscopy. Fleiss\u2019-kappa-values (14% cut-off) were the highest (moderate) using the original recommendation on light-microscope (Kappa 0.58). Fleiss\u2019 kappa values (20% cut-off) were the highest (Kappa 0.48 each) in analyzing hotspots on light-microscopy and digital-analysis. No methodologies using digital-analysis were superior to the methods on light microscope. Conclusion: Our results show that all methods on light-microscopy for Ki-67 assessment in large tissue sections resulted in a good intra-observer reliability. Region analysis and individual review (the original recommendation) on light-microscopy yielded the highest inter-observer reliability. These results show slight improvement to previously published data on poor-reproducibility and thus might be a practical-pragmatic way for routine assessment of Ki-67 Index in G2 breast carcinomas. "], "author_display": ["Zsuzsanna Varga", "Estelle Cassoly", "Qiyu Li", "Christian Oehlschlegel", "Coya Tapia", "Hans Anton Lehr", "Dirk Klingbiel", "Beat Th\u00fcrlimann", "Thomas Ruhstaller"], "article_type": "Research Article", "score": 0.53285605, "title_display": "Standardization for Ki-67 Assessment in Moderately Differentiated Breast Cancer. A Retrospective Analysis of the SAKK 28/12 Study", "publication_date": "2015-04-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0123435"}, {"journal": "PLoS ONE", "abstract": ["Background: Heterogeneity has a key role in meta-analysis methods and can greatly affect conclusions. However, true levels of heterogeneity are unknown and often researchers assume homogeneity. We aim to: a) investigate the prevalence of unobserved heterogeneity and the validity of the assumption of homogeneity; b) assess the performance of various meta-analysis methods; c) apply the findings to published meta-analyses. Methods and Findings: We accessed 57,397 meta-analyses, available in the Cochrane Library in August 2012. Using simulated data we assessed the performance of various meta-analysis methods in different scenarios. The prevalence of a zero heterogeneity estimate in the simulated scenarios was compared with that in the Cochrane data, to estimate the degree of unobserved heterogeneity in the latter. We re-analysed all meta-analyses using all methods and assessed the sensitivity of the statistical conclusions. Levels of unobserved heterogeneity in the Cochrane data appeared to be high, especially for small meta-analyses. A bootstrapped version of the DerSimonian-Laird approach performed best in both detecting heterogeneity and in returning more accurate overall effect estimates. Re-analysing all meta-analyses with this new method we found that in cases where heterogeneity had originally been detected but ignored, 17\u201320% of the statistical conclusions changed. Rates were much lower where the original analysis did not detect heterogeneity or took it into account, between 1% and 3%. Conclusions: When evidence for heterogeneity is lacking, standard practice is to assume homogeneity and apply a simpler fixed-effect meta-analysis. We find that assuming homogeneity often results in a misleading analysis, since heterogeneity is very likely present but undetected. Our new method represents a small improvement but the problem largely remains, especially for very small meta-analyses. One solution is to test the sensitivity of the meta-analysis conclusions to assumed moderate and large degrees of heterogeneity. Equally, whenever heterogeneity is detected, it should not be ignored. "], "author_display": ["Evangelos Kontopantelis", "David A. Springate", "David Reeves"], "article_type": "Research Article", "score": 0.53183585, "title_display": "A Re-Analysis of the Cochrane Library Data: The Dangers of Unobserved Heterogeneity in Meta-Analyses", "publication_date": "2013-07-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0069930"}, {"abstract": ["Background: Systematic reviews are increasingly informing policies in tuberculosis (TB) care and control. They may also be a source of questions for future research. As part of the process of developing the International Roadmap for TB Research, we did a systematic review of published systematic reviews on TB, to identify research priorities that are most frequently suggested in reviews. Methodology/Principal Findings: We searched EMBASE, MEDLINE, Web of Science, and the Cochrane Library for systematic reviews and meta-analyses on any aspect of TB published between 2005 and 2010. One reviewer extracted data and a second reviewer independently extracted data from a random subset of included studies. In total, 137 systematic reviews, with 141 research questions, were included in this review. We used the UK Health Research Classification System (HRCS) to help us classify the research questions and priorities. The three most common research topics were in the area of detection, screening and diagnosis of TB (32.6%), development and evaluation of treatments and therapeutic interventions (23.4%), and TB aetiology and risk factors (19.9%). The research priorities determined were mainly focused on the discovery and evaluation of bacteriological TB tests and drug-resistant TB tests and immunological tests. Other important topics of future research were genetic susceptibility linked to TB and disease determinants attributed to HIV/TB. Evaluation of drug treatments for TB, drug-resistant TB and HIV/TB were also frequently proposed research topics. Conclusions: Systematic reviews are a good source of key research priorities. Findings from our survey have informed the development of the International Roadmap for TB Research by the TB Research Movement. "], "author_display": ["Ioana Nicolau", "Daphne Ling", "Lulu Tian", "Christian Lienhardt", "Madhukar Pai"], "article_type": "Research Article", "score": 0.5318341, "title_display": "Research Questions and Priorities for Tuberculosis: A Survey of Published Systematic Reviews and Meta-Analyses", "publication_date": "2012-07-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0042479"}, {"journal": "PLoS ONE", "abstract": ["\nThere has been a substantial increase in research activity on autism during the past decade. Research into effective ways of responding to the immediate needs of autistic people is, however, less advanced, as are efforts at translating basic science research into service provision. Involving community members in research is one potential way of reducing this gap. This study therefore investigated the views of community involvement in autism research both from the perspectives of autism researchers and of community members, including autistic adults, family members and practitioners. Results from a large-scale questionnaire study (n\u200a=\u200a1,516) showed that researchers perceive themselves to be engaged with the autism community but that community members, most notably autistic people and their families, did not share this view. Focus groups/interviews with 72 participants further identified the potential benefits and remaining challenges to involvement in research, especially regarding the distinct perspectives of different stakeholders. Researchers were skeptical about the possibilities of dramatically increasing community engagement, while community members themselves spoke about the challenges to fully understanding and influencing the research process. We suggest that the lack of a shared approach to community engagement in UK autism research represents a key roadblock to translational endeavors.\n"], "author_display": ["Elizabeth Pellicano", "Adam Dinsmore", "Tony Charman"], "article_type": "Research Article", "score": 0.52951986, "title_display": "Views on Researcher-Community Engagement in Autism Research in the United Kingdom: A Mixed-Methods Study", "publication_date": "2014-10-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0109946"}, {"journal": "PLoS ONE", "abstract": ["\nEmpirical mode decomposition (EMD) is an adaptive method for nonlinear, non-stationary signal analysis. However, the upper and lower envelopes fitted by cubic spline interpolation (CSI) may often occur overshoots. In this paper, a new envelope fitting method based on the flattest constrained interpolation is proposed. The proposed method effectively integrates the difference between extremes into the cost function, and applies a chaos particle swarm optimization method to optimize the derivatives of the interpolation nodes. The proposed method was tested on three different types of data: ascertain signal, random signals and real electrocardiogram signals. The experimental results show that: (1) The proposed flattest envelope effectively solves the overshoots caused by CSI method and the artificial bends caused by piecewise parabola interpolation (PPI) method. (2) The index of orthogonality of the intrinsic mode functions (IMFs) based on the proposed method is 0.04054, 0.02222\u00b10.01468 and 0.04013\u00b10.03953 for the ascertain signal, random signals and electrocardiogram signals, respectively, which is lower than the CSI method and the PPI method, and means the IMFs are more orthogonal. (3) The index of energy conversation of the IMFs based on the proposed method is 0.96193, 0.93501\u00b10.03290 and 0.93041\u00b10.00429 for the ascertain signal, random signals and electrocardiogram signals, respectively, which is closer to 1 than the other two methods and indicates the total energy deviation amongst the components is smaller. (4) The comparisons of the Hilbert spectrums show that the proposed method overcomes the mode mixing problems very well, and make the instantaneous frequency more physically meaningful."], "author_display": ["Weifang Zhu", "Heming Zhao", "Dehui Xiang", "Xinjian Chen"], "article_type": "Research Article", "score": 0.5277314, "title_display": "A Flattest Constrained Envelope Approach for Empirical Mode Decomposition", "publication_date": "2013-04-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0061739"}, {"journal": "PLOS ONE", "abstract": ["Importance: Despite the rapidly declining number of physician-investigators, there is no consistent structure within medical education so far for involving medical students in research. Objective: To conduct an integrated mixed-methods systematic review and meta-analysis of published studies about medical students' participation in research, and to evaluate the evidence in order to guide policy decision-making regarding this issue. Evidence Review: We followed the PRISMA statement guidelines during the preparation of this review and meta-analysis. We searched various databases as well as the bibliographies of the included studies between March 2012 and September 2013. We identified all relevant quantitative and qualitative studies assessing the effect of medical student participation in research, without restrictions regarding study design or publication date. Prespecified outcome-specific quality criteria were used to judge the admission of each quantitative outcome into the meta-analysis. Initial screening of titles and abstracts resulted in the retrieval of 256 articles for full-text assessment. Eventually, 79 articles were included in our study, including eight qualitative studies. An integrated approach was used to combine quantitative and qualitative studies into a single synthesis. Once all included studies were identified, a data-driven thematic analysis was performed. Findings and Conclusions: Medical student participation in research is associated with improved short- and long- term scientific productivity, more informed career choices and improved knowledge about-, interest in- and attitudes towards research. Financial worries, gender, having a higher degree (MSc or PhD) before matriculation and perceived competitiveness of the residency of choice are among the factors that affect the engagement of medical students in research and/or their scientific productivity. Intercalated BSc degrees, mandatory graduation theses and curricular research components may help in standardizing research education during medical school. "], "author_display": ["Mohamed Amgad", "Marco Man Kin Tsui", "Sarah J. Liptrott", "Emad Shash"], "article_type": "Research Article", "score": 0.52682525, "title_display": "Medical Student Research: An Integrated Mixed-Methods Systematic Review and Meta-Analysis", "publication_date": "2015-06-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0127470"}, {"journal": "PLoS ONE", "abstract": ["Purpose: To compare the performance of newly proposed point-wise linear regression (PLR) with the binomial test (binomial PLR) against mean deviation (MD) trend analysis and permutation analyses of PLR (PoPLR), in detecting global visual field (VF) progression in glaucoma. Methods: 15 VFs (Humphrey Field Analyzer, SITA standard, 24-2) were collected from 96 eyes of 59 open angle glaucoma patients (6.0 \u00b1 1.5 [mean \u00b1 standard deviation] years). Using the total deviation of each point on the 2nd to 16th VFs (VF2-16), linear regression analysis was carried out. The numbers of VF test points with a significant trend at various probability levels (p<0.025, 0.05, 0.075 and 0.1) were investigated with the binomial test (one-side). A VF series was defined as \u201csignificant\u201d if the median p-value from the binomial test was <0.025. Similarly, the progression analysis was carried out using only second to sixth VFs (VF2-6). The performance of each method was evaluated using the \u2018consistency measures\u2019; proportion both significant (PBS):  both VF series (VF2-6 and VF2-16) were \u201csignificant\u201d, proportion both were not significant (PBNS): both were \u201cnot significant\u201d, proportion inconsistently significant (PIS): VF2-16 was \u201cnot significant\u201d but VF2-6 was \u201csignificant\u201d. A similar analysis was carried out using VF2-7 and VF2-15 series, and the performance was compared with MD trend analysis and PoPLR. Results: The PBS of the binomial PLR method (0.14 to 0.86) was significantly higher than MD trend analysis (0.04 to 0.89) and PoPLR (0.09 to 0.93). The PIS of the proposed method (0.0 to 0.17) was significantly lower than the MD approach (0.0 to 0.67) and PoPLR (0.07 to 0.33). The PBNS of the three approaches were not significantly different. Conclusions: The binomial BLR method gives more consistent results than MD trend analysis and PoPLR, hence it will be helpful as a tool to \u2018flag\u2019 possible VF deterioration. "], "author_display": ["Ayako Karakawa", "Hiroshi Murata", "Hiroyo Hirasawa", "Chihiro Mayama", "Ryo Asaoka"], "article_type": "Research Article", "score": 0.5266253, "title_display": "Detection of Progression of Glaucomatous Visual Field Damage Using the Point-Wise Method with the Binomial Test", "publication_date": "2013-10-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0078630"}, {"journal": "PLoS ONE", "abstract": ["Background: Clinician-scientists play an important role in translating between research and clinical practice. Significant concerns about a decline in their numbers have been raised. Potential barriers for career entry and progress are explored in this study. Methods: Case-study research methods were used to identify barriers perceived by clinician-scientists and their research teams in two Canadian laboratories. These perceptions were then compared against statistical analysis of data from Canadian Institutes of Health Research (CIHR) databases on grant and award performance of clinician-scientists and non-clinical PhDs for fiscal years 2000 to 2008. Results: Three main barriers were identified through qualitative analysis: research training, research salaries, and research grants. We then looked for evidence of these barriers in the Canada-wide statistical dataset for our study period. Clinician-scientists had a small but statistically significant higher mean number of degrees (3.3) than non-clinical scientists (3.2), potentially confirming the perception of longer training times. But evidence of the other two barriers was equivocal. For example, while overall growth in salary awards was minimal, awards to clinician-scientists increased by 45% compared to 6.3% for non-clinical PhDs. Similarly, in terms of research funding, awards to clinician-scientists increased by more than 25% compared with 5% for non-clinical PhDs. However, clinician-scientist-led grants funded under CIHR's Clinical thematic area decreased significantly from 61% to 51% (p-value<0.001) suggesting that clinician-scientists may be shifting their attention to other research domains. Conclusion: While clinician-scientists continue to perceive barriers to career entry and progress, quantitative results suggest improvements over the last decade. Clinician-scientists are awarded an increasing proportion of CIHR research grants and salary awards. Given the translational importance of this group, however, it may be prudent to adopt specific policy and funding incentives to ensure the ongoing viability of the career path. "], "author_display": ["Bryn Lander", "Gillian E. Hanley", "Janet Atkinson-Grosjean"], "article_type": "Research Article", "score": 0.52647626, "title_display": "Clinician-Scientists in Canada: Barriers to Career Entry and Progress", "publication_date": "2010-10-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0013168"}, {"journal": "PLoS ONE", "abstract": ["\nIn this research, we examine how restrictive policy influenced performance in human embryonic stem cell research (hESC) between 1998 and 2008. In previous research, researchers argued whether restrictive policy decreased the performance of stem cell research in some nations, especially in the US. Here, we hypothesize that this policy influenced specific subfields of the hESC research. To investigate the selective policy effects, we categorize hESC research publications into three subfields\u2014derivation, differentiation, and medical application research. Our analysis shows that restrictive policy had different effects on different subfields. In general, the US outperformed in overall hESC research throughout these periods. In the derivation of hESC, however, the US almost lost its competence under restrictive policy. Interestingly, the US scientific community showed prominent resilience in hESC research through international collaboration. We concluded that the US resilience and performance stemmed from the wide breadth of research portfolio of US scientists across the hESC subfields, combined with their strategic efforts to collaborate internationally on derivation research.\n"], "author_display": ["Seongwuk Moon", "Seong Beom Cho"], "article_type": "Research Article", "score": 0.5247975, "title_display": "Differential Impact of Science Policy on Subfields of Human Embryonic Stem Cell Research", "publication_date": "2014-04-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0086395"}, {"journal": "PLoS ONE", "abstract": ["\nIn an effort to deal with more complicated evaluation situations, scientists have focused their efforts on dynamic comprehensive evaluation research. How to make full use of the subjective and objective information has become one of the noteworthy content. In this paper, a dynamic comprehensive evaluation method with subjective and objective information is proposed. We use the combination weighting method to determine the index weight. Analysis hierarchy process method is applied to dispose the subjective information, and criteria importance through intercriteria correlation method is used to handle the objective information. And for the time weight determination, we consider both time distance and information size to embody the principle of esteeming the present over the past. And then the linear weighted average model is constructed to make the evaluation process more practicable. Finally, an example is presented to illustrate the effectiveness of this method. Overall, the results suggest that the proposed method is reasonable and effective.\n"], "author_display": ["Dinglin Liu", "Xianglian Zhao"], "article_type": "Research Article", "score": 0.5236893, "title_display": "Method and Application for Dynamic Comprehensive Evaluation with Subjective and Objective Information", "publication_date": "2013-12-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0083323"}, {"journal": "PLoS ONE", "abstract": ["Background: Synthesizing research evidence using systematic and rigorous methods has become a key feature of evidence-based medicine and knowledge translation. Systematic reviews (SRs) may or may not include a meta-analysis depending on the suitability of available data. They are often being criticised as \u2018secondary research\u2019 and denied the status of original research. Scientific journals play an important role in the publication process. How they appraise a given type of research influences the status of that research in the scientific community. We investigated the attitudes of editors of core clinical journals towards SRs and their value for publication. Methods: We identified the 118 journals labelled as \u201ccore clinical journals\u201d by the National Library of Medicine, USA in April 2009. The journals\u2019 editors were surveyed by email in 2009 and asked whether they considered SRs as original research projects; whether they published SRs; and for which section of the journal they would consider a SR manuscript. Results: The editors of 65 journals (55%) responded. Most respondents considered SRs to be original research (71%) and almost all journals (93%) published SRs. Several editors regarded the use of Cochrane methodology or a meta-analysis as quality criteria; for some respondents these criteria were premises for the consideration of SRs as original research. Journals placed SRs in various sections such as \u201cReview\u201d or \u201cFeature article\u201d. Characterization of non-responding journals showed that about two thirds do publish systematic reviews. Discussion: Currently, the editors of most core clinical journals consider SRs original research. Our findings are limited by a non-responder rate of 45%. Individual comments suggest that this is a grey area and attitudes differ widely. A debate about the definition of \u2018original research\u2019 in the context of SRs is warranted. "], "author_display": ["Joerg J. Meerpohl", "Florian Herrle", "Gerd Antes", "Erik von Elm"], "article_type": "Research Article", "score": 0.52195954, "title_display": "Scientific Value of Systematic Reviews: Survey of Editors of Core Clinical Journals", "publication_date": "2012-05-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0035732"}, {"journal": "PLoS ONE", "abstract": ["\nThe number of methods for pre-processing and analysis of gene expression data continues to increase, often making it difficult to select the most appropriate approach. We present a simple procedure for comparative estimation of a variety of methods for microarray data pre-processing and analysis. Our approach is based on the use of real microarray data in which controlled fold changes are introduced into 20% of the data to provide a metric for comparison with the unmodified data. The data modifications can be easily applied to raw data measured with any technological platform and retains all the complex structures and statistical characteristics of the real-world data. The power of the method is illustrated by its application to the quantitative comparison of different methods of normalization and analysis of microarray data. Our results demonstrate that the method of controlled modifications of real experimental data provides a simple tool for assessing the performance of data preprocessing and analysis methods.\n"], "author_display": ["Mikhail G. Dozmorov", "Joel M. Guthridge", "Robert E. Hurst", "Igor M. Dozmorov"], "article_type": "Research Article", "score": 0.5215516, "title_display": "A Comprehensive and Universal Method for Assessing the Performance of Differential Gene Expression Analyses", "publication_date": "2010-09-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0012657"}, {"abstract": ["Background: Despite the availability of effective interventions and public recognition of the severity of the problem, rabies continues to suffer neglect by programme planners in India and other low and middle income countries. We investigate whether this state of \u2018policy impasse\u2019 is due to, at least in part, the research community not catering to the information needs of the policy makers. Methods & Findings: Our objective was to review the research output on rabies from India and examine its alignment with national policy priorities. A systematic literature review of all rabies research articles published from India between 2001 and 2011 was conducted. The distribution of conducted research was compared to the findings of an earlier research prioritization exercise. It was found that a total of 93 research articles were published from India since 2001, out of which 61% consisted of laboratory based studies focussing on rabies virus. Animals were the least studied group, comprising only 8% of the research output. One third of the articles were published in three journals focussing on vaccines and infectious disease epidemiology and the top 4 institutions (2 each from the animal and human health sectors) collectively produced 49% of the national research output. Biomedical research related to development of new interventions dominated the total output as opposed to the identified priority domains of socio-politic-economic research, basic epidemiological research and research to improve existing interventions. Conclusion: The paper highlights the gaps between rabies research and policy needs, and makes the case for developing a strategic research agenda that focusses on rabies control as an expected outcome. Author Summary: Rabies is among the most widely spread zoonoses (diseases that are naturally transmitted between vertebrate animals and humans) in humans in most Asian, African and Latin American countries. Even though researchers have demonstrated effectiveness of strategies to control rabies at the population level, such as post exposure prophylaxis in humans and animal birth control and immunization among dogs, are well known, policy makers in most countries are hesitant to implement these strategies. This paper examines the disconnect that prevents the translation of scientific research outputs into effective policies. We contrasted the type of research papers published on rabies from India in the last eleven years with a previously identified set of priority research options. We found that most published research articles related to biomedical research focussing on development of new interventions. This was in contrast to policy and systems-related research and research to improve the performance of existing interventions that were identified as priority research options for India earlier. The findings of our study highlight the importance of moving beyond a purely researcher-driven agenda and suggest the need to promote research that has a vision of rabies control in the near future. "], "author_display": ["Manish Kakkar", "Vidya Venkataramanan", "Sampath Krishnan", "Ritu Singh Chauhan", "Syed Shahid Abbas", "on behalf of Roadmap to Combat Zoonoses in India (RCZI) initiative "], "article_type": "Research Article", "score": 0.5192323, "title_display": "Moving from Rabies Research to Rabies Control: Lessons from India", "publication_date": "2012-08-07T00:00:00Z", "eissn": "1935-2735", "id": "10.1371/journal.pntd.0001748"}, {"journal": "PLoS ONE", "abstract": ["\nIn DNA methylation, methyl groups are covalently bound to CpG dinucleotides. However, the assumption that methyl groups are not lost during routine DNA extraction has not been empirically tested. To avoid nonbiological associations in DNA methylation studies, it is essential to account for potential batch effect bias in the assessment of this epigenetic mechanism. Our purpose was to determine if the DNA isolation method is an independent source of variability in methylation status. We quantified Global DNA Methylation (GDM) by luminometric methylation assay (LUMA), comparing the results from 3 different DNA isolation methods. In the controlled analysis (n\u200a=\u200a9), GDM differed slightly for the same individual depending on extraction method. In the population analysis (n\u200a=\u200a580) there were significant differences in GDM between the 3 DNA isolation methods (medians, 78.1%, 76.5% and 75.1%; p<0.001). A systematic review of published data from LUMA GDM studies that specify DNA extraction methods is concordant with our findings. DNA isolation method is a source of GDM variability measured with LUMA. To avoid possible bias, the method used should be reported and taken into account in future DNA methylation studies.\n"], "author_display": ["Carolina Soriano-T\u00e1rraga", "Jordi Jim\u00e9nez-Conde", "Eva Giralt-Steinhauer", "\u00c1ngel Ois", "Ana Rodr\u00edguez-Campello", "Elisa Cuadrado-Godia", "Israel Fern\u00e1ndez-Cadenas", "Joan Montaner", "Gavin Lucas", "Roberto Elosua", "Jaume Roquer", "GeneStroke \u201cThe Spanish Stroke Genetics Consortium\u201d "], "article_type": "Research Article", "score": 0.5181054, "title_display": "DNA Isolation Method Is a Source of Global DNA Methylation Variability Measured with LUMA. Experimental Analysis and a Systematic Review", "publication_date": "2013-04-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0060750"}, {"journal": "PLOS ONE", "abstract": ["Objective: The aims of this study were to assess participatory methods for obtaining community views on child health research. Background: Community participation in research is recognised as an important part of the research process; however, there has been inconsistency in its implementation and application in Australia. The Western Australian Telethon Kids Institute Participation Program employs a range of methods for fostering active involvement of community members in its research. These include public discussion forums, called Community Conversations. While participation levels are good, the attendees represent only a sub-section of the Western Australian population. Therefore, we conducted a telephone survey of randomly selected households to evaluate its effectiveness in eliciting views from a broader cross-section of the community about our research agenda and community participation in research, and whether the participants would be representative of the general population. We also conducted two Conversations, comparing the survey as a recruitment tool and normal methods using the Participation Program. Results: While the telephone survey was a good method for eliciting community views about research, there were marked differences in the profile of study participants compared to the general population (e.g. 78% vs 50% females). With a 26% response rate, the telephone survey was also more expensive than a Community Conversation. The cold calling approach proved an unsuccessful recruitment method, with only two out of a possible 816 telephone respondents attending a Conversation. Conclusion: While the results showed that both of the methods produced useful input for our research program, we could not conclude that either method gained input that was representative of the entire community. The Conversations were relatively low-cost and provided more in-depth information about one subject, whereas the telephone survey provided information across a greater range of subjects, and allowed more quantitative analysis. "], "author_display": ["Wavne Rikkers", "Katrina Boterhoven de Haan", "David Lawrence", "Anne McKenzie", "Kirsten Hancock", "Hayley Haines", "Daniel Christensen", "Stephen R. Zubrick"], "article_type": "Research Article", "score": 0.51624775, "title_display": "Two Methods for Engaging with the Community in Setting Priorities for Child Health Research: Who Engages?", "publication_date": "2015-05-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0125969"}, {"journal": "PLOS ONE", "abstract": ["\nAccurate quantification of creatinine (Cre) is important to estimate glomerular filtration rate (GFR). Differences among various methods of Cre quantification were previously noted. This study aims to develop a liquid chromatography tandem mass spectrometry (LC-MS/MS) method for serum Cre and compare this method with clinical routine methods. LC-MS/MS analysis was performed on API 4000 triple quadrupole mass spectrometer coupled with an Agilent 1200 liquid chromatography system. After adding isotope-labeled Cre-d3 as internal standard, serum samples were prepared via a one-step protein precipitation with methanol. The LC-MS/MS method was compared with frequently used enzymatic method and Jaffe method. This developed method, with a total run time of 3 min, had a lower limit of quantification of 4.4 \u03bcmol/L, a total imprecision of 1.15%\u20133.84%, and an average bias of 1.06%. No significant matrix effect, carryover, and interference were observed for the LC-MS/MS method. The reference intervals of serum Cre measured by LC-MS/MS assay were 41\u201379 \u03bcmol/L for adult women, and 46\u2013101 \u03bcmol/L for adult men. Using LC-MS/MS as a reference, the enzymatic method showed an average bias of -2.1% and the Jaffe method showed a substantial average bias of 11.7%. Compared with the LC-MS/MS method, significant negative bias was observed for the enzymatic and Jaffe methods in hemolytic and lipimic samples. We developed a simple, specific, and accurate LC-MS/MS method to analyze serum Cre. Discordance existed among different methods.\n"], "author_display": ["Meixian Ou", "Yunxiao Song", "Shuijun Li", "Gangyi Liu", "Jingying Jia", "Menqi Zhang", "Haichen Zhang", "Chen Yu"], "article_type": "Research Article", "score": 0.51571304, "title_display": "LC-MS/MS Method for Serum Creatinine: Comparison with Enzymatic Method and Jaffe Method", "publication_date": "2015-07-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0133912"}, {"journal": "PLoS ONE", "abstract": ["Objective: Tissue biobanks are an important source for discovery and validation studies aiming for new proteins that are causally related with disease development. There is an increasing demand for accurate and reproducible histological characterization, especially for subsequent analysis and interpretation of data in association studies. We assessed reproducibility of one semiquantative and two quantitative methods for histological tissue characterization. We introduce a new automated method for whole digital slide quantification. Carotid atherosclerotic plaques were used to test reproducibility. Methods: 50 atherosclerotic plaques that were obtained during carotid endarterectomy were analysed. For the semiquantitative analysis, 6 different plaque characteristics were scored in categories by two independent observers, and Cohen's \u03ba was used to test intra- and interobserver reproducibility. The computer-aided method (assessed by two independent observers) and automated method were tested on CD68 (for macrophages) and \u03b1 smooth muscle actin (for smooth muscle cells) stainings. Agreement for these two methods (done on a continuous scale) was assessed by intraclass correlation coefficients (ICCs). Results: For the semiquantitative analysis, \u03ba values ranged from 0.55 to 0.69 for interobserver variability, and were slightly higher for intraobserver reproducibility in both observers. The computer-aided method yielded intra- and interobserver ICCs between 0.6 and 0.9. The new automated method performed most optimal regarding reproducibility, with ICCs ranging from 0.92 to 0.97. Conclusions: The analysis of performance of three methods for histological slide characterization on carotid atherosclerotic plaques showed high precision and agreement in repeated measurements for the automated method for whole digital slide quantification. We suggest that this method can fulfill the need for reproducible histological quantification. "], "author_display": ["Joyce E. P. Vrijenhoek", "Bastiaan G. L. Nelissen", "Evelyn Velema", "Kristy Vons", "Jean-Paul P. M. de Vries", "Marinus J. C. Eijkemans", "Hester M. den. Ruijter", "Gert Jan de Borst", "Frans L. Moll", "Gerard Pasterkamp"], "article_type": "Research Article", "score": 0.5156845, "title_display": "High Reproducibility of Histological Characterization by Whole Virtual Slide Quantification; An Example Using Carotid Plaque Specimens", "publication_date": "2014-12-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0115907"}, {"journal": "PLoS ONE", "abstract": ["\n        Evaluative bibliometrics uses advanced techniques to assess the impact of scholarly work in the context of other scientific work and usually compares the relative scientific contributions of research groups or institutions. Using publications from the National Institute of Allergy and Infectious Diseases (NIAID) HIV/AIDS extramural clinical trials networks, we assessed the presence, performance, and impact of papers published in 2006\u20132008. Through this approach, we sought to expand traditional bibliometric analyses beyond citation counts to include normative comparisons across journals and fields, visualization of co-authorship across the networks, and assess the inclusion of publications in reviews and syntheses. Specifically, we examined the research output of the networks in terms of the a) presence of papers in the scientific journal hierarchy ranked on the basis of journal influence measures, b) performance of publications on traditional bibliometric measures, and c) impact of publications in comparisons with similar publications worldwide, adjusted for journals and fields. We also examined collaboration and interdisciplinarity across the initiative, through network analysis and modeling of co-authorship patterns. Finally, we explored the uptake of network produced publications in research reviews and syntheses. Overall, the results suggest the networks are producing highly recognized work, engaging in extensive interdisciplinary collaborations, and having an impact across several areas of HIV-related science. The strengths and limitations of the approach for evaluation and monitoring research initiatives are discussed.\n      "], "author_display": ["Scott R. Rosas", "Jonathan M. Kagan", "Jeffrey T. Schouten", "Perry A. Slack", "William M. K. Trochim"], "article_type": "Research Article", "score": 0.51531726, "title_display": "Evaluating Research and Impact: A Bibliometric Analysis of Research by the NIH/NIAID HIV/AIDS Clinical Trials Networks", "publication_date": "2011-03-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0017428"}, {"journal": "PLoS ONE", "abstract": ["\nSince hub nodes have been found to play important roles in many networks, highly connected hub genes are expected to play an important role in biology as well. However, the empirical evidence remains ambiguous. An open question is whether (or when) hub gene selection leads to more meaningful gene lists than a standard statistical analysis based on significance testing when analyzing genomic data sets (e.g., gene expression or DNA methylation data). Here we address this question for the special case when multiple genomic data sets are available. This is of great practical importance since for many research questions multiple data sets are publicly available. In this case, the data analyst can decide between a standard statistical approach (e.g., based on meta-analysis) and a co-expression network analysis approach that selects intramodular hubs in consensus modules. We assess the performance of these two types of approaches according to two criteria. The first criterion evaluates the biological insights gained and is relevant in basic research. The second criterion evaluates the validation success (reproducibility) in independent data sets and often applies in clinical diagnostic or prognostic applications. We compare meta-analysis with consensus network analysis based on weighted correlation network analysis (WGCNA) in three comprehensive and unbiased empirical studies: (1) Finding genes predictive of lung cancer survival, (2) finding methylation markers related to age, and (3) finding mouse genes related to total cholesterol. The results demonstrate that intramodular hub gene status with respect to consensus modules is more useful than a meta-analysis p-value when identifying biologically meaningful gene lists (reflecting criterion 1). However, standard meta-analysis methods perform as good as (if not better than) a consensus network approach in terms of validation success (criterion 2). The article also reports a comparison of meta-analysis techniques applied to gene expression data and presents novel R functions for carrying out consensus network analysis, network based screening, and meta analysis.\n"], "author_display": ["Peter Langfelder", "Paul S. Mischel", "Steve Horvath"], "article_type": "Research Article", "score": 0.5149536, "title_display": "When Is Hub Gene Selection Better than Standard Meta-Analysis?", "publication_date": "2013-04-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0061505"}, {"journal": "PLOS ONE", "abstract": ["\nFunding has been viewed in the literature as one of the main determinants of scientific activities. Also, at an individual level, securing funding is one of the most important factors for a researcher, enabling him/her to carry out research projects. However, not everyone is successful in obtaining the necessary funds. The main objective of this work is to measure the effect of several important factors such as past productivity, scientific collaboration or career age of researchers, on the amount of funding that is allocated to them. For this purpose, the paper estimates a temporal non-linear multiple regression model. According to the results, although past productivity of researchers positively affects the funding level, our findings highlight the significant role of networking and collaboration. It was observed that being a member of large scientific teams and getting connected to productive researchers who have also a good control over the collaboration network and the flow of information can increase the chances for securing more money. In fact, our results show that in the quest for the research money it is more important how researchers build their collaboration network than what publications they produce and whether they are cited.\n"], "author_display": ["Ashkan Ebadi", "Andrea Schiffauerova"], "article_type": "Research Article", "score": 0.51457095, "title_display": "How to Receive More Funding for Your Research? Get Connected to the Right People!", "publication_date": "2015-07-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0133061"}, {"journal": "PLOS ONE", "abstract": ["Background: Analyzing high throughput genomics data is a complex and compute intensive task, generally requiring numerous software tools and large reference data sets, tied together in successive stages of data transformation and visualisation. A computational platform enabling best practice genomics analysis ideally meets a number of requirements, including: a wide range of analysis and visualisation tools, closely linked to large user and reference data sets; workflow platform(s) enabling accessible, reproducible, portable analyses, through a flexible set of interfaces; highly available, scalable computational resources; and flexibility and versatility in the use of these resources to meet demands and expertise of a variety of users. Access to an appropriate computational platform can be a significant barrier to researchers, as establishing such a platform requires a large upfront investment in hardware, experience, and expertise. Results: We designed and implemented the Genomics Virtual Laboratory (GVL) as a middleware layer of machine images, cloud management tools, and online services that enable researchers to build arbitrarily sized compute clusters on demand, pre-populated with fully configured bioinformatics tools, reference datasets and workflow and visualisation options. The platform is flexible in that users can conduct analyses through web-based (Galaxy, RStudio, IPython Notebook) or command-line interfaces, and add/remove compute nodes and data resources as required. Best-practice tutorials and protocols provide a path from introductory training to practice. The GVL is available on the OpenStack-based Australian Research Cloud (http://nectar.org.au) and the Amazon Web Services cloud. The principles, implementation and build process are designed to be cloud-agnostic. Conclusions: This paper provides a blueprint for the design and implementation of a cloud-based Genomics Virtual Laboratory. We discuss scope, design considerations and technical and logistical constraints, and explore the value added to the research community through the suite of services and resources provided by our implementation. "], "author_display": ["Enis Afgan", "Clare Sloggett", "Nuwan Goonasekera", "Igor Makunin", "Derek Benson", "Mark Crowe", "Simon Gladman", "Yousef Kowsar", "Michael Pheasant", "Ron Horst", "Andrew Lonie"], "article_type": "Research Article", "score": 0.5131899, "title_display": "Genomics Virtual Laboratory: A Practical Bioinformatics Workbench for the Cloud", "publication_date": "2015-10-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0140829"}, {"journal": "PLOS ONE", "abstract": ["\nNotwithstanding that \u2018public engagement\u2019 is conceptualised differently internationally and in different academic disciplines, higher education institutions largely accept the importance of public engagement with research. However, there is limited evidence on how researchers conceptualise engagement, their views on what constitutes engagement and the communities they would (or would not) like to engage with. This paper presents the results of a survey of researchers in the Open University that sought to gather data to fill these gaps. This research was part of an action research project designed to embed engagement in the routine practices of researchers at all levels. The findings indicate that researchers have a relatively narrow view of public engagement with research and the communities with which they interact. It also identified that very few strategically evaluate their public engagement activities. We conclude by discussing some of the interventions we have introduced with the aim of broadening and deepening future researcher engagement.\n"], "author_display": ["Ann Grand", "Gareth Davies", "Richard Holliman", "Anne Adams"], "article_type": "Research Article", "score": 0.5116714, "title_display": "Mapping Public Engagement with Research in a UK University", "publication_date": "2015-04-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0121874"}, {"journal": "PLoS ONE", "abstract": ["Background: A fundamental aspect of epidemiological studies concerns the estimation of factor-outcome associations to identify risk factors, prognostic factors and potential causal factors. Because reliable estimates for these associations are important, there is a growing interest in methods for combining the results from multiple studies in individual participant data meta-analyses (IPD-MA). When there is substantial heterogeneity across studies, various random-effects meta-analysis models are possible that employ a one-stage or two-stage method. These are generally thought to produce similar results, but empirical comparisons are few. Objective: We describe and compare several one- and two-stage random-effects IPD-MA methods for estimating factor-outcome associations from multiple risk-factor or predictor finding studies with a binary outcome. One-stage methods use the IPD of each study and meta-analyse using the exact binomial distribution, whereas two-stage methods reduce evidence to the aggregated level (e.g. odds ratios) and then meta-analyse assuming approximate normality. We compare the methods in an empirical dataset for unadjusted and adjusted risk-factor estimates. Results: Though often similar, on occasion the one-stage and two-stage methods provide different parameter estimates and different conclusions. For example, the effect of erythema and its statistical significance was different for a one-stage (OR\u200a=\u200a1.35, ) and univariate two-stage (OR\u200a=\u200a1.55, ). Estimation issues can also arise: two-stage models suffer unstable estimates when zero cell counts occur and one-stage models do not always converge. Conclusion: When planning an IPD-MA, the choice and implementation (e.g. univariate or multivariate) of a one-stage or two-stage method should be prespecified in the protocol as occasionally they lead to different conclusions about which factors are associated with outcome. Though both approaches can suffer from estimation challenges, we recommend employing the one-stage method, as it uses a more exact statistical approach and accounts for parameter correlation. "], "author_display": ["Thomas P. A. Debray", "Karel G. M. Moons", "Ghada Mohammed Abdallah Abo-Zaid", "Hendrik Koffijberg", "Richard David Riley"], "article_type": "Research Article", "score": 0.50884223, "title_display": "Individual Participant Data Meta-Analysis for a Binary Outcome: One-Stage or Two-Stage?", "publication_date": "2013-04-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0060650"}, {"journal": "PLOS ONE", "abstract": ["\nP-Type ATPases are part of the regulatory system of the cell where they are responsible for transporting ions and lipids through the cell membrane. These pumps are found in all eukaryotes and their malfunction has been found to cause several severe diseases. Knowing which substrate is pumped by a certain P-Type ATPase is therefore vital. The P-Type ATPases can be divided into 11 subtypes based on their specificity, that is, the substrate that they pump. Determining the subtype experimentally is time-consuming. Thus it is of great interest to be able to accurately predict the subtype based on the amino acid sequence only. We present an approach to P-Type ATPase sequence classification based on the k-nearest neighbors, similar to a homology search, and show that this method provides performs very well and, to the best of our knowledge, better than any existing method despite its simplicity. The classifier is made available as a web service at http://services.birc.au.dk/patbox/ which also provides access to a database of potential P-Type ATPases and their predicted subtypes.\n"], "author_display": ["Dan S\u00f8ndergaard", "Christian N\u00f8rgaard Storm Pedersen"], "article_type": "Research Article", "score": 0.5070089, "title_display": "PATBox: A Toolbox for Classification and Analysis of P-Type ATPases", "publication_date": "2015-09-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0139571"}, {"journal": "PLOS ONE", "abstract": ["\nEarly integration of research education into medical curricula is crucial for evidence-based practice. Yet, many medical students are graduating with no research experience due to the lack of such integration in their medical school programs. The purpose of this study was to explore the impact of a peer-organized, extra-curricular research methodology course on the attitudes of medical students towards research and future academic careers. Twenty one medical students who participated in a peer-organized research course were enrolled in three focus group discussions to explore their experiences, perceptions and attitudes towards research after the course. Discussions were conducted using a semi-structured interview guide, and were transcribed and thematically analyzed for major and minor themes identification. Our findings indicate that students\u2019 perceptions of research changed after the course from being difficult initially to becoming possible. Participants felt that their research skills and critical thinking were enhanced and that they would develop research proposals and abstracts successfully. Students praised the peer-assisted teaching approach as being successful in enhancing the learning environment and filling the curricular gap. In conclusion, peer-organized extra-curricular research courses may be a useful option to promote research interest and skills of medical students when gaps in research education in medical curricula exist.\n"], "author_display": ["Bassel Nazha", "Rony H. Salloum", "Akl C. Fahed", "Mona Nabulsi"], "article_type": "Research Article", "score": 0.5047667, "title_display": "Students\u2019 Perceptions of Peer-Organized Extra-Curricular Research Course during Medical School: A Qualitative Study", "publication_date": "2015-03-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0119375"}, {"journal": "PLoS ONE", "abstract": ["Background: Biomedical researchers are now often faced with situations where it is necessary to test a large number of hypotheses simultaneously, eg, in comparative gene expression studies using high-throughput microarray technology. To properly control false positive errors the FDR (false discovery rate) approach has become widely used in multiple testing. The accurate estimation of FDR requires the proportion of true null hypotheses being accurately estimated. To date many methods for estimating this quantity have been proposed. Typically when a new method is introduced, some simulations are carried out to show the improved accuracy of the new method. However, the simulations are often very limited to covering only a few points in the parameter space. Results: Here I have carried out extensive in silico experiments to compare some commonly used methods for estimating the proportion of true null hypotheses. The coverage of these simulations is unprecedented thorough over the parameter space compared to typical simulation studies in the literature. Thus this work enables us to draw conclusions globally as to the performance of these different methods. It was found that a very simple method gives the most accurate estimation in a dominantly large area of the parameter space. Given its simplicity and its overall superior accuracy I recommend its use as the first choice for estimating the proportion of true null hypotheses in multiple testing. "], "author_display": ["Shu-Dong Zhang"], "article_type": "Research Article", "score": 0.503536, "title_display": "Towards Accurate Estimation of the Proportion of True Null Hypotheses in Multiple Testing", "publication_date": "2011-04-22T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0018874"}, {"journal": "PLoS ONE", "abstract": ["Background: The past decade has seen several high-level events and documents committing to strengthening the field of health policy and systems research (HPSR) as a critical input to strengthening health systems. Specifically, they called for increased production, capacity to undertake and funding for HPSR. The objective of this paper is to assess the extent to which progress has been achieved, an important feedback for stakeholders in this field. Methods and Finding: Two sources of data have been used. The first is a bibliometric analysis to assess growth in production of HPSR between 2003 and 2009. The six building blocks of the health system were used to define the scope of this search. The second is a survey of 96 research institutions undertaken in 2010 to assess the capacity and funding availability to undertake HPSR, compared with findings from the same survey undertaken in 2000 and 2008. Both analyses focus on HPSR relevant to low-income and middle-income countries (LMICs). Overall, we found an increasing trend of publications on HPSR in LMICs, although only 4% were led by authors from low-income countries (LICs). This is consistent with findings from the institutional survey, where despite improvements in infrastructure of research institutions, a minimal change has been seen in the level of experience of researchers within LIC institutions. Funding availability in LICs has increased notably to institutions in Sub-Saharan Africa; nonetheless, the overall increase has been modest in all regions. Conclusion: Although progress has been made in both the production and funding availability for HPSR, capacity to undertake the research locally has grown at a much slower pace, particularly in LICs where there is most need for this research. A firm commitment to dedicate a proportion of all future funding for research to building capacity may be the only solution to turn the tide. "], "author_display": ["Taghreed Adam", "Saad Ahmad", "Maryam Bigdeli", "Abdul Ghaffar", "John-Arne R\u00f8ttingen"], "article_type": "Research Article", "score": 0.50347596, "title_display": "Trends in Health Policy and Systems Research over the Past Decade: Still Too Little Capacity in Low-Income Countries", "publication_date": "2011-11-22T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0027263"}, {"abstract": ["\n        Genetic researchers often collect disease related quantitative traits in addition to disease status because they are interested in understanding the pathophysiology of disease processes. In genome-wide association (GWA) studies, these quantitative phenotypes may be relevant to disease development and serve as intermediate phenotypes or they could be behavioral or other risk factors that predict disease risk. Statistical tests combining both disease status and quantitative risk factors should be more powerful than case-control studies, as the former incorporates more information about the disease. In this paper, we proposed a modified inverse-variance weighted meta-analysis method to combine disease status and quantitative intermediate phenotype information. The simulation results showed that when an intermediate phenotype was available, the inverse-variance weighted method had more power than did a case-control study of complex diseases, especially in identifying susceptibility loci having minor effects. We further applied this modified meta-analysis to a study of imputed lung cancer genotypes with smoking data in 1154 cases and 1137 matched controls. The most significant SNPs came from the CHRNA3-CHRNA5-CHRNB4 region on chromosome 15q24\u201325.1, which has been replicated in many other studies. Our results confirm that this CHRNA region is associated with both lung cancer development and smoking behavior. We also detected three significant SNPs\u2014rs1800469, rs1982072, and rs2241714\u2014in the promoter region of the TGFB1 gene on chromosome 19 (p\u200a=\u200a1.46\u00d710\u22125, 1.18\u00d710\u22125, and 6.57\u00d710\u22126, respectively). The SNP rs1800469 is reported to be associated with chronic obstructive pulmonary disease and lung cancer in cigarette smokers. The present study is the first GWA study to replicate this result. Signals in the 3q26 region were also identified in the meta-analysis. We demonstrate the intermediate phenotype can potentially enhance the power of complex disease association analysis and the modified meta-analysis method is robust to incorporate intermediate phenotype or other quantitative risk factor in the analysis.\n      "], "author_display": ["Yafang Li", "Jian Huang", "Christopher I. Amos"], "article_type": "Research Article", "score": 0.5030922, "title_display": "Genetic Association Analysis of Complex Diseases Incorporating Intermediate Phenotype Information", "publication_date": "2012-10-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0046612"}, {"journal": "PLOS ONE", "abstract": ["\nmicroRNAs (miRNAs) are important gene regulators at post-transcriptional level, and inferring miRNA-mRNA regulatory relationships is a crucial problem. Consequently, several computational methods of predicting miRNA targets have been proposed using expression data with or without sequence based miRNA target information. A typical procedure for applying and evaluating such a method is i) collecting matched miRNA and mRNA expression profiles in a specific condition, e.g. a cancer dataset from The Cancer Genome Atlas (TCGA), ii) applying the new computational method to the selected dataset, iii) validating the predictions against knowledge from literature and third-party databases, and comparing the performance of the method with some existing methods. This procedure is time consuming given the time elapsed when collecting and processing data, repeating the work from existing methods, searching for knowledge from literature and third-party databases to validate the results, and comparing the results from different methods. The time consuming procedure prevents researchers from quickly testing new computational models, analysing new datasets, and selecting suitable methods for assisting with the experiment design. Here, we present an R package, miRLAB, for automating the procedure of inferring and validating miRNA-mRNA regulatory relationships. The package provides a complete set of pipelines for testing new methods and analysing new datasets. miRLAB includes a pipeline to obtain matched miRNA and mRNA expression datasets directly from TCGA, 12 benchmark computational methods for inferring miRNA-mRNA regulatory relationships, the functions for validating the predictions using experimentally validated miRNA target data and miRNA perturbation data, and the tools for comparing the results from different computational methods.\n"], "author_display": ["Thuc Duy Le", "Junpeng Zhang", "Lin Liu", "Huawen Liu", "Jiuyong Li"], "article_type": "Research Article", "score": 0.502715, "title_display": "miRLAB: An R Based Dry Lab for Exploring miRNA-mRNA Regulatory Relationships", "publication_date": "2015-12-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0145386"}, {"journal": "PLOS Computational Biology", "abstract": ["\nBy aggregating data for complex traits in a biologically meaningful way, gene and gene-set analysis constitute a valuable addition to single-marker analysis. However, although various methods for gene and gene-set analysis currently exist, they generally suffer from a number of issues. Statistical power for most methods is strongly affected by linkage disequilibrium between markers, multi-marker associations are often hard to detect, and the reliance on permutation to compute p-values tends to make the analysis computationally very expensive. To address these issues we have developed MAGMA, a novel tool for gene and gene-set analysis. The gene analysis is based on a multiple regression model, to provide better statistical performance. The gene-set analysis is built as a separate layer around the gene analysis for additional flexibility. This gene-set analysis also uses a regression structure to allow generalization to analysis of continuous properties of genes and simultaneous analysis of multiple gene sets and other gene properties. Simulations and an analysis of Crohn\u2019s Disease data are used to evaluate the performance of MAGMA and to compare it to a number of other gene and gene-set analysis tools. The results show that MAGMA has significantly more power than other tools for both the gene and the gene-set analysis, identifying more genes and gene sets associated with Crohn\u2019s Disease while maintaining a correct type 1 error rate. Moreover, the MAGMA analysis of the Crohn\u2019s Disease data was found to be considerably faster as well.\nAuthor Summary: Gene and gene-set analysis are statistical methods for analysing multiple genetic markers simultaneously to determine their joint effect. These methods can be used when the effects of individual markers is too weak to detect, which is a common problem when studying polygenic traits. Moreover, gene-set analysis can provide additional insight into functional and biological mechanisms underlying the genetic component of a trait. Although a number of methods for gene and gene-set analysis are available however, they generally suffer from various statistical issues and can be very time-consuming to run. We have therefore developed a new method called MAGMA to address these issues, and have compared it to a number of existing tools. Our results show that MAGMA detects more associated genes and gene-sets than other methods, and is also considerably faster. The way the method is set up also makes it highly flexible. This makes it suitable as a basis for more general statistical analyses aimed at investigating more complex research questions. "], "author_display": ["Christiaan A. de Leeuw", "Joris M. Mooij", "Tom Heskes", "Danielle Posthuma"], "article_type": "Research Article", "score": 0.49990195, "title_display": "MAGMA: Generalized Gene-Set Analysis of GWAS Data", "publication_date": "2015-04-17T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1004219"}, {"journal": "PLOS Neglected Tropical Diseases", "abstract": ["Background: Research on Neglected Tropical Diseases (NTDs) has increased in recent decades, and significant need-gaps in diagnostic and treatment tools remain. Analysing bibliometric data from published research is a powerful method for revealing research efforts, partnerships and expertise. We aim to identify and map NTD research networks in Germany and their partners abroad to enable an informed and transparent evaluation of German contributions to NTD research. Methodology/Principal Findings: A SCOPUS database search for articles with German author affiliations that were published between 2002 and 2012 was conducted for kinetoplastid and helminth diseases. Open-access tools were used for data cleaning and scientometrics (OpenRefine), geocoding (OpenStreetMaps) and to create (Table2Net), visualise and analyse co-authorship networks (Gephi). From 26,833 publications from around the world that addressed 11 diseases, we identified 1,187 (4.4%) with at least one German author affiliation, and we processed 972 publications for the five most published-about diseases. Of those, we extracted 4,007 individual authors and 863 research institutions to construct co-author networks. The majority of co-authors outside Germany were from high-income countries and Brazil. Collaborations with partners on the African continent remain scattered. NTD research within Germany was distributed among 220 research institutions. We identified strong performers on an individual level by using classic parameters (number of publications, h-index) and social network analysis parameters (betweenness centrality). The research network characteristics varied strongly between diseases. Conclusions/Significance: The share of NTD publications with German affiliations is approximately half of its share in other fields of medical research. This finding underlines the need to identify barriers and expand Germany\u2019s otherwise strong research activities towards NTDs. A geospatial analysis of research collaborations with partners abroad can support decisions to strengthen research capacity, particularly in low- and middle-income countries, which were less involved in collaborations than high-income countries. Identifying knowledge hubs within individual researcher networks complements traditional scientometric indicators that are used to identify opportunities for collaboration. Using free tools to analyse research processes and output could facilitate data-driven health policies. Our findings contribute to the prioritisation of efforts in German NTD research at a time of impending local and global policy decisions. Author Summary: Neglected tropical disease research has changed considerably in recent decades, and the German government is committed to addressing its past neglect of NTD research. Our aim was to use an innovative social network analysis of bibliometric data to map neglected tropical disease research networks that are inside of and affiliated with Germany, thereby enabling data-driven health policy decision-making. We created and analysed co-author networks from publications in the SCOPUS database, with a focus on five diseases. We found that Germany's share of global publication output for NTDs is approximately half that of other medical research fields. Furthermore, we identified institutions with prominent NTD research within Germany and strong research collaborations between German institutions and partners abroad, mostly in other high-income countries. This allowed an assessment of strong collaborations for further development, e.g., for research capacity strengthening in low-income-countries, but also for identifying missed opportunities for collaboration within the network. Through co-authorship network analysis of individual researcher networks, we identified strong performers by using classic bibliometric parameters, and we identified academic talent by social network analysis parameters on an individual level. "], "author_display": ["Max Ernst Bender", "Suzanne Edwards", "Peter von Philipsborn", "Fridolin Steinbeis", "Thomas Keil", "Peter Tinnemann"], "article_type": "Research Article", "score": 0.49815333, "title_display": "Using Co-authorship Networks to Map and Analyse Global Neglected Tropical Disease Research with an Affiliation to Germany", "publication_date": "2015-12-31T00:00:00Z", "eissn": "1935-2735", "id": "10.1371/journal.pntd.0004182"}, {"journal": "PLoS ONE", "abstract": ["\nWe investigated commonly used methods (Autocorrelation, Enright, and Discrete Fourier Transform) to estimate the periodicity of oscillatory data and determine which method most accurately estimated periods while being least vulnerable to the presence of noise. Both simulated and experimental data were used in the analysis performed. We determined the significance of calculated periods by applying these methods to several random permutations of the data and then calculating the probability of obtaining the period's peak in the corresponding periodograms. Our analysis suggests that the Enright method is the most accurate for estimating the period of oscillatory data. We further show that to accurately estimate the period of oscillatory data, it is necessary that at least five cycles of data are sampled, using at least four data points per cycle. These results suggest that the Enright method should be more widely applied in order to improve the analysis of oscillatory data.\n"], "author_display": ["M\u00e1rcio Mour\u00e3o", "Leslie Satin", "Santiago Schnell"], "article_type": "Research Article", "score": 0.4968084, "title_display": "Optimal Experimental Design to Estimate Statistically Significant Periods of Oscillations in Time Course Data", "publication_date": "2014-04-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0093826"}, {"journal": "PLoS ONE", "abstract": ["Background: There is increasing recognition of sex/gender differences in health and the importance of identifying differential effects of interventions for men and women. Yet, to whom the research evidence does or does not apply, with regard to sex/gender, is often insufficiently answered. This is also true for systematic reviews which synthesize results of primary studies. A lack of analysis and reporting of evidence on sex/gender raises concerns about the applicability of systematic reviews. To bridge this gap, this pilot study aimed to translate knowledge about sex/gender analysis (SGA) into a user-friendly \u2018briefing note\u2019 format and evaluate its potential in aiding the implementation of SGA in systematic reviews. Methods: Our Sex/Gender Methods Group used an interactive process to translate knowledge about sex/gender into briefing notes, a concise communication tool used by policy and decision makers. The briefing notes were developed in collaboration with three Cochrane Collaboration review groups (HIV/AIDS, Hypertension, and Musculoskeletal) who were also the target knowledge users of the briefing notes. Briefing note development was informed by existing systematic review checklists, literature on sex/gender, in-person and virtual meetings, and consultation with topic experts. Finally, we held a workshop for potential users to evaluate the notes. Results: Each briefing note provides tailored guidance on considering sex/gender to reviewers who are planning or conducting systematic reviews and includes the rationale for considering sex/gender, with examples specific to each review group\u2019s focus. Review authors found that the briefing notes provided welcome guidance on implementing SGA that was clear and concise, but also identified conceptual and implementation challenges. Conclusions: Sex/gender briefing notes are a promising knowledge translation tool. By encouraging sex/gender analysis and equity considerations in systematic reviews, the briefing notes can assist systematic reviewers in ensuring the applicability of research evidence, with the goal of improved health outcomes for diverse populations. "], "author_display": ["Marion Doull", "Vivian Welch", "Lorri Puil", "Vivien Runnels", "Stephanie E. Coen", "Beverley Shea", "Jennifer O\u2019Neill", "Cornelia Borkhoff", "Sari Tudiver", "Madeline Boscoe"], "article_type": "Research Article", "score": 0.49628636, "title_display": "Development and Evaluation of \u2018Briefing Notes\u2019 as a Novel Knowledge Translation Tool to Aid the Implementation of Sex/Gender Analysis in Systematic Reviews: A Pilot Study", "publication_date": "2014-11-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0110786"}, {"journal": "PLoS ONE", "abstract": ["\n        Principal-oscillation-pattern (POP) analysis is a multivariate and systematic technique for identifying the dynamic characteristics of a system from time-series data. In this study, we demonstrate the first application of POP analysis to genome-wide time-series gene-expression data. We use POP analysis to infer oscillation patterns in gene expression. Typically, a genomic system matrix cannot be directly estimated because the number of genes is usually much larger than the number of time points in a genomic study. Thus, we first identify the POPs of the eigen-genomic system that consists of the first few significant eigengenes obtained by singular value decomposition. By using the linear relationship between eigengenes and genes, we then infer the POPs of the genes. Both simulation data and real-world data are used in this study to demonstrate the applicability of POP analysis to genomic data. We show that POP analysis not only compares favorably with experiments and existing computational methods, but that it also provides complementary information relative to other approaches.\n      "], "author_display": ["Daifeng Wang", "Ari Arapostathis", "Claus O. Wilke", "Mia K. Markey"], "article_type": "Research Article", "score": 0.49562117, "title_display": "Principal-Oscillation-Pattern Analysis of Gene Expression", "publication_date": "2012-01-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0028805"}, {"journal": "PLOS ONE", "abstract": ["Background: There is increasing recognition of the importance of sharing research data within the international scientific community, but also of the ethical and social challenges this presents, particularly in the context of structural inequities and varied capacity in international research. Public involvement is essential to building locally responsive research policies, including on data sharing, but little research has involved stakeholders from low-to-middle income countries. Methods: Between January and June 2014, a qualitative study was conducted in Kenya involving sixty stakeholders with varying experiences of research in a deliberative process to explore views on benefits and challenges in research data sharing. In-depth interviews and extended small group discussions based on information sharing and facilitated debate were used to collect data. Data were analysed using Framework Analysis, and charting flow and dynamics in debates. Findings: The findings highlight both the opportunities and challenges of communicating about this complex and relatively novel topic for many stakeholders. For more and less research-experienced stakeholders, ethical research data sharing is likely to rest on the development and implementation of appropriate trust-building processes, linked to local perceptions of benefits and challenges. The central nature of trust is underpinned by uncertainties around who might request what data, for what purpose and when. Key benefits perceived in this consultation were concerned with the promotion of public health through science, with legitimate beneficiaries defined differently by different groups. Important challenges were risks to the interests of study participants, communities and originating researchers through stigmatisation, loss of privacy, impacting autonomy and unfair competition, including through forms of intentional and unintentional 'misuse' of data. Risks were also seen for science. Discussion: Given background structural inequities in much international research, building trust in this low-to-middle income setting includes ensuring that the interests of study participants, primary communities and originating researchers will be promoted as far as possible, as well as protected. Important ways of building trust in data sharing include involving the public in policy development and implementation, promoting scientific collaborations around data sharing and building close partnerships between researchers and government health authorities to provide checks and balances on data sharing, and promote near and long-term translational benefits. "], "author_display": ["Irene Jao", "Francis Kombe", "Salim Mwalukore", "Susan Bull", "Michael Parker", "Dorcas Kamuya", "Sassy Molyneux", "Vicki Marsh"], "article_type": "Research Article", "score": 0.49560755, "title_display": "Research Stakeholders\u2019 Views on Benefits and Challenges for Public Health Research Data Sharing in Kenya: The Importance of Trust and Social Relations", "publication_date": "2015-09-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0135545"}, {"abstract": ["\n        With increasing interest in the carbon cycle on arid land, there is an urgent need to quantify both soil organic carbon (SOC) and inorganic carbon (SIC) thus to assess various methods. Here, we present a study employing three methods for determinations of SOC and SIC in the Yanqi Basin of northwest China. We use an elemental analyzer for both SOC and SIC, the Walkley-Black method for SOC, a modified pressure calcimeter method for SIC, and a simple loss-on-ignition (LOI) procedure for determinations of SOC and SIC. Our analyses show that all three approaches produce consistently low values for SOC (1\u201314 g kg\u22121) and high values for SIC (8\u201353 g kg\u22121). The Walkley-Black method provides an accurate estimate of SOC with 100% recovery for most soil samples. The pressure calcimeter method is as accurate as the elemental analysis for measuring SIC. In addition, SOC and SIC can be accurately estimated using a two-step LOI approach, i.e., (1) combustion at 375\u00b0C for 17 hours to estimate SOC, and (2) subsequent combustion at 800\u00b0C for 12 hours to estimate SIC. There are strong linear relationships for both SOC and SIC between the elemental analysis and LOI method, which demonstrates the capability of the two-step LOI technique for estimating SOC and SIC in this arid region.\n      "], "author_display": ["Xiujun Wang", "Jiaping Wang", "Juan Zhang"], "article_type": "Research Article", "score": 0.49199677, "title_display": "Comparisons of Three Methods for Organic and Inorganic Carbon in Calcareous Soils of Northwestern China", "publication_date": "2012-08-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0044334"}, {"journal": "PLoS ONE", "abstract": ["Background: Patient adherence is an important issue for health service providers and health researchers. However, the knowledge structure of diverse research on treatment adherence is unclear. This study used co-word analysis and social network analysis techniques to analyze research literature on adherence, and to show their knowledge structure and evolution over time. Methods: Published scientific papers about treatment adherence were retrieved from Web of Science (2000 to May 2011). A total of 2308 relevant articles were included: 788 articles published in 2000\u20132005 and 1520 articles published in 2006\u20132011. The keywords of each article were extracted by using the software Biblexcel, and the synonym and isogenous words were merged manually. The frequency of keywords and their co-occurrence frequency were counted. High frequency keywords were selected to yield the co-words matrix. Finally the decomposition maps were used to comb the complex knowledge structures. Results: Research themes were more general in the first period (2000 to 2005), and more extensive with many more new terms in the second period (2006 to 2011). Research on adherence has covered more and more diseases, populations and methods, but other diseases/conditions are not as hot as HIV/AIDS and have not become specialty themes/sub-directions. Most studies originated from the United States. Conclusion: The dynamic of this field is mainly divergent, with increasing number of new sub-directions of research. Future research is required to investigate specific directions and converge as well to construct a general paradigm in this field. "], "author_display": ["Juan Zhang", "Jun Xie", "Wanli Hou", "Xiaochen Tu", "Jing Xu", "Fujian Song", "Zhihong Wang", "Zuxun Lu"], "article_type": "Research Article", "score": 0.490928, "title_display": "Mapping the Knowledge Structure of Research on Patient Adherence: Knowledge Domain Visualization Based Co-Word Analysis and Social Network Analysis", "publication_date": "2012-04-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0034497"}, {"journal": "PLoS ONE", "abstract": ["\nProtein-RNA complexes play key roles in several cellular processes by the interactions of amino acids with RNA. To understand the recognition mechanism, it is important to identify the specific amino acids involved in RNA binding. Various computational methods have been developed for predicting RNA binding residues from protein sequence. However, their performances mainly depend on the training dataset, feature selection for developing a model and learning capacity of the model. Hence, it is important to reveal the correspondence between the performance of methods and properties of RNA-binding proteins (RBPs). In this work, we have collected all available RNA binding residues prediction methods and revealed their performances on unbiased, stringent and diverse datasets for RBPs with less than 25% sequence identity based on structural class, fold, superfamily, family, protein function, RNA type, RNA strand and RNA conformation. The best methods for each type of RBPs and the type of RBPs, which require further refinement in prediction, have been brought out. We also analyzed the performance of these methods for the disordered regions, structures which are not included in the training dataset and recently solved structures. The reliability of prediction is better than randomly choosing any method or combination of methods. This approach would be a valuable resource for biologists to choose the best method based on the type of RBPs for designing their experiments and the tool is freely accessible online at www.iitm.ac.in/bioinfo/RNA-protein/.\n"], "author_display": ["R. Nagarajan", "M. Michael Gromiha"], "article_type": "Research Article", "score": 0.490781, "title_display": "Prediction of RNA Binding Residues: An Extensive Analysis Based on Structure and Function to Select the Best Predictor", "publication_date": "2014-03-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0091140"}, {"journal": "PLoS ONE", "abstract": ["Background: A serious worldwide effort to strengthen research based knowledge translation (KT) has begun in recent years and some countries, particularly developed ones, are trying to incorporate KT in their health and health research systems. Keeping in mind the recent economic depression and the need to perform more efficient research, we aimed to assess and compare the KT status of selected health research institutes in the Eastern Mediterranean Regions' countries, and to identify their strengths and weaknesses in the field. Methods: After finding the focal points that would steer the focus group discussions (FGDs) and help complete the \u2018Self Assessment Tool for Research Institutes\u2019 (SATORI) tool, each focal point held two FGDs in which researchers, research authorities and other individuals specified in detail further in the study were held. The scores obtained by each institute were evaluated quantitatively, and the transcriptions were analyzed qualitatively with OpenCode software. Results: For ease of analysis the 50 items of the SATORI were classified into 7 main domains: \u2018priority setting\u2019, \u2018research quality and timeliness\u2019, \u2018researchers\u2019 KT capacities', \u2018facilities and pre-requisites of KT\u2019, \u2018processes and regulations supporting KT\u2019, \u2018interaction with research users\u2019, and \u2018promoting and evaluating the use of knowledge\u2019. Based on the scoring system, the strongest domain was \u2018research quality and timeliness\u2019. \u2018Priority setting\u2019 was the weakest domain of all. The remaining domains were more or less equal in strength and were not in a favorable state. The qualitative findings confirmed the quantitative findings. Conclusions: The main problem, it seems, is that a KT climate does not exist in the region. And despite the difference in the contexts, there are many similarities in the region's institutes included in this study. Collaborative efforts can play a role in creating this climate by steering countries towards KT and suggesting regional strategic directions according to their needs. "], "author_display": ["Katayoun Maleki", "Randah R. Hamadeh", "Jaleh Gholami", "Ahmed Mandil", "Saima Hamid", "Zahid Ahmad Butt", "Abdulaziz Bin Saeed", "Dalia Y. M. El Kheir", "Mohammed Saleem", "Sahar Maqsoud", "Najibullah Safi", "Ban A. Abdul-Majeed", "Reza Majdzadeh"], "article_type": "Research Article", "score": 0.49010292, "title_display": "The Knowledge Translation Status in Selected Eastern-Mediterranean Universities and Research Institutes", "publication_date": "2014-09-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0103732"}, {"journal": "PLOS ONE", "abstract": ["\nAs pharmacodynamic drug-drug interactions (PD DDIs) could lead to severe adverse effects in patients, it is important to identify potential PD DDIs in drug development. The signaling starting from drug targets is propagated through protein-protein interaction (PPI) networks. PD DDIs could occur by close interference on the same targets or within the same pathways as well as distant interference through cross-talking pathways. However, most of the previous approaches have considered only close interference by measuring distances between drug targets or comparing target neighbors. We have applied a random walk with restart algorithm to simulate signaling propagation from drug targets in order to capture the possibility of their distant interference. Cross validation with DrugBank and Kyoto Encyclopedia of Genes and Genomes DRUG shows that the proposed method outperforms the previous methods significantly. We also provide a web service with which PD DDIs for drug pairs can be analyzed at http://biosoft.kaist.ac.kr/targetrw.\n"], "author_display": ["Kyunghyun Park", "Docyong Kim", "Suhyun Ha", "Doheon Lee"], "article_type": "Research Article", "score": 0.4894808, "title_display": "Predicting Pharmacodynamic Drug-Drug Interactions through Signaling Propagation Interference on Protein-Protein Interaction Networks", "publication_date": "2015-10-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0140816"}, {"journal": "PLoS ONE", "abstract": ["Background: Passive exposure to environmental tobacco smoke (ETS) is estimated to exert a major burden of disease. Currently, numerous countries have taken legal actions to protect the population against ETS. Numerous studies have been conducted in this field. Therefore, scientometric methods should be used to analyze the accumulated data since there is no such approach available so far. Methods and Results: A combination of scientometric methods and novel visualizing procedures were used, including density-equalizing mapping and radar charting techniques. 6,580 ETS-related studies published between 1900 and 2008 were identified in the ISI database. Using different scientometric approaches, a continuous increase of both quantitative and qualitative parameters was found. The combination with density-equalizing calculations demonstrated a leading position of the United States (2,959 items published) in terms of quantitative research activities. Charting techniques demonstrated that there are numerous bi- and multilateral networks between different countries and institutions in this field. Again, a leading position of American institutions was found. Conclusions: This is the first comprehensive scientometric analysis of data on global scientific activities in the field of environmental tobacco smoke research. The present findings can be used as a benchmark for funding allocation processes. "], "author_display": ["Karin Vitzthum", "Cristian Scutaru", "Lindy Musial-Bright", "David Quarcoo", "Tobias Welte", "Michael Spallek", "Beatrix Groneberg-Kloft"], "article_type": "Research Article", "score": 0.48942685, "title_display": "Scientometric Analysis and Combined Density-Equalizing Mapping of Environmental Tobacco Smoke (ETS) Research", "publication_date": "2010-06-22T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0011254"}, {"journal": "PLoS ONE", "abstract": ["\n        Pathway analysis has been proposed as a complement to single SNP analyses in GWAS. This study compared pathway analysis methods using two lung cancer GWAS data sets based on four studies: one a combined data set from Central Europe and Toronto (CETO); the other a combined data set from Germany and MD Anderson (GRMD). We searched the literature for pathway analysis methods that were widely used, representative of other methods, and had available software for performing analysis. We selected the programs EASE, which uses a modified Fishers Exact calculation to test for pathway associations, GenGen (a version of Gene Set Enrichment Analysis (GSEA)), which uses a Kolmogorov-Smirnov-like running sum statistic as the test statistic, and SLAT, which uses a p-value combination approach. We also included a modified version of the SUMSTAT method (mSUMSTAT), which tests for association by averaging \u03c72 statistics from genotype association tests. There were nearly 18000 genes available for analysis, following mapping of more than 300,000 SNPs from each data set. These were mapped to 421 GO level 4 gene sets for pathway analysis. Among the methods designed to be robust to biases related to gene size and pathway SNP correlation (GenGen, mSUMSTAT and SLAT), the mSUMSTAT approach identified the most significant pathways (8 in CETO and 1 in GRMD). This included a highly plausible association for the acetylcholine receptor activity pathway in both CETO (FDR\u22640.001) and GRMD (FDR\u200a=\u200a0.009), although two strong association signals at a single gene cluster (CHRNA3-CHRNA5-CHRNB4) drive this result, complicating its interpretation. Few other replicated associations were found using any of these methods. Difficulty in replicating associations hindered our comparison, but results suggest mSUMSTAT has advantages over the other approaches, and may be a useful pathway analysis tool to use alongside other methods such as the commonly used GSEA (GenGen) approach.\n      "], "author_display": ["Gordon Fehringer", "Geoffrey Liu", "Laurent Briollais", "Paul Brennan", "Christopher I. Amos", "Margaret R. Spitz", "Heike Bickeb\u00f6ller", "H. Erich Wichmann", "Angela Risch", "Rayjean J. Hung"], "article_type": "Research Article", "score": 0.48886716, "title_display": "Comparison of Pathway Analysis Approaches Using Lung Cancer GWAS Data Sets", "publication_date": "2012-02-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0031816"}, {"journal": "PLoS ONE", "abstract": ["\nGene set methods aim to assess the overall evidence of association of a set of genes with a phenotype, such as disease or a quantitative trait. Multiple approaches for gene set analysis of expression data have been proposed. They can be divided into two types: competitive and self-contained. Benefits of self-contained methods include that they can be used for genome-wide, candidate gene, or pathway studies, and have been reported to be more powerful than competitive methods. We therefore investigated ten self-contained methods that can be used for continuous, discrete and time-to-event phenotypes. To assess the power and type I error rate for the various previously proposed and novel approaches, an extensive simulation study was completed in which the scenarios varied according to: number of genes in a gene set, number of genes associated with the phenotype, effect sizes, correlation between expression of genes within a gene set, and the sample size. In addition to the simulated data, the various methods were applied to a pharmacogenomic study of the drug gemcitabine. Simulation results demonstrated that overall Fisher's method and the global model with random effects have the highest power for a wide range of scenarios, while the analysis based on the first principal component and Kolmogorov-Smirnov test tended to have lowest power. The methods investigated here are likely to play an important role in identifying pathways that contribute to complex traits.\n"], "author_display": ["Brooke L. Fridley", "Gregory D. Jenkins", "Joanna M. Biernacka"], "article_type": "Research Article", "score": 0.4887776, "title_display": "Self-Contained Gene-Set Analysis of Expression Data: An Evaluation of Existing and Novel Methods", "publication_date": "2010-09-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0012693"}, {"journal": "PLOS ONE", "abstract": ["Background: Treatment by covariate interactions can be explored in reviews using interaction analyses (e.g., subgroup analysis). Such analyses can provide information on how the covariate modifies the treatment effect and is an important methodological approach for personalising medicine. Guidance exists regarding how to apply such analyses but little is known about whether authors follow the guidance. Methods: Using published recommendations, we developed criteria to assess how well interaction analyses were designed, applied, interpreted, and reported. The Cochrane Database of Systematic Reviews was searched (8th August 2013). We applied the criteria to the most recently published review, with an accessible protocol, for each Cochrane Review Group. We excluded review updates, diagnostic test accuracy reviews, withdrawn reviews, and overviews of reviews. Data were summarised regarding reviews, covariates, and analyses. Results: Each of the 52 included reviews planned or did interaction analyses; 51 reviews (98%) planned analyses and 33 reviews (63%) applied analyses. The type of analysis planned and the type subsequently applied (e.g., sensitivity or subgroup analysis) was discrepant in 24 reviews (46%). No review reported how or why each covariate had been chosen; 22 reviews (42%) did state each covariate a priori in the protocol but no review identified each post-hoc covariate as such. Eleven reviews (21%) mentioned five covariates or less. One review reported planning to use a method to detect interactions (i.e., interaction test) for each covariate; another review reported applying the method for each covariate. Regarding interpretation, only one review reported whether an interaction was detected for each covariate and no review discussed the importance, or plausibility, of the results, or the possibility of confounding for each covariate. Conclusions: Interaction analyses in Cochrane Reviews can be substantially improved. The proposed criteria can be used to help guide the reporting and conduct of analyses. "], "author_display": ["Sarah Donegan", "Lisa Williams", "Sofia Dias", "Catrin Tudur-Smith", "Nicky Welton"], "article_type": "Research Article", "score": 0.488133, "title_display": "Exploring Treatment by Covariate Interactions Using Subgroup Analysis and Meta-Regression in Cochrane Reviews: A Review of Recent Practice", "publication_date": "2015-06-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0128804"}, {"journal": "PLOS Neglected Tropical Diseases", "abstract": ["\nThe current Ebola outbreak poses a threat to individual and global public health. Although the disease has been of interest to the scientific community since 1976, an effective vaccination approach is still lacking. This fact questions past global public health strategies, which have not foreseen the possible impact of this infectious disease. To quantify the global research activity in this field, a scientometric investigation was conducted. We analyzed the research output of countries, individual institutions and their collaborative networks. The resulting research architecture indicated that American and European countries played a leading role regarding output activity, citations and multi- and bilateral cooperations. When related to population numbers, African countries, which usually do not dominate the global research in other medical fields, were among the most prolific nations. We conclude that the field of Ebola research is constantly progressing, and the research landscape is influenced by economical and infrastructural factors as well as historical relations between countries and outbreak events.\nAuthor Summary: For the first time in the history of the disease, the Ebola virus left its local setting and affected people not only in isolated rural areas, but reached larger towns and cities leading to worldwide repercussions. This development prompted a joint global response to this health threat. This encompassed not only immediate relief efforts, but also an up search in global research work. In this study, the scientific output in Ebola research available in one of the mayor medical search platforms was characterized. We studied among others the origin of research, the collaboration between countries and the research topics. Partly, the obtained data was weighted against economic parameters. We attained a detailed map of the research activities from the discovery of Ebola in 1976 up to today. Our research provides the first overview of the worldwide Ebola research output. It might help stakeholders in Ebola research to better plan investigations with a global perspective. "], "author_display": ["David Quarcoo", "D\u00f6rthe Br\u00fcggmann", "Doris Klingelh\u00f6fer", "David A. Groneberg"], "article_type": "Research Article", "score": 0.4872592, "title_display": "Ebola and Its Global Research Architecture\u2014Need for an Improvement", "publication_date": "2015-09-25T00:00:00Z", "eissn": "1935-2735", "id": "10.1371/journal.pntd.0004083"}, {"journal": "PLoS ONE", "abstract": ["Background: In sub-Saharan Africa, a large proportion of HIV positive patients on antiretroviral therapy (ART) are lost to follow-up, some of whom are dead. The objective of this study was to validate methods used to correct mortality estimates for loss-to-follow-up using a cohort with complete death ascertainment. Methods: Routinely collected data from HIV patients initiating first line antiretroviral therapy (ART) at the Infectious Diseases Institute (IDI) (Routine Cohort) was used. Three methods to estimate mortality after initiation were: 1) standard Kaplan-Meier estimation (uncorrected method) that uses passively observed data; 2) double-sampling methods by Frangakis and Rubin (F&R) where deaths obtained from patient tracing studies are given a higher weight than those passively ascertained; 3) Nomogram proposed by Egger et al. Corrected mortality estimates in the Routine Cohort, were compared with the estimates from the IDI research observational cohort (Research Cohort), which was used as the \u201cgold-standard\u201d. Results: We included 5,633 patients from the Routine Cohort and 559 from the Research Cohort. Uncorrected mortality estimates (95% confidence interval [1]) in the Routine Cohort at 1, 2 and 3 years were 5.5% (4.9%\u20136.3%), 6.6% (5.9%\u20137.5%) and 7.4% (6.5%\u20138.5%), respectively. The F&R corrected estimates at 1, 2 and 3 years were 11.2% (5.8%\u201321.2%), 15.8% (9.9%\u201324.8%) and 18.5% (12.3% \u201327.2%) respectively. The estimates obtained from the Research Cohort were 15.6% (12.8%\u201318.9%), 17.5% (14.6%\u201321.0%) and 19.0% (15.3%\u201321.9%) at 1, 2 and 3 years respectively. Using the nomogram method in the Routine Cohort, the corrected programme-level mortality estimate in year 1 was 11.9% (8.0%\u201315.7%). Conclusion: Mortality adjustments provided by the F&R and nomogram methods are adequate and should be employed to correct mortality for loss-to-follow-up in large HIV care centres in Sub-Saharan Africa. "], "author_display": ["Agnes N. Kiragga", "Barbara Castelnuovo", "Rachel Musomba", "Jonathan Levin", "Andrew Kambugu", "Yukari C. Manabe", "Constantin T. Yiannoutsos", "Noah Kiwanuka"], "article_type": "Research Article", "score": 0.48656392, "title_display": "Comparison of Methods for Correction of Mortality Estimates for Loss to Follow-Up after ART Initiation: A Case of the Infectious Diseases Institute, Uganda", "publication_date": "2013-12-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0083524"}, {"abstract": ["\n        Neurotree is an online database that documents the lineage of academic mentorship in neuroscience. Modeled on the tree format typically used to describe biological genealogies, the Neurotree web site provides a concise summary of the intellectual history of neuroscience and relationships between individuals in the current neuroscience community. The contents of the database are entirely crowd-sourced: any internet user can add information about researchers and the connections between them. As of July 2012, Neurotree has collected information from 10,000 users about 35,000 researchers and 50,000 mentor relationships, and continues to grow. The present report serves to highlight the utility of Neurotree as a resource for academic research and to summarize some basic analysis of its data. The tree structure of the database permits a variety of graphical analyses. We find that the connectivity and graphical distance between researchers entered into Neurotree early has stabilized and thus appears to be mostly complete. The connectivity of more recent entries continues to mature. A ranking of researcher fecundity based on their mentorship reveals a sustained period of influential researchers from 1850\u20131950, with the most influential individuals active at the later end of that period. Finally, a clustering analysis reveals that some subfields of neuroscience are reflected in tightly interconnected mentor-trainee groups.\n      "], "author_display": ["Stephen V. David", "Benjamin Y. Hayden"], "article_type": "Research Article", "score": 0.48635334, "title_display": "Neurotree: A Collaborative, Graphical Database of the Academic Genealogy of Neuroscience", "publication_date": "2012-10-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0046608"}, {"journal": "PLoS ONE", "abstract": ["\nRecent advances in big data and analytics research have provided a wealth of large data sets that are too big to be analyzed in their entirety, due to restrictions on computer memory or storage size. New Bayesian methods have been developed for data sets that are large only due to large sample sizes. These methods partition big data sets into subsets and perform independent Bayesian Markov chain Monte Carlo analyses on the subsets. The methods then combine the independent subset posterior samples to estimate a posterior density given the full data set. These approaches were shown to be effective for Bayesian models including logistic regression models, Gaussian mixture models and hierarchical models. Here, we introduce the R package parallelMCMCcombine which carries out four of these techniques for combining independent subset posterior samples. We illustrate each of the methods using a Bayesian logistic regression model for simulation data and a Bayesian Gamma model for real data; we also demonstrate features and capabilities of the R package. The package assumes the user has carried out the Bayesian analysis and has produced the independent subposterior samples outside of the package. The methods are primarily suited to models with unknown parameters of fixed dimension that exist in continuous parameter spaces. We envision this tool will allow researchers to explore the various methods for their specific applications and will assist future progress in this rapidly developing field.\n"], "author_display": ["Alexey Miroshnikov", "Erin M. Conlon"], "article_type": "Research Article", "score": 0.4859736, "title_display": "parallelMCMCcombine: An R Package for Bayesian Methods for Big Data and Analytics", "publication_date": "2014-09-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0108425"}, {"journal": "PLoS ONE", "abstract": ["Background: Fluorescence-based methods have been proposed to aid caries lesion detection. Summarizing and analysing findings of studies about fluorescence-based methods could clarify their real benefits. Objective: We aimed to perform a comprehensive systematic review and meta-analysis to evaluate the accuracy of fluorescence-based methods in detecting caries lesions. Data Source: Two independent reviewers searched PubMed, Embase and Scopus through June 2012 to identify papers/articles published. Other sources were checked to identify non-published literature. Study Eligibility Criteria, Participants and Diagnostic Methods: The eligibility criteria were studies that: (1) have assessed the accuracy of fluorescence-based methods of detecting caries lesions on occlusal, approximal or smooth surfaces, in both primary or permanent human teeth, in the laboratory or clinical setting; (2) have used a reference standard; and (3) have reported sufficient data relating to the sample size and the accuracy of methods. Study Appraisal and Synthesis Methods: A diagnostic 2\u00d72 table was extracted from included studies to calculate the pooled sensitivity, specificity and overall accuracy parameters (Diagnostic Odds Ratio and Summary Receiver-Operating curve). The analyses were performed separately for each method and different characteristics of the studies. The quality of the studies and heterogeneity were also evaluated. Results: Seventy five studies met the inclusion criteria from the 434 articles initially identified. The search of the grey or non-published literature did not identify any further studies. In general, the analysis demonstrated that the fluorescence-based method tend to have similar accuracy for all types of teeth, dental surfaces or settings. There was a trend of better performance of fluorescence methods in detecting more advanced caries lesions. We also observed moderate to high heterogeneity and evidenced publication bias. Conclusions: Fluorescence-based devices have similar overall performance; however, better accuracy in detecting more advanced caries lesions has been observed. "], "author_display": ["Thais Gimenez", "Mariana Minatel Braga", "Daniela Procida Raggio", "Chris Deery", "David N. Ricketts", "Fausto Medeiros Mendes"], "article_type": "Research Article", "score": 0.48570687, "title_display": "Fluorescence-Based Methods for Detecting Caries Lesions: Systematic Review, Meta-Analysis and Sources of Heterogeneity", "publication_date": "2013-04-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0060421"}, {"journal": "PLoS ONE", "abstract": ["Introduction: The concepts of \u2018sex\u2019 and \u2018gender\u2019 are both of vital importance in medicine and health sciences. However, the meaning of these concepts has seldom been discussed in the medical literature. The aim of this study was to explore what the concepts of \u2018sex\u2019 and \u2018gender\u2019 meant for gender researchers based in a medical faculty. Methods: Sixteen researchers took part in focus group discussions. The analysis was performed in several steps. The participating researchers read the text and discussed ideas for analysis in national and international workshops. The data were analysed using qualitative content analysis. The authors performed independent preliminary analyses, which were further developed and intensively discussed between the authors. Results: The analysis of meanings of the concepts of \u2018sex\u2019 and \u2018gender\u2019 for gender researchers based in a medical faculty resulted in three categories; \u201cSex as more than biology\u201d, with the subcategories \u2018sex\u2019 is not simply biological, \u2018sex\u2019 as classification, and \u2018sex\u2019 as fluid and changeable; \u201dGender as a multiplicity of power-related constructions\u201d, with the subcategories: \u2018gender\u2019 as constructions, \u2018gender\u2019 power dimensions, and \u2018gender\u2019 as doing femininities and masculinities; \u201cSex and gender as interwoven\u201d, with the subcategories: \u2018sex\u2019 and \u2018gender\u2019 as inseparable and embodying \u2018sex\u2019 and \u2018gender\u2019. Conclusions: Gender researchers within medicine pointed out the importance of looking beyond a dichotomous view of the concepts of \u2018sex\u2019 and \u2018gender\u2019. The perception of the concepts was that \u2018sex\u2019 and \u2018gender\u2019 were intertwined. Further research is needed to explore how \u2018sex\u2019 and \u2018gender\u2019 interact. "], "author_display": ["Lena Alex", "Anncristine Fjellman Wiklund", "Berit Lundman", "Monica Christianson", "Anne Hammarstr\u00f6m"], "article_type": "Research Article", "score": 0.48568317, "title_display": "Beyond a Dichotomous View of the Concepts of \u2018Sex\u2019 and \u2018Gender\u2019 Focus Group Discussions among Gender Researchers at a Medical Faculty", "publication_date": "2012-11-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0050275"}, {"journal": "PLoS ONE", "abstract": ["\nIndirect estimation methodologies of the total fertility rate (TFR) have a long history within demography and have provided important techniques applied demographers can use when data is sparse or lacking. However new methodologies for approximating the total fertility rate have not been proposed in nearly 30 years. This study presents a novel method for indirectly approximating the total fertility rate using an algebraic rearrangement of the general fertility rate (GFR) through the known relationship between GFR and TFR. It then compares the proposed method to the well-known Bogue-Palmore method. These methods are compared in 196 countries and include overall errors as well as characteristics of the countries that contribute to fertility behavior. Additionally, these methods were compared geographically to find any geographical patterns. We find this novel method is not only simpler than the Bogue-Palmore method, requiring fewer data inputs, but also has reduced algebraic and absolute errors when compared with the Bogue-Palmore method and specifically outperforms the Bogue-Palmore method in developing countries. We find that our novel method may be useful estimation procedure for demographers.\n"], "author_display": ["Matt Hauer", "Jack Baker", "Warren Brown"], "article_type": "Research Article", "score": 0.4854853, "title_display": "Indirect Estimates of Total Fertility Rate Using Child Woman/Ratio: A Comparison with the Bogue-Palmore Method", "publication_date": "2013-06-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0067226"}, {"journal": "PLoS ONE", "abstract": ["Background: In randomised trials of medical interventions, the most reliable analysis follows the intention-to-treat (ITT) principle. However, the ITT analysis requires that missing outcome data have to be imputed. Different imputation techniques may give different results and some may lead to bias. In anti-obesity drug trials, many data are usually missing, and the most used imputation method is last observation carried forward (LOCF). LOCF is generally considered conservative, but there are more reliable methods such as multiple imputation (MI). Objectives: To compare four different methods of handling missing data in a 60-week placebo controlled anti-obesity drug trial on topiramate. Methods: We compared an analysis of complete cases with datasets where missing body weight measurements had been replaced using three different imputation methods: LOCF, baseline carried forward (BOCF) and MI. Results: 561 participants were randomised. Compared to placebo, there was a significantly greater weight loss with topiramate in all analyses: 9.5 kg (SE 1.17) in the complete case analysis (N\u200a=\u200a86), 6.8 kg (SE 0.66) using LOCF (N\u200a=\u200a561), 6.4 kg (SE 0.90) using MI (N\u200a=\u200a561) and 1.5 kg (SE 0.28) using BOCF (N\u200a=\u200a561). Conclusions: The different imputation methods gave very different results. Contrary to widely stated claims, LOCF did not produce a conservative (i.e., lower) efficacy estimate compared to MI. Also, LOCF had a lower SE than MI. "], "author_display": ["Anders W. J\u00f8rgensen", "Lars H. Lundstr\u00f8m", "J\u00f8rn Wetterslev", "Arne Astrup", "Peter C. G\u00f8tzsche"], "article_type": "Research Article", "score": 0.48525733, "title_display": "Comparison of Results from Different Imputation Techniques for Missing Data from an Anti-Obesity Drug Trial", "publication_date": "2014-11-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0111964"}, {"journal": "PLoS Neglected Tropical Diseases", "abstract": ["Background: This study designed and applied accessible yet systematic methods to generate baseline information about the patterns and structure of Canada's neglected tropical disease (NTD) research network; a network that, until recently, was formed and functioned on the periphery of strategic Canadian research funding. Methodology: Multiple methods were used to conduct this study, including: (1) a systematic bibliometric procedure to capture archival NTD publications and co-authorship data; (2) a country-level \u201ccore-periphery\u201d network analysis to measure and map the structure of Canada's NTD co-authorship network including its size, density, cliques, and centralization; and (3) a statistical analysis to test the correlation between the position of countries in Canada's NTD network (\u201ck-core measure\u201d) and the quantity and quality of research produced. Principal Findings: Over the past sixty years (1950\u20132010), Canadian researchers have contributed to 1,079 NTD publications, specializing in Leishmania, African sleeping sickness, and leprosy. Of this work, 70% of all first authors and co-authors (n\u200a=\u200a4,145) have been Canadian. Since the 1990s, however, a network of international co-authorship activity has been emerging, with representation of researchers from 62 different countries; largely researchers from OECD countries (e.g. United States and United Kingdom) and some non-OECD countries (e.g. Brazil and Iran). Canada has a core-periphery NTD international research structure, with a densely connected group of OECD countries and some African nations, such as Uganda and Kenya. Sitting predominantly on the periphery of this research network is a cluster of 16 non-OECD nations that fall within the lowest GDP percentile of the network. Conclusion/Significance: The publication specialties, composition, and position of NTD researchers within Canada's NTD country network provide evidence that while Canadian researchers currently remain the overall gatekeepers of the NTD research they generate; there is opportunity to leverage existing research collaborations and help advance regions and NTD areas that are currently under-developed. Author Summary: This study applies co-authorship network analysis to generate baseline information about the patterns and structure of Canada's neglected tropical disease (NTD) publication activity and research network. Researchers, public and private funders, not-for-profit organizations, and policy makers may use the methodology or study findings for targeting, monitoring, and assessing Canada's contribution to a research field that is ready for attention and advancements. Future studies could use the findings to comparatively analyze the emergence of specific NTD research amongst institutional networks or further examine attributes and mechanisms that support and impede Canadian involvement in NTD research production and collaborative North\u2013South research partnerships. "], "author_display": ["Kaye Phillips", "Jillian Clare Kohler", "Peter Pennefather", "Halla Thorsteinsdottir", "Joseph Wong"], "article_type": "Research Article", "score": 0.48521435, "title_display": "Canada's Neglected Tropical Disease Research Network: Who's in the Core\u2014Who's on the Periphery?", "publication_date": "2013-12-05T00:00:00Z", "eissn": "1935-2735", "id": "10.1371/journal.pntd.0002568"}, {"journal": "PLoS ONE", "abstract": ["Background: In recent years the \u201cnoninferiority\u201d trial has emerged as the new standard design for HIV drug development among antiretroviral patients often with a primary endpoint based on the difference in success rates between the two treatment groups. Different statistical methods have been introduced to provide confidence intervals for that difference. The main objective is to investigate whether the choice of the statistical method changes the conclusion of the trials. Methods: We presented 11 trials published in 2010 using a difference in proportions as the primary endpoint. In these trials, 5 different statistical methods have been used to estimate such confidence intervals. The five methods are described and applied to data from the 11 trials. The noninferiority of the new treatment is not demonstrated if the prespecified noninferiority margin it includes in the confidence interval of the treatment difference. Results: Results indicated that confidence intervals can be quite different according to the method used. In many situations, however, conclusions of the trials are not altered because point estimates of the treatment difference were too far from the prespecified noninferiority margins. Nevertheless, in few trials the use of different statistical methods led to different conclusions. In particular the use of \u201cexact\u201d methods can be very confusing. Conclusion: Statistical methods used to estimate confidence intervals in noninferiority trials have a strong impact on the conclusion of such trials. "], "author_display": ["Philippe Flandre"], "article_type": "Research Article", "score": 0.48433816, "title_display": "Statistical Methods in Recent HIV Noninferiority Trials: Reanalysis of 11 Trials", "publication_date": "2011-09-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0022871"}, {"journal": "PLoS Neglected Tropical Diseases", "abstract": ["Background: Measuring the impact of capacity strengthening support is a priority for the international development community. Several frameworks exist for monitoring and evaluating funding results and modalities. Based on its long history of support, we report on the impact of individual and institutional capacity strengthening programmes conducted by the UNICEF/UNDP/World Bank/WHO Special Programme for Research and Training in Tropical Diseases (TDR) and on the factors that influenced the outcome of its Research Capacity Strengthening (RCS) activities. Methodology and Principal Findings: A mix of qualitative and quantitative methods (questionnaires and in-depth interviews) was applied to a selected group of 128 individual and 20 institutional capacity development grant recipients that completed their training/projects between 2000 and 2008. A semi-structured interview was also conducted on site with scientists from four institutions. Most of the grantees, both individual and institutional, reported beneficial results from the grant. However, glaring inequities stemming from gender imbalances and a language bias towards English were identified. The study showed that skills improvement through training contributed to better formulation of research proposals, but not necessarily to improved project implementation or communication of results. Appreciation of the institutional grants' impact varied among recipient countries. The least developed countries saw the programmes as essential for supporting basic infrastructure and activities. Advanced developing countries perceived the research grants as complementary to available resources, and particularly suitable for junior researchers who were not yet able to compete for major international grants. Conclusion: The study highlights the need for a more equitable process to improve the effectiveness of health research capacity strengthening activities. Support should be tailored to the existing research capacity in disease endemic countries and should focus on strengthening national health research systems, particularly in the least developing countries. The engagement of stakeholders at country level would facilitate the design of more specific and comprehensive strategies based on local needs. Author Summary: The UNICEF/UNDP/World Bank /WHO Special Programme for Research and Training in Tropical Diseases (TDR) has over the 2000\u20132008 period supported the development of individual and institutional grants. Although the TDR research capacity development programmes has had a substantial impact on the development of tropical disease research and research capacity in disease endemic countries, a review of the lessons learnt and benefits of this approach has never been completed. A study was conducted to analyse TDR's inputs in research capacity in endemic countries and to assist TDR in the improvement of its future activities. An analysis (by variables of gender, age, language, country of origin, country of studies, type of grant, scientific interest etc) of the grantees that have benefited from TDR support in terms of their career development and research capacity, including any important financial implications was conducted. The study identify opportunities that are a broader relevance to objectives to international development agencies such as addressing inequities such as the gender imbalance language bias towards English and building a supportive research environment in DECs in which researchers can develop their scientific career and pursue their research. "], "author_display": ["Happiness Minja", "Christian Nsanzabana", "Christine Maure", "Axel Hoffmann", "Susan Rumisha", "Olumide Ogundahunsi", "Fabio Zicker", "Marcel Tanner", "Pascal Launois"], "article_type": "Research Article", "score": 0.48419586, "title_display": "Impact of Health Research Capacity Strengthening in Low- and Middle-Income Countries: The Case of WHO/TDR Programmes", "publication_date": "2011-10-11T00:00:00Z", "eissn": "1935-2735", "id": "10.1371/journal.pntd.0001351"}, {"abstract": ["\n        It has been suggested that pathway analysis can complement single-SNP analysis in exploring genomewide association data. Pathway analysis incorporates the available biological knowledge of genes and SNPs and is expected to improve the chances of revealing the underlying genetic architecture of complex traits. Methods for pathway analysis can be classified as competitive (enrichment) or self-contained (association) according to the hypothesis tested. Although association tests are statistically more powerful than enrichment tests they can be difficult to calibrate because biases in analysis accumulate across multiple SNPs or genes. Furthermore, enrichment tests can be more scientifically relevant than association tests, as they detect pathways with relatively more evidence for association than the remaining genes. Here we show how some well known association tests can be simply adapted to test for enrichment, and compare their performance to some established enrichment tests. We propose versions of the Adaptive Rank Truncated Product (ARTP), Tail Strength Measure and Fisher\u2019s combination of p-values for testing the enrichment null hypothesis. We compare the behaviour of these proposed methods with the established Hypergeometric Test and Gene-Set Enrichment Analysis (GSEA). The results of the simulation study show that the modified version of the ARTP method has generally the best performance across the situations considered. The methods were also applied for finding enriched pathways for body mass index (BMI) and platelet function phenotypes. The pathway analysis of BMI identified the Vasoactive Intestinal Peptide pathway as significantly associated with BMI. This pathway has been previously reported as associated with BMI and the risk of obesity. The ARTP method was the method that identified the largest number of enriched pathways across all tested pathway databases and phenotypes. The simulation and data application results are in agreement with previous work on association tests and suggests that the ARTP should be preferred for both enrichment and association testing.\n      "], "author_display": ["Marina Evangelou", "Augusto Rendon", "Willem H. Ouwehand", "Lorenz Wernisch", "Frank Dudbridge"], "article_type": "Research Article", "score": 0.48404944, "title_display": "Comparison of Methods for Competitive Tests of Pathway Analysis", "publication_date": "2012-07-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0041018"}, {"journal": "PLoS ONE", "abstract": ["Background: Many techniques are proposed for the quantification of tumor heterogeneity as an imaging biomarker for differentiation between tumor types, tumor grading, response monitoring and outcome prediction. However, in clinical practice these methods are barely used. This study evaluates the reported performance of the described methods and identifies barriers to their implementation in clinical practice. Methodology: The Ovid, Embase, and Cochrane Central databases were searched up to 20 September 2013. Heterogeneity analysis methods were classified into four categories, i.e., non-spatial methods (NSM), spatial grey level methods (SGLM), fractal analysis (FA) methods, and filters and transforms (F&T). The performance of the different methods was compared. Principal Findings: Of the 7351 potentially relevant publications, 209 were included. Of these studies, 58% reported the use of NSM, 49% SGLM, 10% FA, and 28% F&T. Differentiation between tumor types, tumor grading and/or outcome prediction was the goal in 87% of the studies. Overall, the reported area under the curve (AUC) ranged from 0.5 to 1 (median 0.87). No relation was found between the performance and the quantification methods used, or between the performance and the imaging modality. A negative correlation was found between the tumor-feature ratio and the AUC, which is presumably caused by overfitting in small datasets. Cross-validation was reported in 63% of the classification studies. Retrospective analyses were conducted in 57% of the studies without a clear description. Conclusions: In a research setting, heterogeneity quantification methods can differentiate between tumor types, grade tumors, and predict outcome and monitor treatment effects. To translate these methods to clinical practice, more prospective studies are required that use external datasets for validation: these datasets should be made available to the community to facilitate the development of new and improved methods. "], "author_display": ["Lejla Alic", "Wiro J. Niessen", "Jifke F. Veenland"], "article_type": "Research Article", "score": 0.48328227, "title_display": "Quantification of Heterogeneity as a Biomarker in Tumor Imaging: A Systematic Review", "publication_date": "2014-10-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0110300"}, {"journal": "PLOS ONE", "abstract": ["\nProfessional codes of ethics are social contracts among members of a professional group, which aim to instigate, encourage and nurture ethical behaviour and prevent professional misconduct, including research and publication. Despite the existence of codes of ethics, research misconduct remains a serious problem. A survey of codes of ethics from 795 professional organizations from the Illinois Institute of Technology\u2019s Codes of Ethics Collection showed that 182 of them (23%) used research integrity and research ethics terminology in their codes, with differences across disciplines: while the terminology was common in professional organizations in social sciences (82%), mental health (71%), sciences (61%), other organizations had no statements (construction trades, fraternal social organizations, real estate) or a few of them (management, media, engineering). A subsample of 158 professional organizations we judged to be directly involved in research significantly more often had statements on research integrity/ethics terminology than the whole sample: an average of 10.4% of organizations with a statement (95% CI = 10.4-23-5%) on any of the 27 research integrity/ethics terms compared to 3.3% (95% CI = 2.1\u20134.6%), respectively (P<0.001). Overall, 62% of all statements addressing research integrity/ethics concepts used prescriptive language in describing the standard of practice. Professional organizations should define research integrity and research ethics issues in their ethics codes and collaborate within and across disciplines to adequately address responsible conduct of research and meet contemporary needs of their communities.\n"], "author_display": ["Dubravka Komi\u0107", "Stjepan Ljudevit Maru\u0161i\u0107", "Ana Maru\u0161i\u0107"], "article_type": "Research Article", "score": 0.48255754, "title_display": "Research Integrity and Research Ethics in Professional Codes of Ethics: Survey of Terminology Used by Professional Organizations across Research Disciplines", "publication_date": "2015-07-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0133662"}, {"journal": "PLOS ONE", "abstract": ["\nThe compartment model analysis using medical imaging data is the well-established but extremely time consuming technique for quantifying the changes in microvascular physiology of targeted organs in clinical patients after antivascular therapies. In this paper, we present a first graphics processing unit-accelerated method for compartmental modeling of medical imaging data. Using this approach, we performed the analysis of dynamic contrast-enhanced magnetic resonance imaging data from bevacizumab-treated glioblastoma patients in less than one minute per slice without losing accuracy. This approach reduced the computation time by more than 120-fold comparing to a central processing unit-based method that performed the analogous analysis steps in serial and more than 17-fold comparing to the algorithm that optimized for central processing unit computation. The method developed in this study could be of significant utility in reducing the computational times required to assess tumor physiology from dynamic contrast-enhanced magnetic resonance imaging data in preclinical and clinical development of antivascular therapies and related fields.\n"], "author_display": ["Yu-Han H. Hsu", "Ziyin Huang", "Gregory Z. Ferl", "Chee M. Ng"], "article_type": "Research Article", "score": 0.48245412, "title_display": "GPU-Accelerated Compartmental Modeling Analysis of DCE-MRI Data from Glioblastoma Patients Treated with Bevacizumab", "publication_date": "2015-03-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0118421"}, {"journal": "PLoS ONE", "abstract": ["\n        Qualitative and quantitative methods are being developed to measure the impacts of research on society, but they suffer from serious drawbacks associated with linking a piece of research to its subsequent impacts. We have developed a method to derive impact scores for individual research publications according to their contribution to answering questions of quantified importance to end users of research. To demonstrate the approach, here we evaluate the impacts of research into means of conserving wild bee populations in the UK. For published papers, there is a weak positive correlation between our impact score and the impact factor of the journal. The process identifies publications that provide high quality evidence relating to issues of strong concern. It can also be used to set future research agendas.\n      "], "author_display": ["William J. Sutherland", "David Goulson", "Simon G. Potts", "Lynn V. Dicks"], "article_type": "Research Article", "score": 0.48242658, "title_display": "Quantifying the Impact and Relevance of Scientific Research", "publication_date": "2011-11-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0027537"}, {"journal": "PLoS ONE", "abstract": ["Background: The length of the huntingtin (HTT) CAG repeat is strongly correlated with both age at onset of Huntington\u2019s disease (HD) symptoms and age at death of HD patients. Dichotomous analysis comparing HD to controls is widely used to study the effects of HTT CAG repeat expansion. However, a potentially more powerful approach is a continuous analysis strategy that takes advantage of all of the different CAG lengths, to capture effects that are expected to be critical to HD pathogenesis. Methodology/Principal Findings: We used continuous and dichotomous approaches to analyze microarray gene expression data from 107 human control and HD lymphoblastoid cell lines. Of all probes found to be significant in a continuous analysis by CAG length, only 21.4% were so identified by a dichotomous comparison of HD versus controls. Moreover, of probes significant by dichotomous analysis, only 33.2% were also significant in the continuous analysis. Simulations revealed that the dichotomous approach would require substantially more than 107 samples to either detect 80% of the CAG-length correlated changes revealed by continuous analysis or to reduce the rate of significant differences that are not CAG length-correlated to 20% (n\u200a=\u200a133 or n\u200a=\u200a206, respectively). Given the superior power of the continuous approach, we calculated the correlation structure between HTT CAG repeat lengths and gene expression levels and created a freely available searchable website, \u201cHD CAGnome,\u201d that allows users to examine continuous relationships between HTT CAG and expression levels of \u223c20,000 human genes. Conclusions/Significance: Our results reveal limitations of dichotomous approaches compared to the power of continuous analysis to study a disease where human genotype-phenotype relationships strongly support a role for a continuum of CAG length-dependent changes. The compendium of HTT CAG length-gene expression level relationships found at the HD CAGnome now provides convenient routes for discovery of candidates influenced by the HD mutation. "], "author_display": ["Ekaterina I. Galkina", "Aram Shin", "Kathryn R. Coser", "Toshi Shioda", "Isaac S. Kohane", "Ihn Sik Seong", "Vanessa C. Wheeler", "James F. Gusella", "Marcy E. MacDonald", "Jong-Min Lee"], "article_type": "Research Article", "score": 0.48236907, "title_display": "HD CAGnome: A Search Tool for Huntingtin CAG Repeat Length-Correlated Genes", "publication_date": "2014-04-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0095556"}, {"journal": "PLoS ONE", "abstract": ["\nLists of clinical codes are the foundation for research undertaken using electronic medical records (EMRs). If clinical code lists are not available, reviewers are unable to determine the validity of research, full study replication is impossible, researchers are unable to make effective comparisons between studies, and the construction of new code lists is subject to much duplication of effort. Despite this, the publication of clinical codes is rarely if ever a requirement for obtaining grants, validating protocols, or publishing research. In a representative sample of 450 EMR primary research articles indexed on PubMed, we found that only 19 (5.1%) were accompanied by a full set of published clinical codes and 32 (8.6%) stated that code lists were available on request. To help address these problems, we have built an online repository where researchers using EMRs can upload and download lists of clinical codes. The repository will enable clinical researchers to better validate EMR studies, build on previous code lists and compare disease definitions across studies. It will also assist health informaticians in replicating database studies, tracking changes in disease definitions or clinical coding practice through time and sharing clinical code information across platforms and data sources as research objects.\n"], "author_display": ["David A. Springate", "Evangelos Kontopantelis", "Darren M. Ashcroft", "Ivan Olier", "Rosa Parisi", "Edmore Chamapiwa", "David Reeves"], "article_type": "Research Article", "score": 0.48235974, "title_display": "ClinicalCodes: An Online Clinical Codes Repository to Improve the Validity and Reproducibility of Research Using Electronic Medical Records", "publication_date": "2014-06-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0099825"}, {"journal": "PLOS ONE", "abstract": ["\nThe human serum proteome has been extensively screened for biomarkers. However, the large dynamic range of protein concentrations in serum and the presence of highly abundant and large molecular weight proteins, make identification and detection changes in the amount of low-molecular weight proteins (LMW, molecular weight \u2264 30kDa) difficult. Here, we developed a gel-filter method including four layers of different concentration of tricine SDS-PAGE-based gels to block high-molecular weight proteins and enrich LMW proteins. By utilizing this method, we identified 1,576 proteins (n = 2) from 10 \u03bcL serum. Among them, 559 (n = 2) proteins belonged to LMW proteins. Furthermore, this gel-filter method could identify 67.4% and 39.8% more LMW proteins than that in representative methods of glycine SDS-PAGE and optimized-DS, respectively. By utilizing SILAC-AQUA approach with labeled recombinant protein as internal standard, the recovery rate for GST spiked in serum during the treatment of gel-filter, optimized-DS, and ProteoMiner was 33.1 \u00b1 0.01%, 18.7 \u00b1 0.01% and 9.6 \u00b1 0.03%, respectively. These results demonstrate that the gel-filter method offers a rapid, highly reproducible and efficient approach for screening biomarkers from serum through proteomic analyses.\n"], "author_display": ["Lingsheng Chen", "Linhui Zhai", "Yanchang Li", "Ning Li", "Chengpu Zhang", "Lingyan Ping", "Lei Chang", "Junzhu Wu", "Xiangping Li", "Deshun Shi", "Ping Xu"], "article_type": "Research Article", "score": 0.48196396, "title_display": "Development of Gel-Filter Method for High Enrichment of Low-Molecular Weight Proteins from Serum", "publication_date": "2015-02-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0115862"}, {"journal": "PLOS ONE", "abstract": ["Background: Cancer health disparities research depends on access to biospecimens from diverse racial/ethnic populations. This multimethodological study, using mixed methods for quantitative and qualitative analysis of survey results, assessed barriers, concerns, and practices for sharing biospecimens/data among researchers working with biospecimens from minority populations in a 5 state region of the United States (Arizona, Colorado, New Mexico, Oklahoma, and Texas). The ultimate goals of this research were to understand data sharing barriers among biomedical researchers; guide strategies to increase participation in biospecimen research; and strengthen collaborative opportunities among researchers. Methods and Population: Email invitations to anonymous participants (n = 605 individuals identified by the NIH RePORT database), resulted in 112 responses. The survey assessed demographics, specimen collection data, and attitudes about virtual biorepositories. Respondents were primarily principal investigators at PhD granting institutions (91.1%) conducting basic (62.3%) research; most were non-Hispanic White (63.4%) and men (60.6%). The low response rate limited the statistical power of the analyses, further the number of respondents for each survey question was variable. Results: Findings from this study identified barriers to biospecimen research, including lack of access to sufficient biospecimens, and limited availability of diverse tissue samples. Many of these barriers can be attributed to poor annotation of biospecimens, and researchers\u2019 unwillingness to share existing collections. Addressing these barriers to accessing biospecimens is essential to combating cancer in general and cancer health disparities in particular. This study confirmed researchers\u2019 willingness to participate in a virtual biorepository (n = 50 respondents agreed). However, researchers in this region listed clear specifications for establishing and using such a biorepository: specifications related to standardized procedures, funding, and protections of human subjects and intellectual property. The results help guide strategies to increase data sharing behaviors and to increase participation of researchers with multiethnic biospecimen collections in collaborative research endeavors Conclusions: Data sharing by researchers is essential to leveraging knowledge and resources needed for the advancement of research on cancer health disparities. Although U.S. funding entities have guidelines for data and resource sharing, future efforts should address researcher preferences in order to promote collaboration to address cancer health disparities. "], "author_display": ["Mai H. Oushy", "Rebecca Palacios", "Alan E. C. Holden", "Amelie G. Ramirez", "Kipling J. Gallion", "Mary A. O\u2019Connell"], "article_type": "Research Article", "score": 0.48141065, "title_display": "To Share or Not to Share? A Survey of Biomedical Researchers in the U.S. Southwest, an Ethnically Diverse Region", "publication_date": "2015-09-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0138239"}, {"journal": "PLoS ONE", "abstract": ["\n        The production and turnover of fine roots play substantial roles in the biogeochemical cycles of terrestrial ecosystems. However, the disparity among the estimates of both production and turnover, particularly due to technical limitations, has been debated for several decades. Here, we conducted a meta-analysis to compare published estimates of fine root production and turnover rates derived from different methods at the same sites and at the same sampling time. On average, the estimates of fine root production and turnover rates were 87% and 124% higher, respectively, by indirect methods than by direct methods. The substantially higher fine root production and turnover estimated by indirect methods, on which most global carbon models are based, indicate the necessity of re-assessing the global carbon model predictions for atmospheric carbon sequestration in soils as a result of the production and turnover of fine roots.\n      "], "author_display": ["Z. Y. Yuan", "Han Y. H. Chen"], "article_type": "Research Article", "score": 0.48137894, "title_display": "Indirect Methods Produce Higher Estimates of Fine Root Production and Turnover Rates than Direct Methods", "publication_date": "2012-11-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0048989"}, {"journal": "PLoS ONE", "abstract": ["Introduction: Systematic reviewer authors intending to include all randomized participants in their meta-analyses need to make assumptions about the outcomes of participants with missing data. Objective: The objective of this paper is to provide systematic reviewer authors with a relatively simple guidance for addressing dichotomous data for participants excluded from analyses of randomized trials. Methods: This guide is based on a review of the Cochrane handbook and published methodological research. The guide deals with participants excluded from the analysis who were considered \u2018non-adherent to the protocol\u2019 but for whom data are available, and participants with missing data. Results: Systematic reviewer authors should include data from \u2018non-adherent\u2019 participants excluded from the primary study authors' analysis but for whom data are available. For missing, unavailable participant data, authors may conduct a complete case analysis (excluding those with missing data) as the primary analysis. Alternatively, they may conduct a primary analysis that makes plausible assumptions about the outcomes of participants with missing data. When the primary analysis suggests important benefit, sensitivity meta-analyses using relatively extreme assumptions that may vary in plausibility can inform the extent to which risk of bias impacts the confidence in the results of the primary analysis. The more plausible assumptions draw on the outcome event rates within the trial or in all trials included in the meta-analysis. The proposed guide does not take into account the uncertainty associated with assumed events. Conclusions: This guide proposes methods for handling participants excluded from analyses of randomized trials. These methods can help in establishing the extent to which risk of bias impacts meta-analysis results. "], "author_display": ["Elie A. Akl", "Bradley C. Johnston", "Pablo Alonso-Coello", "Ignacio Neumann", "Shanil Ebrahim", "Matthias Briel", "Deborah J. Cook", "Gordon H. Guyatt"], "article_type": "Research Article", "score": 0.4796754, "title_display": "Addressing Dichotomous Data for Participants Excluded from Trial Analysis: A Guide for Systematic Reviewers", "publication_date": "2013-02-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0057132"}, {"journal": "PLoS ONE", "abstract": ["\nTargeted resequencing by massively parallel sequencing has become an effective and affordable way to survey small to large portions of the genome for genetic variation. Despite the rapid development in open source software for analysis of such data, the practical implementation of these tools through construction of sequencing analysis pipelines still remains a challenging and laborious activity, and a major hurdle for many small research and clinical laboratories. We developed TREVA (Targeted REsequencing Virtual Appliance), making pre-built pipelines immediately available as a virtual appliance. Based on virtual machine technologies, TREVA is a solution for rapid and efficient deployment of complex bioinformatics pipelines to laboratories of all sizes, enabling reproducible results. The analyses that are supported in TREVA include: somatic and germline single-nucleotide and insertion/deletion variant calling, copy number analysis, and cohort-based analyses such as pathway and significantly mutated genes analyses. TREVA is flexible and easy to use, and can be customised by Linux-based extensions if required. TREVA can also be deployed on the cloud (cloud computing), enabling instant access without investment overheads for additional hardware. TREVA is available at http://bioinformatics.petermac.org/treva/.\n"], "author_display": ["Jason Li", "Maria A. Doyle", "Isaam Saeed", "Stephen Q. Wong", "Victoria Mar", "David L. Goode", "Franco Caramia", "Ken Doig", "Georgina L. Ryland", "Ella R. Thompson", "Sally M. Hunter", "Saman K. Halgamuge", "Jason Ellul", "Alexander Dobrovic", "Ian G. Campbell", "Anthony T. Papenfuss", "Grant A. McArthur", "Richard W. Tothill"], "article_type": "Research Article", "score": 0.47888637, "title_display": "Bioinformatics Pipelines for Targeted Resequencing and Whole-Exome Sequencing of Human and Mouse Genomes: A Virtual Appliance Approach for Instant Deployment", "publication_date": "2014-04-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0095217"}, {"journal": "PLoS Medicine", "abstract": ["Background: Blinding is a cornerstone of therapeutic evaluation because lack of blinding can bias treatment effect estimates. An inventory of the blinding methods would help trialists conduct high-quality clinical trials and readers appraise the quality of results of published trials. We aimed to systematically classify and describe methods to establish and maintain blinding of patients and health care providers and methods to obtain blinding of outcome assessors in randomized controlled trials of pharmacologic treatments. Methods and Findings: We undertook a systematic review of all reports of randomized controlled trials assessing pharmacologic treatments with blinding published in 2004 in high impact-factor journals from Medline and the Cochrane Methodology Register. We used a standardized data collection form to extract data. The blinding methods were classified according to whether they primarily (1) established blinding of patients or health care providers, (2) maintained the blinding of patients or health care providers, and (3) obtained blinding of assessors of the main outcomes. We identified 819 articles, with 472 (58%) describing the method of blinding. Methods to establish blinding of patients and/or health care providers concerned mainly treatments provided in identical form, specific methods to mask some characteristics of the treatments (e.g., added flavor or opaque coverage), or use of double dummy procedures or simulation of an injection. Methods to avoid unblinding of patients and/or health care providers involved use of active placebo, centralized assessment of side effects, patients informed only in part about the potential side effects of each treatment, centralized adapted dosage, or provision of sham results of complementary investigations. The methods reported for blinding outcome assessors mainly relied on a centralized assessment of complementary investigations, clinical examination (i.e., use of video, audiotape, or photography), or adjudication of clinical events. Conclusions: This review classifies blinding methods and provides a detailed description of methods that could help trialists overcome some barriers to blinding in clinical trials and readers interpret the quality of pharmalogic trials. \n        Following a systematic review of all reports of randomized controlled trials assessing pharmacologic treatments involving blinding, a classification of blinding methods is proposed.\n      Background.: In evidence-based medicine, good-quality randomized controlled trials are generally considered to be the most reliable source of information about the effects of different treatments, such as drugs. In a randomized trial, patients are assigned to receive one treatment or another by the play of chance. This technique helps makes sure that the two groups of patients receiving the different treatments are equivalent at the start of the trial. Proper randomization also prevents doctors from controlling or affecting which treatment patients get, which could distort the results. An additional tool that is also used to make trials more precise is \u201cblinding.\u201d Blinding involves taking steps to prevent patients, doctors, or other people involved in the trial (e.g., those people recording measurements) from finding out which patients got what treatment. Properly done, blinding should make sure the results of a trial are more accurate. This is because in an unblinded study, participants may respond better if they know they have received a promising new treatment (or worse if they only got placebo or an old drug); doctors may \u201cwant\u201d a particular treatment to do better in the trial, and unthinking bias could creep into their measurements or actions; the same applies for practitioners and researchers who record patients' outcomes in the trial. However, blinding is not a simple, single step; the people carrying out the trial often have to set up a variety of different procedures that depend on the type of trial that is being done. Why Was This Study Done?: The researchers here wanted to thoroughly examine different methods that have been used to achieve blinding in randomized trials of drug treatments, and to describe and classify them. They hoped that a better understanding of the different blinding methods would help people doing trials to design better trials in the future, and also help readers to interpret the quality of trials that had been done. What Did the Researchers Do and Find?: This group of researchers conducted what is called a \u201csystematic review.\u201d They systematically searched the published medical literature to find all randomized, blinded drug trials published in 2004 in a number of different \u201chigh-impact\u201d journals (journals whose articles are often mentioned in other articles). Then, the researchers classified information from the published trial reports. The researchers ended up with 819 trial reports, and nearly 60% of them described how blinding was done. Their classification of blinding was divided up into three main areas. First, they detailed methods used to hide which drugs are given to particular patients, such as preparing identically appearing treatments; using strong flavors to mask taste; matching the colors of pills; using saline injections and so on. Second, they described a number of methods that could be used to reduce the risk of unblinding (of doctors or patients), such as using an \u201cactive placebo\u201d (a sugar pill that mimics some of the expected side effects of the drug treatment). Finally, they defined methods for blinded measurement of outcomes (such as using a central committee to collect data). What Do These Findings Mean?: The researchers' classification will help people to work out how different techniques can be used to achieve, and keep, blinding in a trial. This will assist others to understand whether any particular trial was likely to have been blinded properly, and therefore work out whether the results are reliable. The researchers also suggest that, generally, blinding methods are not described in enough detail in published scientific papers, and recommend that guidelines for describing results of randomized trials be improved. Additional Information.: Please access these Web sites via the online version of this summary at http://dx.doi.org/10.1371/journal.pmed.0030425. "], "author_display": ["Isabelle Boutron", "Candice Estellat", "Lydia Guittet", "Agnes Dechartres", "David L Sackett", "Asbj\u00f8rn Hr\u00f3bjartsson", "Philippe Ravaud"], "article_type": "Research Article", "score": 0.47839636, "title_display": "Methods of Blinding in Reports of Randomized Controlled Trials Assessing Pharmacologic Treatments: A Systematic Review", "publication_date": "2006-10-31T00:00:00Z", "eissn": "1549-1676", "id": "10.1371/journal.pmed.0030425"}, {"journal": "PLoS ONE", "abstract": ["\nTraditional permutation (TradPerm) tests are usually considered the gold standard for multiple testing corrections. However, they can be difficult to complete for the meta-analyses of genetic association studies based on multiple single nucleotide polymorphism loci as they depend on individual-level genotype and phenotype data to perform random shuffles, which are not easy to obtain. Most meta-analyses have therefore been performed using summary statistics from previously published studies. To carry out a permutation using only genotype counts without changing the size of the TradPerm P-value, we developed a Monte Carlo permutation (MCPerm) method. First, for each study included in the meta-analysis, we used a two-step hypergeometric distribution to generate a random number of genotypes in cases and controls. We then carried out a meta-analysis using these random genotype data. Finally, we obtained the corrected permutation P-value of the meta-analysis by repeating the entire process N times. We used five real datasets and five simulation datasets to evaluate the MCPerm method and our results showed the following: (1) MCPerm requires only the summary statistics of the genotype, without the need for individual-level data; (2) Genotype counts generated by our two-step hypergeometric distributions had the same distributions as genotype counts generated by shuffling; (3) MCPerm had almost exactly the same permutation P-values as TradPerm (r\u200a=\u200a0.999; P<2.2e-16); (4) The calculation speed of MCPerm is much faster than that of TradPerm. In summary, MCPerm appears to be a viable alternative to TradPerm, and we have developed it as a freely available R package at CRAN: http://cran.r-project.org/web/packages/MCPerm/index.html.\n"], "author_display": ["Yongshuai Jiang", "Lanying Zhang", "Fanwu Kong", "Mingming Zhang", "Hongchao Lv", "Guiyou Liu", "Mingzhi Liao", "Rennan Feng", "Jin Li", "Ruijie Zhang"], "article_type": "Research Article", "score": 0.47802907, "title_display": "MCPerm: A Monte Carlo Permutation Method for Accurately Correcting the Multiple Testing in a Meta-Analysis of Genetic Association Studies", "publication_date": "2014-02-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0089212"}, {"journal": "PLoS ONE", "abstract": ["\nFor scientific, ethical and economic reasons, experiments involving animals should be appropriately designed, correctly analysed and transparently reported. This increases the scientific validity of the results, and maximises the knowledge gained from each experiment. A minimum amount of relevant information must be included in scientific publications to ensure that the methods and results of a study can be reviewed, analysed and repeated. Omitting essential information can raise scientific and ethical concerns. We report the findings of a systematic survey of reporting, experimental design and statistical analysis in published biomedical research using laboratory animals. Medline and EMBASE were searched for studies reporting research on live rats, mice and non-human primates carried out in UK and US publicly funded research establishments. Detailed information was collected from 271 publications, about the objective or hypothesis of the study, the number, sex, age and/or weight of animals used, and experimental and statistical methods. Only 59% of the studies stated the hypothesis or objective of the study and the number and characteristics of the animals used. Appropriate and efficient experimental design is a critical component of high-quality science. Most of the papers surveyed did not use randomisation (87%) or blinding (86%), to reduce bias in animal selection and outcome assessment. Only 70% of the publications that used statistical methods described their methods and presented the results with a measure of error or variability. This survey has identified a number of issues that need to be addressed in order to improve experimental design and reporting in publications describing research using animals. Scientific publication is a powerful and important source of information; the authors of scientific publications therefore have a responsibility to describe their methods and results comprehensively, accurately and transparently, and peer reviewers and journal editors share the responsibility to ensure that published studies fulfil these criteria.\n"], "author_display": ["Carol Kilkenny", "Nick Parsons", "Ed Kadyszewski", "Michael F. W. Festing", "Innes C. Cuthill", "Derek Fry", "Jane Hutton", "Douglas G. Altman"], "article_type": "Research Article", "score": 0.47755125, "title_display": "Survey of the Quality of Experimental Design, Statistical Analysis and Reporting of Research Using Animals", "publication_date": "2009-11-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0007824"}, {"journal": "PLoS ONE", "abstract": ["\nSystematic reviews that employ network meta-analysis are undertaken and published with increasing frequency while related statistical methodology is evolving. Future statistical developments and evaluation of the existing methodologies could be motivated by the characteristics of the networks of interventions published so far in order to tackle real rather than theoretical problems. Based on the recently formed network meta-analysis literature we aim to provide an insight into the characteristics of networks in healthcare research. We searched PubMed until end of 2012 for meta-analyses that used any form of indirect comparison. We collected data from networks that compared at least four treatments regarding their structural characteristics as well as characteristics of their analysis. We then conducted a descriptive analysis of the various network characteristics. We included 186 networks of which 35 (19%) were star-shaped (treatments were compared to a common comparator but not between themselves). The median number of studies per network was 21 and the median number of treatments compared was 6. The majority (85%) of the non-star shaped networks included at least one multi-arm study. Synthesis of data was primarily done via network meta-analysis fitted within a Bayesian framework (113 (61%) networks). We were unable to identify the exact method used to perform indirect comparison in a sizeable number of networks (18 (9%)). In 32% of the networks the investigators employed appropriate statistical methods to evaluate the consistency assumption; this percentage is larger among recently published articles. Our descriptive analysis provides useful information about the characteristics of networks of interventions published the last 16 years and the methods for their analysis. Although the validity of network meta-analysis results highly depends on some basic assumptions, most authors did not report and evaluate them adequately. Reviewers and editors need to be aware of these assumptions and insist on their reporting and accuracy.\n"], "author_display": ["Adriani Nikolakopoulou", "Anna Chaimani", "Areti Angeliki Veroniki", "Haris S. Vasiliadis", "Christopher H. Schmid", "Georgia Salanti"], "article_type": "Research Article", "score": 0.47624964, "title_display": "Characteristics of Networks of Interventions: A Description of a Database of 186 Published Networks", "publication_date": "2014-01-22T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0086754"}, {"journal": "PLoS ONE", "abstract": ["\nDNA microarray technologies are used extensively to profile the expression levels of thousands of genes under various conditions, yielding extremely large data-matrices. Thus, analyzing this information and extracting biologically relevant knowledge becomes a considerable challenge. A classical approach for tackling this challenge is to use clustering (also known as one-way clustering) methods where genes (or respectively samples) are grouped together based on the similarity of their expression profiles across the set of all samples (or respectively genes). An alternative approach is to develop biclustering methods to identify local patterns in the data. These methods extract subgroups of genes that are co-expressed across only a subset of samples and may feature important biological or medical implications. In this study we evaluate 13 biclustering and 2 clustering (k-means and hierarchical) methods. We use several approaches to compare their performance on two real gene expression data sets. For this purpose we apply four evaluation measures in our analysis: (1) we examine how well the considered (bi)clustering methods differentiate various sample types; (2) we evaluate how well the groups of genes discovered by the (bi)clustering methods are annotated with similar Gene Ontology categories; (3) we evaluate the capability of the methods to differentiate genes that are known to be specific to the particular sample types we study and (4) we compare the running time of the algorithms. In the end, we conclude that as long as the samples are well defined and annotated, the contamination of the samples is limited, and the samples are well replicated, biclustering methods such as Plaid and SAMBA are useful for discovering relevant subsets of genes and samples.\n"], "author_display": ["Ali Oghabian", "Sami Kilpinen", "Sampsa Hautaniemi", "Elena Czeizler"], "article_type": "Research Article", "score": 0.47607628, "title_display": "Biclustering Methods: Biological Relevance and Application in Gene Expression Analysis", "publication_date": "2014-03-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0090801"}, {"journal": "PLOS ONE", "abstract": ["\nA new method of DTM construction based on quadrangular irregular networks (QINs) that considers all the original data points and has a topological matrix is presented. A numerical test and a real-world example are used to comparatively analyse the accuracy of QINs against classical interpolation methods and other DTM representation methods, including SPLINE, KRIGING and triangulated irregular networks (TINs). The numerical test finds that the QIN method is the second-most accurate of the four methods. In the real-world example, DTMs are constructed using QINs and the three classical interpolation methods. The results indicate that the QIN method is the most accurate method tested. The difference in accuracy rank seems to be caused by the locations of the data points sampled. Although the QIN method has drawbacks, it is an alternative method for DTM construction.\n"], "author_display": ["Mengjun Kang", "Mingjun Wang", "Qingyun Du"], "article_type": "Research Article", "score": 0.47564593, "title_display": "A Method of DTM Construction Based on Quadrangular Irregular Networks and Related Error Analysis", "publication_date": "2015-05-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0127592"}, {"journal": "PLoS ONE", "abstract": ["Background: Recently, Cipriani and colleagues examined the relative efficacy of 12 new-generation antidepressants on major depression using network meta-analytic methods. They found that some of these medications outperformed others in patient response to treatment. However, several methodological criticisms have been raised about network meta-analysis and Cipriani's analysis in particular which creates the concern that the stated superiority of some antidepressants relative to others may be unwarranted. Materials and Methods: A Monte Carlo simulation was conducted which involved replicating Cipriani's network meta-analysis under the null hypothesis (i.e., no true differences between antidepressants). The following simulation strategy was implemented: (1) 1000 simulations were generated under the null hypothesis (i.e., under the assumption that there were no differences among the 12 antidepressants), (2) each of the 1000 simulations were network meta-analyzed, and (3) the total number of false positive results from the network meta-analyses were calculated. Findings: Greater than 7 times out of 10, the network meta-analysis resulted in one or more comparisons that indicated the superiority of at least one antidepressant when no such true differences among them existed. Interpretation: Based on our simulation study, the results indicated that under identical conditions to those of the 117 RCTs with 236 treatment arms contained in Cipriani et al.'s meta-analysis, one or more false claims about the relative efficacy of antidepressants will be made over 70% of the time. As others have shown as well, there is little evidence in these trials that any antidepressant is more effective than another. The tendency of network meta-analyses to generate false positive results should be considered when conducting multiple comparison analyses. "], "author_display": ["A. C. Del Re", "Glen I. Spielmans", "Christoph Fl\u00fcckiger", "Bruce E. Wampold"], "article_type": "Research Article", "score": 0.47481343, "title_display": "Efficacy of New Generation Antidepressants: Differences Seem Illusory", "publication_date": "2013-06-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0063509"}, {"journal": "PLoS ONE", "abstract": ["\nWhile genome-wide gene expression data are generated at an increasing rate, the repertoire of approaches for pattern discovery in these data is still limited. Identifying subtle patterns of interest in large amounts of data (tens of thousands of profiles) associated with a certain level of noise remains a challenge. A microarray time series was recently generated to study the transcriptional program of the mouse segmentation clock, a biological oscillator associated with the periodic formation of the segments of the body axis. A method related to Fourier analysis, the Lomb-Scargle periodogram, was used to detect periodic profiles in the dataset, leading to the identification of a novel set of cyclic genes associated with the segmentation clock. Here, we applied to the same microarray time series dataset four distinct mathematical methods to identify significant patterns in gene expression profiles. These methods are called: Phase consistency, Address reduction, Cyclohedron test and Stable persistence, and are based on different conceptual frameworks that are either hypothesis- or data-driven. Some of the methods, unlike Fourier transforms, are not dependent on the assumption of periodicity of the pattern of interest. Remarkably, these methods identified blindly the expression profiles of known cyclic genes as the most significant patterns in the dataset. Many candidate genes predicted by more than one approach appeared to be true positive cyclic genes and will be of particular interest for future research. In addition, these methods predicted novel candidate cyclic genes that were consistent with previous biological knowledge and experimental validation in mouse embryos. Our results demonstrate the utility of these novel pattern detection strategies, notably for detection of periodic profiles, and suggest that combining several distinct mathematical approaches to analyze microarray datasets is a valuable strategy for identifying genes that exhibit novel, interesting transcriptional patterns.\n"], "author_display": ["Mary-Lee Dequ\u00e9ant", "Sebastian Ahnert", "Herbert Edelsbrunner", "Thomas M. A. Fink", "Earl F. Glynn", "Gaye Hattem", "Andrzej Kudlicki", "Yuriy Mileyko", "Jason Morton", "Arcady R. Mushegian", "Lior Pachter", "Maga Rowicka", "Anne Shiu", "Bernd Sturmfels", "Olivier Pourqui\u00e9"], "article_type": "Research Article", "score": 0.47476488, "title_display": "Comparison of Pattern Detection Methods in Microarray Time Series of the Segmentation Clock", "publication_date": "2008-08-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0002856"}, {"journal": "PLoS ONE", "abstract": ["\nWe analyzed the impact of a requirement introduced in December 2010 that all applicants to the Canadian Institutes of Health Research indicate whether their research designs accounted for sex or gender. We aimed to inform research policy by understanding the extent to which applicants across health research disciplines accounted for sex and gender. We conducted a descriptive statistical analysis to identify trends in application data from three research funding competitions (December 2010, June 2011, and December 2011) (N\u200a=\u200a1459). We also conducted a qualitative thematic analysis of applicants' responses. Here we show that the proportion of applicants responding affirmatively to the questions on sex and gender increased over time (48% in December 2011, compared to 26% in December 2010). Biomedical researchers were least likely to report accounting for sex and gender. Analysis by discipline-specific peer review panel showed variation in the likelihood that a given panel will fund grants with a stated focus on sex or gender. These findings suggest that mandatory questions are one way of encouraging the uptake of sex and gender in health research, yet there remain persistent disparities across disciplines. These disparities represent opportunities for policy intervention by health research funders.\n"], "author_display": ["Joy Johnson", "Zena Sharman", "Bilkis Vissandj\u00e9e", "Donna E. Stewart"], "article_type": "Research Article", "score": 0.4745948, "title_display": "Does a Change in Health Research Funding Policy Related to the Integration of Sex and Gender Have an Impact?", "publication_date": "2014-06-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0099900"}, {"journal": "PLoS ONE", "abstract": ["\nRegional-based association analysis instead of individual testing of each SNP was introduced in genome-wide association studies to increase the power of gene mapping, especially for rare genetic variants. For regional association tests, the kernel machine-based regression approach was recently proposed as a more powerful alternative to collapsing-based methods. However, the vast majority of existing algorithms and software for the kernel machine-based regression are applicable only to unrelated samples. In this paper, we present a new method for the kernel machine-based regression association analysis of quantitative traits in samples of related individuals. The method is based on the GRAMMAR+ transformation of phenotypes of related individuals, followed by use of existing kernel machine-based regression software for unrelated samples. We compared the performance of kernel-based association analysis on the material of the Genetic Analysis Workshop 17 family sample and real human data by using our transformation, the original untransformed trait, and environmental residuals. We demonstrated that only the GRAMMAR+ transformation produced type I errors close to the nominal value and that this method had the highest empirical power. The new method can be applied to analysis of related samples by using existing software for kernel-based association analysis developed for unrelated samples.\n"], "author_display": ["Nadezhda M. Belonogova", "Gulnara R. Svishcheva", "Cornelia M. van Duijn", "Yurii S. Aulchenko", "Tatiana I. Axenovich"], "article_type": "Research Article", "score": 0.4740572, "title_display": "Region-Based Association Analysis of Human Quantitative Traits in Related Individuals", "publication_date": "2013-06-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0065395"}, {"journal": "PLoS Computational Biology", "abstract": ["\nIt is well known that Affymetrix microarrays are widely used to predict genome-wide gene expression and genome-wide genetic polymorphisms from RNA and genomic DNA hybridization experiments, respectively. It has recently been proposed to integrate the two predictions by use of RNA microarray data only. Although the ability to detect single feature polymorphisms (SFPs) from RNA microarray data has many practical implications for genome study in both sequenced and unsequenced species, it raises enormous challenges for statistical modelling and analysis of microarray gene expression data for this objective. Several methods are proposed to predict SFPs from the gene expression profile. However, their performance is highly vulnerable to differential expression of genes. The SFPs thus predicted are eventually a reflection of differentially expressed genes rather than genuine sequence polymorphisms. To address the problem, we developed a novel statistical method to separate the binding affinity between a transcript and its targeting probe and the parameter measuring transcript abundance from perfect-match hybridization values of Affymetrix gene expression data. We implemented a Bayesian approach to detect SFPs and to genotype a segregating population at the detected SFPs. Based on analysis of three Affymetrix microarray datasets, we demonstrated that the present method confers a significantly improved robustness and accuracy in detecting the SFPs that carry genuine sequence polymorphisms when compared to its rivals in the literature. The method developed in this paper will provide experimental genomicists with advanced analytical tools for appropriate and efficient analysis of their microarray experiments and biostatisticians with insightful interpretation of Affymetrix microarray data.\nAuthor Summary: One of the ultimate goals of genomics is to explore structural and functional variations of all genes in a genome. High-density oligo-microarray techniques enable prediction of genome-wide gene expression and genome-wide genetic polymorphisms from using RNA and genomic DNA samples, respectively. A recent proposal to integrate the two predictions by use of RNA microarray data alone has great practical implications in genomics. However, it is essential but very challenging to develop an appropriate analytical method for detecting genetic polymorphisms (SFPs) from RNA expression data, which are inherently coupled with various sources of biological and technical variations. This paper presents a novel statistical approach to detect SFPs from gene expression data. We demonstrated that the new method is significantly more robust to variation due to differential expression of genes and improves the reliability of calling SFPs that bear genuine sequence polymorphisms than the other five methods in the mainstream literature on SFP prediction from microarray data. The improved predictability of detecting SFPs not only confers accuracy in evaluating gene expression from microarray information, but also opens up an opportunity to integrate structural and functional analyses by using only one set of microarray data. "], "author_display": ["Minghui Wang", "Xiaohua Hu", "Gang Li", "Lindsey J. Leach", "Elena Potokina", "Arnis Druka", "Robbie Waugh", "Michael J. Kearsey", "Zewei Luo"], "article_type": "Research Article", "score": 0.47395426, "title_display": "Robust Detection and Genotyping of Single Feature Polymorphisms from Gene Expression Data", "publication_date": "2009-03-13T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1000317"}, {"journal": "PLOS ONE", "abstract": ["\nIn order to improve the performance of voltage source converter-high voltage direct current (VSC-HVDC) system, we propose an improved auto-disturbance rejection control (ADRC) method based on least squares support vector machines (LSSVM) in the rectifier side. Firstly, we deduce the high frequency transient mathematical model of VSC-HVDC system. Then we investigate the ADRC and LSSVM principles. We ignore the tracking differentiator in the ADRC controller aiming to improve the system dynamic response speed. On this basis, we derive the mathematical model of ADRC controller optimized by LSSVM for direct current voltage loop. Finally we carry out simulations to verify the feasibility and effectiveness of our proposed control method. In addition, we employ the time-frequency representation methods, i.e., Wigner-Ville distribution (WVD) and adaptive optimal kernel (AOK) time-frequency representation, to demonstrate our proposed method performs better than the traditional method from the perspective of energy distribution in time and frequency plane.\n"], "author_display": ["Ying-Pei Liu", "Hai-Ping Liang", "Zhong-Ke Gao"], "article_type": "Research Article", "score": 0.47348133, "title_display": "The Application of Auto-Disturbance Rejection Control Optimized by Least Squares Support Vector Machines Method and Time-Frequency Representation in Voltage Source Converter-High Voltage Direct Current System", "publication_date": "2015-06-22T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0130135"}, {"journal": "PLoS ONE", "abstract": ["\nErosion of dentin results in a complex multi-layered lesion. Several methods have been used to measure erosive substance loss of dentin, but were found to have only limited agreement, in parts because they assess different structural parameters. The present study compared the agreement of four different methods (transversal microradiography [TMR], Confocal Laser Scanning Microscopy [CLSM], Laser Profilometry [LPM] and modified Knoop Hardness measurement [KHM]) to measure erosive substance loss in vitro. Ninety-six dentin specimens were prepared from bovine roots, embedded, ground, polished and covered with nail-varnish except for an experimental window. Erosion was performed for 1 h using citric acid concentrations of 0.00% (control), 0.07%, 0.25% and 1.00% (n\u200a=\u200a24/group). Adjacent surfaces served as sound reference. Two examiners independently determined the substance loss. After 1 h erosion with 1% citric acid solution, substance losses (mean\u00b1SD) of 12.0\u00b11.3 \u00b5m (TMR), 2.9\u00b11.3 \u00b5m (LPM), 3.9\u00b11.3 \u00b5m (KHM) and 17.0\u00b12.6 \u00b5m (CLSM) were detected. ROC curve analysis found all methods to have high accuracy for discriminating different degrees of erosive substance loss (AUC 0.83\u20131.00). Stepwise discriminatory analysis found TMR and CLSM to have the highest discriminatory power. All methods showed significant relative and proportional bias (p<0.001). The smallest albeit significant disagreement was found between LPM and KHM. No significant inter-rater bias was detected except for KHM. LPM is prone to underestimate erosive loss, possibly due to detection of the organic surface layer. KHM was not found suitable to measure erosive loss in dentin. TMR and CLSM detected the loss of mineralised tissue, showed high reliability, and had the highest discriminatory power. Different methods might be suitable to measure different structural parameters.\n"], "author_display": ["Falk Schwendicke", "Geert Felstehausen", "Clifton Carey", "Christof D\u00f6rfer"], "article_type": "Research Article", "score": 0.4730001, "title_display": "Comparison of Four Methods to Assess Erosive Substance Loss of Dentin", "publication_date": "2014-09-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0108064"}, {"journal": "PLoS ONE", "abstract": ["Background: High-throughput RNA interference (RNAi) screening has become a widely used approach to elucidating gene functions. However, analysis and annotation of large data sets generated from these screens has been a challenge for researchers without a programming background. Over the years, numerous data analysis methods were produced for plate quality control and hit selection and implemented by a few open-access software packages. Recently, strictly standardized mean difference (SSMD) has become a widely used method for RNAi screening analysis mainly due to its better control of false negative and false positive rates and its ability to quantify RNAi effects with a statistical basis. We have developed GUItars to enable researchers without a programming background to use SSMD as both a plate quality and a hit selection metric to analyze large data sets. Results: The software is accompanied by an intuitive graphical user interface for easy and rapid analysis workflow. SSMD analysis methods have been provided to the users along with traditionally-used z-score, normalized percent activity, and t-test methods for hit selection. GUItars is capable of analyzing large-scale data sets from screens with or without replicates. The software is designed to automatically generate and save numerous graphical outputs known to be among the most informative high-throughput data visualization tools capturing plate-wise and screen-wise performances. Graphical outputs are also written in HTML format for easy access, and a comprehensive summary of screening results is written into tab-delimited output files. Conclusion: With GUItars, we demonstrated robust SSMD-based analysis workflow on a 3840-gene small interfering RNA (siRNA) library and identified 200 siRNAs that increased and 150 siRNAs that decreased the assay activities with moderate to stronger effects. GUItars enables rapid analysis and illustration of data from large- or small-scale RNAi screens using SSMD and other traditional analysis methods. The software is freely available at http://sourceforge.net/projects/guitars/. "], "author_display": ["Asli N. Goktug", "Su Sien Ong", "Taosheng Chen"], "article_type": "Research Article", "score": 0.4725289, "title_display": "GUItars: A GUI Tool for Analysis of High-Throughput RNA Interference Screening Data", "publication_date": "2012-11-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0049386"}, {"journal": "PLoS ONE", "abstract": ["\nRNAseq and microarray methods are frequently used to measure gene expression level. While similar in purpose, there are fundamental differences between the two technologies. Here, we present the largest comparative study between microarray and RNAseq methods to date using The Cancer Genome Atlas (TCGA) data. We found high correlations between expression data obtained from the Affymetrix one-channel microarray and RNAseq (Spearman correlations coefficients of \u223c0.8). We also observed that the low abundance genes had poorer correlations between microarray and RNAseq data than high abundance genes. As expected, due to measurement and normalization differences, Agilent two-channel microarray and RNAseq data were poorly correlated (Spearman correlations coefficients of only \u223c0.2). By examining the differentially expressed genes between tumor and normal samples we observed reasonable concordance in directionality between Agilent two-channel microarray and RNAseq data, although a small group of genes were found to have expression changes reported in opposite directions using these two technologies. Overall, RNAseq produces comparable results to microarray technologies in term of expression profiling. The RNAseq normalization methods RPKM and RSEM produce similar results on the gene level and reasonably concordant results on the exon level. Longer exons tended to have better concordance between the two normalization methods than shorter exons.\n"], "author_display": ["Yan Guo", "Quanhu Sheng", "Jiang Li", "Fei Ye", "David C. Samuels", "Yu Shyr"], "article_type": "Research Article", "score": 0.4722565, "title_display": "Large Scale Comparison of Gene Expression Levels by Microarrays and RNAseq Using TCGA Data", "publication_date": "2013-08-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0071462"}, {"journal": "PLOS Neglected Tropical Diseases", "abstract": ["Background: The European & Developing Countries Clinical Trials Partnership (EDCTP) is a partnership of European and sub-Saharan African countries that aims to accelerate the development of medical interventions against poverty-related diseases (PRDs). A bibliometric analysis was conducted to 1) measure research output from European and African researchers on PRDs, 2) describe collaboration patterns, and 3) assess the citation impact of clinical research funded by EDCTP. Methodology/Principal Findings: Disease-specific research publications were identified in Thomson Reuters Web of Science using search terms in titles, abstracts and keywords. Publication data, including citation counts, were extracted for 2003\u20132011. Analyses including output, share of global papers, normalised citation impact (NCI), and geographical distribution are presented. Data are presented as five-year moving averages. European EDCTP member countries accounted for ~33% of global research output in PRDs and sub-Saharan African countries for ~10% (2007\u20132011). Both regions contributed more to the global research output in malaria (43.4% and 22.2%, respectively). The overall number of PRD papers from sub-Saharan Africa increased markedly (>47%) since 2003, particularly for HIV/AIDS (102%) and tuberculosis (TB) (81%), and principally involving Southern and East Africa. For 2007\u20132011, European and sub-Saharan African research collaboration on PRDs was highly cited compared with the world average (NCI in brackets): HIV/AIDS 1.62 (NCI: 1.16), TB 2.11 (NCI: 1.06), malaria 1.81 (NCI: 1.22), and neglected infectious diseases 1.34 (NCI: 0.97). The NCI of EDCTP-funded papers for 2003\u20132011 was exceptionally high for HIV/AIDS (3.24), TB (4.08) and HIV/TB co-infection (5.10) compared with global research benchmarks (1.14, 1.05 and 1.35, respectively). Conclusions: The volume and citation impact of papers from sub-Saharan Africa has increased since 2003, as has collaborative research between Europe and sub-Saharan Africa. >90% of publications from EDCTP-funded research were published in high-impact journals and are highly cited. These findings corroborate the benefit of collaborative research on PRDs. Author Summary: The European & Developing Countries Clinical Trials Partnership (EDCTP) was created in 2003 as a European response to the global health crisis caused by the three main poverty-related diseases (PRDs) of HIV/AIDS, tuberculosis and malaria. EDCTP funds research focusing on clinical trials for diagnosing, preventing and treating these diseases. We conducted a bibliometric analysis to 1) measure research output and citation impact from European and African researchers working on PRDs, 2) describe collaboration patterns, and 3) assess the citation impact of research funded by EDCTP. Citation analysis is a commonly used bibliometric tool to analyse scientific literature. Overall, the volume and citation impact of papers from sub-Saharan Africa has increased since 2003, as has collaborative research between Europe and sub-Saharan Africa. Papers arising from collaborative research had a higher citation impact than non-collaborative research and >90% of publications from EDCTP-funded research projects were published in high-impact journals. These results suggest that research on PRDs in sub-Saharan Africa is growing and that the EDCTP partnership contributes to high-impact, collaborative research published in high-impact journals. By providing research funds and supporting activities to strengthen the research environment, the partnership contributes to sub-Saharan African researchers taking the lead in PRD research. "], "author_display": ["J. Gabrielle Breugelmans", "Michael M. Makanga", "Ana L\u00facia V. Cardoso", "Sophie B. Mathewson", "Bethan R. Sheridan-Jones", "Karen A. Gurney", "Charles S. Mgone"], "article_type": "Research Article", "score": 0.47175843, "title_display": "Bibliometric Assessment of European and Sub-Saharan African Research Output on Poverty-Related and Neglected Infectious Diseases from 2003 to 2011", "publication_date": "2015-08-11T00:00:00Z", "eissn": "1935-2735", "id": "10.1371/journal.pntd.0003997"}, {"journal": "PLOS ONE", "abstract": ["\nComparing DNA or protein sequences plays an important role in the functional analysis of genomes. Despite many methods available for sequences comparison, few methods retain the information content of sequences. We propose a new approach, the Yau-Hausdorff method, which considers all translations and rotations when seeking the best match of graphical curves of DNA or protein sequences. The complexity of this method is lower than that of any other two dimensional minimum Hausdorff algorithm. The Yau-Hausdorff method can be used for measuring the similarity of DNA sequences based on two important tools: the Yau-Hausdorff distance and graphical representation of DNA sequences. The graphical representations of DNA sequences conserve all sequence information and the Yau-Hausdorff distance is mathematically proved as a true metric. Therefore, the proposed distance can preciously measure the similarity of DNA sequences. The phylogenetic analyses of DNA sequences by the Yau-Hausdorff distance show the accuracy and stability of our approach in similarity comparison of DNA or protein sequences. This study demonstrates that Yau-Hausdorff distance is a natural metric for DNA and protein sequences with high level of stability. The approach can be also applied to similarity analysis of protein sequences by graphic representations, as well as general two dimensional shape matching.\n"], "author_display": ["Kun Tian", "Xiaoqian Yang", "Qin Kong", "Changchuan Yin", "Rong L. He", "Stephen S.-T. Yau"], "article_type": "Research Article", "score": 0.47054538, "title_display": "Two Dimensional Yau-Hausdorff Distance with Applications on Comparison of DNA and Protein Sequences", "publication_date": "2015-09-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0136577"}, {"journal": "PLoS ONE", "abstract": ["\nAs palmprints are captured using non-contact devices, image blur is inevitably generated because of the defocused status. This degrades the recognition performance of the system. To solve this problem, we propose a stable-feature extraction method based on a Vese\u2013Osher (VO) decomposition model to recognize blurred palmprints effectively. A Gaussian defocus degradation model is first established to simulate image blur. With different degrees of blurring, stable features are found to exist in the image which can be investigated by analyzing the blur theoretically. Then, a VO decomposition model is used to obtain structure and texture layers of the blurred palmprint images. The structure layer is stable for different degrees of blurring (this is a theoretical conclusion that needs to be further proved via experiment). Next, an algorithm based on weighted robustness histogram of oriented gradients (WRHOG) is designed to extract the stable features from the structure layer of the blurred palmprint image. Finally, a normalized correlation coefficient is introduced to measure the similarity in the palmprint features. We also designed and performed a series of experiments to show the benefits of the proposed method. The experimental results are used to demonstrate the theoretical conclusion that the structure layer is stable for different blurring scales. The WRHOG method also proves to be an advanced and robust method of distinguishing blurred palmprints. The recognition results obtained using the proposed method and data from two palmprint databases (PolyU and Blurred\u2013PolyU) are stable and superior in comparison to previous high-performance methods (the equal error rate is only 0.132%). In addition, the authentication time is less than 1.3 s, which is fast enough to meet real-time demands. Therefore, the proposed method is a feasible way of implementing blurred palmprint recognition.\n"], "author_display": ["Danfeng Hong", "Jian Su", "Qinggen Hong", "Zhenkuan Pan", "Guodong Wang"], "article_type": "Research Article", "score": 0.47004285, "title_display": "Blurred Palmprint Recognition Based on Stable-Feature Extraction Using a Vese\u2013Osher Decomposition Model", "publication_date": "2014-07-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0101866"}, {"journal": "PLoS ONE", "abstract": ["\n        Knowledge of the transmission field (B1+) of radio-frequency coils is crucial for high field (B0\u200a=\u200a3.0 T) and ultrahigh field (B0\u22657.0 T) magnetic resonance applications to overcome constraints dictated by electrodynamics in the short wavelength regime with the ultimate goal to improve the image quality. For this purpose B1+ mapping methods are used, which are commonly magnitude-based. In this study an analysis of five phase-based methods for three-dimensional mapping of the B1+ field is presented. The five methods are implemented in a 3D gradient-echo technique. Each method makes use of different RF-pulses (composite or off-resonance pulses) to encode the effective intensity of the B1+ field into the phase of the magnetization. The different RF-pulses result in different trajectories of the magnetization, different use of the transverse magnetization and different sensitivities to B1+ inhomogeneities and frequency offsets, as demonstrated by numerical simulations. The characterization of the five methods also includes phantom experiments and in vivo studies of the human brain at 3.0 T and at 7.0 T. It is shown how the characteristics of each method affect the quality of the B1+ maps. Implications for in vivo B1+ mapping at 3.0 T and 7.0 T are discussed.\n      "], "author_display": ["Flavio Carinci", "Davide Santoro", "Federico von Samson-Himmelstjerna", "Tomasz Dawid Lindel", "Matthias Alexander Dieringer", "Thoralf Niendorf"], "article_type": "Research Article", "score": 0.46987772, "title_display": "Characterization of Phase-Based Methods Used for Transmission Field Uniformity Mapping: A Magnetic Resonance Study at 3.0 T and 7.0 T", "publication_date": "2013-03-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0057982"}, {"journal": "PLoS ONE", "abstract": ["Background: Although scientific innovation has been a long-standing topic of interest for historians, philosophers and cognitive scientists, few studies in biomedical research have examined from researchers' perspectives how high impact publications are developed and why they are consistently produced by a small group of researchers. Our objective was therefore to interview a group of researchers with a track record of high impact publications to explore what mechanism they believe contribute to the generation of high impact publications. Methodology/Principal Findings: Researchers were located in universities all over the globe and interviews were conducted by phone. All interviews were transcribed using standard qualitative methods. A Grounded Theory approach was used to code each transcript, later aggregating concept and categories into overarching explanation model. The model was then translated into a System Dynamics mathematical model to represent its structure and behavior. Five emerging themes were found in our study. First, researchers used heuristics or rules of thumb that came naturally to them. Second, these heuristics were reinforced by positive feedback from their peers and mentors. Third, good communication skills allowed researchers to provide feedback to their peers, thus closing a positive feedback loop. Fourth, researchers exhibited a number of psychological attributes such as curiosity or open-mindedness that constantly motivated them, even when faced with discouraging situations. Fifth, the system is dominated by randomness and serendipity and is far from a linear and predictable environment. Some researchers, however, took advantage of this randomness by incorporating mechanisms that would allow them to benefit from random findings. The aggregation of these themes into a policy model represented the overall expected behavior of publications and their impact achieved by high impact researchers. Conclusions: The proposed selection mechanism provides insights that can be translated into research coaching programs as well as research policy models to optimize the introduction of high impact research at a broad scale among institutional and governmental agencies. "], "author_display": ["Hilary Zelko", "Guilherme Roberto Zammar", "Ana Paula Bonilauri Ferreira", "Amruta Phadtare", "Jatin Shah", "Ricardo Pietrobon"], "article_type": "Research Article", "score": 0.46978736, "title_display": "Selection Mechanisms Underlying High Impact Biomedical Research - A Qualitative Analysis and Causal Model", "publication_date": "2010-05-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0010535"}, {"journal": "PLoS ONE", "abstract": ["Objective: Government funders of biomedical research are under increasing pressure to demonstrate societal benefits of their investments. A number of published studies attempted to correlate research funding levels with the societal burden for various diseases, with mixed results. We examined whether research funded by the Department of Veterans Affairs (VA) is well aligned with current and projected veterans\u2019 health needs. The organizational structure of the VA makes it a particularly suitable setting for examining these questions. Methods: We used the publication patterns and dollar expenditures of VA-funded researchers to characterize the VA research portfolio by disease. We used health care utilization data from the VA for the same diseases to define veterans\u2019 health needs. We then measured the level of correlation between the two and identified disease groups that were under- or over-represented in the research portfolio relative to disease expenditures. Finally, we used historic health care utilization trends combined with demographic projections to identify diseases and conditions that are increasing in costs and/or patient volume and consequently represent potential targets for future research investments. Results: We found a significant correlation between research volume/expenditures and health utilization. Some disease groups were slightly under- or over-represented, but these deviations were relatively small. Diseases and conditions with the increasing utilization trend at the VA included hypertension, hypercholesterolemia, diabetes, hearing loss, sleeping disorders, complications of pregnancy, and several mental disorders. Conclusions: Research investments at the VA are well aligned with veteran health needs. The VA can continue to meet these needs by supporting research on the diseases and conditions with a growing number of patients, costs of care, or both. Our approach can be used by other funders of disease research to characterize their portfolios and to plan research investments. "], "author_display": ["Luba Katz", "Rebecca V. Fink", "Samuel R. Bozeman", "Barbara J. McNeil"], "article_type": "Research Article", "score": 0.469684, "title_display": "Using Health Care Utilization and Publication Patterns to Characterize the Research Portfolio and to Plan Future Research Investments", "publication_date": "2014-12-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0114873"}, {"abstract": ["\n        Various types of genomic data (e.g., SNPs and mRNA transcripts) have been employed to identify risk genes for complex diseases. However, the analysis of these data has largely been performed in isolation. Combining these multiple data for integrative analysis can take advantage of complementary information and thus can have higher power to identify genes (and/or their functions) that would otherwise be impossible with individual data analysis. Due to the different nature, structure, and format of diverse sets of genomic data, multiple genomic data integration is challenging. Here we address the problem by developing a sparse representation based clustering (SRC) method for integrative data analysis. As an example, we applied the SRC method to the integrative analysis of 376821 SNPs in 200 subjects (100 cases and 100 controls) and expression data for 22283 genes in 80 subjects (40 cases and 40 controls) to identify significant genes for osteoporosis (OP). Comparing our results with previous studies, we identified some genes known related to OP risk (e.g., \u2018THSD4\u2019, \u2018CRHR1\u2019, \u2018HSD11B1\u2019, \u2018THSD7A\u2019, \u2018BMPR1B\u2019 \u2018ADCY10\u2019, \u2018PRL\u2019, \u2018CA8\u2019,\u2019ESRRA\u2019, \u2018CALM1\u2019, \u2018CALM1\u2019, \u2018SPARC\u2019, and \u2018LRP1\u2019). Moreover, we uncovered novel osteoporosis susceptible genes (\u2018DICER1\u2019, \u2018PTMA\u2019, etc.) that were not found previously but play functionally important roles in osteoporosis etiology from existing studies. In addition, the SRC method identified genes can lead to higher accuracy for the diagnosis/classification of osteoporosis subjects when compared with the traditional T-test and Fisher-exact test, which further validates the proposed SRC approach for integrative analysis.\n      "], "author_display": ["Hongbao Cao", "Shufeng Lei", "Hong-Wen Deng", "Yu-Ping Wang"], "article_type": "Research Article", "score": 0.46952495, "title_display": "Identification of Genes for Complex Diseases Using Integrated Analysis of Multiple Types of Genomic Data", "publication_date": "2012-09-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0042755"}, {"journal": "PLoS ONE", "abstract": ["\nDifferences exist among analysis results of agriculture monitoring and crop production based on remote sensing observations, which are obtained at different spatial scales from multiple remote sensors in same time period, and processed by same algorithms, models or methods. These differences can be mainly quantitatively described from three aspects, i.e. multiple remote sensing observations, crop parameters estimation models, and spatial scale effects of surface parameters. Our research proposed a new method to analyse and correct the differences between multi-source and multi-scale spatial remote sensing surface reflectance datasets, aiming to provide references for further studies in agricultural application with multiple remotely sensed observations from different sources. The new method was constructed on the basis of physical and mathematical properties of multi-source and multi-scale reflectance datasets. Theories of statistics were involved to extract statistical characteristics of multiple surface reflectance datasets, and further quantitatively analyse spatial variations of these characteristics at multiple spatial scales. Then, taking the surface reflectance at small spatial scale as the baseline data, theories of Gaussian distribution were selected for multiple surface reflectance datasets correction based on the above obtained physical characteristics and mathematical distribution properties, and their spatial variations. This proposed method was verified by two sets of multiple satellite images, which were obtained in two experimental fields located in Inner Mongolia and Beijing, China with different degrees of homogeneity of underlying surfaces. Experimental results indicate that differences of surface reflectance datasets at multiple spatial scales could be effectively corrected over non-homogeneous underlying surfaces, which provide database for further multi-source and multi-scale crop growth monitoring and yield prediction, and their corresponding consistency analysis evaluation.\n"], "author_display": ["Yingying Dong", "Ruisen Luo", "Haikuan Feng", "Jihua Wang", "Jinling Zhao", "Yining Zhu", "Guijun Yang"], "article_type": "Research Article", "score": 0.46775696, "title_display": "Analysing and Correcting the Differences between Multi-Source and Multi-Scale Spatial Remote Sensing Observations", "publication_date": "2014-11-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0111642"}, {"journal": "PLoS ONE", "abstract": ["Background: Medical education curricula in developing countries should emphasize training in health research. This study compares the knowledge and attitudes towards health research between undergraduate medical students undertaking Problem Based Learning (PBL) versus conventional Lecture Based Learning (LBL). Methods: Two groups comprising 66 (LBL) and 84 (PBL) 4th and 5th year students from the medical college of Aga Khan University were administered a structured and validated questionnaire. Knowledge and attitudes of the two groups were recorded on a scale (graduated in percentages) and compared for statistical difference. Results: PBL students scored 54.0% while LBL students scored 55.5% on the knowledge scale [p-value; 0.63]. On the attitudes scale, PBL students scored 75.5% against a 66.7% score of LBL students [p-value; 0.021]. A higher proportion of PBL students (89%) had participated in research activities compared to LBL students (74%) and thus felt more confident in conducting research and writing a scientific paper. Conclusion: The PBL students showed slightly healthier attitudes towards health research compared to LBL students. Both groups demonstrated a similar level of knowledge about health research. The positive impact of the PBL curriculum on attitudes of medical students towards health research may help in improving research output from developing countries in future. "], "author_display": ["Hassan Khan", "Ather M. Taqui", "Muhammad Rizwanulhaq Khawaja", "Zafar Fatmi"], "article_type": "Research Article", "score": 0.4661647, "title_display": "Problem-Based Versus Conventional Curricula: Influence on Knowledge and Attitudes of Medical Students Towards Health Research", "publication_date": "2007-07-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0000632"}, {"journal": "PLoS ONE", "abstract": ["Background: Survival analysis using time-updated CD4+ counts during antiretroviral therapy is frequently employed to determine risk of clinical events. The time-point when the CD4+ count is assumed to change potentially biases effect estimates but methods used to estimate this are infrequently reported. Methods: This study examined the effect of three different estimation methods: assuming i) a constant CD4+ count from date of measurement until the date of next measurement, ii) a constant CD4+ count from the midpoint of the preceding interval until the midpoint of the subsequent interval and iii) a linear interpolation between consecutive CD4+ measurements to provide additional midpoint measurements. Person-time, tuberculosis rates and hazard ratios by CD4+ stratum were compared using all available CD4+ counts (measurement frequency 1\u20133 months) and 6 monthly measurements from a clinical cohort. Simulated data were used to compare the extent of bias introduced by these methods. Results: The midpoint method gave the closest fit to person-time spent with low CD4+ counts and for hazard ratios for outcomes both in the clinical dataset and the simulated data. Conclusion: The midpoint method presents a simple option to reduce bias in time-updated CD4+ analysis, particularly at low CD4 cell counts and rapidly increasing counts after ART initiation. "], "author_display": ["Katharina Kranzer", "James J. Lewis", "Richard G. White", "Judith R. Glynn", "Stephen D. Lawn", "Keren Middelkoop", "Linda-Gail Bekker", "Robin Wood"], "article_type": "Research Article", "score": 0.46533805, "title_display": "Antiretroviral Treatment Cohort Analysis Using Time-Updated CD4 Counts: Assessment of Bias with Different Analytic Methods", "publication_date": "2011-11-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0027763"}, {"journal": "PLoS ONE", "abstract": ["\n        Recent advances in the field of intravital imaging have for the first time allowed us to conduct pharmacokinetic and pharmacodynamic studies at the single cell level in live animal models. Due to these advances, there is now a critical need for automated analysis of pharmacokinetic data. To address this, we began by surveying common thresholding methods to determine which would be most appropriate for identifying fluorescently labeled drugs in intravital imaging. We then developed a segmentation algorithm that allows semi-automated analysis of pharmacokinetic data at the single cell level. Ultimately, we were able to show that drug concentrations can indeed be extracted from serial intravital imaging in an automated fashion. We believe that the application of this algorithm will be of value to the analysis of intravital microscopy imaging particularly when imaging drug action at the single cell level.\n      "], "author_display": ["Randy J. Giedt", "Peter D. Koch", "Ralph Weissleder"], "article_type": "Research Article", "score": 0.4650281, "title_display": "Single Cell Analysis of Drug Distribution by Intravital Imaging", "publication_date": "2013-04-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0060988"}, {"journal": "PLoS ONE", "abstract": ["Background: Sharing of epidemiological and clinical data sets among researchers is poor at best, in detriment of science and community at large. The purpose of this paper is therefore to (1) describe a novel Web application designed to share information on study data sets focusing on epidemiological clinical research in a collaborative environment and (2) create a policy model placing this collaborative environment into the current scientific social context. Methodology: The Database of Databases application was developed based on feedback from epidemiologists and clinical researchers requiring a Web-based platform that would allow for sharing of information about epidemiological and clinical study data sets in a collaborative environment. This platform should ensure that researchers can modify the information. A Model-based predictions of number of publications and funding resulting from combinations of different policy implementation strategies (for metadata and data sharing) were generated using System Dynamics modeling. Principal Findings: The application allows researchers to easily upload information about clinical study data sets, which is searchable and modifiable by other users in a wiki environment. All modifications are filtered by the database principal investigator in order to maintain quality control. The application has been extensively tested and currently contains 130 clinical study data sets from the United States, Australia, China and Singapore. Model results indicated that any policy implementation would be better than the current strategy, that metadata sharing is better than data-sharing, and that combined policies achieve the best results in terms of publications. Conclusions: Based on our empirical observations and resulting model, the social network environment surrounding the application can assist epidemiologists and clinical researchers contribute and search for metadata in a collaborative environment, thus potentially facilitating collaboration efforts among research communities distributed around the globe. "], "author_display": ["Elias C\u00e9sar Araujo de Carvalho", "Adelia Portero Batilana", "Julie Simkins", "Henrique Martins", "Jatin Shah", "Dimple Rajgor", "Anand Shah", "Scott Rockart", "Ricardo Pietrobon"], "article_type": "Research Article", "score": 0.46496025, "title_display": "Application Description and Policy Model in Collaborative Environment for Sharing of Information on Epidemiological and Clinical Research Data Sets", "publication_date": "2010-02-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0009314"}, {"journal": "PLOS ONE", "abstract": ["\nGood quality medical research generally requires not only an expertise in the chosen medical field of interest but also a sound knowledge of statistical methodology. The number of medical research articles which have been published in Indian medical journals has increased quite substantially in the past decade. The aim of this study was to collate all evidence on study design quality and statistical analyses used in selected leading Indian medical journals. Ten (10) leading Indian medical journals were selected based on impact factors and all original research articles published in 2003 (N = 588) and 2013 (N = 774) were categorized and reviewed. A validated checklist on study design, statistical analyses, results presentation, and interpretation was used for review and evaluation of the articles. Main outcomes considered in the present study were \u2013 study design types and their frequencies, error/defects proportion in study design, statistical analyses, and implementation of CONSORT checklist in RCT (randomized clinical trials). From 2003 to 2013: The proportion of erroneous statistical analyses did not decrease (\u03c72=0.592, \u03a6=0.027, p=0.4418), 25% (80/320) in 2003 compared to 22.6% (111/490) in 2013. Compared with 2003, significant improvement was seen in 2013; the proportion of papers using statistical tests increased significantly (\u03c72=26.96, \u03a6=0.16, p<0.0001) from 42.5% (250/588) to 56.7 % (439/774). The overall proportion of errors in study design decreased significantly (\u03c72=16.783, \u03a6=0.12 p<0.0001), 41.3% (243/588) compared to 30.6% (237/774). In 2013, randomized clinical trials designs has remained very low (7.3%, 43/588) with majority showing some errors (41 papers, 95.3%). Majority of the published studies were retrospective in nature both in 2003 [79.1% (465/588)] and in 2013 [78.2% (605/774)]. Major decreases in error proportions were observed in both results presentation (\u03c72=24.477, \u03a6=0.17, p<0.0001), 82.2% (263/320) compared to 66.3% (325/490) and interpretation (\u03c72=25.616, \u03a6=0.173, p<0.0001), 32.5% (104/320) compared to 17.1% (84/490), though some serious ones were still present. Indian medical research seems to have made no major progress regarding using correct statistical analyses, but error/defects in study designs have decreased significantly. Randomized clinical trials are quite rarely published and have high proportion of methodological problems.\n"], "author_display": ["Shabbeer Hassan", "Rajashree Yellur", "Pooventhan Subramani", "Poornima Adiga", "Manoj Gokhale", "Manasa S. Iyer", "Shreemathi S. Mayya"], "article_type": "Research Article", "score": 0.4643526, "title_display": "Research Design and Statistical Methods in Indian Medical Journals: A Retrospective Survey", "publication_date": "2015-04-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0121268"}, {"journal": "PLoS ONE", "abstract": ["\nResearch endeavours require the collaborative effort of an increasing number of individuals. International scientific collaborations are particularly important for HIV and HPV co-infection studies, since the burden of disease is rising in developing countries, but most experts and research funds are found in developed countries, where the prevalence of HIV is low. The objective of our study was to investigate patterns of international scientific collaboration in HIV and HPV research using social network analysis. Through a systematic review of the literature, we obtained epidemiological data, as well as data on countries and authors involved in co-infection studies. The collaboration network was analysed in respect to the following: centrality, density, modularity, connected components, distance, clustering and spectral clustering. We observed that for many low- and middle-income countries there were no epidemiological estimates of HPV infection of the cervix among HIV-infected individuals. Most studies found only involved researchers from the same country (64%). Studies derived from international collaborations including high-income countries and either low- or middle-income countries had on average three times larger sample sizes than those including only high-income countries or low-income countries. The high global clustering coefficient (0.9) coupled with a short average distance between researchers (4.34) suggests a \u201csmall-world phenomenon.\u201d Researchers from high-income countries seem to have higher degree centrality and tend to cluster together in densely connected communities. We found a large well-connected community, which encompasses 70% of researchers, and 49 other small isolated communities. Our findings suggest that in the field of HIV and HPV, there seems to be both room and incentives for researchers to engage in collaborations between countries of different income-level. Through international collaboration resources available to researchers in high-income countries can be efficiently used to enroll more participants in low- and middle-income countries.\n"], "author_display": ["Tazio Vanni", "Marco Mesa-Frias", "Ruben Sanchez-Garcia", "Rafael Roesler", "Gilberto Schwartsmann", "Marcelo Z. Goldani", "Anna M. Foss"], "article_type": "Research Article", "score": 0.46402353, "title_display": "International Scientific Collaboration in HIV and HPV: A Network Analysis", "publication_date": "2014-03-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0093376"}, {"journal": "PLoS ONE", "abstract": ["\nThe common formula used for converting a chi-square test into a correlation coefficient for use as an effect size in meta-analysis has a hidden assumption which may be violated in specific instances, leading to an overestimation of the effect size. A corrected formula is provided.\n"], "author_display": ["Michael S. Rosenberg"], "article_type": "Research Article", "score": 0.46388993, "title_display": "A Generalized Formula for Converting Chi-Square Tests to Effect Sizes for Meta-Analysis", "publication_date": "2010-04-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0010059"}, {"journal": "PLoS ONE", "abstract": ["\n        Aging and age-related disease represents a substantial quantity of current natural, social and behavioral science research efforts. Presently, no centralized system exists for tracking aging research projects across numerous research disciplines. The multidisciplinary nature of this research complicates the understanding of underlying project categories, the establishment of project relations, and the development of a unified project classification scheme. We have developed a highly visual database, the International Aging Research Portfolio (IARP), available at AgingPortfolio.org to address this issue. The database integrates information on research grants, peer-reviewed publications, and issued patent applications from multiple sources. Additionally, the database uses flexible project classification mechanisms and tools for analyzing project associations and trends. This system enables scientists to search the centralized project database, to classify and categorize aging projects, and to analyze the funding aspects across multiple research disciplines. The IARP is designed to provide improved allocation and prioritization of scarce research funding, to reduce project overlap and improve scientific collaboration thereby accelerating scientific and medical progress in a rapidly growing area of research. Grant applications often precede publications and some grants do not result in publications, thus, this system provides utility to investigate an earlier and broader view on research activity in many research disciplines. This project is a first attempt to provide a centralized database system for research grants and to categorize aging research projects into multiple subcategories utilizing both advanced machine algorithms and a hierarchical environment for scientific collaboration.\n      "], "author_display": ["Alex Zhavoronkov", "Charles R. Cantor"], "article_type": "Research Article", "score": 0.46384832, "title_display": "Methods for Structuring Scientific Knowledge from Many Areas Related to Aging Research", "publication_date": "2011-07-22T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0022597"}, {"journal": "PLOS ONE", "abstract": ["Background: In general, the individual patient-level data (IPD) collected in clinical trials are not available to independent researchers to conduct economic evaluations; researchers only have access to published survival curves and summary statistics. Thus, methods that use published survival curves and summary statistics to reproduce statistics for economic evaluations are essential. Four methods have been identified: two traditional methods 1) least squares method, 2) graphical method; and two recently proposed methods by 3) Hoyle and Henley, 4) Guyot et al. The four methods were first individually reviewed and subsequently assessed regarding their abilities to estimate mean survival through a simulation study. Methods: A number of different scenarios were developed that comprised combinations of various sample sizes, censoring rates and parametric survival distributions. One thousand simulated survival datasets were generated for each scenario, and all methods were applied to actual IPD. The uncertainty in the estimate of mean survival time was also captured. Results: All methods provided accurate estimates of the mean survival time when the sample size was 500 and a Weibull distribution was used. When the sample size was 100 and the Weibull distribution was used, the Guyot et al. method was almost as accurate as the Hoyle and Henley method; however, more biases were identified in the traditional methods. When a lognormal distribution was used, the Guyot et al. method generated noticeably less bias and a more accurate uncertainty compared with the Hoyle and Henley method. Conclusions: The traditional methods should not be preferred because of their remarkable overestimation. When the Weibull distribution was used for a fitted model, the Guyot et al. method was almost as accurate as the Hoyle and Henley method. However, if the lognormal distribution was used, the Guyot et al. method was less biased compared with the Hoyle and Henley method. "], "author_display": ["Xiaomin Wan", "Liubao Peng", "Yuanjian Li"], "article_type": "Research Article", "score": 0.46300596, "title_display": "A Review and Comparison of Methods for Recreating Individual Patient Data from Published Kaplan-Meier Survival Curves for Economic Evaluations: A Simulation Study", "publication_date": "2015-03-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0121353"}, {"journal": "PLOS ONE", "abstract": ["\nFast and computationally less complex feature extraction for moving object detection using aerial images from unmanned aerial vehicles (UAVs) remains as an elusive goal in the field of computer vision research. The types of features used in current studies concerningmoving object detection are typically chosen based on improving detection rate rather than on providing fast and computationally less complex feature extraction methods. Because moving object detection using aerial images from UAVs involves motion as seen from a certain altitude, effective and fast feature extraction is a vital issue for optimum detection performance. This research proposes a two-layer bucket approach based on a new feature extraction algorithm referred to as the moment-based feature extraction algorithm (MFEA). Because a moment represents thecoherent intensity of pixels and motion estimation is a motion pixel intensity measurement, this research used this relation to develop the proposed algorithm. The experimental results reveal the successful performance of the proposed MFEA algorithm and the proposed methodology.\n"], "author_display": ["A. F. M. Saifuddin Saif", "Anton Satria Prabuwono", "Zainal Rasyid Mahayuddin"], "article_type": "Research Article", "score": 0.46223307, "title_display": "Moment Feature Based Fast Feature Extraction Algorithm for Moving Object Detection Using Aerial Images", "publication_date": "2015-06-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0126212"}, {"journal": "PLoS ONE", "abstract": ["\n        The output of state-of-the-art reverse-engineering methods for biological networks is often based on the fitting of a mathematical model to the data. Typically, different datasets do not give single consistent network predictions but rather an ensemble of inconsistent networks inferred under the same reverse-engineering method that are only consistent with the specific experimentally measured data. Here, we focus on an alternative approach for combining the information contained within such an ensemble of inconsistent gene networks called meta-analysis, to make more accurate predictions and to estimate the reliability of these predictions. We review two existing meta-analysis approaches; the Fisher transformation combined coefficient test (FTCCT) and Fisher's inverse combined probability test (FICPT); and compare their performance with five well-known methods, ARACNe, Context Likelihood or Relatedness network (CLR), Maximum Relevance Minimum Redundancy (MRNET), Relevance Network (RN) and Bayesian Network (BN). We conducted in-depth numerical ensemble simulations and demonstrated for biological expression data that the meta-analysis approaches consistently outperformed the best gene regulatory network inference (GRNI) methods in the literature. Furthermore, the meta-analysis approaches have a low computational complexity. We conclude that the meta-analysis approaches are a powerful tool for integrating different datasets to give more accurate and reliable predictions for biological networks.\n      "], "author_display": ["Azree Nazri", "Pietro Lio"], "article_type": "Research Article", "score": 0.46190453, "title_display": "Investigating Meta-Approaches for Reconstructing Gene Networks in a Mammalian Cellular Context", "publication_date": "2012-01-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0028713"}, {"journal": "PLOS ONE", "abstract": ["\nComparative co-localization analysis of transcription factors (TFs) and epigenetic marks (EMs) in specific biological contexts is one of the most critical areas of ChIP-Seq data analysis beyond peak calling. Yet there is a significant lack of user-friendly and powerful tools geared towards co-localization analysis based exploratory research. Most tools currently used for co-localization analysis are command line only and require extensive installation procedures and Linux expertise. Online tools partially address the usability issues of command line tools, but slow response times and few customization features make them unsuitable for rapid data-driven interactive exploratory research. We have developed PAPST: Peak Assignment and Profile Search Tool, a user-friendly yet powerful platform with a unique design, which integrates both gene-centric and peak-centric co-localization analysis into a single package. Most of PAPST\u2019s functions can be completed in less than five seconds, allowing quick cycles of data-driven hypothesis generation and testing. With PAPST, a researcher with or without computational expertise can perform sophisticated co-localization pattern analysis of multiple TFs and EMs, either against all known genes or a set of genomic regions obtained from public repositories or prior analysis. PAPST is a versatile, efficient, and customizable tool for genome-wide data-driven exploratory research. Creatively used, PAPST can be quickly applied to any genomic data analysis that involves a comparison of two or more sets of genomic coordinate intervals, making it a powerful tool for a wide range of exploratory genomic research. We first present PAPST\u2019s general purpose features then apply it to several public ChIP-Seq data sets to demonstrate its rapid execution and potential for cutting-edge research with a case study in enhancer analysis. To our knowledge, PAPST is the first software of its kind to provide efficient and sophisticated post peak-calling ChIP-Seq data analysis as an easy-to-use interactive application. PAPST is available at https://github.com/paulbible/papst and is a public domain work.\n"], "author_display": ["Paul W. Bible", "Yuka Kanno", "Lai Wei", "Stephen R. Brooks", "John J. O\u2019Shea", "Maria I. Morasso", "Rasiah Loganantharaj", "Hong-Wei Sun"], "article_type": "Research Article", "score": 0.4612471, "title_display": "PAPST, a User Friendly and Powerful Java Platform for ChIP-Seq Peak Co-Localization Analysis and Beyond", "publication_date": "2015-05-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0127285"}, {"journal": "PLoS ONE", "abstract": ["Background: Overviews of systematic reviews compile data from multiple systematic reviews (SRs) and are a new method of evidence synthesis. Objectives: To describe the methodological approaches in overviews of interventions. Design: Descriptive study. Methods: We searched 4 databases from 2000 to July 2011; we handsearched Evidence-based Child Health: A Cochrane Review Journal. We defined an overview as a study that: stated a clear objective; examined an intervention; used explicit methods to identify SRs; collected and synthesized outcome data from the SRs; and intended to include only SRs. We did not restrict inclusion by population characteristics (e.g., adult or children only). Two researchers independently screened studies and applied eligibility criteria. One researcher extracted data with verification by a second. We conducted a descriptive analysis. Results: From 2,245 citations, 75 overviews were included. The number of overviews increased from 1 in 2000 to 14 in 2010. The interventions were pharmacological (n\u200a=\u200a20, 26.7%), non-pharmacological (n\u200a=\u200a26, 34.7%), or both (n\u200a=\u200a29, 38.7%). Inclusion criteria were clearly stated in 65 overviews. Thirty-three (44%) overviews searched at least 2 databases. The majority reported the years and databases searched (n\u200a=\u200a46, 61%), and provided key words (n\u200a=\u200a58, 77%). Thirty-nine (52%) overviews included Cochrane SRs only. Two reviewers independently screened and completed full text review in 29 overviews (39%). Methods of data extraction were reported in 45 (60%). Information on quality of individual studies was extracted from the original SRs in 27 (36%) overviews. Quality assessment of the SRs was performed in 28 (37%) overviews; at least 9 different tools were used. Quality of the body of evidence was assessed in 13 (17%) overviews. Most overviews provided a narrative or descriptive analysis of the included SRs. One overview conducted indirect analyses and the other conducted mixed treatment comparisons. Publication bias was discussed in 18 (24%) overviews. Conclusions: This study shows considerable variation in the methods used for overviews. There is a need for methodological rigor and consistency in overviews, as well as empirical evidence to support the methods employed. "], "author_display": ["Lisa Hartling", "Annabritt Chisholm", "Denise Thomson", "Donna M. Dryden"], "article_type": "Research Article", "score": 0.4610634, "title_display": "A Descriptive Analysis of Overviews of Reviews Published between 2000 and 2011", "publication_date": "2012-11-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0049667"}, {"journal": "PLoS ONE", "abstract": ["\n        Accurate determination of circadian phase is necessary for research and clinical purposes because of the influence of the master circadian pacemaker on multiple physiologic functions. Melatonin is presently the most accurate marker of the activity of the human circadian pacemaker. Current methods of analyzing the plasma melatonin rhythm can be grouped into three categories: curve-fitting, threshold-based and physiologically-based linear differential equations. To determine which method provides the most accurate assessment of circadian phase, we compared the ability to fit the data and the variability of phase estimates for seventeen different markers of melatonin phase derived from these methodological categories. We used data from three experimental conditions under which circadian rhythms - and therefore calculated melatonin phase - were expected to remain constant or progress uniformly. Melatonin profiles from older subjects and subjects with lower melatonin amplitude were less likely to be fit by all analysis methods. When circadian drift over multiple study days was algebraically removed, there were no significant differences between analysis methods of melatonin onsets (P\u200a=\u200a0.57), but there were significant differences between those of melatonin offsets (P<0.0001). For a subset of phase assessment methods, we also examined the effects of data loss on variability of phase estimates by systematically removing data in 2-hour segments. Data loss near onset of melatonin secretion differentially affected phase estimates from the methods, with some methods incorrectly assigning phases too early while other methods assigning phases too late; missing data at other times did not affect analyses of the melatonin profile. We conclude that melatonin data set characteristics, including amplitude and completeness of data collection, differentially affect the results depending on the melatonin analysis method used.\n      "], "author_display": ["Hadassa Klerman", "Melissa A. St. Hilaire", "Richard E. Kronauer", "Joshua J. Gooley", "Claude Gronfier", "Joseph T. Hull", "Steven W. Lockley", "Nayantara Santhi", "Wei Wang", "Elizabeth B. Klerman"], "article_type": "Research Article", "score": 0.46101278, "title_display": "Analysis Method and Experimental Conditions Affect Computed Circadian Phase from Melatonin Data", "publication_date": "2012-04-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0033836"}, {"journal": "PLoS ONE", "abstract": ["Background: The recent decline in fertility in India has been unprecedented especially in southern India, where fertility is almost exclusively controlled by means of permanent contraceptive methods, mainly female sterilization, which constitutes about two-thirds of overall contraceptive use. Many Indian women undergo sterilization at relatively young ages as a consequence of early marriage and childbearing in short birth intervals. This research aims to investigate the socioeconomic factors determining the choices for alternative contraceptive choices against the dominant preference for sterilization among married women in India. Methods: Data for this study are drawn from the 2005\u201306 National Family Health Surveys focusing on a sample of married women who reported having used a method of contraception in the five years preceding the survey. A multilevel multinomial logit regression is used to estimate the impact of socioeconomic factors on contraceptive choices, differentiating temporary modern or traditional methods versus sterilization. Findings: Religious affiliation, women's education and occupation had overarching influence on method choices amongst recent users. Muslim women were at higher odds of choosing a traditional or modern temporary method than sterilization. Higher level of women's education increased the odds of modern temporary method choices but the education effect on traditional method choices was only marginally significant. Recent users belonging to wealthier households had higher odds of choosing modern methods over sterilization. Exposure to family planning messages through radio had a positive effect on modern and traditional method choices. Community variations in method choices were highly significant. Conclusion: The persistent dominance of sterilization in the Indian family planning programme is largely determined by socioeconomic conditions. Reproductive health programmes should address the socioeconomic barriers and consider multiple cost-effective strategies such as mass media to promote awareness of modern temporary methods. "], "author_display": ["Isabel Tiago de Oliveira", "Jos\u00e9 G. Dias", "Sabu S. Padmadas"], "article_type": "Research Article", "score": 0.460969, "title_display": "Dominance of Sterilization and Alternative Choices of Contraception in India: An Appraisal of the Socioeconomic Impact", "publication_date": "2014-01-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0086654"}, {"journal": "PLoS ONE", "abstract": ["\nIn this paper, a randomized numerical approach is used to obtain approximate solutions for a class of nonlinear Fredholm integral equations of the second kind. The proposed approach contains two steps: at first, we define a discretized form of the integral equation by quadrature formula methods and solution of this discretized form converges to the exact solution of the integral equation by considering some conditions on the kernel of the integral equation. And then we convert the problem to an optimal control problem by introducing an artificial control function. Following that, in the next step, solution of the discretized form is approximated by a kind of Monte Carlo (MC) random search algorithm. Finally, some examples are given to show the efficiency of the proposed approach.\n"], "author_display": ["Zhimin Hong", "Zaizai Yan", "Jiao Yan"], "article_type": "Research Article", "score": 0.46060723, "title_display": "Random Search Algorithm for Solving the Nonlinear Fredholm Integral Equations of the Second Kind", "publication_date": "2014-07-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0103068"}, {"journal": "PLOS ONE", "abstract": ["\nThe present paper introduces a condition number estimation method for preconditioned matrices. The newly developed method provides reasonable results, while the conventional method which is based on the Lanczos connection gives meaningless results. The Lanczos connection based method provides the condition numbers of coefficient matrices of systems of linear equations with information obtained through the preconditioned conjugate gradient method. Estimating the condition number of preconditioned matrices is sometimes important when describing the effectiveness of new preconditionerers or selecting adequate preconditioners. Operating a preconditioner on a coefficient matrix is the simplest method of estimation. However, this is not possible for large-scale computing, especially if computation is performed on distributed memory parallel computers. This is because, the preconditioned matrices become dense, even if the original matrices are sparse. Although the Lanczos connection method can be used to calculate the condition number of preconditioned matrices, it is not considered to be applicable to large-scale problems because of its weakness with respect to numerical errors. Therefore, we have developed a robust and parallelizable method based on Hager\u2019s method. The feasibility studies are curried out for the diagonal scaling preconditioner and the SSOR preconditioner with a diagonal matrix, a tri-daigonal matrix and Pei\u2019s matrix. As a result, the Lanczos connection method contains around 10% error in the results even with a simple problem. On the other hand, the new method contains negligible errors. In addition, the newly developed method returns reasonable solutions when the Lanczos connection method fails with Pei\u2019s matrix, and matrices generated with the finite element method.\n"], "author_display": ["Noriyuki Kushida"], "article_type": "Research Article", "score": 0.46010953, "title_display": "Condition Number Estimation of Preconditioned Matrices", "publication_date": "2015-03-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0122331"}, {"journal": "PLoS ONE", "abstract": ["Background: 16S rRNA gene pyrosequencing approach has revolutionized studies in microbial ecology. While primer selection and short read length can affect the resulting microbial community profile, little is known about the influence of pyrosequencing methods on the sequencing throughput and the outcome of microbial community analyses. The aim of this study is to compare differences in output, ease, and cost among three different amplicon pyrosequencing methods for the Roche/454 Titanium platform Methodology/Principal Findings: The following three pyrosequencing methods for 16S rRNA genes were selected in this study: Method-1 (standard method) is the recommended method for bi-directional sequencing using the LIB-A kit; Method-2 is a new option designed in this study for unidirectional sequencing with the LIB-A kit; and Method-3 uses the LIB-L kit for unidirectional sequencing. In our comparison among these three methods using 10 different environmental samples, Method-2 and Method-3 produced 1.5\u20131.6 times more useable reads than the standard method (Method-1), after quality-based trimming, and did not compromise the outcome of microbial community analyses. Specifically, Method-3 is the most cost-effective unidirectional amplicon sequencing method as it provided the most reads and required the least effort in consumables management. Conclusions: Our findings clearly demonstrated that alternative pyrosequencing methods for 16S rRNA genes could drastically affect sequencing output (e.g. number of reads before and after trimming) but have little effect on the outcomes of microbial community analysis. This finding is important for both researchers and sequencing facilities utilizing 16S rRNA gene pyrosequencing for microbial ecological studies. "], "author_display": ["Hideyuki Tamaki", "Chris L. Wright", "Xiangzhen Li", "Qiaoyan Lin", "Chiachi Hwang", "Shiping Wang", "Jyothi Thimmapuram", "Yoichi Kamagata", "Wen-Tso Liu"], "article_type": "Research Article", "score": 0.45979095, "title_display": "Analysis of 16S rRNA Amplicon Sequencing Options on the Roche/454 Next-Generation Titanium Sequencing Platform", "publication_date": "2011-09-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0025263"}, {"journal": "PLoS ONE", "abstract": ["Aims: This survey aims to describe the perception of barriers to and facilitators of research utilization by registered nurses in Sichuan province, China, and to explore the factors influencing the perceptions of the barriers to and facilitators of research utilization. Methods: A cross sectional survey design and a double cluster sampling method were adopted. A total of 590 registered nurses from 3 tertiary level hospitals in Sichuan province, China, were recruited in a period from September 2006 to January 2007.  A modified BARRUERS Scale and a Facilitators Scale were used. Data were analyzed with descriptive statistics, rank transformation test, and multiple linear regression.  Results: Barriers related to the setting subscale were more influential than barriers related to other subscales. The lack of authority was ranked as the top greatest barrier (15.7%), followed by the lack of time (13.4%) and language barrier (15.0%). Additional barriers identified were the reluctance of patients to research utilization, the lack of funding, and the lack of legal protection. The top three greatest facilitators were enhancing managerial support (36.9%), advancing education to increase knowledge base (21.1%), and increasing time for reviewing and implementing (17.5%), while cooperation of patients to research utilization, establishing a panel to evaluate researches, and funding were listed as additional facilitators. Hospital, educational background, research experience, and knowledge on evidence-based nursing were the factors influencing perceptions of the barriers and facilitators. Conclusions: Nurses in China are facing a number of significant barriers in research utilization. Enhancing managerial support might be the most promising facilitator, given Chinese traditional culture and existing health care system. Hospital, educational background, research experience and knowledge on evidence-based nursing should be taken into account to promote research utilization. The BARRIERS Scale should consider funding and involvement of patients in research utilization. "], "author_display": ["Li-Ping Wang", "Xiao-Lian Jiang", "Lei Wang", "Guo-Rong Wang", "Yang-Jing Bai"], "article_type": "Research Article", "score": 0.45959488, "title_display": "Barriers to and Facilitators of Research Utilization: A Survey of Registered Nurses in China", "publication_date": "2013-11-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0081908"}, {"journal": "PLoS ONE", "abstract": ["Background: India has the highest number of HIV infected persons in the world after South Africa. Much HIV related behavioral, clinical and laboratory based research is ongoing in India. Yet little is known on Indian HIV patients' knowledge of research, their processes of decision making and motives for participation. We aimed to explore these areas among HIV infected individuals to understand their reasons for participating in research. Methodology/Principal Findings: This is a cross sectional survey among 173 HIV infected adults at a tertiary level hospital in Bangalore, India, done between October 2010 and January 2011. A pre-tested questionnaire was administered to the participants by trained research assistants to assess their knowledge regarding research, willingness to participate, decision making and determinants of participation. Participants were presented with five hypothetical HIV research studies. Each study had a different level of intervention and time commitment. Of respondents, 103(60%), said that research meant \u2018to discover something new\u2019 and 138(80%) were willing to participate in research. A third of the respondents were unaware of their right to refuse participation. Willingness to participate in research varied with level of intervention. It was the lowest for the hypothetical study involving sensitive questions followed by the hypothetical drug trial; and was the highest for the hypothetical cross sectional questionnaire based study (p<0.0015). Individual health benefits and altruism were the primary motives for participation in research and indicate the presence of therapeutic misconception. Women were less likely to make autonomous decisions for participation in interventional studies. Conclusions/Significance: Despite a majority willing to participate, over a third of respondents did not have any knowledge of research or the voluntary nature of participation. This has ethical implications. Researchers need to focus on enabling potential research participants understand the concepts of research, promote autonomous decisions, especially by women and restrict therapeutic misconception. "], "author_display": ["Rashmi J. Rodrigues", "Jimmy Antony", "Shubha Krishnamurthy", "Anita Shet", "Ayesha De Costa"], "article_type": "Research Article", "score": 0.45937836, "title_display": "\u2018What Do I Know? Should I Participate?\u2019 Considerations on Participation in HIV Related Research among HIV Infected Adults in Bangalore, South India", "publication_date": "2013-02-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0053054"}, {"journal": "PLoS ONE", "abstract": ["Background: Cellular activities are governed by the physical and the functional interactions among several proteins involved in various biological pathways. With the availability of sequenced genomes and high-throughput experimental data one can identify genome-wide protein-protein interactions using various computational techniques. Comparative assessments of these techniques in predicting protein interactions have been frequently reported in the literature but not their ability to elucidate a particular biological pathway. Methods: Towards the goal of understanding the prediction capabilities of interactions among the specific biological pathway proteins, we report the analyses of 14 biological pathways of Escherichia coli catalogued in KEGG database using five protein-protein functional linkage prediction methods. These methods are phylogenetic profiling, gene neighborhood, co-presence of orthologous genes in the same gene clusters, a mirrortree variant, and expression similarity. Conclusions: Our results reveal that the prediction of metabolic pathway protein interactions continues to be a challenging task for all methods which possibly reflect flexible/independent evolutionary histories of these proteins. These methods have predicted functional associations of proteins involved in amino acids, nucleotide, glycans and vitamins & co-factors pathways slightly better than the random performance on carbohydrate, lipid and energy metabolism. We also make similar observations for interactions involved among the environmental information processing proteins. On the contrary, genetic information processing or specialized processes such as motility related protein-protein linkages that occur in the subset of organisms are predicted with comparable accuracy. Metabolic pathways are best predicted by using neighborhood of orthologous genes whereas phyletic pattern is good enough to reconstruct central dogma pathway protein interactions. We have also shown that the effective use of a particular prediction method depends on the pathway under investigation. In case one is not focused on specific pathway, gene expression similarity method is the best option. "], "author_display": ["Vijaykumar Yogesh Muley", "Akash Ranjan"], "article_type": "Research Article", "score": 0.45916757, "title_display": "Evaluation of Physical and Functional Protein-Protein Interaction Prediction Methods for Detecting Biological Pathways", "publication_date": "2013-01-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0054325"}, {"journal": "PLoS ONE", "abstract": ["\n        This paper reports data from semi-structured interviews on how 26 Australian civil servants, ministers and ministerial advisors find and evaluate researchers with whom they wish to consult or collaborate. Policymakers valued researchers who had credibility across the three attributes seen as contributing to trustworthiness: competence (an exemplary academic reputation complemented by pragmatism, understanding of government processes, and effective collaboration and communication skills); integrity (independence, \u201cauthenticity\u201d, and faithful reporting of research); and benevolence (commitment to the policy reform agenda). The emphases given to these assessment criteria appeared to be shaped in part by policymakers' roles and the type and phase of policy development in which they were engaged. Policymakers are encouraged to reassess their methods for engaging researchers and to maximise information flow and support in these relationships. Researchers who wish to influence policy are advised to develop relationships across the policy community, but also to engage in other complementary strategies for promoting research-informed policy, including the strategic use of mass media.\n      "], "author_display": ["Abby S. Haynes", "Gemma E. Derrick", "Sally Redman", "Wayne D. Hall", "James A. Gillespie", "Simon Chapman", "Heidi Sturk"], "article_type": "Research Article", "score": 0.45907003, "title_display": "Identifying Trustworthy Experts: How Do Policymakers Find and Assess Public Health Researchers Worth Consulting or Collaborating With?", "publication_date": "2012-03-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0032665"}, {"journal": "PLoS ONE", "abstract": ["\n        Genes involved in disease that are not common are often difficult to identify; a method that pinpoints them from a small number of unrelated patients will be of great help. In order to establish such a method that detects recessive genes identical-by-descent, we modified homozygosity mapping (HM) so that it is constructed on the basis of homozygosity haplotype (HM on HH) analysis. An analysis using 6 unrelated patients with Siiyama-type \u03b11-antitrypsin deficiency, a disease caused by a founder gene, the correct gene locus was pinpointed from data of any 2 patients (length: 1.2\u201321.8 centimorgans, median: 1.6 centimorgans). For a test population in which these 6 patients and 54 healthy subjects were scrambled, the approach accurately identified these 6 patients and pinpointed the locus to a 1.4-centimorgan fragment. Analyses using synthetic data revealed that the analysis works well for IBD fragment derived from a most recent common ancestor (MRCA) who existed less than 60 generations ago. The analysis is unsuitable for the genes with a frequency in general population more than 0.1. Thus, HM on HH analysis is a powerful technique, applicable to a small number of patients not known to be related, and will accelerate the identification of disease-causing genes for recessive conditions.\n      "], "author_display": ["Koichi Hagiwara", "Hiroyuki Morino", "Jun Shiihara", "Tomoaki Tanaka", "Hitoshi Miyazawa", "Tomoko Suzuki", "Masakazu Kohda", "Yasushi Okazaki", "Kuniaki Seyama", "Hideshi Kawakami"], "article_type": "Research Article", "score": 0.45767146, "title_display": "Homozygosity Mapping on Homozygosity Haplotype Analysis to Detect Recessive Disease-Causing Genes from a Small Number of Unrelated, Outbred Patients", "publication_date": "2011-09-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0025059"}, {"journal": "PLOS ONE", "abstract": ["\nA DNA library is a collection of DNA fragments cloned into vectors and stored individually in host cells, and is a valuable resource for molecular cloning, gene physical mapping, and genome sequencing projects. To take the best advantage of a DNA library, a good screening method is needed. After describing pooling strategies and issues that should be considered in DNA library screening, here we report an efficient colony multiplex quantitative PCR-based 3-step, 3-dimension, and binary-code (3S3DBC) method we used to screen genes from a planarian genomic DNA fosmid library. This method requires only 3 rounds of PCR reactions and only around 6 hours to distinguish one or more desired clones from a large DNA library. According to the particular situations in different research labs, this method can be further modified and simplified to suit their requirements.\n"], "author_display": ["Yang An", "Atsushi Toyoda", "Chen Zhao", "Asao Fujiyama", "Kiyokazu Agata"], "article_type": "Research Article", "score": 0.45762548, "title_display": "A Colony Multiplex Quantitative PCR-Based 3S3DBC Method and Variations of It for Screening DNA Libraries", "publication_date": "2015-02-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0116997"}, {"journal": "PLoS ONE", "abstract": ["\nOne of the most important steps in biomedical longitudinal studies is choosing a good experimental design that can provide high accuracy in the analysis of results with a minimum sample size. Several methods for constructing efficient longitudinal designs have been developed based on power analysis and the statistical model used for analyzing the final results. However, development of this technology is not available to practitioners through user-friendly software. In this paper we introduce LADES (Longitudinal Analysis and Design of Experiments Software) as an alternative and easy-to-use tool for conducting longitudinal analysis and constructing efficient longitudinal designs. LADES incorporates methods for creating cost-efficient longitudinal designs, unequal longitudinal designs, and simple longitudinal designs. In addition, LADES includes different methods for analyzing longitudinal data such as linear mixed models, generalized estimating equations, among others. A study of European eels is reanalyzed in order to show LADES capabilities. Three treatments contained in three aquariums with five eels each were analyzed. Data were collected from 0 up to the 12th week post treatment for all the eels (complete design). The response under evaluation is sperm volume. A linear mixed model was fitted to the results using LADES. The complete design had a power of 88.7% using 15 eels. With LADES we propose the use of an unequal design with only 14 eels and 89.5% efficiency. LADES was developed as a powerful and simple tool to promote the use of statistical methods for analyzing and creating longitudinal experiments in biomedical research.\n"], "author_display": ["Alan V\u00e1zquez-Alcocer", "Daniel Ladislao Garz\u00f3n-Cortes", "Rosa Mar\u00eda S\u00e1nchez-Casas"], "article_type": "Research Article", "score": 0.45716298, "title_display": "LADES: A Software for Constructing and Analyzing Longitudinal Designs in Biomedical Research", "publication_date": "2014-07-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0100570"}, {"journal": "PLoS ONE", "abstract": ["\nJoint association analysis of multiple traits in a genome-wide association study (GWAS), i.e. a multivariate GWAS, offers several advantages over analyzing each trait in a separate GWAS. In this study we directly compared a number of multivariate GWAS methods using simulated data. We focused on six methods that are implemented in the software packages PLINK, SNPTEST, MultiPhen, BIMBAM, PCHAT and TATES, and also compared them to standard univariate GWAS, analysis of the first principal component of the traits, and meta-analysis of univariate results. We simulated data (N\u200a=\u200a1000) for three quantitative traits and one bi-allelic quantitative trait locus (QTL), and varied the number of traits associated with the QTL (explained variance 0.1%), minor allele frequency of the QTL, residual correlation between the traits, and the sign of the correlation induced by the QTL relative to the residual correlation. We compared the power of the methods using empirically fixed significance thresholds (\u03b1\u200a=\u200a0.05). Our results showed that the multivariate methods implemented in PLINK, SNPTEST, MultiPhen and BIMBAM performed best for the majority of the tested scenarios, with a notable increase in power for scenarios with an opposite sign of genetic and residual correlation. All multivariate analyses resulted in a higher power than univariate analyses, even when only one of the traits was associated with the QTL. Hence, use of multivariate GWAS methods can be recommended, even when genetic correlations between traits are weak.\n"], "author_display": ["Tessel E. Galesloot", "Kristel van Steen", "Lambertus A. L. M. Kiemeney", "Luc L. Janss", "Sita H. Vermeulen"], "article_type": "Research Article", "score": 0.4571116, "title_display": "A Comparison of Multivariate Genome-Wide Association Methods", "publication_date": "2014-04-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0095923"}, {"journal": "PLoS ONE", "abstract": ["\n        Using multiple detection methods can increase the number, kind, and distribution of individuals sampled, which may increase accuracy and precision and reduce cost of population abundance estimates. However, when variables influencing abundance are of interest, if individuals detected via different methods are influenced by the landscape differently, separate analysis of multiple detection methods may be more appropriate. We evaluated the effects of combining two detection methods on the identification of variables important to local abundance using detections of grizzly bears with hair traps (systematic) and bear rubs (opportunistic). We used hierarchical abundance models (N-mixture models) with separate model components for each detection method. If both methods sample the same population, the use of either data set alone should (1) lead to the selection of the same variables as important and (2) provide similar estimates of relative local abundance. We hypothesized that the inclusion of 2 detection methods versus either method alone should (3) yield more support for variables identified in single method analyses (i.e. fewer variables and models with greater weight), and (4) improve precision of covariate estimates for variables selected in both separate and combined analyses because sample size is larger. As expected, joint analysis of both methods increased precision as well as certainty in variable and model selection. However, the single-method analyses identified different variables and the resulting predicted abundances had different spatial distributions. We recommend comparing single-method and jointly modeled results to identify the presence of individual heterogeneity between detection methods in N-mixture models, along with consideration of detection probabilities, correlations among variables, and tolerance to risk of failing to identify variables important to a subset of the population. The benefits of increased precision should be weighed against those risks. The analysis framework presented here will be useful for other species exhibiting heterogeneity by detection method.\n      "], "author_display": ["Tabitha A. Graves", "J. Andrew Royle", "Katherine C. Kendall", "Paul Beier", "Jeffrey B. Stetz", "Amy C. Macleod"], "article_type": "Research Article", "score": 0.45644903, "title_display": "Balancing Precision and Risk: Should Multiple Detection Methods Be Analyzed Separately in N-Mixture Models?", "publication_date": "2012-12-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0049410"}, {"journal": "PLoS ONE", "abstract": ["\nSample preparation is key to the success of proteomics studies. In the present study, two sample preparation methods were tested for their suitability on the mature, recalcitrant leaves of six representative perennial plants (grape, plum, pear, peach, orange, and ramie). An improved sample preparation method was obtained: Tris and Triton X-100 were added together instead of CHAPS to the lysis buffer, and a 20% TCA-water solution and 100% precooled acetone were added after the protein extraction for the further purification of protein. This method effectively eliminates nonprotein impurities and obtains a clear two-dimensional gel electrophoresis array. The method facilitates the separation of high-molecular-weight proteins and increases the resolution of low-abundance proteins. This method provides a widely applicable and economically feasible technology for the proteomic study of the mature, recalcitrant leaves of perennial plants.\n"], "author_display": ["Deng Gang", "Zhong Xinyue", "Zhang Na", "Lao Chengying", "Wang Bo", "Peng Dingxiang", "Liu Lijun"], "article_type": "Research Article", "score": 0.4561027, "title_display": "A Proteomics Sample Preparation Method for Mature, Recalcitrant Leaves of Perennial Plants", "publication_date": "2014-07-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0102175"}, {"journal": "PLoS ONE", "abstract": ["Background: Caenorhbditis elegans has being vigorously used as a model organism in many research fields and often accompanied by administrating with various drugs. The methods of delivering drugs to worms are varied from one study to another, which make difficult in comparing results between studies. Methodology/Principal Findings: We evaluated the drug absorption efficiency in C. elegans using five frequently used methods with resveratrol with low aqueous solubility and water-soluble 5-Fluoro-2\u2032-deoxyuridine (FUDR) as positive compounds. The drugs were either applied to the LB medium with bacteria OP50, before spreading onto Nematode Growth Medium (NGM) plates (LB medium method), or to the NGM with live (NGM live method) or dead bacteria (NGM dead method), or spotting the drug solution to the surface of plates directly (spot dead method), or growing the worms in liquid medium (liquid growing method). The concentration of resveratrol and FUDR increased gradually within C. elegans and reached the highest during 12 hours to one day and then decreased slowly. At the same time point, the higher the drug concentration, the higher the metabolism rate. The drug concentrations in worms fed with dead bacteria were higher than with live bacteria at the same time point. Consistently, the drug concentration in medium with live bacteria decreased much faster than in medium with dead bacteria, reach to about half of the original concentration within 12 hours. Conclusion: Resveratrol with low aqueous solubility and water-soluble FUDR have the same absorption and metabolism pattern. The drug metabolism rate in worms was both dosage and time dependent. NGM dead method and liquid growing method achieved the best absorption efficiency in worms. The drug concentration within worms was comparable with that in mice, providing a bridge for dose translation from worms to mammals. "], "author_display": ["Shan-Qing Zheng", "Ai-Jun Ding", "Guo-Ping Li", "Gui-Sheng Wu", "Huai-Rong Luo"], "article_type": "Research Article", "score": 0.45602334, "title_display": "Drug Absorption Efficiency in <i>Caenorhbditis elegans</i> Delivered by Different Methods", "publication_date": "2013-02-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0056877"}, {"journal": "PLOS ONE", "abstract": ["\nImage segmentation is an indispensable process in the visualization of human tissues, particularly during clinical analysis of brain magnetic resonance (MR) images. For many human experts, manual segmentation is a difficult and time consuming task, which makes an automated brain MR image segmentation method desirable. In this regard, this paper presents a new segmentation method for brain MR images, integrating judiciously the merits of rough-fuzzy computing and multiresolution image analysis technique. The proposed method assumes that the major brain tissues, namely, gray matter, white matter, and cerebrospinal fluid from the MR images are considered to have different textural properties. The dyadic wavelet analysis is used to extract the scale-space feature vector for each pixel, while the rough-fuzzy clustering is used to address the uncertainty problem of brain MR image segmentation. An unsupervised feature selection method is introduced, based on maximum relevance-maximum significance criterion, to select relevant and significant textural features for segmentation problem, while the mathematical morphology based skull stripping preprocessing step is proposed to remove the non-cerebral tissues like skull. The performance of the proposed method, along with a comparison with related approaches, is demonstrated on a set of synthetic and real brain MR images using standard validity indices.\n"], "author_display": ["Pradipta Maji", "Shaswati Roy"], "article_type": "Research Article", "score": 0.45569193, "title_display": "Rough-Fuzzy Clustering and Unsupervised Feature Selection for Wavelet Based MR Image Segmentation", "publication_date": "2015-04-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0123677"}, {"journal": "PLoS ONE", "abstract": ["Background: The interest in prognostic reviews is increasing, but to properly review existing evidence an accurate search filer for finding prediction research is needed. The aim of this paper was to validate and update two previously introduced search filters for finding prediction research in Medline: the Ingui filter and the Haynes Broad filter. Methodology/Principal Findings: Based on a hand search of 6 general journals in 2008 we constructed two sets of papers. Set 1 consisted of prediction research papers (n\u200a=\u200a71), and set 2 consisted of the remaining papers (n\u200a=\u200a1133). Both search filters were validated in two ways, using diagnostic accuracy measures as performance measures. First, we compared studies in set 1 (reference) with studies retrieved by the search strategies as applied in Medline. Second, we compared studies from 4 published systematic reviews (reference) with studies retrieved by the search filter as applied in Medline. Next \u2013 using word frequency methods \u2013 we constructed an additional search string for finding prediction research. Both search filters were good in identifying clinical prediction models: sensitivity ranged from 0.94 to 1.0 using our hand search as reference, and 0.78 to 0.89 using the systematic reviews as reference. This latter performance measure even increased to around 0.95 (range 0.90 to 0.97) when either search filter was combined with the additional string that we developed. Retrieval rate of explorative prediction research was poor, both using our hand search or our systematic review as reference, and even combined with our additional search string: sensitivity ranged from 0.44 to 0.85. Conclusions/Significance: Explorative prediction research is difficult to find in Medline, using any of the currently available search filters. Yet, application of either the Ingui filter or the Haynes broad filter results in a very low number missed clinical prediction model studies. "], "author_display": ["Geert-Jan Geersing", "Walter Bouwmeester", "Peter Zuithoff", "Rene Spijker", "Mariska Leeflang", "Karel Moons"], "article_type": "Research Article", "score": 0.45562062, "title_display": "Search Filters for Finding Prognostic and Diagnostic Prediction Studies in Medline to Enhance Systematic Reviews", "publication_date": "2012-02-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0032844"}, {"journal": "PLoS ONE", "abstract": ["\nThe frequency with which scientists fabricate and falsify data, or commit other forms of scientific misconduct is a matter of controversy. Many surveys have asked scientists directly whether they have committed or know of a colleague who committed research misconduct, but their results appeared difficult to compare and synthesize. This is the first meta-analysis of these surveys.\nTo standardize outcomes, the number of respondents who recalled at least one incident of misconduct was calculated for each question, and the analysis was limited to behaviours that distort scientific knowledge: fabrication, falsification, \u201ccooking\u201d of data, etc\u2026 Survey questions on plagiarism and other forms of professional misconduct were excluded. The final sample consisted of 21 surveys that were included in the systematic review, and 18 in the meta-analysis.\nA pooled weighted average of 1.97% (N\u200a=\u200a7, 95%CI: 0.86\u20134.45) of scientists admitted to have fabricated, falsified or modified data or results at least once \u2013a serious form of misconduct by any standard\u2013 and up to 33.7% admitted other questionable research practices. In surveys asking about the behaviour of colleagues, admission rates were 14.12% (N\u200a=\u200a12, 95% CI: 9.91\u201319.72) for falsification, and up to 72% for other questionable research practices. Meta-regression showed that self reports surveys, surveys using the words \u201cfalsification\u201d or \u201cfabrication\u201d, and mailed surveys yielded lower percentages of misconduct. When these factors were controlled for, misconduct was reported more frequently by medical/pharmacological researchers than others.\nConsidering that these surveys ask sensitive questions and have other limitations, it appears likely that this is a conservative estimate of the true prevalence of scientific misconduct.\n"], "author_display": ["Daniele Fanelli"], "article_type": "Research Article", "score": 0.4555161, "title_display": "How Many Scientists Fabricate and Falsify Research? A Systematic Review and Meta-Analysis of Survey Data", "publication_date": "2009-05-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0005738"}, {"journal": "PLOS ONE", "abstract": ["\nIn most cases, fabrics such as curtains, skirts, suit pants and so on are draped under their own gravity parallel to fabric plane while the gravity is perpendicular to fabric plane in traditional drape testing method. As a result, it does not conform to actual situation and the test data is not convincing enough. To overcome this problem, this paper presents a novel method which simulates the real mechanical conditions and ensures the gravity is parallel to the fabric plane. This method applied a low-cost Kinect Sensor device to capture the 3-dimensional (3D) drape profile, thus we obtained the drape degree parameters and aesthetic parameters by 3D reconstruction and image processing and analysis techniques. The experiment was conducted on our self-devised drape-testing instrument by choosing different kinds of weave structure fabrics as our testing samples and the results were compared with those of traditional method and subjective evaluation. Through regression and correlation analysis we found that this novel testing method was significantly correlated with the traditional and subjective evaluation method. We achieved a new, non-contact 3D measurement method for drape testing, namely unidirectional fabric drape testing method. This method is more suitable for evaluating drape behavior because it is more in line with actual mechanical conditions of draped fabrics and has a well consistency with the requirements of visual and aesthetic style of fabrics.\n"], "author_display": ["Zaihuan Mei", "Wei Shen", "Yan Wang", "Jingzhi Yang", "Ting Zhou", "Hua Zhou"], "article_type": "Research Article", "score": 0.45543253, "title_display": "Unidirectional Fabric Drape Testing Method", "publication_date": "2015-11-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0143648"}, {"journal": "PLOS ONE", "abstract": ["\nThe torque output accuracy of the IPMSM in electric vehicles using a state of the art MTPA strategy highly depends on the accuracy of machine parameters, thus, a torque estimation method is necessary for the safety of the vehicle. In this paper, a torque estimation method based on flux estimator with a modified low pass filter is presented. Moreover, by taking into account the non-ideal characteristic of the inverter, the torque estimation accuracy is improved significantly. The effectiveness of the proposed method is demonstrated through MATLAB/Simulink simulation and experiment.\n"], "author_display": ["Zhihong Wu", "Ke Lu", "Yuan Zhu"], "article_type": "Research Article", "score": 0.45518115, "title_display": "A Practical Torque Estimation Method for Interior Permanent Magnet Synchronous Machine in Electric Vehicles", "publication_date": "2015-06-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0130923"}, {"journal": "PLoS ONE", "abstract": ["Background: \u201cCumulative meta-analysis\u201d describes a statistical procedure to calculate, retrospectively, summary estimates from the results of similar trials every time the results of a further trial in the series had become available. In the early 1990s, comparisons of cumulative meta-analyses of treatments for myocardial infarction with advice promulgated through medical textbooks showed that research had continued long after robust estimates of treatment effects had accumulated, and that medical textbooks had overlooked strong, existing evidence from trials. Cumulative meta-analyses have subsequently been used to assess what could have been known had new studies been informed by systematic reviews of relevant existing evidence and how waste might have been reduced. Methods and Findings: We used a systematic approach to identify and summarise the findings of cumulative meta-analyses of studies of the effects of clinical interventions, published from 1992 to 2012. Searches were done of PubMed, MEDLINE, EMBASE, the Cochrane Methodology Register and Science Citation Index. A total of 50 eligible reports were identified, including more than 1,500 cumulative meta-analyses. A variety of themes are illustrated with specific examples. The studies showed that initially positive results became null or negative in meta-analyses as more trials were done; that early null or negative results were over-turned; that stable results (beneficial, harmful and neutral) would have been seen had a meta-analysis been done before the new trial; and that additional trials had been much too small to resolve the remaining uncertainties. Conclusions: This large, unique collection of cumulative meta-analyses highlights how a review of the existing evidence might have helped researchers, practitioners, patients and funders make more informed decisions and choices about new trials over decades of research. This would have led to earlier uptake of effective interventions in practice, less exposure of trial participants to less effective treatments, and reduced waste resulting from unjustified research. "], "author_display": ["Mike Clarke", "Anne Brice", "Iain Chalmers"], "article_type": "Research Article", "score": 0.45434642, "title_display": "Accumulating Research: A Systematic Account of How Cumulative Meta-Analyses Would Have Provided Knowledge, Improved Health, Reduced Harm and Saved Resources", "publication_date": "2014-07-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0102670"}, {"abstract": ["Background: Emphasis is increasingly being placed on the monitoring and comparison of clinical outcomes between healthcare providers. Funnel plots have become a standard graphical methodology to identify outliers and comprise plotting an outcome summary statistic from each provider against a specified \u2018target\u2019 together with upper and lower control limits. With discrete probability distributions it is not possible to specify the exact probability that an observation from an \u2018in-control\u2019 provider will fall outside the control limits. However, general probability characteristics can be set and specified using interpolation methods. Guidelines recommend that providers falling outside such control limits should be investigated, potentially with significant consequences, so it is important that the properties of the limits are understood. Methods: Control limits for funnel plots for the Standardised Mortality Ratio (SMR) based on the Poisson distribution were calculated using three proposed interpolation methods and the probability calculated of an \u2018in-control\u2019 provider falling outside of the limits. Examples using published data were shown to demonstrate the potential differences in the identification of outliers. Results: The first interpolation method ensured that the probability of an observation of an \u2018in control\u2019 provider falling outside either limit was always less than a specified nominal probability (p). The second method resulted in such an observation falling outside either limit with a probability that could be either greater or less than p, depending on the expected number of events. The third method led to a probability that was always greater than, or equal to, p. Conclusion: The use of different interpolation methods can lead to differences in the identification of outliers. This is particularly important when the expected number of events is small. We recommend that users of these methods be aware of the differences, and specify which interpolation method is to be used prior to any analysis. "], "author_display": ["Bradley N. Manktelow", "Sarah E. Seaton"], "article_type": "Research Article", "score": 0.45384213, "title_display": "Specifying the Probability Characteristics of Funnel Plot Control Limits: An Investigation of Three Approaches", "publication_date": "2012-09-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0045723"}, {"abstract": ["\n        Within functional magnetic resonance imaging (fMRI), the use of the traditional general linear model (GLM) based analysis methods is often restricted to strictly controlled research setups requiring a parametric activation model. Instead, Inter-Subject Correlation (ISC) method is based on voxel-wise correlation between the time series of the subjects, which makes it completely non-parametric and thus suitable for naturalistic stimulus paradigms such as movie watching. In this study, we compared an ISC based analysis results with those of a GLM based in five distinct controlled research setups. We used International Consortium for Brain Mapping functional reference battery (FRB) fMRI data available from the Laboratory of Neuro Imaging image data archive. The selected data included measurements from 37 right-handed subjects, who all had performed the same five tasks from FRB. The GLM was expected to locate activations accurately in FRB data and thus provide good grounds for investigating relationship between ISC and stimulus induced fMRI activation. The statistical maps of ISC and GLM were compared with two measures. The first measure was the Pearson's correlation between the non-thresholded ISC test-statistics and absolute values of the GLM Z-statistics. The average correlation value over five tasks was 0.74. The second was the Dice index between the activation regions of the methods. The average Dice value over the tasks and three threshold levels was 0.73. The results of this study indicated how the data driven ISC analysis found the same foci as the model-based GLM analysis. The agreement of the results is highly interesting, because ISC is applicable in situations where GLM is not suitable, for example, when analyzing data from a naturalistic stimuli experiment.\n      "], "author_display": ["Juha Pajula", "Jukka-Pekka Kauppi", "Jussi Tohka"], "article_type": "Research Article", "score": 0.4535671, "title_display": "Inter-Subject Correlation in fMRI: Method Validation against Stimulus-Model Based Analysis", "publication_date": "2012-08-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0041196"}, {"journal": "PLoS ONE", "abstract": ["\n        This study deals with an effective nucleic acids extraction method from various strains of Botryococcus braunii which possesses an extensive extracellular matrix. A method combining freeze/thaw and bead-beating with heterogeneous diameter of silica/zirconia beads was optimized to isolate DNA and RNA from microalgae, especially from B. braunii. Eukaryotic Microalgal Nucleic Acids Extraction (EMNE) method developed in this study showed at least 300 times higher DNA yield in all strains of B. braunii with high integrity and 50 times reduced working volume compared to commercially available DNA extraction kits. High quality RNA was also extracted using this method and more than two times the yield compared to existing methods. Real-time experiments confirmed the quality and quantity of the input DNA and RNA extracted using EMNE method. The method was also applied to other eukaryotic microalgae, such as diatoms, Chlamydomonas sp., Chlorella sp., and Scenedesmus sp. resulting in higher efficiencies. Cost-effectiveness analysis of DNA extraction by various methods revealed that EMNE method was superior to commercial kits and other reported methods by >15%. This method would immensely contribute to area of microalgal genomics.\n      "], "author_display": ["Byung-Hyuk Kim", "Rishiram Ramanan", "Dae-Hyun Cho", "Gang-Guk Choi", "Hyun-Joon La", "Chi-Yong Ahn", "Hee-Mock Oh", "Hee-Sik Kim"], "article_type": "Research Article", "score": 0.45356253, "title_display": "Simple, Rapid and Cost-Effective Method for High Quality Nucleic Acids Extraction from Different Strains of <i>Botryococcus braunii</i>", "publication_date": "2012-05-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0037770"}, {"journal": "PLOS ONE", "abstract": ["Background: In Europe, men have lower rates of attempted suicide compared to women and at the same time a higher rate of completed suicides, indicating major gender differences in lethality of suicidal behaviour. The aim of this study was to analyse the extent to which these gender differences in lethality can be explained by factors such as choice of more lethal methods or lethality differences within the same suicide method or age. In addition, we explored gender differences in the intentionality of suicide attempts. Methods and Findings: Methods. Design: Epidemiological study using a combination of self-report and official data. Setting: Mental health care services in four European countries: Germany, Hungary, Ireland, and Portugal. Data basis: Completed suicides derived from official statistics for each country (767 acts, 74.4% male) and assessed suicide attempts excluding habitual intentional self-harm (8,175 acts, 43.2% male). Findings Main Results: Suicidal acts (fatal and non-fatal) were 3.4 times more lethal in men than in women (lethality 13.91% (regarding 4106 suicidal acts) versus 4.05% (regarding 4836 suicidal acts)), the difference being significant for the methods hanging, jumping, moving objects, sharp objects and poisoning by substances other than drugs. Median age at time of suicidal behaviour (35\u201344 years) did not differ between males and females. The overall gender difference in lethality of suicidal behaviour was explained by males choosing more lethal suicide methods (odds ratio (OR) = 2.03; 95% CI = 1.65 to 2.50; p < 0.000001) and additionally, but to a lesser degree, by a higher lethality of suicidal acts for males even within the same method (OR = 1.64; 95% CI = 1.32 to 2.02; p = 0.000005). Results of a regression analysis revealed neither age nor country differences were significant predictors for gender differences in the lethality of suicidal acts. The proportion of serious suicide attempts among all non-fatal suicidal acts with known intentionality (NFSAi) was significantly higher in men (57.1%; 1,207 of 2,115 NFSAi) than in women (48.6%; 1,508 of 3,100 NFSAi) (\u03c72 = 35.74; p < 0.000001). Main limitations of the study: Due to restrictive data security regulations to ensure anonymity in Ireland, specific ages could not be provided because of the relatively low absolute numbers of suicide in the Irish intervention and control region. Therefore, analyses of the interaction between gender and age could only be conducted for three of the four countries. Attempted suicides were assessed for patients presenting to emergency departments or treated in hospitals. An unknown rate of attempted suicides remained undetected. This may have caused an overestimation of the lethality of certain methods. Moreover, the detection of attempted suicides and the registration of completed suicides might have differed across the four countries. Some suicides might be hidden and misclassified as undetermined deaths. Conclusions: Men more often used highly lethal methods in suicidal behaviour, but there was also a higher method-specific lethality which together explained the large gender differences in the lethality of suicidal acts. Gender differences in the lethality of suicidal acts were fairly consistent across all four European countries examined. Males and females did not differ in age at time of suicidal behaviour. Suicide attempts by males were rated as being more serious independent of the method used, with the exceptions of attempted hanging, suggesting gender differences in intentionality associated with suicidal behaviour. These findings contribute to understanding of the spectrum of reasons for gender differences in the lethality of suicidal behaviour and should inform the development of gender specific strategies for suicide prevention. "], "author_display": ["Roland Mergl", "Nicole Koburger", "Katherina Heinrichs", "Andr\u00e1s Sz\u00e9kely", "M\u00f3nika Ditta T\u00f3th", "James Coyne", "S\u00f3nia Quint\u00e3o", "Ella Arensman", "Claire Coffey", "Margaret Maxwell", "Airi V\u00e4rnik", "Chantal van Audenhove", "David McDaid", "Marco Sarchiapone", "Armin Schmidtke", "Axel Genz", "Ricardo Gusm\u00e3o", "Ulrich Hegerl"], "article_type": "Research Article", "score": 0.45355064, "title_display": "What Are Reasons for the Large Gender Differences in the Lethality of Suicidal Acts? An Epidemiological Analysis in Four European Countries", "publication_date": "2015-07-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0129062"}, {"journal": "PLOS ONE", "abstract": ["\nRegion-based association analysis is a more powerful tool for gene mapping than testing of individual genetic variants, particularly for rare genetic variants. The most powerful methods for regional mapping are based on the functional data analysis approach, which assumes that the regional genome of an individual may be considered as a continuous stochastic function that contains information about both linkage and linkage disequilibrium. Here, we extend this powerful approach, earlier applied only to independent samples, to the samples of related individuals. To this end, we additionally include a random polygene effects in functional linear model used for testing association between quantitative traits and multiple genetic variants in the region. We compare the statistical power of different methods using Genetic Analysis Workshop 17 mini-exome family data and a wide range of simulation scenarios. Our method increases the power of regional association analysis of quantitative traits compared with burden-based and kernel-based methods for the majority of the scenarios. In addition, we estimate the statistical power of our method using regions with small number of genetic variants, and show that our method retains its advantage over burden-based and kernel-based methods in this case as well. The new method is implemented as the R-function \u2018famFLM\u2019 using two types of basis functions: the B-spline and Fourier bases. We compare the properties of the new method using models that differ from each other in the type of their function basis. The models based on the Fourier basis functions have an advantage in terms of speed and power over the models that use the B-spline basis functions and those that combine B-spline and Fourier basis functions. The \u2018famFLM\u2019 function is distributed under GPLv3 license and is freely available at http://mga.bionet.nsc.ru/soft/famFLM/.\n"], "author_display": ["Gulnara R. Svishcheva", "Nadezhda M. Belonogova", "Tatiana I. Axenovich"], "article_type": "Research Article", "score": 0.4534362, "title_display": "Region-Based Association Test for Familial Data under Functional Linear Models", "publication_date": "2015-06-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0128999"}, {"journal": "PLoS ONE", "abstract": ["\nOne of the key aspects of computational systems biology is the investigation on the dynamic biological processes within cells. Computational models are often required to elucidate the mechanisms and principles driving the processes because of the nonlinearity and complexity. The models usually incorporate a set of parameters that signify the physical properties of the actual biological systems. In most cases, these parameters are estimated by fitting the model outputs with the corresponding experimental data. However, this is a challenging task because the available experimental data are frequently noisy and incomplete. In this paper, a new hybrid optimization method is proposed to estimate these parameters from the noisy and incomplete experimental data. The proposed method, called Swarm-based Chemical Reaction Optimization, integrates the evolutionary searching strategy employed by the Chemical Reaction Optimization, into the neighbouring searching strategy of the Firefly Algorithm method. The effectiveness of the method was evaluated using a simulated nonlinear model and two biological models: synthetic transcriptional oscillators, and extracellular protease production models. The results showed that the accuracy and computational speed of the proposed method were better than the existing Differential Evolution, Firefly Algorithm and Chemical Reaction Optimization methods. The reliability of the estimated parameters was statistically validated, which suggests that the model outputs produced by these parameters were valid even when noisy and incomplete experimental data were used. Additionally, Akaike Information Criterion was employed to evaluate the model selection, which highlighted the capability of the proposed method in choosing a plausible model based on the experimental data. In conclusion, this paper presents the effectiveness of the proposed method for parameter estimation and model selection problems using noisy and incomplete experimental data. This study is hoped to provide a new insight in developing more accurate and reliable biological models based on limited and low quality experimental data.\n"], "author_display": ["Afnizanfaizal Abdullah", "Safaai Deris", "Mohd Saberi Mohamad", "Sohail Anwar"], "article_type": "Research Article", "score": 0.45336825, "title_display": "An Improved Swarm Optimization for Parameter Estimation and Biological Model Selection", "publication_date": "2013-04-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0061258"}, {"journal": "PLOS ONE", "abstract": ["\nFollowing the rapid development of social media, sentiment analysis has become an important social media mining technique. The performance of automatic sentiment analysis primarily depends on feature selection and sentiment classification. While information gain (IG) and support vector machines (SVM) are two important techniques, few studies have optimized both approaches in sentiment analysis. The effectiveness of applying a global optimization approach to sentiment analysis remains unclear. We propose a global optimization-based sentiment analysis (PSOGO-Senti) approach to improve sentiment analysis with IG for feature selection and SVM as the learning engine. The PSOGO-Senti approach utilizes a particle swarm optimization algorithm to obtain a global optimal combination of feature dimensions and parameters in the SVM. We evaluate the PSOGO-Senti model on two datasets from different fields. The experimental results showed that the PSOGO-Senti model can improve binary and multi-polarity Chinese sentiment analysis. We compared the optimal feature subset selected by PSOGO-Senti with the features in the sentiment dictionary. The results of this comparison indicated that PSOGO-Senti can effectively remove redundant and noisy features and can select a domain-specific feature subset with a higher-explanatory power for a particular sentiment analysis task. The experimental results showed that the PSOGO-Senti approach is effective and robust for sentiment analysis tasks in different domains. By comparing the improvements of two-polarity, three-polarity and five-polarity sentiment analysis results, we found that the five-polarity sentiment analysis delivered the largest improvement. The improvement of the two-polarity sentiment analysis was the smallest. We conclude that the PSOGO-Senti achieves higher improvement for a more complicated sentiment analysis task. We also compared the results of PSOGO-Senti with those of the genetic algorithm (GA) and grid search method. From the results of this comparison, we found that PSOGO-Senti is more suitable for improving a difficult multi-polarity sentiment analysis problem.\n"], "author_display": ["Xinmiao Li", "Jing Li", "Yukeng Wu"], "article_type": "Research Article", "score": 0.45329708, "title_display": "A Global Optimization Approach to Multi-Polarity Sentiment Analysis", "publication_date": "2015-04-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0124672"}, {"journal": "PLOS ONE", "abstract": ["Purpose of the study: This study seeks to explore methods for conducting economic evaluations alongside multinational trials by conducting a systematic review of the methods used in practice and the challenges that are typically faced by the researchers who conducted the economic evaluations. Methods: A review was conducted for the period 2002 to 2012, with potentially relevant articles identified by searching the Medline, Embase and NHS EED databases. Studies were included if they were full economic evaluations conducted alongside a multinational trial. Results: A total of 44 studies out of a possible 2667 met the inclusion criteria. Methods used for the analyses varied between studies, indicating a lack of consensus on how economic evaluation alongside multinational studies should be carried out. The most common challenge appeared to be related to addressing differences between countries, which potentially hinders the generalisability and transferability of results. Other challenges reported included inadequate sample sizes and choosing cost-effectiveness thresholds. Conclusions: It is recommended that additional guidelines be developed to aid researchers in this area and that these be based on an understanding of the challenges associated with multinational trials and the strengths and limitations of alternative approaches. Guidelines should focus on ensuring that results will aid decision makers in their individual countries. "], "author_display": ["Raymond Oppong", "Sue Jowett", "Tracy E. Roberts"], "article_type": "Research Article", "score": 0.45307428, "title_display": "Economic Evaluation alongside Multinational Studies: A Systematic Review of Empirical Studies", "publication_date": "2015-06-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0131949"}, {"journal": "PLoS ONE", "abstract": ["Introduction: Mortality data provide essential evidence on the health status of populations in crisis-affected and resource-poor settings and to guide and assess relief operations. Retrospective surveys are commonly used to collect mortality data in such populations, but require substantial resources and have important methodological limitations. We evaluated the feasibility of an alternative method for rapidly quantifying mortality (the informant method). The study objective was to assess the economic feasibility of the informant method. Methods: The informant method captures deaths through an exhaustive search for all deaths occurring in a population over a defined and recent recall period, using key community informants and next-of-kin of decedents. Between July and October 2008, we implemented and evaluated the informant method in: Kabul, Afghanistan; Mae La camp for Karen refugees, Thai-Burma border; Chiradzulu District, Malawi; and Lugufu and Mtabila refugee camps, Tanzania. We documented the time and cost inputs for the informant method in each site, and compared these with projections for hypothetical retrospective mortality surveys implemented in the same site with a 6 month recall period and with a 30 day recall period. Findings: The informant method was estimated to require an average of 29% less time inputs and 33% less monetary inputs across all four study sites when compared with retrospective surveys with a 6 month recall period, and 88% less time inputs and 86% less monetary inputs when compared with retrospective surveys with a 1 month recall period. Verbal autopsy questionnaires were feasible and efficient, constituting only 4% of total person-time for the informant method's implementation in Chiradzulu District. Conclusions: The informant method requires fewer resources and incurs less respondent burden. The method's generally impressive feasibility and the near real-time mortality data it provides warrant further work to develop the method given the importance of mortality measurement in such settings. "], "author_display": ["Bayard Roberts", "Oliver W. Morgan", "Mohammed Ghaus Sultani", "Peter Nyasulu", "Sunday Rwebangila", "Egbert Sondorp", "Daniel Chandramohan", "Francesco Checchi"], "article_type": "Research Article", "score": 0.45274168, "title_display": "Economic Feasibility of a New Method to Estimate Mortality in Crisis-Affected and Resource-Poor Settings", "publication_date": "2011-09-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0025175"}, {"journal": "PLoS ONE", "abstract": ["\nIn blood oxygen level dependent (BOLD) functional magnetic resonance imaging (fMRI), assessing functional connectivity between and within brain networks from datasets acquired during steady-state conditions has become increasingly common. However, in contrast to connectivity analyses based on task-evoked signal changes, selecting the optimal spatial location of the regions of interest (ROIs) whose timecourses will be extracted and used in subsequent analyses is not straightforward. Moreover, it is also unknown how different choices of the precise anatomical locations within given brain regions influence the estimates of functional connectivity under steady-state conditions. The objective of the present study was to assess the variability in estimates of functional connectivity induced by different anatomical choices of ROI locations for a given brain network. We here targeted the default mode network (DMN) sampled during both resting-state and a continuous verbal 2-back working memory task to compare four different methods to extract ROIs in terms of ROI features (spatial overlap, spatial functional heterogeneity), signal features (signal distribution, mean, variance, correlation) as well as strength of functional connectivity as a function of condition. We show that, while different ROI selection methods produced quantitatively different results, all tested ROI selection methods agreed on the final conclusion that functional connectivity within the DMN decreased during the continuous working memory task compared to rest.\n"], "author_display": ["Guillaume Marrelec", "Peter Fransson"], "article_type": "Research Article", "score": 0.45273715, "title_display": "Assessing the Influence of Different ROI Selection Strategies on Functional Connectivity Analyses of fMRI Data Acquired During Steady-State Conditions", "publication_date": "2011-04-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0014788"}, {"journal": "PLoS ONE", "abstract": ["Background: Pedigree studies of complex heritable diseases often feature nominal or ordinal phenotypic measurements and missing genetic marker or phenotype data. Methodology: We have developed a Bayesian method for Linkage analysis of Ordinal and Categorical traits (LOCate) that can analyze complex genealogical structure for family groups and incorporate missing data. LOCate uses a Gibbs sampling approach to assess linkage, incorporating a simulated tempering algorithm for fast mixing. While our treatment is Bayesian, we develop a LOD (log of odds) score estimator for assessing linkage from Gibbs sampling that is highly accurate for simulated data. LOCate is applicable to linkage analysis for ordinal or nominal traits, a versatility which we demonstrate by analyzing simulated data with a nominal trait, on which LOCate outperforms LOT, an existing method which is designed for ordinal traits. We additionally demonstrate our method's versatility by analyzing a candidate locus (D2S1788) for panic disorder in humans, in a dataset with a large amount of missing data, which LOT was unable to handle. Conclusion: LOCate's accuracy and applicability to both ordinal and nominal traits will prove useful to researchers interested in mapping loci for categorical traits. "], "author_display": ["Abra Brisbin", "Myrna M. Weissman", "Abby J. Fyer", "Steven P. Hamilton", "James A. Knowles", "Carlos D. Bustamante", "Jason G. Mezey"], "article_type": "Research Article", "score": 0.4524409, "title_display": "Bayesian Linkage Analysis of Categorical Traits for Arbitrary Pedigree Designs", "publication_date": "2010-08-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0012307"}, {"journal": "PLOS ONE", "abstract": ["Background: Patient and public involvement (PPI) is advocated in clinical trials yet evidence on how to optimise its impact is limited. We explored researchers' and PPI contributors' accounts of the impact of PPI within trials and factors likely to influence its impact. Methods: Semi-structured qualitative interviews with researchers and PPI contributors accessed through a cohort of randomised clinical trials. Analysis of transcripts of audio-recorded interviews was informed by the principles of the constant comparative method, elements of content analysis and informant triangulation. Results: We interviewed 21 chief investigators, 10 trial managers and 17 PPI contributors from 28 trials. The accounts of informants within the same trials were largely in agreement. Over half the informants indicted PPI had made a difference within a trial, through contributions that influenced either an aspect of a trial, or how researchers thought about a trial. According to informants, the opportunity for PPI to make a difference was influenced by two main factors: whether chief investigators had goals and plans for PPI and the quality of the relationship between the research team and the PPI contributors. Early involvement of PPI contributors and including them in responsive (e.g. advisory groups) and managerial (e.g. trial management groups) roles were more likely to achieve impact compared to late involvement and oversight roles (e.g. trial steering committees). Conclusion: Those seeking to enhance PPI in trials should develop goals for PPI at an early stage that fits the needs of the trial, plan PPI implementation in accordance with these goals, invest in developing good relationships between PPI contributors and researchers, and favour responsive and managerial roles for contributors in preference to oversight-only roles. These features could be used by research funders in judging PPI in trial grant applications and to inform policies to optimise PPI within trials. "], "author_display": ["Louise Dudley", "Carrol Gamble", "Jennifer Preston", "Deborah Buck", "The EPIC Patient Advisory Group ", "Bec Hanley", "Paula Williamson", "Bridget Young"], "article_type": "Research Article", "score": 0.45234048, "title_display": "What Difference Does Patient and Public Involvement Make and What Are Its Pathways to Impact? Qualitative Study of Patients and Researchers from a Cohort of Randomised Clinical Trials", "publication_date": "2015-06-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0128817"}, {"journal": "PLoS Medicine", "abstract": [": Jonathan Cook and colleagues systematically reviewed the literature for methods of determining the target difference for use in calculating the necessary sample size for clinical trials, and discuss which methods are best for various types of trials. Background: Randomised controlled trials (RCTs) are widely accepted as the preferred study design for evaluating healthcare interventions. When the sample size is determined, a (target) difference is typically specified that the RCT is designed to detect. This provides reassurance that the study will be informative, i.e., should such a difference exist, it is likely to be detected with the required statistical precision. The aim of this review was to identify potential methods for specifying the target difference in an RCT sample size calculation. Methods and Findings: A comprehensive systematic review of medical and non-medical literature was carried out for methods that could be used to specify the target difference for an RCT sample size calculation. The databases searched were MEDLINE, MEDLINE In-Process, EMBASE, the Cochrane Central Register of Controlled Trials, the Cochrane Methodology Register, PsycINFO, Science Citation Index, EconLit, the Education Resources Information Center (ERIC), and Scopus (for in-press publications); the search period was from 1966 or the earliest date covered, to between November 2010 and January 2011. Additionally, textbooks addressing the methodology of clinical trials and International Conference on Harmonisation of Technical Requirements for Registration of Pharmaceuticals for Human Use (ICH) tripartite guidelines for clinical trials were also consulted. A narrative synthesis of methods was produced. Studies that described a method that could be used for specifying an important and/or realistic difference were included. The search identified 11,485 potentially relevant articles from the databases searched. Of these, 1,434 were selected for full-text assessment, and a further nine were identified from other sources. Fifteen clinical trial textbooks and the ICH tripartite guidelines were also reviewed. In total, 777 studies were included, and within them, seven methods were identified\u2014anchor, distribution, health economic, opinion-seeking, pilot study, review of the evidence base, and standardised effect size. Conclusions: A variety of methods are available that researchers can use for specifying the target difference in an RCT sample size calculation. Appropriate methods may vary depending on the aim (e.g., specifying an important difference versus a realistic difference), context (e.g., research question and availability of data), and underlying framework adopted (e.g., Bayesian versus conventional statistical approach). Guidance on the use of each method is given. No single method provides a perfect solution for all contexts. Background: A clinical trial is a research study in which human volunteers are randomized to receive a given intervention or not, and outcomes are measured in both groups to determine the effect of the intervention. Randomized controlled trials (RCTs) are widely accepted as the preferred study design because by randomly assigning participants to groups, any differences between the two groups, other than the intervention under study, are due to chance. To conduct a RCT, investigators calculate how many patients they need to enroll to determine whether the intervention is effective. The number of patients they need to enroll depends on how effective the intervention is expected to be, or would need to be in order to be clinically important. The assumed difference between the two groups is the target difference. A larger target difference generally means that fewer patients need to be enrolled, relative to a smaller target difference. The target difference and number of patients enrolled contribute to the study's statistical precision, and the ability of the study to determine whether the intervention is effective. Selecting an appropriate target difference is important from both a scientific and ethical standpoint. Why Was This Study Done?: There are several ways to determine an appropriate target difference. The authors wanted to determine what methods for specifying the target difference are available and when they can be used. What Did the Researchers Do and Find?: To identify studies that used a method for determining an important and/or realistic difference, the investigators systematically surveyed the research literature. Two reviewers screened each of the abstracts chosen, and a third reviewer was consulted if necessary. The authors identified seven methods to determine target differences. They evaluated the studies to establish similarities and differences of each application. Points about the strengths and limitations of the method and how frequently the method was chosen were also noted. What Do these Findings Mean?: The study draws attention to an understudied but important part of designing a clinical trial. Enrolling the right number of patients is very important\u2014too few patients and the study may not be able to answer the study question; too many and the study will be more expensive and more difficult to conduct, and will unnecessarily expose more patients to any study risks. The target difference may also be helpful in interpreting the results of the trial. The authors discuss the pros and cons of different ways to calculate target differences and which methods are best for which types of studies, to help inform researchers designing such studies. Additional Information: Please access these websites via the online version of this summary at http://dx.doi.org/10.1371/journal.pmed.1001645. "], "author_display": ["Jenni Hislop", "Temitope E. Adewuyi", "Luke D. Vale", "Kirsten Harrild", "Cynthia Fraser", "Tara Gurung", "Douglas G. Altman", "Andrew H. Briggs", "Peter Fayers", "Craig R. Ramsay", "John D. Norrie", "Ian M. Harvey", "Brian Buckley", "Jonathan A. Cook", "for the DELTA group "], "article_type": "Research Article", "score": 0.45221514, "title_display": "Methods for Specifying the Target Difference in a Randomised Controlled Trial: The Difference ELicitation in TriAls (DELTA) Systematic Review", "publication_date": "2014-05-13T00:00:00Z", "eissn": "1549-1676", "id": "10.1371/journal.pmed.1001645"}, {"journal": "PLoS ONE", "abstract": ["\n        Previous studies have shown that the identification and analysis of both abundant and rare k-mers or \u201cDNA words of length k\u201d in genomic sequences using suitable statistical background models can reveal biologically significant sequence elements. Other studies have investigated the uni/multimodal distribution of k-mer abundances or \u201ck-mer spectra\u201d in different DNA sequences. However, the existing background models are affected to varying extents by compositional bias. Moreover, the distribution of k-mer abundances in the context of related genomes has not been studied previously. Here, we present a novel statistical background model for calculating k-mer enrichment in DNA sequences based on the average of the frequencies of the two (k-1) mers for each k-mer. Comparison of our null model with the commonly used ones, including Markov models of different orders and the single mismatch model, shows that our method is more robust to compositional AT-rich bias and detects many additional, repeat-poor over-abundant k-mers that are biologically meaningful. Analysis of overrepresented genomic k-mers (4\u2264k\u226416) from four yeast species using this model showed that the fraction of overrepresented DNA words falls linearly as k increases; however, a significant number of overabundant k-mers exists at higher values of k. Finally, comparative analysis of k-mer abundance scores across four yeast species revealed a mixture of unimodal and multimodal spectra for the various genomic sub-regions analyzed.\n      "], "author_display": ["Ramkumar Hariharan", "Reji Simon", "M. Radhakrishna Pillai", "Todd D. Taylor"], "article_type": "Research Article", "score": 0.45210493, "title_display": "Comparative Analysis of DNA Word Abundances in Four Yeast Genomes Using a Novel Statistical Background Model", "publication_date": "2013-03-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0058038"}, {"journal": "PLoS ONE", "abstract": ["\n        There exist several methods for calculating the fractal dimension of objects represented as 2D digital images. For example, Box counting, Minkowski dilation or Fourier analysis can be employed. However, there appear to be some limitations. It is not possible to calculate only the fractal dimension of an irregular region of interest in an image or to perform the calculations in a particular direction along a line on an arbitrary angle through the image. The calculations must be made for the whole image. In this paper, a new method to overcome these limitations is proposed. 2D images are appropriately prepared in order to apply 1D signal analyses, originally developed to investigate nonlinear time series. The Higuchi dimension of these 1D signals is calculated using Higuchi's algorithm, and it is shown that both regions of interests and directional dependencies can be evaluated independently of the whole picture. A thorough validation of the proposed technique and a comparison of the new method to the Fourier dimension, a common two dimensional method for digital images, are given. The main result is that Higuchi's algorithm allows a direction dependent as well as direction independent analysis. Actual values for the fractal dimensions are reliable and an effective treatment of regions of interests is possible. Moreover, the proposed method is not restricted to Higuchi's algorithm, as any 1D method of analysis, can be applied.\n      "], "author_display": ["Helmut Ahammer"], "article_type": "Research Article", "score": 0.45190936, "title_display": "Higuchi Dimension of Digital Images", "publication_date": "2011-09-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0024796"}, {"journal": "PLoS ONE", "abstract": ["\n        To understand the role of human microbiota in health and disease, we need to study effects of environmental and other epidemiological variables on the composition of microbial communities. The composition of a microbial community may depend on multiple factors simultaneously. Therefore we need multivariate methods for detecting, analyzing and visualizing the interactions between environmental variables and microbial communities. We provide two different approaches for multivariate analysis of these complex combined datasets: (i) We select variables that correlate with overall microbiota composition and microbiota members that correlate with the metadata using canonical correlation analysis, determine independency of the observed correlations in a multivariate regression analysis, and visualize the effect size and direction of the observed correlations using heatmaps; (ii) We select variables and microbiota members using univariate or bivariate regression analysis, followed by multivariate regression analysis, and visualize the effect size and direction of the observed correlations using heatmaps. We illustrate the results of both approaches using a dataset containing respiratory microbiota composition and accompanying metadata. The two different approaches provide slightly different results; with approach (i) using canonical correlation analysis to select determinants and microbiota members detecting fewer and stronger correlations only and approach (ii) using univariate or bivariate analyses to select determinants and microbiota members detecting a similar but broader pattern of correlations. The proposed approaches both detect and visualize independent correlations between multiple environmental variables and members of the microbial community. Depending on the size of the datasets and the hypothesis tested one can select the method of preference.\n      "], "author_display": ["Xinhui Wang", "Marinus J. C. Eijkemans", "Jacco Wallinga", "Giske Biesbroek", "Krzysztof Trzci\u0144ski", "Elisabeth A. M. Sanders", "Debby Bogaert"], "article_type": "Research Article", "score": 0.4518757, "title_display": "Multivariate Approach for Studying Interactions between Environmental Variables and Microbial Communities", "publication_date": "2012-11-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0050267"}, {"abstract": ["\n        The investigation of associations between rare genetic variants and diseases or phenotypes has two goals. Firstly, the identification of which genes or genomic regions are associated, and secondly, discrimination of associated variants from background noise within each region. Over the last few years, many new methods have been developed which associate genomic regions with phenotypes. However, classical methods for high-dimensional data have received little attention. Here we investigate whether several classical statistical methods for high-dimensional data: ridge regression (RR), principal components regression (PCR), partial least squares regression (PLS), a sparse version of PLS (SPLS), and the LASSO are able to detect associations with rare genetic variants. These approaches have been extensively used in statistics to identify the true associations in data sets containing many predictor variables. Using genetic variants identified in three genes that were Sanger sequenced in 1998 individuals, we simulated continuous phenotypes under several different models, and we show that these feature selection and feature extraction methods can substantially outperform several popular methods for rare variant analysis. Furthermore, these approaches can identify which variants are contributing most to the model fit, and therefore both goals of rare variant analysis can be achieved simultaneously with the use of regression regularization methods. These methods are briefly illustrated with an analysis of adiponectin levels and variants in the ADIPOQ gene.\n      "], "author_display": ["ChangJiang Xu", "Martin Ladouceur", "Zari Dastani", "J. Brent Richards", "Antonio Ciampi", "Celia M. T. Greenwood"], "article_type": "Research Article", "score": 0.45121437, "title_display": "Multiple Regression Methods Show Great Potential for Rare Variant Association Tests", "publication_date": "2012-08-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0041694"}, {"journal": "PLoS ONE", "abstract": ["\nGene set analysis allows the inclusion of knowledge from established gene sets, such as gene pathways, and potentially improves the power of detecting differentially expressed genes. However, conventional methods of gene set analysis focus on gene marginal effects in a gene set, and ignore gene interactions which may contribute to complex human diseases. In this study, we propose a method of gene interaction enrichment analysis, which incorporates knowledge of predefined gene sets (e.g. gene pathways) to identify enriched gene interaction effects on a phenotype of interest. In our proposed method, we also discuss the reduction of irrelevant genes and the extraction of a core set of gene interactions for an identified gene set, which contribute to the statistical variation of a phenotype of interest. The utility of our method is demonstrated through analyses on two publicly available microarray datasets. The results show that our method can identify gene sets that show strong gene interaction enrichments. The enriched gene interactions identified by our method may provide clues to new gene regulation mechanisms related to the studied phenotypes. In summary, our method offers a powerful tool for researchers to exhaustively examine the large numbers of gene interactions associated with complex human diseases, and can be a useful complement to classical gene set analyses which only considers single genes in a gene set.\n"], "author_display": ["Jigang Zhang", "Jian Li", "Hong-Wen Deng"], "article_type": "Research Article", "score": 0.4505574, "title_display": "Identifying Gene Interaction Enrichment for Gene Expression Data", "publication_date": "2009-11-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0008064"}, {"journal": "PLOS ONE", "abstract": ["\nNext Generation Sequencing (NGS) methods are driving profound changes in biomedical research, with a growing impact on patient care. Many academic medical centers are evaluating potential models to prepare for the rapid increase in NGS information needs. This study sought to investigate (1) how and where sequencing data is generated and analyzed, (2) research objectives and goals for NGS, (3) workforce capacity and unmet needs, (4) storage capacity and unmet needs, (5) available and anticipated funding resources, and (6) future challenges. As a precursor to informed decision making at our institution, we undertook a systematic needs assessment of investigators using survey methods. We recruited 331 investigators from over 60 departments and divisions at the University of Pittsburgh Schools of Health Sciences and had 140 respondents, or a 42% response rate. Results suggest that both sequencing and analysis bottlenecks currently exist. Significant educational needs were identified, including both investigator-focused needs, such as selection of NGS methods suitable for specific research objectives, and program-focused needs, such as support for training an analytic workforce. The absence of centralized infrastructure was identified as an important institutional gap. Key principles for organizations managing this change were formulated based on the survey responses. This needs assessment provides an in-depth case study which may be useful to other academic medical centers as they identify and plan for future needs.\n"], "author_display": ["Albert Geskin", "Elizabeth Legowski", "Anish Chakka", "Uma R Chandran", "M. Michael Barmada", "William A. LaFramboise", "Jeremy Berg", "Rebecca S. Jacobson"], "article_type": "Research Article", "score": 0.44978493, "title_display": "Needs Assessment for Research Use of High-Throughput Sequencing at a Large Academic Medical Center", "publication_date": "2015-06-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0131166"}, {"journal": "PLoS ONE", "abstract": ["Background: Research ethics consultation programs are being established with a goal of addressing the ethical, societal, and policy considerations associated with biomedical research. A number of these programs are modelled after clinical ethics consultation services that began to be institutionalized in the 1980s. Our objective was to determine biomedical science researchers' perceived need for and utility of research ethics consultation, through examination of their perceptions of whether they and their institutions faced ethical, social or policy issues (outside those mandated by regulation) and examination of willingness to seek advice in addressing these issues. We conducted telephone interviews and focus groups in 2006 with researchers from Stanford University and a mailed survey in December 2006 to 7 research universities in the U.S. Findings: A total of 16 researchers were interviewed (75% response rate), 29 participated in focus groups, and 856 responded to the survey (50% response rate). Approximately half of researchers surveyed (51%) reported that they would find a research ethics consultation service at their institution moderately, very or extremely useful, while over a third (36%) reported that such a service would be useful to them personally. Respondents conducting human subjects research were more likely to find such a service very to extremely useful to them personally than respondents not conducting human subjects research (20% vs 10%; chi2 p<0.001). Conclusion: Our findings indicate that biomedical researchers do encounter and anticipate encountering ethical and societal questions and concerns and a substantial proportion, especially clinical researchers, would likely use a consultation service if they were aware of it. These findings provide data to inform the development of such consultation programs in general. "], "author_display": ["Jennifer B. McCormick", "Angie M. Boyce", "Mildred K. Cho"], "article_type": "Research Article", "score": 0.44973367, "title_display": "Biomedical Scientists' Perceptions of Ethical and Social Implications: Is There a Role for Research Ethics Consultation?", "publication_date": "2009-03-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0004659"}, {"journal": "PLoS ONE", "abstract": ["Background: Medication errors are an important source of potentially preventable morbidity and mortality. The PINCER study, a cluster randomised controlled trial, is one of the world\u2019s first experimental studies aiming to reduce the risk of such medication related potential for harm in general practice. Bayesian analyses can improve the clinical interpretability of trial findings. Methods: Experts were asked to complete a questionnaire to elicit opinions of the likely effectiveness of the intervention for the key outcomes of interest - three important primary care medication errors. These were averaged to generate collective prior distributions, which were then combined with trial data to generate Bayesian posterior distributions. The trial data were analysed in two ways: firstly replicating the trial reported cohort analysis acknowledging pairing of observations, but excluding non-paired observations; and secondly as cross-sectional data, with no exclusions, but without acknowledgement of the pairing. Frequentist and Bayesian analyses were compared. Findings: Bayesian evaluations suggest that the intervention is able to reduce the likelihood of one of the medication errors by about 50 (estimated to be between 20% and 70%). However, for the other two main outcomes considered, the evidence that the intervention is able to reduce the likelihood of prescription errors is less conclusive. Conclusions: Clinicians are interested in what trial results mean to them, as opposed to what trial results suggest for future experiments. This analysis suggests that the PINCER intervention is strongly effective in reducing the likelihood of one of the important errors; not necessarily effective in reducing the other errors. Depending on the clinical importance of the respective errors, careful consideration should be given before implementation, and refinement targeted at the other errors may be something to consider. "], "author_display": ["Karla Hemming", "Peter J. Chilton", "Richard J. Lilford", "Anthony Avery", "Aziz Sheikh"], "article_type": "Research Article", "score": 0.44973263, "title_display": "Bayesian Cohort and Cross-Sectional Analyses of the PINCER Trial: A Pharmacist-Led Intervention to Reduce Medication Errors in Primary Care", "publication_date": "2012-06-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0038306"}, {"journal": "PLoS ONE", "abstract": ["Background: Citation analysis has become an important tool for research performance assessment in the medical sciences. However, different areas of medical research may have considerably different citation practices, even within the same medical field. Because of this, it is unclear to what extent citation-based bibliometric indicators allow for valid comparisons between research units active in different areas of medical research. Methodology: A visualization methodology is introduced that reveals differences in citation practices between medical research areas. The methodology extracts terms from the titles and abstracts of a large collection of publications and uses these terms to visualize the structure of a medical field and to indicate how research areas within this field differ from each other in their average citation impact. Results: Visualizations are provided for 32 medical fields, defined based on journal subject categories in the Web of Science database. The analysis focuses on three fields: Cardiac & cardiovascular systems, Clinical neurology, and Surgery. In each of these fields, there turn out to be large differences in citation practices between research areas. Low-impact research areas tend to focus on clinical intervention research, while high-impact research areas are often more oriented on basic and diagnostic research. Conclusions: Popular bibliometric indicators, such as the h-index and the impact factor, do not correct for differences in citation practices between medical fields. These indicators therefore cannot be used to make accurate between-field comparisons. More sophisticated bibliometric indicators do correct for field differences but still fail to take into account within-field heterogeneity in citation practices. As a consequence, the citation impact of clinical intervention research may be substantially underestimated in comparison with basic and diagnostic research. "], "author_display": ["Nees Jan van Eck", "Ludo Waltman", "Anthony F. J. van Raan", "Robert J. M. Klautz", "Wilco C. Peul"], "article_type": "Research Article", "score": 0.4493637, "title_display": "Citation Analysis May Severely Underestimate the Impact of Clinical Research as Compared to Basic Research", "publication_date": "2013-04-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0062395"}, {"journal": "PLoS ONE", "abstract": ["\nSteady-state Visual Evoked Potential (SSVEP) outperforms the other types of ERPs for Brain-computer Interface (BCI), and thus it is widely employed. In order to apply SSVEP-based BCI to real life situations, it is important to improve the accuracy and transfer rate of the system. Aimed at this target, many SSVEP extraction methods have been proposed. All these methods are based directly on the properties of SSVEP, such as power and phase. In this study, we first filtered out the target frequencies from the original EEG to get a new signal and then computed the similarity between the original EEG and the new signal. Based on this similarity, SSVEP in the original EEG can be identified. This method is referred to as SOB (Similarity of Background). The SOB method is used to detect SSVEP in 1s-length and 3s-length EEG segments respectively. The accuracy of detection is compared with its peers computed by the widely-used Power Spectrum (PS) method and the Canonical Coefficient (CC) method. The comparison results illustrate that the SOB method can lead to a higher accuracy than the PS method and CC method when detecting a short period SSVEP signal.\n"], "author_display": ["Zhenghua Wu"], "article_type": "Research Article", "score": 0.44917238, "title_display": "SSVEP Extraction Based on the Similarity of Background EEG", "publication_date": "2014-04-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0093884"}, {"journal": "PLOS ONE", "abstract": ["\nCanonical correlation analysis (CCA) has been widely used in the detection of the steady-state visual evoked potentials (SSVEPs) in brain-computer interfaces (BCIs). The standard CCA method, which uses sinusoidal signals as reference signals, was first proposed for SSVEP detection without calibration. However, the detection performance can be deteriorated by the interference from the spontaneous EEG activities. Recently, various extended methods have been developed to incorporate individual EEG calibration data in CCA to improve the detection performance. Although advantages of the extended CCA methods have been demonstrated in separate studies, a comprehensive comparison between these methods is still missing. This study performed a comparison of the existing CCA-based SSVEP detection methods using a 12-class SSVEP dataset recorded from 10 subjects in a simulated online BCI experiment. Classification accuracy and information transfer rate (ITR) were used for performance evaluation. The results suggest that individual calibration data can significantly improve the detection performance. Furthermore, the results showed that the combination method based on the standard CCA and the individual template based CCA (IT-CCA) achieved the highest performance.\n"], "author_display": ["Masaki Nakanishi", "Yijun Wang", "Yu-Te Wang", "Tzyy-Ping Jung"], "article_type": "Research Article", "score": 0.4483663, "title_display": "A Comparison Study of Canonical Correlation Analysis Based Methods for Detecting Steady-State Visual Evoked Potentials", "publication_date": "2015-10-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0140703"}, {"journal": "PLoS ONE", "abstract": ["\n        It is widely thought that resting state functional connectivity likely reflects functional interaction among brain areas and that different functional areas interact with different sets of brain areas. A method for mapping areal boundaries has been formulated based on the large-scale spatial characteristics of regional interaction revealed by resting state functional connectivity. In the present study, we present a novel analysis for areal boundary mapping that requires only the signal timecourses within a region of interest, without reference to the information from outside the region. The areal boundaries were generated by the novel analysis and were compared with those generated by the previously-established standard analysis. The boundaries were robust and reproducible across the two analyses, in two regions of interest tested. These results suggest that the information for areal boundaries is readily available inside the region of interest.\n      "], "author_display": ["Satoshi Hirose", "Takamitsu Watanabe", "Koji Jimura", "Masaki Katsura", "Akira Kunimatsu", "Osamu Abe", "Kuni Ohtomo", "Yasushi Miyashita", "Seiki Konishi"], "article_type": "Research Article", "score": 0.44809726, "title_display": "Local Signal Time-Series during Rest Used for Areal Boundary Mapping in Individual Human Brains", "publication_date": "2012-05-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0036496"}, {"abstract": ["Background: Individual participant data (IPD) meta-analyses that obtain \u201craw\u201d data from studies rather than summary data typically adopt a \u201ctwo-stage\u201d approach to analysis whereby IPD within trials generate summary measures, which are combined using standard meta-analytical methods. Recently, a range of \u201cone-stage\u201d approaches which combine all individual participant data in a single meta-analysis have been suggested as providing a more powerful and flexible approach. However, they are more complex to implement and require statistical support. This study uses a dataset to compare \u201ctwo-stage\u201d and \u201cone-stage\u201d models of varying complexity, to ascertain whether results obtained from the approaches differ in a clinically meaningful way. Methods and Findings: We included data from 24 randomised controlled trials, evaluating antiplatelet agents, for the prevention of pre-eclampsia in pregnancy. We performed two-stage and one-stage IPD meta-analyses to estimate overall treatment effect and to explore potential treatment interactions whereby particular types of women and their babies might benefit differentially from receiving antiplatelets. Two-stage and one-stage approaches gave similar results, showing a benefit of using anti-platelets (Relative risk 0.90, 95% CI 0.84 to 0.97). Neither approach suggested that any particular type of women benefited more or less from antiplatelets. There were no material differences in results between different types of one-stage model. Conclusions: For these data, two-stage and one-stage approaches to analysis produce similar results. Although one-stage models offer a flexible environment for exploring model structure and are useful where across study patterns relating to types of participant, intervention and outcome mask similar relationships within trials, the additional insights provided by their usage may not outweigh the costs of statistical support for routine application in syntheses of randomised controlled trials. Researchers considering undertaking an IPD meta-analysis should not necessarily be deterred by a perceived need for sophisticated statistical methods when combining information from large randomised trials. "], "author_display": ["Gavin B. Stewart", "Douglas G. Altman", "Lisa M. Askie", "Lelia Duley", "Mark C. Simmonds", "Lesley A. Stewart"], "article_type": "Research Article", "score": 0.44787103, "title_display": "Statistical Analysis of Individual Participant Data Meta-Analyses: A Comparison of Methods and Recommendations for Practice", "publication_date": "2012-10-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0046042"}, {"journal": "PLoS ONE", "abstract": ["\nIn vegetation science and forest management, tree density is often used as a variable. To determine the value of this variable, reliable field methods are necessary. When vegetation is sparse or not easily accessible, the use of sample plots is not feasible in the field. Therefore, plotless methods, like the Point Centred Quarter Method, are often used as an alternative. In this study we investigate the accuracy of different plotless sampling methods. To this end, tree densities of a mangrove forest were determined and compared with estimates provided by several plotless methods. None of these methods proved accurate across all field sites with mean underestimations up to 97% and mean overestimations up to 53% in the field. Applying the methods to different vegetation patterns shows that when random spatial distributions were used the true density was included within the 95% confidence limits of all the plotless methods tested. It was also found that, besides aggregation and regularity, density trends often found in mangroves contribute to the unreliability. This outcome raises questions about the use of plotless sampling in forest monitoring and management, as well as for estimates of density-based carbon sequestration. We give recommendations to minimize errors in vegetation surveys and recommendations for further in-depth research.\n"], "author_display": ["Renske Hijbeek", "Nico Koedam", "Md Nabiul Islam Khan", "James Gitundu Kairo", "Johan Schoukens", "Farid Dahdouh-Guebas"], "article_type": "Research Article", "score": 0.44753796, "title_display": "An Evaluation of Plotless Sampling Using Vegetation Simulations and Field Data from a Mangrove Forest", "publication_date": "2013-06-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0067201"}, {"journal": "PLOS ONE", "abstract": ["Objective: To propose a new algorithm facilitating automated analysis of 1p and 19q status by FISH technique in oligodendroglial tumors with software packages available in the majority of institutions using this technique. Methods: We documented all green/red (G/R) probe signal combinations in a retrospective series of 53 oligodendroglial tumors according to literature guidelines (Algorithm 1) and selected only the most significant combinations for a new algorithm (Algorithm 2). This second algorithm was then validated on a prospective internal series of 45 oligodendroglial tumors and on an external series of 36 gliomas. Results: Algorithm 2 utilizes 24 G/R combinations which represent less than 40% of combinations observed with Algorithm 1. The new algorithm excludes some common G/R combinations (1/1, 3/2) and redefines the place of others (defining 1/2 as compatible with normal and 3/3, 4/4 and 5/5 as compatible with imbalanced chromosomal status). The new algorithm uses the combination + ratio method of signal probe analysis to give the best concordance between manual and automated analysis on samples of 100 tumor cells (91% concordance for 1p and 89% concordance for 19q) and full concordance on samples of 200 tumor cells. This highlights the value of automated analysis as a means to identify cases in which a larger number of tumor cells should be studied by manual analysis. Validation of this algorithm on a second series from another institution showed a satisfactory concordance (89%, \u03ba = 0.8). Conclusion: Our algorithm can be easily implemented on all existing FISH analysis software platforms and should facilitate multicentric evaluation and standardization of 1p/19q assessment in gliomas with reduction of the professional and technical time required. "], "author_display": ["C\u00e9line Duval", "Marie de Tayrac", "Karine Michaud", "Florian Cabillic", "Claudie Paquet", "Peter Vincent Gould", "St\u00e9phan Saikali"], "article_type": "Research Article", "score": 0.44742593, "title_display": "Automated Analysis of 1p/19q Status by FISH in Oligodendroglial Tumors: Rationale and Proposal of an Algorithm", "publication_date": "2015-07-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0132125"}, {"journal": "PLoS ONE", "abstract": ["Objectives: Validation of a cost effective in-house method for HIV-1 drug resistance genotyping using plasma samples. Design: The validation includes the establishment of analytical performance characteristics such as accuracy, reproducibility, precision and sensitivity. Methods: The accuracy was assessed by comparing 26 paired Virological Quality Assessment (VQA) proficiency testing panel sequences generated by in-house and ViroSeq Genotyping System 2.0 (Celera Diagnostics, US) as a gold standard. The reproducibility and precision were carried out on five samples with five replicates representing multiple HIV-1 subtypes (A, B, C) and resistance patterns. The amplification sensitivity was evaluated on HIV-1 positive plasma samples (n\u200a=\u200a88) with known viral loads ranges from 1000\u20131.8 million RNA copies/ml. Results: Comparison of the nucleotide sequences generated by ViroSeq and in-house method showed 99.41\u00b10.46 and 99.68\u00b10.35% mean nucleotide and amino acid identity respectively. Out of 135 Stanford HIVdb listed HIV-1 drug resistance mutations, partial discordance was observed at 15 positions and complete discordance was absent. The reproducibility and precision study showed high nucleotide sequence identities i.e. 99.88\u00b10.10 and 99.82\u00b10.20 respectively. The in-house method showed 100% analytical sensitivity on the samples with HIV-1 viral load >1000 RNA copies/ml. The cost of running the in-house method is only 50% of that for ViroSeq method (112$ vs 300$), thus making it cost effective. Conclusions: The validated cost effective in-house method may be used to collect surveillance data on the emergence and transmission of HIV-1 drug resistance in resource limited countries. Moreover, the wide applications of a cost effective and validated in-house method for HIV-1 drug resistance testing will facilitate the decision making for the appropriate management of HIV infected patients. "], "author_display": ["Devidas N. Chaturbhuj", "Amit P. Nirmalkar", "Ramesh S. Paranjape", "Srikanth P. Tripathy"], "article_type": "Research Article", "score": 0.44739714, "title_display": "Evaluation of a Cost Effective In-House Method for HIV-1 Drug Resistance Genotyping Using Plasma Samples", "publication_date": "2014-02-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0087441"}, {"journal": "PLoS ONE", "abstract": ["Background: Exploratory factor analysis is a commonly used statistical technique in metabolic syndrome research to uncover latent structure amongst metabolic variables. The application of factor analysis requires methodological decisions that reflect the hypothesis of the metabolic syndrome construct. These decisions often raise the complexity of the interpretation from the output. We propose two alternative techniques developed from cluster analysis which can achieve a clinically relevant structure, whilst maintaining intuitive advantages of clustering methodology. Methods: Two advanced techniques of clustering in the VARCLUS and matroid methods are discussed and implemented on a metabolic syndrome data set to analyze the structure of ten metabolic risk factors. The subjects were selected from the normative aging study based in Boston, Massachusetts. The sample included a total of 847 men aged between 21 and 81 years who provided complete data on selected risk factors during the period 1987 to 1991. Results: Four core components were identified by the clustering methods. These are labelled obesity, lipids, insulin resistance and blood pressure. The exploratory factor analysis with oblique rotation suggested an overlap of the loadings identified on the insulin resistance and obesity factors. The VARCLUS and matroid analyses separated these components and were able to demonstrate associations between individual risk factors. Conclusions: An oblique rotation can be selected to reflect the clinical concept of a single underlying syndrome, however the results are often difficult to interpret. Factor loadings must be considered along with correlations between the factors. The correlated components produced by the VARCLUS and matroid analyses are not overlapped, which allows for a simpler application of the methodologies and interpretation of the results. These techniques encourage consistency in the interpretation whilst remaining faithful to the construct under study. "], "author_display": ["Andrew Woolston", "Yu-Kang Tu", "Paul D. Baxter", "Mark S. Gilthorpe"], "article_type": "Research Article", "score": 0.44736814, "title_display": "A Comparison of Different Approaches to Unravel the Latent Structure within Metabolic Syndrome", "publication_date": "2012-04-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0034410"}, {"journal": "PLoS ONE", "abstract": ["\nA Compound fault signal usually contains multiple characteristic signals and strong confusion noise, which makes it difficult to separate week fault signals from them through conventional ways, such as FFT-based envelope detection, wavelet transform or empirical mode decomposition individually. In order to improve the compound faults diagnose of rolling bearings via signals\u2019 separation, the present paper proposes a new method to identify compound faults from measured mixed-signals, which is based on ensemble empirical mode decomposition (EEMD) method and independent component analysis (ICA) technique. With the approach, a vibration signal is firstly decomposed into intrinsic mode functions (IMF) by EEMD method to obtain multichannel signals. Then, according to a cross correlation criterion, the corresponding IMF is selected as the input matrix of ICA. Finally, the compound faults can be separated effectively by executing ICA method, which makes the fault features more easily extracted and more clearly identified. Experimental results validate the effectiveness of the proposed method in compound fault separating, which works not only for the outer race defect, but also for the rollers defect and the unbalance fault of the experimental system.\n"], "author_display": ["Huaqing Wang", "Ruitong Li", "Gang Tang", "Hongfang Yuan", "Qingliang Zhao", "Xi Cao"], "article_type": "Research Article", "score": 0.4469257, "title_display": "A Compound Fault Diagnosis for Rolling Bearings Method Based on Blind Source Separation and Ensemble Empirical Mode Decomposition", "publication_date": "2014-10-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0109166"}, {"journal": "PLoS ONE", "abstract": ["Background & Objective: Genome-wide profiles of tumors obtained using functional genomics platforms are being deposited to the public repositories at an astronomical scale, as a result of focused efforts by individual laboratories and large projects such as the Cancer Genome Atlas (TCGA) and the International Cancer Genome Consortium. Consequently, there is an urgent need for reliable tools that integrate and interpret these data in light of current knowledge and disseminate results to biomedical researchers in a user-friendly manner. We have built the canEvolve web portal to meet this need. Results: canEvolve query functionalities are designed to fulfill most frequent analysis needs of cancer researchers with a view to generate novel hypotheses. canEvolve stores gene, microRNA (miRNA) and protein expression profiles, copy number alterations for multiple cancer types, and protein-protein interaction information. canEvolve allows querying of results of primary analysis, integrative analysis and network analysis of oncogenomics data. The querying for primary analysis includes differential gene and miRNA expression as well as changes in gene copy number measured with SNP microarrays. canEvolve provides results of integrative analysis of gene expression profiles with copy number alterations and with miRNA profiles as well as generalized integrative analysis using gene set enrichment analysis. The network analysis capability includes storage and visualization of gene co-expression, inferred gene regulatory networks and protein-protein interaction information. Finally, canEvolve provides correlations between gene expression and clinical outcomes in terms of univariate survival analysis. Conclusion: At present canEvolve provides different types of information extracted from 90 cancer genomics studies comprising of more than 10,000 patients. The presence of multiple data types, novel integrative analysis for identifying regulators of oncogenesis, network analysis and ability to query gene lists/pathways are distinctive features of canEvolve. canEvolve will facilitate integrative and meta-analysis of oncogenomics datasets. Availability: The canEvolve web portal is available at http://www.canevolve.org/. "], "author_display": ["Mehmet Kemal Samur", "Zhenyu Yan", "Xujun Wang", "Qingyi Cao", "Nikhil C. Munshi", "Cheng Li", "Parantu K. Shah"], "article_type": "Research Article", "score": 0.44682923, "title_display": "canEvolve: A Web Portal for Integrative Oncogenomics", "publication_date": "2013-02-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0056228"}, {"journal": "PLoS ONE", "abstract": ["Background: The p value obtained from a significance test provides no information about the magnitude or importance of the underlying phenomenon. Therefore, additional reporting of effect size is often recommended. Effect sizes are theoretically independent from sample size. Yet this may not hold true empirically: non-independence could indicate publication bias. Methods: We investigate whether effect size is independent from sample size in psychological research. We randomly sampled 1,000 psychological articles from all areas of psychological research. We extracted p values, effect sizes, and sample sizes of all empirical papers, and calculated the correlation between effect size and sample size, and investigated the distribution of p values. Results: We found a negative correlation of r\u200a=\u200a\u2212.45 [95% CI: \u2212.53; \u2212.35] between effect size and sample size. In addition, we found an inordinately high number of p values just passing the boundary of significance. Additional data showed that neither implicit nor explicit power analysis could account for this pattern of findings. Conclusion: The negative correlation between effect size and samples size, and the biased distribution of p values indicate pervasive publication bias in the entire field of psychology. "], "author_display": ["Anton K\u00fchberger", "Astrid Fritz", "Thomas Scherndl"], "article_type": "Research Article", "score": 0.4459728, "title_display": "Publication Bias in Psychology: A Diagnosis Based on the Correlation between Effect Size and Sample Size", "publication_date": "2014-09-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0105825"}, {"journal": "PLOS ONE", "abstract": ["\nUnderstanding emerging areas of a multidisciplinary research field is crucial for researchers, policymakers and other stakeholders. For them a knowledge structure based on longitudinal bibliographic data can be an effective instrument. But with the vast amount of available online information it is often hard to understand the knowledge structure for data. In this paper, we present a novel approach for retrieving online bibliographic data and propose a framework for exploring knowledge structure. We also present several longitudinal analyses to interpret and visualize the last 20 years of published obesity research data.\n"], "author_display": ["Shahadat Uddin", "Arif Khan", "Louise A. Baur"], "article_type": "Research Article", "score": 0.4458921, "title_display": "A Framework to Explore the Knowledge Structure of Multidisciplinary Research Fields", "publication_date": "2015-04-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0123537"}, {"journal": "PLoS ONE", "abstract": ["\n        Functional analysis of large sets of genes and proteins is becoming more and more necessary with the increase of experimental biomolecular data at omic-scale. Enrichment analysis is by far the most popular available methodology to derive functional implications of sets of cooperating genes. The problem with these techniques relies in the redundancy of resulting information, that in most cases generate lots of trivial results with high risk to mask the reality of key biological events. We present and describe a computational method, called GeneTerm Linker, that filters and links enriched output data identifying sets of associated genes and terms, producing metagroups of coherent biological significance. The method uses fuzzy reciprocal linkage between genes and terms to unravel their functional convergence and associations. The algorithm is tested with a small set of well known interacting proteins from yeast and with a large collection of reference sets from three heterogeneous resources: multiprotein complexes (CORUM), cellular pathways (SGD) and human diseases (OMIM). Statistical Precision, Recall and balanced F-score are calculated showing robust results, even when different levels of random noise are included in the test sets. Although we could not find an equivalent method, we present a comparative analysis with a widely used method that combines enrichment and functional annotation clustering. A web application to use the method here proposed is provided at http://gtlinker.cnb.csic.es.\n      "], "author_display": ["Celia Fontanillo", "Ruben Nogales-Cadenas", "Alberto Pascual-Montano", "Javier De Las Rivas"], "article_type": "Research Article", "score": 0.44588292, "title_display": "Functional Analysis beyond Enrichment: Non-Redundant Reciprocal Linkage of Genes and Biological Terms", "publication_date": "2011-09-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0024289"}, {"journal": "PLoS ONE", "abstract": ["Background: A public that is an informed partner in clinical research is important for ethical, methodological, and operational reasons. There are indications that the public is unaware or misinformed, and not sufficiently engaged in clinical research but studies on the topic are lacking. PARTAKE \u2013 Public Awareness of Research for Therapeutic Advancements through Knowledge and Empowerment is a program aimed at increasing public awareness and partnership in clinical research. The PARTAKE Survey is a component of the program. Objective: To study public knowledge and perceptions of clinical research. Methods: A 40-item questionnaire combining multiple-choice and open-ended questions was administered to 175 English- or Hindi-speaking individuals in 8 public locations representing various socioeconomic strata in New Delhi, India. Results: Interviewees were 18\u201384 old (mean: 39.6, SD\u00b116.6), 23.6% female, 68.6% employed, 7.3% illiterate, 26.3% had heard of research, 2.9% had participated and 58.9% expressed willingness to participate in clinical research. The following perceptions were reported (% true/% false/% not aware): \u2018research benefits society\u2019 (94.1%/3.5%/2.3%), \u2018the government protects against unethical clinical research\u2019 (56.7%/26.3%/16.9%), \u2018research hospitals provide better care\u2019 (67.2%/8.7%/23.9%), \u2018confidentiality is adequately protected\u2019 (54.1%/12.3%/33.5%), \u2018participation in research is voluntary\u2019 (85.3%/5.8%/8.7%); \u2018participants treated like \u2018guinea pigs\u2019\u2019 (20.7%/53.2%/26.0%), and \u2018compensation for participation is adequate\u2019 (24.7%/12.9%/62.3%). Conclusions: Results suggest the Indian public is aware of some key features of clinical research (e.g., purpose, value, voluntary nature of participation), and supports clinical research in general but is unaware of other key features (e.g., compensation, confidentiality, protection of human participants) and exhibits some distrust in the conduct and reporting of clinical trials. Larger, cross-cultural surveys are required to inform educational programs addressing these issues. "], "author_display": ["Tal Burt", "Savita Dhillon", "Pooja Sharma", "Danish Khan", "Deepa MV", "Sazid Alam", "Sarika Jain", "Bhavana Alapati", "Sanjay Mittal", "Padam Singh"], "article_type": "Research Article", "score": 0.44584084, "title_display": "PARTAKE Survey of Public Knowledge and Perceptions of Clinical Research in India", "publication_date": "2013-07-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0068666"}, {"journal": "PLoS ONE", "abstract": ["Background: The interplay between the workflow for clinical tasks and research data collection is often overlooked, ultimately making it ineffective. Questions/purposes: To the best of our knowledge, no previous studies have developed standards that allow for the comparison of workflow models derived from clinical and research tasks toward the improvement of data collection processes Methods: In this study we used the term dissonance for the occurrences where there was a discord between clinical and research workflows. We developed workflow models for a translational research study in psychiatry and the clinic where its data collection was carried out. After identifying points of dissonance between clinical and research models we derived a corresponding classification system that ultimately enabled us to re-engineer the data collection workflow. We considered (1) the number of patients approached for enrollment and (2) the number of patients enrolled in the study as indicators of efficiency in research workflow. We also recorded the number of dissonances before and after the workflow modification. Results: We identified 22 episodes of dissonance across 6 dissonance categories: actor, communication, information, artifact, time, and space. We were able to eliminate 18 episodes of dissonance and increase the number of patients approached and enrolled in research study trough workflow modification. Conclusion: The classification developed in this study is useful for guiding the identification of dissonances and reveal modifications required to align the workflow of data collection and the clinical setting. The methodology described in this study can be used by researchers to standardize data collection process. "], "author_display": ["Luciana Cofiel", "D\u00e9bora U. Bassi", "Ryan Kumar Ray", "Ricardo Pietrobon", "Helena Brentani"], "article_type": "Research Article", "score": 0.44574714, "title_display": "Detecting Dissonance in Clinical and Research Workflow for Translational Psychiatric Registries", "publication_date": "2013-09-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0075167"}, {"journal": "PLoS ONE", "abstract": ["\nHow easy is it to reproduce the results found in a typical computational biology paper? Either through experience or intuition the reader will already know that the answer is with difficulty or not at all. In this paper we attempt to quantify this difficulty by reproducing a previously published paper for different classes of users (ranging from users with little expertise to domain experts) and suggest ways in which the situation might be improved. Quantification is achieved by estimating the time required to reproduce each of the steps in the method described in the original paper and make them part of an explicit workflow that reproduces the original results. Reproducing the method took several months of effort, and required using new versions and new software that posed challenges to reconstructing and validating the results. The quantification leads to \u201creproducibility maps\u201d that reveal that novice researchers would only be able to reproduce a few of the steps in the method, and that only expert researchers with advance knowledge of the domain would be able to reproduce the method in its entirety. The workflow itself is published as an online resource together with supporting software and data. The paper concludes with a brief discussion of the complexities of requiring reproducibility in terms of cost versus benefit, and a desiderata with our observations and guidelines for improving reproducibility. This has implications not only in reproducing the work of others from published papers, but reproducing work from one\u2019s own laboratory.\n"], "author_display": ["Daniel Garijo", "Sarah Kinnings", "Li Xie", "Lei Xie", "Yinliang Zhang", "Philip E. Bourne", "Yolanda Gil"], "article_type": "Research Article", "score": 0.4455482, "title_display": "Quantifying Reproducibility in Computational Biology: The Case of the Tuberculosis Drugome", "publication_date": "2013-11-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0080278"}, {"journal": "PLoS ONE", "abstract": ["Background: The indirect comparison of two interventions can be valuable in many situations. However, the quality of an indirect comparison will depend on several factors including the chosen methodology and validity of underlying assumptions. Published indirect comparisons are increasingly more common in the medical literature, but as yet, there are no published recommendations of how they should be reported. Our aim is to systematically review the quality of published indirect comparisons to add to existing empirical data suggesting that improvements can be made when reporting and applying indirect comparisons. Methodology/Findings: Reviews applying statistical methods to indirectly compare the clinical effectiveness of two interventions using randomised controlled trials were eligible. We searched (1966\u20132008) Database of Abstracts and Reviews of Effects, The Cochrane library, and Medline. Full review publications were assessed for eligibility. Specific criteria to assess quality were developed and applied. Forty-three reviews were included. Adequate methodology was used to calculate the indirect comparison in 41 reviews. Nineteen reviews assessed the similarity assumption using sensitivity analysis, subgroup analysis, or meta-regression. Eleven reviews compared trial-level characteristics. Twenty-four reviews assessed statistical homogeneity. Twelve reviews investigated causes of heterogeneity. Seventeen reviews included direct and indirect evidence for the same comparison; six reviews assessed consistency. One review combined both evidence types. Twenty-five reviews urged caution in interpretation of results, and 24 reviews indicated when results were from indirect evidence by stating this term with the result. Conclusions: This review shows that the underlying assumptions are not routinely explored or reported when undertaking indirect comparisons. We recommend, therefore, that the quality of indirect comparisons should be improved, in particular, by assessing assumptions and reporting the assessment methods applied. We propose that the quality criteria applied in this article may provide a basis to help review authors carry out indirect comparisons and to aid appropriate interpretation. "], "author_display": ["Sarah Donegan", "Paula Williamson", "Carrol Gamble", "Catrin Tudur-Smith"], "article_type": "Research Article", "score": 0.44538647, "title_display": "Indirect Comparisons: A Review of Reporting and Methodological Quality", "publication_date": "2010-11-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0011054"}, {"journal": "PLoS ONE", "abstract": ["Introduction: Some have suggested the quality of reporting of network meta-analyses (a technique used to synthesize information to compare multiple interventions) is sub-optimal. We sought to review information addressing this claim. Objective: To conduct an overview of existing evaluations of quality of reporting in network meta-analyses and indirect treatment comparisons, and to compile a list of topics which may require detailed reporting guidance to enhance future reporting quality. Methods: An electronic search of Medline and the Cochrane Registry of methodologic studies (January 2004\u2013August 2013) was performed by an information specialist. Studies describing findings from quality of reporting assessments were sought. Screening of abstracts and full texts was performed by two team members. Descriptors related to all aspects of reporting a network meta-analysis were summarized. Results: We included eight reports exploring the quality of reporting of network meta-analyses. From past reviews, authors found several aspects of network meta-analyses were inadequately reported, including primary information about literature searching, study selection, and risk of bias evaluations; statement of the underlying assumptions for network meta-analysis, as well as efforts to verify their validity; details of statistical models used for analyses (including information for both Bayesian and Frequentist approaches); completeness of reporting of findings; and approaches for summarizing probability measures as additional important considerations. Conclusions: While few studies were identified, several deficiencies in the current reporting of network meta-analyses were observed. These findings reinforce the need to develop reporting guidance for network meta-analyses. Findings from this review will be used to guide next steps in the development of reporting guidance for network meta-analysis in the format of an extension of the PRISMA (Preferred Reporting Items for Systematic reviews and Meta-Analysis) Statement. "], "author_display": ["Brian Hutton", "Georgia Salanti", "Anna Chaimani", "Deborah M. Caldwell", "Chris Schmid", "Kristian Thorlund", "Edward Mills", "Ferr\u00e1n Catal\u00e1-L\u00f3pez", "Lucy Turner", "Douglas G. Altman", "David Moher"], "article_type": "Research Article", "score": 0.44534764, "title_display": "The Quality of Reporting Methods and Results in Network Meta-Analyses: An Overview of Reviews and Suggestions for Improvement", "publication_date": "2014-03-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0092508"}, {"journal": "PLOS ONE", "abstract": ["\nLarge-scale cohort studies are currently being designed to investigate the human microbiome in health and disease. Adequate sampling strategies are required to limit bias due to shifts in microbial communities during sampling and storage. Therefore, we examined the impact of different sampling and storage conditions on the stability of fecal microbial communities in healthy and diseased subjects. Fecal samples from 10 healthy controls, 10 irritable bowel syndrome and 8 inflammatory bowel disease patients were collected on site, aliquoted immediately after defecation and stored at -80\u00b0C, -20\u00b0C for 1 week, at +4\u00b0C or room temperature for 24 hours. Fecal transport swabs (FecalSwab, Copan) were collected and stored for 48-72 hours at room temperature. We used pyrosequencing of the 16S gene to investigate the stability of microbial communities. Alpha diversity did not differ between all storage methods and -80\u00b0C, except for the fecal swabs. UPGMA clustering and principal coordinate analysis showed significant clustering by test subject (p<0.001) but not by storage method. Bray-Curtis dissimilarity and (un)weighted UniFrac showed a significant higher distance between fecal swabs and -80\u00b0C versus the other methods and -80\u00b0C samples (p<0.009). The relative abundance of Ruminococcus and Enterobacteriaceae did not differ between the storage methods versus -80\u00b0C, but was higher in fecal swabs (p<0.05). Storage up to 24 hours (at +4\u00b0C or room temperature) or freezing at -20\u00b0C did not significantly alter the fecal microbial community structure compared to direct freezing of samples from healthy subjects and patients with gastrointestinal disorders.\n"], "author_display": ["Danyta I. Tedjo", "Daisy M. A. E. Jonkers", "Paul H. Savelkoul", "Ad A. Masclee", "Niels van Best", "Marieke J. Pierik", "John Penders"], "article_type": "Research Article", "score": 0.44527578, "title_display": "The Effect of Sampling and Storage on the Fecal Microbiota Composition in Healthy and Diseased Subjects", "publication_date": "2015-05-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0126685"}, {"journal": "PLOS ONE", "abstract": ["\nThe annual suicide rate in South Korea is the highest among the developed countries. Paraquat is a highly lethal herbicide, commonly used in South Korea as a means for suicide. We have studied the effect of the 2011 paraquat prohibition on the national suicide rate and method of suicide in South Korea. We obtained the monthly suicide rate from 2005 to 2013 in South Korea. In our analyses, we adjusted for the effects of celebrity suicides, and economic, meteorological, and seasonal factors on suicide rate. We employed change point analysis to determine the effect of paraquat prohibition on suicide rate over time, and the results were verified by structural change analysis, an alternative statistical method. After the paraquat prohibition period in South Korea, there was a significant reduction in the total suicide rate and suicide rate by poisoning with herbicides or fungicides in all age groups and in both genders. The estimated suicide rates during this period decreased by 10.0% and 46.1% for total suicides and suicides by poisoning of herbicides or fungicides, respectively. In addition, method substitution effect of paraquat prohibition was found in suicide by poisoning by carbon monoxide, which did not exceed the reduction in the suicide rate of poisoning with herbicides or fungicides. In South Korea, paraquat prohibition led to a lower rate of suicide by paraquat poisoning, as well as a reduction in the overall suicide rate. Paraquat prohibition should be considered as a national suicide prevention strategy in developing and developed countries alongside careful observation for method substitution effects.\n"], "author_display": ["Woojae Myung", "Geung-Hee Lee", "Hong-Hee Won", "Maurizio Fava", "David Mischoulon", "Maren Nyer", "Doh Kwan Kim", "Jung-Yoon Heo", "Hong Jin Jeon"], "article_type": "Research Article", "score": 0.4452702, "title_display": "Paraquat Prohibition and Change in the Suicide Rate and Methods in South Korea", "publication_date": "2015-06-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0128980"}, {"journal": "PLOS ONE", "abstract": ["Background: Significant efforts are underway within the biomedical research community to encourage sharing and reuse of research data in order to enhance research reproducibility and enable scientific discovery. While some technological challenges do exist, many of the barriers to sharing and reuse are social in nature, arising from researchers\u2019 concerns about and attitudes toward sharing their data. In addition, clinical and basic science researchers face their own unique sets of challenges to sharing data within their communities. This study investigates these differences in experiences with and perceptions about sharing data, as well as barriers to sharing among clinical and basic science researchers. Methods: Clinical and basic science researchers in the Intramural Research Program at the National Institutes of Health were surveyed about their attitudes toward and experiences with sharing and reusing research data. Of 190 respondents to the survey, the 135 respondents who identified themselves as clinical or basic science researchers were included in this analysis. Odds ratio and Fisher\u2019s exact tests were the primary methods to examine potential relationships between variables. Worst-case scenario sensitivity tests were conducted when necessary. Results and Discussion: While most respondents considered data sharing and reuse important to their work, they generally rated their expertise as low. Sharing data directly with other researchers was common, but most respondents did not have experience with uploading data to a repository. A number of significant differences exist between the attitudes and practices of clinical and basic science researchers, including their motivations for sharing, their reasons for not sharing, and the amount of work required to prepare their data. Conclusions: Even within the scope of biomedical research, addressing the unique concerns of diverse research communities is important to encouraging researchers to share and reuse data. Efforts at promoting data sharing and reuse should be aimed at solving not only technological problems, but also addressing researchers\u2019 concerns about sharing their data. Given the varied practices of individual researchers and research communities, standardizing data practices like data citation and repository upload could make sharing and reuse easier. "], "author_display": ["Lisa M. Federer", "Ya-Ling Lu", "Douglas J. Joubert", "Judith Welsh", "Barbara Brandys"], "article_type": "Research Article", "score": 0.44504607, "title_display": "Biomedical Data Sharing and Reuse: Attitudes and Practices of Clinical and Scientific Research Staff", "publication_date": "2015-06-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0129506"}, {"journal": "PLOS ONE", "abstract": ["\nRNA-sequencing is rapidly becoming the method of choice for studying the full complexity of transcriptomes, however with increasing dimensionality, accurate gene ranking is becoming increasingly challenging. This paper proposes an accurate and sensitive gene ranking method that implements discriminant non-negative matrix factorization (DNMF) for RNA-seq data. To the best of our knowledge, this is the first work to explore the utility of DNMF for gene ranking. When incorporating Fisher\u2019s discriminant criteria and setting the reduced dimension as two, DNMF learns two factors to approximate the original gene expression data, abstracting the up-regulated or down-regulated metagene by using the sample label information. The first factor denotes all the genes\u2019 weights of two metagenes as the additive combination of all genes, while the second learned factor represents the expression values of two metagenes. In the gene ranking stage, all the genes are ranked as a descending sequence according to the differential values of the metagene weights. Leveraging the nature of NMF and Fisher\u2019s criterion, DNMF can robustly boost the gene ranking performance. The Area Under the Curve analysis of differential expression analysis on two benchmarking tests of four RNA-seq data sets with similar phenotypes showed that our proposed DNMF-based gene ranking method outperforms other widely used methods. Moreover, the Gene Set Enrichment Analysis also showed DNMF outweighs others. DNMF is also computationally efficient, substantially outperforming all other benchmarked methods. Consequently, we suggest DNMF is an effective method for the analysis of differential gene expression and gene ranking for RNA-seq data.\n"], "author_display": ["Zhilong Jia", "Xiang Zhang", "Naiyang Guan", "Xiaochen Bo", "Michael R. Barnes", "Zhigang Luo"], "article_type": "Research Article", "score": 0.44465396, "title_display": "Gene Ranking of RNA-Seq Data via Discriminant Non-Negative Matrix Factorization", "publication_date": "2015-09-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0137782"}, {"journal": "PLOS ONE", "abstract": ["Objectives: To investigate whether a value of information analysis, commonly applied in health care evaluations, is feasible and meaningful in the field of crime prevention. Methods: Interventions aimed at reducing juvenile delinquency are increasingly being evaluated according to their cost-effectiveness. Results of cost-effectiveness models are subject to uncertainty in their cost and effect estimates. Further research can reduce that parameter uncertainty. The value of such further research can be estimated using a value of information analysis, as illustrated in the current study. We built upon an earlier published cost-effectiveness model that demonstrated the comparison of two interventions aimed at reducing juvenile delinquency. Outcomes were presented as costs per criminal activity free year. Results: At a societal willingness-to-pay of \u20ac71,700 per criminal activity free year, further research to eliminate parameter uncertainty was valued at \u20ac176 million. Therefore, in this illustrative analysis, the value of information analysis determined that society should be willing to spend a maximum of \u20ac176 million in reducing decision uncertainty in the cost-effectiveness of the two interventions. Moreover, the results suggest that reducing uncertainty in some specific model parameters might be more valuable than in others. Conclusions: Using a value of information framework to assess the value of conducting further research in the field of crime prevention proved to be feasible. The results were meaningful and can be interpreted according to health care evaluation studies. This analysis can be helpful in justifying additional research funds to further inform the reimbursement decision in regard to interventions for juvenile delinquents. "], "author_display": ["Hester V. Eeren", "Saskia J. Schawo", "Ron H. J. Scholte", "Jan J. V. Busschbach", "Leona Hakkaart"], "article_type": "Research Article", "score": 0.44418254, "title_display": "Value of Information Analysis Applied to the Economic Evaluation of Interventions Aimed at Reducing Juvenile Delinquency: An Illustration", "publication_date": "2015-07-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0131255"}, {"journal": "PLoS ONE", "abstract": ["Objective: Biomedical literature is increasingly enriched with literature reviews and meta-analyses. We sought to assess the understanding of statistical terms routinely used in such studies, among researchers. Methods: An online survey posing 4 clinically-oriented multiple-choice questions was conducted in an international sample of randomly selected corresponding authors of articles indexed by PubMed. Results: A total of 315 unique complete forms were analyzed (participation rate 39.4%), mostly from Europe (48%), North America (31%), and Asia/Pacific (17%). Only 10.5% of the participants answered correctly all 4 \u201cinterpretation\u201d questions while 9.2% answered all questions incorrectly. Regarding each question, 51.1%, 71.4%, and 40.6% of the participants correctly interpreted statistical significance of a given odds ratio, risk ratio, and weighted mean difference with 95% confidence intervals respectively, while 43.5% correctly replied that no statistical model can adjust for clinical heterogeneity. Clinicians had more correct answers than non-clinicians (mean score \u00b1 standard deviation: 2.27\u00b11.06 versus 1.83\u00b11.14, p<0.001); among clinicians, there was a trend towards a higher score in medical specialists (2.37\u00b11.07 versus 2.04\u00b11.04, p\u200a=\u200a0.06) and a lower score in clinical laboratory specialists (1.7\u00b10.95 versus 2.3\u00b11.06, p\u200a=\u200a0.08). No association was observed between the respondents' region or questionnaire completion time and participants' score. Conclusion: A considerable proportion of researchers, randomly selected from a diverse international sample of biomedical scientists, misinterpreted statistical terms commonly reported in meta-analyses. Authors could be prompted to explicitly interpret their findings to prevent misunderstandings and readers are encouraged to keep up with basic biostatistics. "], "author_display": ["Michael N. Mavros", "Vangelis G. Alexiou", "Konstantinos Z. Vardakas", "Matthew E. Falagas"], "article_type": "Research Article", "score": 0.443978, "title_display": "Understanding of Statistical Terms Routinely Used in Meta-Analyses: An International Survey among Researchers", "publication_date": "2013-01-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0047229"}, {"journal": "PLOS ONE", "abstract": ["Background: The objective of this review was to evaluate the use of all direct and indirect methods used to estimate health utilities in both children and adolescents. Utilities measured pre- and post-intervention are combined with the time over which health states are experienced to calculate quality-adjusted life years (QALYs). Cost-utility analyses (CUAs) estimate the cost-effectiveness of health technologies based on their costs and benefits using QALYs as a measure of benefit. The accurate measurement of QALYs is dependent on using appropriate methods to elicit health utilities. Objective: We sought studies that measured health utilities directly from patients or their proxies. We did not exclude those studies that also included adults in the analysis, but excluded those studies focused only on adults. Methods and Findings: We evaluated 90 studies from a total of 1,780 selected from the databases. 47 (52%) studies were CUAs incorporated into randomised clinical trials; 23 (26%) were health-state utility assessments; 8 (9%) validated methods and 12 (13%) compared existing or new methods. 22 unique direct or indirect calculation methods were used a total of 137 times. Direct calculation through standard gamble, time trade-off and visual analogue scale was used 32 times. The EuroQol EQ-5D was the most frequently-used single method, selected for 41 studies. 15 of the methods used were generic methods and the remaining 7 were disease-specific. 48 of the 90 studies (53%) used some form of proxy, with 26 (29%) using proxies exclusively to estimate health utilities. Conclusions: Several child- and adolescent-specific methods are still being developed and validated, leaving many studies using methods that have not been designed or validated for use in children or adolescents. Several studies failed to justify using proxy respondents rather than administering the methods directly to the patients. Only two studies examined missing responses to the methods administered with respect to the patients\u2019 ages. "], "author_display": ["Dominic Thorrington", "Ken Eames"], "article_type": "Research Article", "score": 0.4437697, "title_display": "Measuring Health Utilities in Children and Adolescents: A Systematic Review of the Literature", "publication_date": "2015-08-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0135672"}, {"journal": "PLoS Computational Biology", "abstract": ["\nModern DNA sequencing technologies enable geneticists to rapidly identify genetic variation among many human genomes. However, isolating the minority of variants underlying disease remains an important, yet formidable challenge for medical genetics. We have developed GEMINI (GEnome MINIng), a flexible software package for exploring all forms of human genetic variation. Unlike existing tools, GEMINI integrates genetic variation with a diverse and adaptable set of genome annotations (e.g., dbSNP, ENCODE, UCSC, ClinVar, KEGG) into a unified database to facilitate interpretation and data exploration. Whereas other methods provide an inflexible set of variant filters or prioritization methods, GEMINI allows researchers to compose complex queries based on sample genotypes, inheritance patterns, and both pre-installed and custom genome annotations. GEMINI also provides methods for ad hoc queries and data exploration, a simple programming interface for custom analyses that leverage the underlying database, and both command line and graphical tools for common analyses. We demonstrate GEMINI's utility for exploring variation in personal genomes and family based genetic studies, and illustrate its ability to scale to studies involving thousands of human samples. GEMINI is designed for reproducibility and flexibility and our goal is to provide researchers with a standard framework for medical genomics.\n"], "author_display": ["Umadevi Paila", "Brad A. Chapman", "Rory Kirchner", "Aaron R. Quinlan"], "article_type": "Research Article", "score": 0.44366747, "title_display": "GEMINI: Integrative Exploration of Genetic Variation and Genome Annotations", "publication_date": "2013-07-18T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1003153"}, {"journal": "PLOS ONE", "abstract": ["Introduction: Gene-set analysis (GSA) methods are used as complementary approaches to genome-wide association studies (GWASs). The single marker association estimates of a predefined set of genes are either contrasted with those of all remaining genes or with a null non-associated background. To pool the p-values from several GSAs, it is important to take into account the concordance of the observed patterns resulting from single marker association point estimates across any given gene set. Here we propose an enhanced version of Fisher\u2019s inverse \u03c72-method META-GSA, however weighting each study to account for imperfect correlation between association patterns. Simulation and Power: We investigated the performance of META-GSA by simulating GWASs with 500 cases and 500 controls at 100 diallelic markers in 20 different scenarios, simulating different relative risks between 1 and 1.5 in gene sets of 10 genes. Wilcoxon\u2019s rank sum test was applied as GSA for each study. We found that META-GSA has greater power to discover truly associated gene sets than simple pooling of the p-values, by e.g. 59% versus 37%, when the true relative risk for 5 of 10 genes was assume to be 1.5. Under the null hypothesis of no difference in the true association pattern between the gene set of interest and the set of remaining genes, the results of both approaches are almost uncorrelated. We recommend not relying on p-values alone when combining the results of independent GSAs. Application: We applied META-GSA to pool the results of four case-control GWASs of lung cancer risk (Central European Study and Toronto/Lunenfeld-Tanenbaum Research Institute Study; German Lung Cancer Study and MD Anderson Cancer Center Study), which had already been analyzed separately with four different GSA methods (EASE; SLAT, mSUMSTAT and GenGen). This application revealed the pathway GO0015291 \u201ctransmembrane transporter activity\u201d as significantly enriched with associated genes (GSA-method: EASE, p = 0.0315 corrected for multiple testing). Similar results were found for GO0015464 \u201cacetylcholine receptor activity\u201d but only when not corrected for multiple testing (all GSA-methods applied; p\u22480.02). "], "author_display": ["Albert Rosenberger", "Stefanie Friedrichs", "Christopher I. Amos", "Paul Brennan", "Gordon Fehringer", "Joachim Heinrich", "Rayjean J. Hung", "Thomas Muley", "Martina M\u00fcller-Nurasyid", "Angela Risch", "Heike Bickeb\u00f6ller"], "article_type": "Research Article", "score": 0.4435588, "title_display": "META-GSA: Combining Findings from Gene-Set Analyses across Several Genome-Wide Association Studies", "publication_date": "2015-10-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0140179"}, {"journal": "PLoS ONE", "abstract": ["Introduction: Health research is one mechanism to improve population-level health and should generally match the health needs of populations. However, there have been limited data to assess the trends in national-level cardiovascular research output, even as cardiovascular disease [CVD] has become the leading cause of morbidity and mortality worldwide. Materials and Methods: We performed a time trends analysis of cardiovascular research publications (1999\u20132008) downloaded from Web of Knowledge using a iteratively-tested cardiovascular bibliometric filter with >90% precision and recall. We evaluated cardiovascular research publications, five-year running actual citation indices [ACIs], and degree of international collaboration measured through the ratio of the fractional count of addresses from one country against all addresses for each publication. Results and Discussion: Global cardiovascular publication volume increased from 40 661 publications in 1999 to 55 284 publications in 2008, which represents a 36% increase. The proportion of cardiovascular publications from high-income, Organization for Economic Cooperation and Development [OECD] countries declined from 93% to 84% of the total share over the study period. High-income, OECD countries generally had higher fractional counts, which suggest less international collaboration, than lower income countries from 1999\u20132008. There was an inverse relationship between cardiovascular publications and age-standardized CVD morbidity and mortality rates, but a direct, curvilinear relationship between cardiovascular publications and Human Development Index from 1999\u20132008. Conclusions: Cardiovascular health research output has increased substantially in the past decade, with a greater share of citations being published from low- and middle-income countries. However, low- and middle-income countries with the higher burdens of cardiovascular disease continue to have lower research output than high-income countries, and thus require targeted research investments to improve cardiovascular health. "], "author_display": ["Mark D. Huffman", "Abigail Baldridge", "Gerald S. Bloomfield", "Lisandro D. Colantonio", "Poornima Prabhakaran", "Vamadevan S. Ajay", "Sarah Suh", "Grant Lewison", "Dorairaj Prabhakaran"], "article_type": "Research Article", "score": 0.4435241, "title_display": "Global Cardiovascular Research Output, Citations, and Collaborations: A Time-Trend, Bibliometric Analysis (1999\u20132008)", "publication_date": "2013-12-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0083440"}, {"journal": "PLoS ONE", "abstract": ["\nOver the past decade rapid advances have occurred in the understanding of RNA expression and its regulation. Quantitative polymerase chain reactions (qPCR) have become the gold standard for quantifying gene expression. Microfluidic next generation, high throughput qPCR now permits the detection of transcript copy number in thousands of reactions simultaneously, dramatically increasing the sensitivity over standard qPCR. Here we present a gene expression analysis method applicable to both standard polymerase chain reactions (qPCR) and high throughput qPCR. This technique is adjusted to the input sample quantity (e.g., the number of cells) and is independent of control gene expression. It is efficiency-corrected and with the use of a universal reference sample (commercial complementary DNA (cDNA)) permits the normalization of results between different batches and between different instruments \u2013 regardless of potential differences in transcript amplification efficiency. Modifications of the input quantity method include (1) the achievement of absolute quantification and (2) a non-efficiency corrected analysis. When compared to other commonly used algorithms the input quantity method proved to be valid. This method is of particular value for clinical studies of whole blood and circulating leukocytes where cell counts are readily available.\n"], "author_display": ["Mateusz G. Adamski", "Patryk Gumann", "Alison E. Baird"], "article_type": "Research Article", "score": 0.443413, "title_display": "A Method for Quantitative Analysis of Standard and High-Throughput qPCR Expression Data Based on Input Sample Quantity", "publication_date": "2014-08-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0103917"}, {"journal": "PLoS ONE", "abstract": ["\nPostdoctoral training is a typical step in the course of an academic career, but very little is known about postdoctoral researchers (PDRs) working in the UK. This study used an online survey to explore, for the first time, relevant environmental factors which may be linked to the research output of PDRs in terms of the number of peer-reviewed articles per year of PDR employment. The findings showed reliable links between the research output and research institutions, time spent as PDR, and parental education, whereas no clear links were observed between PDRs' output and research area, nationality, gender, number of siblings, or work environment. PDRs based in universities tended to publish, on average, more than the ones based in research centres. PDRs with children tended to stay longer in postdoctoral employment than PDRs without children. Moreover, research output tended to be higher in PDRs with fathers educated at secondary or higher level. The work environment did not affect output directly, but about 1/5 of PDRs were not satisfied with their job or institutional support and about 2/3 of them perceived their job prospects as \u201cdifficult\u201d. The results from this exploratory study raise important questions, which need to be addressed in large-scale studies in order to understand (and monitor) how PDRs' family and work environment interact with their research output\u2014an essential step given the crucial role of PDRs in research and development in the country.\n"], "author_display": ["Fatima M. Felisberti", "Rebecca Sear"], "article_type": "Research Article", "score": 0.44311816, "title_display": "Postdoctoral Researchers in the UK: A Snapshot at Factors Affecting Their Research Output", "publication_date": "2014-04-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0093890"}, {"journal": "PLoS ONE", "abstract": ["Background: Zoonotic infections pose a significant public health challenge for low- and middle-income countries and have traditionally been a neglected area of research. The Roadmap to Combat Zoonoses in India (RCZI) initiative conducted an exercise to systematically identify and prioritize research options needed to control zoonoses in India. Methods and Findings: Priority setting methods developed by the Child Health and Nutrition Research Initiative were adapted for the diversity of sectors, disciplines, diseases and populations relevant for zoonoses in India. A multidisciplinary group of experts identified priority zoonotic diseases and knowledge gaps and proposed research options to address key knowledge gaps within the next five years. Each option was scored using predefined criteria by another group of experts. The scores were weighted using relative ranks among the criteria based upon the feedback of a larger reference group. We categorized each research option by type of research, disease targeted, factorials, and level of collaboration required. We analysed the research options by tabulating them along these categories. Seventeen experts generated four universal research themes and 103 specific research options, the majority of which required a high to medium level of collaboration across sectors. Research options designated as pertaining to \u2018social, political and economic\u2019 factorials predominated and scored higher than options focussing on ecological, genetic and biological, or environmental factors. Research options related to \u2018health policy and systems\u2019 scored highest while those related to \u2018research for development of new interventions\u2019 scored the lowest. Conclusions: We methodically identified research themes and specific research options incorporating perspectives of a diverse group of stakeholders. These outputs reflect the diverse nature of challenges posed by zoonoses and should be acceptable across diseases, disciplines, and sectors. The identified research options capture the need for \u2018actionable research\u2019 for advancing the prevention and control of zoonoses in India. "], "author_display": ["Nitin Sekar", "Naman K. Shah", "Syed Shahid Abbas", "Manish Kakkar", "on behalf of the Roadmap to Combat Zoonoses in India (RCZI) Initiative "], "article_type": "Research Article", "score": 0.4430372, "title_display": "Research Options for Controlling Zoonotic Disease in India, 2010\u20132015", "publication_date": "2011-02-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0017120"}, {"journal": "PLOS ONE", "abstract": ["\nWe here describe a convenient method for preparation, fixation and fluorescence analysis of in vitro cultivated metacestode vesicles from E. multilocularis. Parasite materials could be prepared in one hour, did not need to be sectioned, and were subsequently utilized for further whole-mount staining assays directly. Using these preparations, in combination with conventional fluorescence staining techniques, we could detect the expression and subcellular localization of a specific protein and identify in situ proliferative or apoptotic cells in the germinal layer of metacestode vesicles. Based on this approach, future molecular and cellular analysis of Echinococcus metacestode vesicles in the in vitro system will be greatly facilitated.\n"], "author_display": ["Zhe Cheng", "Fan Liu", "Shan Zhu", "Huimin Tian", "Liang Wang", "Yanhai Wang"], "article_type": "Research Article", "score": 0.4427811, "title_display": "A Rapid and Convenient Method for Fluorescence Analysis of <i>In Vitro</i> Cultivated Metacestode Vesicles from <i>Echinococcus multilocularis</i>", "publication_date": "2015-02-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0118215"}, {"journal": "PLOS ONE", "abstract": ["\nEssential proteins are indispensable for living organisms to maintain life activities and play important roles in the studies of pathology, synthetic biology, and drug design. Therefore, besides experiment methods, many computational methods are proposed to identify essential proteins. Based on the centrality-lethality rule, various centrality methods are employed to predict essential proteins in a Protein-protein Interaction Network (PIN). However, neglecting the temporal and spatial features of protein-protein interactions, the centrality scores calculated by centrality methods are not effective enough for measuring the essentiality of proteins in a PIN. Moreover, many methods, which overfit with the features of essential proteins for one species, may perform poor for other species. In this paper, we demonstrate that the centrality-lethality rule also exists in Protein Subcellular Localization Interaction Networks (PSLINs). To do this, a method based on Localization Specificity for Essential protein Detection (LSED), was proposed, which can be combined with any centrality method for calculating the improved centrality scores by taking into consideration PSLINs in which proteins play their roles. In this study, LSED was combined with eight centrality methods separately to calculate Localization-specific Centrality Scores (LCSs) for proteins based on the PSLINs of four species (Saccharomyces cerevisiae, Homo sapiens, Mus musculus and Drosophila melanogaster). Compared to the proteins with high centrality scores measured from the global PINs, more proteins with high LCSs measured from PSLINs are essential. It indicates that proteins with high LCSs measured from PSLINs are more likely to be essential and the performance of centrality methods can be improved by LSED. Furthermore, LSED provides a wide applicable prediction model to identify essential proteins for different species.\n"], "author_display": ["Xiaoqing Peng", "Jianxin Wang", "Jun Wang", "Fang-Xiang Wu", "Yi Pan"], "article_type": "Research Article", "score": 0.44263297, "title_display": "Rechecking the Centrality-Lethality Rule in the Scope of Protein Subcellular Localization Interaction Networks", "publication_date": "2015-06-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0130743"}, {"journal": "PLoS ONE", "abstract": ["\nWe have developed a method focusing on ECG signal de-noising using Independent component analysis (ICA). This approach combines JADE source separation and binary decision tree for identification and subsequent ECG noise removal. In order to to test the efficiency of this method comparison to standard filtering a wavelet- based de-noising method was used. Freely data available at Physionet medical data storage were evaluated. Evaluation criteria was root mean square error (RMSE) between original ECG and filtered data contaminated with artificial noise. Proposed algorithm achieved comparable result in terms of standard noises (power line interference, base line wander, EMG), but noticeably significantly better results were achieved when uncommon noise (electrode cable movement artefact) were compared.\n"], "author_display": ["Jakub Kuzilek", "Vaclav Kremen", "Filip Soucek", "Lenka Lhotska"], "article_type": "Research Article", "score": 0.44234753, "title_display": "Independent Component Analysis and Decision Trees for ECG Holter Recording De-Noising", "publication_date": "2014-06-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0098450"}, {"journal": "PLoS ONE", "abstract": ["\nTo generate information about the monsoon onset and withdrawal we have to choose a monsoon definition and apply it to data. One problem that arises is that false monsoon onsets can hamper our analysis, which is often alleviated by smoothing the data in time or space. Another problem is that local communities or stakeholder groups may define the monsoon differently. We therefore aim to develop a technique that reduces false onsets for high-resolution gridded data, while also being flexible for different requirements that can be tailored to particular end-users. In this study, we explain how we developed our technique and demonstrate how it successfully reduces false onsets and withdrawals. The presented results yield improved information about the monsoon length and its interannual variability. Due to this improvement, we are able to extract information from higher resolution data sets. This implies that we can potentially get a more detailed picture of local climate variations that can be used in more local climate application projects such as community-based adaptations.\n"], "author_display": ["Mathew Alexander Stiller-Reeve", "Thomas Spengler", "Pao-Shin Chu"], "article_type": "Research Article", "score": 0.44223127, "title_display": "Testing a Flexible Method to Reduce False Monsoon Onsets", "publication_date": "2014-08-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0104386"}, {"journal": "PLOS ONE", "abstract": ["\nRecovery of high quality PCR-amplifiable DNA has been the general minimal requirement for DNA extraction methods for bulk molecular analysis. However, modern high through-put community profiling technologies are more sensitive to representativeness and reproducibility of DNA extraction method. Here, we assess the impact of three DNA extraction methods (with different levels of extraction harshness) for assessing hindgut microbiomes from pigs fed with different diets (with different physical properties). DNA extraction from each sample was performed in three technical replicates for each extraction method and sequenced by 16S rRNA amplicon sequencing. Host was the primary driver of molecular sequencing outcomes, particularly on samples analysed by wheat based diets, but higher variability, with one failed extraction occurred on samples from a barley fed pig. Based on these results, an effective method will enable reproducible and quality outcomes on a range of samples, whereas an ineffective method will fail to generate extract, but host (rather than extraction method) remains the primary factor.\n"], "author_display": ["Yang Lu", "Philip Hugenholtz", "Damien John Batstone"], "article_type": "Research Article", "score": 0.44200143, "title_display": "Evaluating DNA Extraction Methods for Community Profiling of Pig Hindgut Microbial Community", "publication_date": "2015-11-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0142720"}, {"journal": "PLoS ONE", "abstract": ["Background: The purpose of this systematic review was to evaluate evidence about authorship issues and provide synthesis of research on authorship across all research fields. Methods: We searched bibliographical databases to identify articles describing empirical quantitive or qualitative research from all scholarly fields on different aspects of authorship. Search was limited to original articles and reviews. Results: The final sample consisted of 123 articles reporting results from 118 studies. Most studies came for biomedical and health research fields and social sciences. Study design was usually a survey (53%) or descriptive study (27%); only 2 studies used randomized design. We identified four 4 general themes common to all research disciplines: authorship perceptions, definitions and practices, defining order of authors on the byline, ethical and unethical authorship practices, and authorship issues related to student/non-research personnel-supervisor collaboration. For 14 survey studies, a meta-analysis showed a pooled weighted average of 29% (95% CI 24% to 35%) researchers reporting their own or others' experience with misuse of authorship. Authorship misuse was reported more often by researcher outside of the USA and UK: 55% (95% CI 45% to 64%) for 4 studies in France, South Africa, India and Bangladesh vs. 23% (95% CI 18% to 28%) in USA/UK or international journal settings. Interpretation: High prevalence of authorship problems may have severe impact on the integrity of the research process, just as more serious forms of research misconduct. There is a need for more methodologically rigorous studies to understand the allocation of publication credit across research disciplines. "], "author_display": ["Ana Maru\u0161i\u0107", "Lana Bo\u0161njak", "Ana Jeron\u010di\u0107"], "article_type": "Research Article", "score": 0.4417236, "title_display": "A Systematic Review of Research on the Meaning, Ethics and Practices of Authorship across Scholarly Disciplines", "publication_date": "2011-09-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0023477"}, {"journal": "PLoS ONE", "abstract": ["\nThis study aims to assess the nanotribology research output at global level using scientometric tools. The SCOPUS database was used to retrieve records related to the nanotribology research for the period 1996\u20132010. Publications were counted on a fractional basis. The level of collaboration and its citation impact were examined. The performance of the most productive countries, institutes and most preferred journals is assessed. Various visualization tools such as the Sci2 tool and Ucinet were employed. The USA ranked top in terms of number of publications, citations per paper and h-index, while Switzerland published a higher percentage of international collaborative papers. The most productive institution was Tsinghua University followed by Ohio State University and Lanzhou Institute of Chemical Physics, CAS. The most preferred journals were Tribology Letters, Wear and Journal of Japanese Society of Tribologists. The result of author keywords analysis reveals that Molecular Dynamics, MEMS, Hard Disk and Diamond like Carbon are major research topics.\n"], "author_display": ["Bakthavachalam Elango", "Periyaswamy Rajendran", "Lutz Bornmann"], "article_type": "Research Article", "score": 0.44162816, "title_display": "Global Nanotribology Research Output (1996\u20132010): A Scientometric Analysis", "publication_date": "2013-12-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0081094"}, {"journal": "PLoS ONE", "abstract": ["Background: Constructing coexpression networks and performing network analysis using large-scale gene expression data sets is an effective way to uncover new biological knowledge; however, the methods used for gene association in constructing these coexpression networks have not been thoroughly evaluated. Since different methods lead to structurally different coexpression networks and provide different information, selecting the optimal gene association method is critical. Methods and Results: In this study, we compared eight gene association methods \u2013 Spearman rank correlation, Weighted Rank Correlation, Kendall, Hoeffding's D measure, Theil-Sen, Rank Theil-Sen, Distance Covariance, and Pearson \u2013 and focused on their true knowledge discovery rates in associating pathway genes and construction coordination networks of regulatory genes. We also examined the behaviors of different methods to microarray data with different properties, and whether the biological processes affect the efficiency of different methods. Conclusions: We found that the Spearman, Hoeffding and Kendall methods are effective in identifying coexpressed pathway genes, whereas the Theil-sen, Rank Theil-Sen, Spearman, and Weighted Rank methods perform well in identifying coordinated transcription factors that control the same biological processes and traits. Surprisingly, the widely used Pearson method is generally less efficient, and so is the Distance Covariance method that can find gene pairs of multiple relationships. Some analyses we did clearly show Pearson and Distance Covariance methods have distinct behaviors as compared to all other six methods. The efficiencies of different methods vary with the data properties to some degree and are largely contingent upon the biological processes, which necessitates the pre-analysis to identify the best performing method for gene association and coexpression network construction. "], "author_display": ["Sapna Kumari", "Jeff Nie", "Huann-Sheng Chen", "Hao Ma", "Ron Stewart", "Xiang Li", "Meng-Zhu Lu", "William M. Taylor", "Hairong Wei"], "article_type": "Research Article", "score": 0.4414595, "title_display": "Evaluation of Gene Association Methods for Coexpression Network Construction and Biological Knowledge Discovery", "publication_date": "2012-11-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0050411"}, {"journal": "PLOS ONE", "abstract": ["\nThis work aimed at combining different segmentation approaches to produce a robust and accurate segmentation result. Three to five segmentation results of the left ventricle were combined using the STAPLE algorithm and the reliability of the resulting segmentation was evaluated in comparison with the result of each individual segmentation method. This comparison was performed using a supervised approach based on a reference method. Then, we used an unsupervised statistical evaluation, the extended Regression Without Truth (eRWT) that ranks different methods according to their accuracy in estimating a specific biomarker in a population. The segmentation accuracy was evaluated by estimating six cardiac function parameters resulting from the left ventricle contour delineation using a public cardiac cine MRI database. Eight different segmentation methods, including three expert delineations and five automated methods, were considered, and sixteen combinations of the automated methods using STAPLE were investigated. The supervised and unsupervised evaluations demonstrated that in most cases, STAPLE results provided better estimates than individual automated segmentation methods. Overall, combining different automated segmentation methods improved the reliability of the segmentation result compared to that obtained using an individual method and could achieve the accuracy of an expert.\n"], "author_display": ["Jessica Lebenberg", "Alain Lalande", "Patrick Clarysse", "Irene Buvat", "Christopher Casta", "Alexandre Cochet", "Constantin Constantinid\u00e8s", "Jean Cousty", "Alain de Cesare", "Stephanie Jehan-Besson", "Muriel Lefort", "Laurent Najman", "Elodie Roullot", "Laurent Sarry", "Christophe Tilmant", "Frederique Frouin", "Mireille Garreau"], "article_type": "Research Article", "score": 0.44141123, "title_display": "Improved Estimation of Cardiac Function Parameters Using a Combination of Independent Automated Segmentation Results in Cardiovascular Magnetic Resonance Imaging", "publication_date": "2015-08-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0135715"}, {"journal": "PLoS ONE", "abstract": ["\nThe generalized and improved -expansion method is a powerful and advantageous mathematical tool for establishing abundant new traveling wave solutions of nonlinear partial differential equations. In this article, we investigate the higher dimensional nonlinear evolution equation, namely, the (3+1)-dimensional modified KdV-Zakharov-Kuznetsev equation via this powerful method. The solutions are found in hyperbolic, trigonometric and rational function form involving more parameters and some of our constructed solutions are identical with results obtained by other authors if certain parameters take special values and some are new. The numerical results described in the figures were obtained with the aid of commercial software Maple.\n"], "author_display": ["Hasibun Naher", "Farah Aini Abdullah", "M. Ali Akbar"], "article_type": "Research Article", "score": 0.4413349, "title_display": "Generalized and Improved <i>(G\u2032/G)</i>-Expansion Method for (3+1)-Dimensional Modified KdV-Zakharov-Kuznetsev Equation", "publication_date": "2013-05-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0064618"}, {"journal": "PLOS ONE", "abstract": ["\nWhen a long distance oil pipeline crosses an earthquake disaster area, inertial force and strong ground motion can cause the pipeline stress to exceed the failure limit, resulting in bending and deformation failure. To date, researchers have performed limited safety analyses of oil pipelines in earthquake disaster areas that include stress analysis. Therefore, using the spectrum method and theory of one-dimensional beam units, CAESAR II is used to perform a dynamic earthquake analysis for an oil pipeline in the XX earthquake disaster area. This software is used to determine if the displacement and stress of the pipeline meet the standards when subjected to a strong earthquake. After performing the numerical analysis, the primary seismic action axial, longitudinal and horizontal displacement directions and the critical section of the pipeline can be located. Feasible project enhancement suggestions based on the analysis results are proposed. The designer is able to utilize this stress analysis method to perform an ultimate design for an oil pipeline in earthquake disaster areas; therefore, improving the safe operation of the pipeline.\n"], "author_display": ["Xiaonan Wu", "Hongfang Lu", "Kun Huang", "Shijuan Wu", "Weibiao Qiao"], "article_type": "Research Article", "score": 0.44088954, "title_display": "Frequency Spectrum Method-Based Stress Analysis for Oil Pipelines in Earthquake Disaster Areas", "publication_date": "2015-02-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0115299"}, {"journal": "PLoS ONE", "abstract": ["Background: Seasonality of suicides is well-known and nearly ubiquitous, but recent evidence showed inconsistent patterns of decreasing or increasing seasonality in different countries. Furthermore, strength of seasonality was hypothesized to be associated with suicide prevalence. This study aimed at pointing out methodological difficulties in examining changes in suicide seasonality. Methododology/Principal Findings: The present study examines the hypothesis of decreasing seasonality with a superior method that allows continuous modeling of seasonality. Suicides in Austria (1970\u20132008, N\u200a=\u200a67,741) were analyzed with complex demodulation, a local (point-in-time specific) version of harmonic analysis. This avoids the need to arbitrarily split the time series, as is common practice in the field of suicide seasonality research, and facilitates incorporating the association with suicide prevalence. Regression models were used to assess time trends and association of amplitude and absolute suicide numbers. Results showed that strength of seasonality was associated with absolute suicide numbers, and that strength of seasonality was stable during the study period when this association was taken into account. Conclusion/Significance: Continuous modeling of suicide seasonality with complex demodulation avoids spurious findings that can result when time series are segmented and analyzed piecewise or when the association with suicide prevalence is disregarded. "], "author_display": ["Ingo W. Nader", "Jakob Pietschnig", "Thomas Niederkrotenthaler", "Nestor D. Kapusta", "Gernot Sonneck", "Martin Voracek"], "article_type": "Research Article", "score": 0.4408434, "title_display": "Suicide Seasonality: Complex Demodulation as a Novel Approach in Epidemiologic Analysis", "publication_date": "2011-02-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0017413"}, {"journal": "PLoS ONE", "abstract": ["\n        The sequential analysis of client and clinician speech in psychotherapy sessions can help to identify and characterize potential mechanisms of treatment and behavior change. Previous studies required coding systems that were time-consuming, expensive, and error-prone. Existing software can be expensive and inflexible, and furthermore, no single package allows for pre-parsing, sequential coding, and assignment of global ratings. We developed a free, open-source, and adaptable program to meet these needs: The CASAA Application for Coding Treatment Interactions (CACTI). Without transcripts, CACTI facilitates the real-time sequential coding of behavioral interactions using WAV-format audio files. Most elements of the interface are user-modifiable through a simple XML file, and can be further adapted using Java through the terms of the GNU Public License. Coding with this software yields interrater reliabilities comparable to previous methods, but at greatly reduced time and expense. CACTI is a flexible research tool that can simplify psychotherapy process research, and has the potential to contribute to the improvement of treatment content and delivery.\n      "], "author_display": ["Lisa H. Glynn", "Kevin A. Hallgren", "Jon M. Houck", "Theresa B. Moyers"], "article_type": "Research Article", "score": 0.4406312, "title_display": "CACTI: Free, Open-Source Software for the Sequential Coding of Behavioral Interactions", "publication_date": "2012-07-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0039740"}, {"journal": "PLoS Genetics", "abstract": ["\nResequencing is an emerging tool for identification of rare disease-associated mutations. Rare mutations are difficult to tag with SNP genotyping, as genotyping studies are designed to detect common variants. However, studies have shown that genetic heterogeneity is a probable scenario for common diseases, in which multiple rare mutations together explain a large proportion of the genetic basis for the disease. Thus, we propose a weighted-sum method to jointly analyse a group of mutations in order to test for groupwise association with disease status. For example, such a group of mutations may result from resequencing a gene. We compare the proposed weighted-sum method to alternative methods and show that it is powerful for identifying disease-associated genes, both on simulated and Encode data. Using the weighted-sum method, a resequencing study can identify a disease-associated gene with an overall population attributable risk (PAR) of 2%, even when each individual mutation has much lower PAR, using 1,000 to 7,000 affected and unaffected individuals, depending on the underlying genetic model. This study thus demonstrates that resequencing studies can identify important genetic associations, provided that specialised analysis methods, such as the weighted-sum method, are used.\nAuthor Summary: Resequencing is an emerging tool for the identification of rare disease-associated mutations. Recent studies have shown that groups of multiple rare mutations together can explain a large proportion of the genetic basis for some diseases. Therefore, we propose a new statistical method for analysing a group of mutations in order to test for groupwise association with disease status. We compare the proposed weighted-sum method to alternative methods and show that it is powerful for identifying disease-associated groups of mutations, both on computer-simulated and real data. By using computer simulations, we further show that resequencing a few thousand individuals is sufficient to perform a genome-wide study of all human genes, if the proposed method is used. This study thus demonstrates that resequencing studies can identify important genetic associations, provided that specialised analysis methods, such as the proposed weighted-sum method, are used. "], "author_display": ["Bo Eskerod Madsen", "Sharon R. Browning"], "article_type": "Research Article", "score": 0.44056007, "title_display": "A Groupwise Association Test for Rare Mutations Using a Weighted Sum Statistic", "publication_date": "2009-02-13T00:00:00Z", "eissn": "1553-7404", "id": "10.1371/journal.pgen.1000384"}, {"journal": "PLoS ONE", "abstract": ["Background: The analysis of microbial communities through DNA sequencing brings many challenges: the integration of different types of data with methods from ecology, genetics, phylogenetics, multivariate statistics, visualization and testing. With the increased breadth of experimental designs now being pursued, project-specific statistical analyses are often needed, and these analyses are often difficult (or impossible) for peer researchers to independently reproduce. The vast majority of the requisite tools for performing these analyses reproducibly are already implemented in R and its extensions (packages), but with limited support for high throughput microbiome census data. Results: Here we describe a software project, phyloseq, dedicated to the object-oriented representation and analysis of microbiome census data in R. It supports importing data from a variety of common formats, as well as many analysis techniques. These include calibration, filtering, subsetting, agglomeration, multi-table comparisons, diversity analysis, parallelized Fast UniFrac, ordination methods, and production of publication-quality graphics; all in a manner that is easy to document, share, and modify. We show how to apply functions from other R packages to phyloseq-represented data, illustrating the availability of a large number of open source analysis techniques. We discuss the use of phyloseq with tools for reproducible research, a practice common in other fields but still rare in the analysis of highly parallel microbiome census data. We have made available all of the materials necessary to completely reproduce the analysis and figures included in this article, an example of best practices for reproducible research. Conclusions: The phyloseq project for R is a new open-source software package, freely available on the web from both GitHub and Bioconductor. "], "author_display": ["Paul J. McMurdie", "Susan Holmes"], "article_type": "Research Article", "score": 0.44053394, "title_display": "phyloseq: An R Package for Reproducible Interactive Analysis and Graphics of Microbiome Census Data", "publication_date": "2013-04-22T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0061217"}, {"journal": "PLoS Computational Biology", "abstract": ["\nTracking bacteria using video microscopy is a powerful experimental approach to probe their motile behaviour. The trajectories obtained contain much information relating to the complex patterns of bacterial motility. However, methods for the quantitative analysis of such data are limited. Most swimming bacteria move in approximately straight lines, interspersed with random reorientation phases. It is therefore necessary to segment observed tracks into swimming and reorientation phases to extract useful statistics. We present novel robust analysis tools to discern these two phases in tracks. Our methods comprise a simple and effective protocol for removing spurious tracks from tracking datasets, followed by analysis based on a two-state hidden Markov model, taking advantage of the availability of mutant strains that exhibit swimming-only or reorientating-only motion to generate an empirical prior distribution. Using simulated tracks with varying levels of added noise, we validate our methods and compare them with an existing heuristic method. To our knowledge this is the first example of a systematic assessment of analysis methods in this field. The new methods are substantially more robust to noise and introduce less systematic bias than the heuristic method. We apply our methods to tracks obtained from the bacterial species Rhodobacter sphaeroides and Escherichia coli. Our results demonstrate that R. sphaeroides exhibits persistence over the course of a tumbling event, which is a novel result with important implications in the study of this and similar species.\nAuthor Summary: Many species of planktonic bacteria are able to propel themselves through a liquid medium by the use of one or more helical flagella. Commonly, the observed motile behaviour consists of a series of approximately straight-line movements, interspersed with random, approximately stationary, reorientation events. This phenomenon is of current interest as it is known to be linked to important bacterial processes such as pathogenicity and biofilm formation. An accepted experimental approach for studying bacterial motility in approximately indigenous conditions is the tracking of cells using a microscope. However, there are currently no validated methods for the analysis of such tracking data. In particular, the identification of reorientation phases, which is complicated by various sources of noise in the data, remains an open challenge. In this paper we present novel methods for analysing large bacterial tracking datasets. We assess the performance of our new methods using computational simulations, and show that they are more reliable than a previously published method. We proceed to analyse previously unpublished tracks from the bacterial species Rhodobacter sphaeroides, an emerging model organism in the field of bacterial motility, and Escherichia coli, a well-studied model bacterium. The analysis demonstrates the novel result that R. sphaeroides exhibits directional persistence over the course of a reorientation event. "], "author_display": ["Gabriel Rosser", "Alexander G. Fletcher", "David A. Wilkinson", "Jennifer A. de Beyer", "Christian A. Yates", "Judith P. Armitage", "Philip K. Maini", "Ruth E. Baker"], "article_type": "Research Article", "score": 0.4401487, "title_display": "Novel Methods for Analysing Bacterial Tracks Reveal Persistence in <i>Rhodobacter sphaeroides</i>", "publication_date": "2013-10-24T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1003276"}, {"journal": "PLOS ONE", "abstract": ["\nIn a meta-analysis with multiple end points of interests that are correlated between or within studies, multivariate approach to meta-analysis has a potential to produce more precise estimates of effects by exploiting the correlation structure between end points. However, under random-effects assumption the multivariate estimation is more complex (as it involves estimation of more parameters simultaneously) than univariate estimation, and sometimes can produce unrealistic parameter estimates. Usefulness of multivariate approach to meta-analysis of the effects of a genetic variant on two or more correlated traits is not well understood in the area of genetic association studies. In such studies, genetic variants are expected to roughly maintain Hardy-Weinberg equilibrium within studies, and also their effects on complex traits are generally very small to modest and could be heterogeneous across studies for genuine reasons. We carried out extensive simulation to explore the comparative performance of multivariate approach with most commonly used univariate inverse-variance weighted approach under random-effects assumption in various realistic meta-analytic scenarios of genetic association studies of correlated end points. We evaluated the performance with respect to relative mean bias percentage, and root mean square error (RMSE) of the estimate and coverage probability of corresponding 95% confidence interval of the effect for each end point. Our simulation results suggest that multivariate approach performs similarly or better than univariate method when correlations between end points within or between studies are at least moderate and between-study variation is similar or larger than average within-study variation for meta-analyses of 10 or more genetic studies. Multivariate approach produces estimates with smaller bias and RMSE especially for the end point that has randomly or informatively missing summary data in some individual studies, when the missing data in the endpoint are imputed with null effects and quite large variance.\n"], "author_display": ["Binod Neupane", "Joseph Beyene"], "article_type": "Research Article", "score": 0.44010922, "title_display": "Multivariate Meta-Analysis of Genetic Association Studies: A Simulation Study", "publication_date": "2015-07-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0133243"}, {"journal": "PLoS ONE", "abstract": ["Background: Real-time quantitative PCR (qPCR) is still the gold-standard technique for gene-expression quantification. Recent technological advances of this method allow for the high-throughput gene-expression analysis, without the limitations of sample space and reagent used. However, non-commercial and user-friendly software for the management and analysis of these data is not available. Results: The recently developed commercial microarrays allow for the drawing of standard curves of multiple assays using the same n-fold diluted samples. Data Analysis Gene (DAG) Expression software has been developed to perform high-throughput gene-expression data analysis using standard curves for relative quantification and one or multiple reference genes for sample normalization. We discuss the application of DAG Expression in the analysis of data from an experiment performed with Fluidigm technology, in which 48 genes and 115 samples were measured. Furthermore, the quality of our analysis was tested and compared with other available methods. Conclusions: DAG Expression is a freely available software that permits the automated analysis and visualization of high-throughput qPCR. A detailed manual and a demo-experiment are provided within the DAG Expression software at http://www.dagexpression.com/dage.zip. "], "author_display": ["Mar\u00eda Ballester", "Rub\u00e9n Cord\u00f3n", "Josep M. Folch"], "article_type": "Research Article", "score": 0.44006324, "title_display": "DAG Expression: High-Throughput Gene Expression Analysis of Real-Time PCR Data Using Standard Curves for Relative Quantification", "publication_date": "2013-11-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0080385"}, {"journal": "PLoS ONE", "abstract": ["\n        Digital communication data has created opportunities to advance the knowledge of human dynamics in many areas, including national security, behavioral health, and consumerism. While digital data uniquely captures the totality of a person's communication, past research consistently shows that a subset of contacts makes up a person's \u201csocial network\u201d of unique resource providers. To address this gap, we analyzed the correspondence between self-reported social network data and email communication data with the objective of identifying the dynamics in e-communication that correlate with a person's perception of a significant network tie. First, we examined the predictive utility of three popular methods to derive social network data from email data based on volume and reciprocity of bilateral email exchanges. Second, we observed differences in the response dynamics along self-reported ties, allowing us to introduce and test a new method that incorporates time-resolved exchange data. Using a range of robustness checks for measurement and misreporting errors in self-report and email data, we find that the methods have similar predictive utility. Although e-communication has lowered communication costs with large numbers of persons, and potentially extended our number of, and reach to contacts, our case results suggest that underlying behavioral patterns indicative of friendship or professional contacts continue to operate in a classical fashion in email interactions.\n      "], "author_display": ["Stefan Wuchty", "Brian Uzzi"], "article_type": "Research Article", "score": 0.43908936, "title_display": "Human Communication Dynamics in Digital Footsteps: A Study of the Agreement between Self-Reported Ties and Email Networks", "publication_date": "2011-11-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0026972"}, {"journal": "PLOS ONE", "abstract": ["\nTo effectively monitor the atmospheric quality of small-scale areas, it is necessary to optimize the locations of the monitoring sites. This study combined geographic parameters extraction by GIS with fuzzy matter-element analysis. Geographic coordinates were extracted by GIS and transformed into rectangular coordinates. These coordinates were input into the Gaussian plume model to calculate the pollutant concentration at each site. Fuzzy matter-element analysis, which is used to solve incompatible problems, was used to select the locations of sites. The matter element matrices were established according to the concentration parameters. The comprehensive correlation functions KA (xj) and KB (xj), which reflect the degree of correlation among monitoring indices, were solved for each site, and a scatter diagram of the sites was drawn to determine the final positions of the sites based on the functions. The sites could be classified and ultimately selected by the scatter diagram. An actual case was tested, and the results showed that 5 positions can be used for monitoring, and the locations conformed to the technical standard. In the results of this paper, the hierarchical clustering method was used to improve the methods. The sites were classified into 5 types, and 7 locations were selected. Five of the 7 locations were completely identical to the sites determined by fuzzy matter-element analysis. The selections according to these two methods are similar, and these methods can be used in combination. In contrast to traditional methods, this study monitors the isolated point pollutant source within a small range, which can reduce the cost of monitoring.\n"], "author_display": ["Jianfa Wu", "Dahao Peng", "Jianhao Ma", "Li Zhao", "Ce Sun", "Huanzhang Ling"], "article_type": "Research Article", "score": 0.43896517, "title_display": "Selection of Atmospheric Environmental Monitoring Sites based on Geographic Parameters Extraction of GIS and Fuzzy Matter-Element Analysis", "publication_date": "2015-04-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0123766"}, {"journal": "PLOS ONE", "abstract": ["\nHere we focus on factor analysis from a best practices point of view, by investigating the factor structure of neuropsychological tests and using the results obtained to illustrate on choosing a reasonable solution. The sample (n=1051 individuals) was randomly divided into two groups: one for exploratory factor analysis (EFA) and principal component analysis (PCA), to investigate the number of factors underlying the neurocognitive variables; the second to test the \u201cbest fit\u201d model via confirmatory factor analysis (CFA). For the exploratory step, three extraction (maximum likelihood, principal axis factoring and principal components) and two rotation (orthogonal and oblique) methods were used. The analysis methodology allowed exploring how different cognitive/psychological tests correlated/discriminated between dimensions, indicating that to capture latent structures in similar sample sizes and measures, with approximately normal data distribution, reflective models with oblimin rotation might prove the most adequate.\n"], "author_display": ["Nadine Correia Santos", "Patr\u00edcio Soares Costa", "Liliana Amorim", "Pedro Silva Moreira", "Pedro Cunha", "Jorge Cotter", "Nuno Sousa"], "article_type": "Research Article", "score": 0.43850568, "title_display": "Exploring the Factor Structure of Neurocognitive Measures in Older Individuals", "publication_date": "2015-04-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0124229"}, {"journal": "PLoS ONE", "abstract": ["\n        Conventional gene selection methods based on principal component analysis (PCA) use only the first principal component (PC) of PCA or sparse PCA to select characteristic genes. These methods indeed assume that the first PC plays a dominant role in gene selection. However, in a number of cases this assumption is not satisfied, so the conventional PCA-based methods usually provide poor selection results. In order to improve the performance of the PCA-based gene selection method, we put forward the gene selection method via weighting PCs by singular values (WPCS). Because different PCs have different importance, the singular values are exploited as the weights to represent the influence on gene selection of different PCs. The ROC curves and AUC statistics on artificial data show that our method outperforms the state-of-the-art methods. Moreover, experimental results on real gene expression data sets show that our method can extract more characteristic genes in response to abiotic stresses than conventional gene selection methods.\n      "], "author_display": ["Jin-Xing Liu", "Yong Xu", "Chun-Hou Zheng", "Yi Wang", "Jing-Yu Yang"], "article_type": "Research Article", "score": 0.43816715, "title_display": "Characteristic Gene Selection via Weighting Principal Components by Singular Values", "publication_date": "2012-07-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0038873"}, {"journal": "PLOS ONE", "abstract": ["Objective: Routinely collected health data, collected for administrative and clinical purposes, without specific a priori research questions, are increasingly used for observational, comparative effectiveness, health services research, and clinical trials. The rapid evolution and availability of routinely collected data for research has brought to light specific issues not addressed by existing reporting guidelines. The aim of the present project was to determine the priorities of stakeholders in order to guide the development of the REporting of studies Conducted using Observational Routinely-collected health Data (RECORD) statement. Methods: Two modified electronic Delphi surveys were sent to stakeholders. The first determined themes deemed important to include in the RECORD statement, and was analyzed using qualitative methods. The second determined quantitative prioritization of the themes based on categorization of manuscript headings. The surveys were followed by a meeting of RECORD working committee, and re-engagement with stakeholders via an online commentary period. Results: The qualitative survey (76 responses of 123 surveys sent) generated 10 overarching themes and 13 themes derived from existing STROBE categories. Highest-rated overall items for inclusion were: Disease/exposure identification algorithms; Characteristics of the population included in databases; and Characteristics of the data. In the quantitative survey (71 responses of 135 sent), the importance assigned to each of the compiled themes varied depending on the manuscript section to which they were assigned. Following the working committee meeting, online ranking by stakeholders provided feedback and resulted in revision of the final checklist. Conclusions: The RECORD statement incorporated the suggestions provided by a large, diverse group of stakeholders to create a reporting checklist specific to observational research using routinely collected health data. Our findings point to unique aspects of studies conducted with routinely collected health data and the perceived need for better reporting of methodological issues. "], "author_display": ["Stuart G. Nicholls", "Pauline Quach", "Erik von Elm", "Astrid Guttmann", "David Moher", "Irene Petersen", "Henrik T. S\u00f8rensen", "Liam Smeeth", "Sin\u00e9ad M. Langan", "Eric I. Benchimol"], "article_type": "Research Article", "score": 0.43800673, "title_display": "The REporting of Studies Conducted Using Observational Routinely-Collected Health Data (RECORD) Statement: Methods for Arriving at Consensus and Developing Reporting Guidelines", "publication_date": "2015-05-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0125620"}, {"journal": "PLoS ONE", "abstract": ["\n        The authors propose a CT image segmentation method using structural analysis that is useful for objects with structural dynamic characteristics. Motivation of our research is from the area of genetic activity. In order to reveal the roles of genes, it is necessary to create mutant mice and measure differences among them by scanning their skeletons with an X-ray CT scanner. The CT image needs to be manually segmented into pieces of the bones. It is a very time consuming to manually segment many mutant mouse models in order to reveal the roles of genes. It is desirable to make this segmentation procedure automatic. Although numerous papers in the past have proposed segmentation techniques, no general segmentation method for skeletons of living creatures has been established. Against this background, the authors propose a segmentation method based on the concept of destruction analogy. To realize this concept, structural analysis is performed using the finite element method (FEM), as structurally weak areas can be expected to break under conditions of stress. The contribution of the method is its novelty, as no studies have so far used structural analysis for image segmentation. The method's implementation involves three steps. First, finite elements are created directly from the pixels of a CT image, and then candidates are also selected in areas where segmentation is thought to be appropriate. The second step involves destruction analogy to find a single candidate with high strain chosen as the segmentation target. The boundary conditions for FEM are also set automatically. Then, destruction analogy is implemented by replacing pixels with high strain as background ones, and this process is iterated until object is decomposed into two parts. Here, CT image segmentation is demonstrated using various types of CT imagery.\n      "], "author_display": ["Hiroyuki Hishida", "Hiromasa Suzuki", "Takashi Michikawa", "Yutaka Ohtake", "Satoshi Oota"], "article_type": "Research Article", "score": 0.43790948, "title_display": "CT Image Segmentation Using FEM with Optimized Boundary Condition", "publication_date": "2012-02-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0031116"}, {"journal": "PLoS ONE", "abstract": ["Background: Complex PCR applications for large genome-scale projects require fast, reliable and often highly sophisticated primer design software applications. Presently, such applications use pipelining methods to utilise many third party applications and this involves file parsing, interfacing and data conversion, which is slow and prone to error. A fully integrated suite of software tools for primer design would considerably improve the development time, the processing speed, and the reliability of bespoke primer design software applications. Results: The PD5 software library is an open-source collection of classes and utilities, providing a complete collection of software building blocks for primer design and analysis. It is written in object-oriented C++ with an emphasis on classes suitable for efficient and rapid development of bespoke primer design programs. The modular design of the software library simplifies the development of specific applications and also integration with existing third party software where necessary. We demonstrate several applications created using this software library that have already proved to be effective, but we view the project as a dynamic environment for building primer design software and it is open for future development by the bioinformatics community. Therefore, the PD5 software library is published under the terms of the GNU General Public License, which guarantee access to source-code and allow redistribution and modification. Conclusions: The PD5 software library is downloadable from Google Code and the accompanying Wiki includes instructions and examples: http://code.google.com/p/primer-design "], "author_display": ["Michael C. Riley", "Wayne Aubrey", "Michael Young", "Amanda Clare"], "article_type": "Research Article", "score": 0.43770972, "title_display": "PD5: A General Purpose Library for Primer Design Software", "publication_date": "2013-11-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0080156"}, {"journal": "PLOS ONE", "abstract": ["\nA recommended field method to assess body composition in adolescent sprint athletes is currently lacking. Existing methods developed for non-athletic adolescents were not longitudinally validated and do not take maturation status into account. This longitudinal study compared two field methods, i.e., a Bio Impedance Analysis (BIA) and a skinfold based equation, with underwater densitometry to track body fat percentage relative to years from age at peak height velocity in adolescent sprint athletes. In this study, adolescent sprint athletes (34 girls, 35 boys) were measured every 6 months during 3 years (age at start = 14.8 \u00b1 1.5yrs in girls and 14.7 \u00b1 1.9yrs in boys). Body fat percentage was estimated in 3 different ways: 1) using BIA with the TANITA TBF 410; 2) using a skinfold based equation; 3) using underwater densitometry which was considered as the reference method. Height for age since birth was used to estimate age at peak height velocity. Cross-sectional analyses were performed using repeated measures ANOVA and Pearson correlations between measurement methods at each occasion. Data were analyzed longitudinally using a multilevel cross-classified model with the PROC Mixed procedure. In boys, compared to underwater densitometry, the skinfold based formula revealed comparable values for body fatness during the study period whereas BIA showed a different pattern leading to an overestimation of body fatness starting from 4 years after age at peak height velocity. In girls, both the skinfold based formula and BIA overestimated body fatness across the whole range of years from peak height velocity. The skinfold based method appears to give an acceptable estimation of body composition during growth as compared to underwater densitometry in male adolescent sprinters. In girls, caution is warranted when interpreting estimations of body fatness by both BIA and a skinfold based formula since both methods tend to give an overestimation.\n"], "author_display": ["Dirk Aerenhouts", "Peter Clarys", "Jan Taeymans", "Jelle Van Cauwenberg"], "article_type": "Research Article", "score": 0.4376951, "title_display": "Estimating Body Composition in Adolescent Sprint Athletes: Comparison of Different Methods in a 3 Years Longitudinal Design", "publication_date": "2015-08-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0136788"}, {"journal": "PLoS ONE", "abstract": ["\n        Copy number variations (CNVs), a common genomic mutation associated with various diseases, are important in research and clinical applications. Whole genome amplification (WGA) and massively parallel sequencing have been applied to single cell CNVs analysis, which provides new insight for the fields of biology and medicine. However, the WGA-induced bias significantly limits sensitivity and specificity for CNVs detection. Addressing these limitations, we developed a practical bioinformatic methodology for CNVs detection at the single cell level using low coverage massively parallel sequencing. This method consists of GC correction for WGA-induced bias removal, binary segmentation algorithm for locating CNVs breakpoints, and dynamic threshold determination for final signals filtering. Afterwards, we evaluated our method with seven test samples using low coverage sequencing (4\u223c9.5%). Four single-cell samples from peripheral blood, whose karyotypes were confirmed by whole genome sequencing analysis, were acquired. Three other test samples derived from blastocysts whose karyotypes were confirmed by SNP-array analysis were also recruited. The detection results for CNVs of larger than 1 Mb were highly consistent with confirmed results reaching 99.63% sensitivity and 97.71% specificity at base-pair level. Our study demonstrates the potential to overcome WGA-bias and to detect CNVs (>1 Mb) at the single cell level through low coverage massively parallel sequencing. It highlights the potential for CNVs research on single cells or limited DNA samples and may prove as a promising tool for research and clinical applications, such as pre-implantation genetic diagnosis/screening, fetal nucleated red blood cells research and cancer heterogeneity analysis.\n      "], "author_display": ["Chunlei Zhang", "Chunsheng Zhang", "Shengpei Chen", "Xuyang Yin", "Xiaoyu Pan", "Ge Lin", "Yueqiu Tan", "Ke Tan", "Zhengfeng Xu", "Ping Hu", "Xuchao Li", "Fang Chen", "Xun Xu", "Yingrui Li", "Xiuqing Zhang", "Hui Jiang", "Wei Wang"], "article_type": "Research Article", "score": 0.4375777, "title_display": "A Single Cell Level Based Method for Copy Number Variation Analysis by Low Coverage Massively Parallel Sequencing", "publication_date": "2013-01-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0054236"}, {"journal": "PLoS ONE", "abstract": ["Background: Hypermethylation of CpG islands in tumor suppressor gene plays an important role in carcinogenesis. Many studies have demonstrated that hypermethylation in promoter region of RAR\u03b2 gene could be found with high prevalence in tumor tissue and autologous controls such as corresponding non-tumor lung tissue, sputum and plasma of the NSCLC patients. But with the small number subjects included in the individual studie, the statistical power is limited. Accordingly, we performed this meta-analysis to further asses the relationship of methylation prevalence between the cancer tissue and atuologous controls (corresponding non-tumor lung tissue, sputum and plasma). Methods: The published articles about RAR\u03b2 gene promoter hypermethyltion were identified using a systematic search strategy in PubMed, EMBASE and CNKI databases. The pooled odds ratio (OR) of RAR\u03b2 promoter methylation in lung cancer tissue versus autologous controls were calculated. Results: Finally, eleven articles, including 1347 tumor tissue samples and 1137 autologous controls were included in this meta-analysis. The pooled odds ratio of RAR\u03b2 promoter methylation in cancer tissue was 3.60 (95%CI: 2.46\u20135.27) compared to autologous controls with random-effect model. Strong and significant correlation between tumor tissue and autologous controls of RAR\u03b2 gene promoter hypermethylation prevalence across studies (Correlation coefficient 0.53) was found. Conclusion: RAR\u03b2 promoter methylation may play an important role in carcinogenesis of the NSCLC. With significant methylation prevalence correlation between tumor tissue and autologous of this gene, methylation detection may be a potential method for searching biomarker for NSCLC. "], "author_display": ["Feng Hua", "Nianzhen Fang", "Xuebing Li", "Siwei Zhu", "Weisan Zhang", "Jundong Gu"], "article_type": "Research Article", "score": 0.4374234, "title_display": "A Meta-Analysis of the Relationship Between <i>RAR\u03b2</i> Gene Promoter Methylation and Non-Small Cell Lung Cancer", "publication_date": "2014-05-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0096163"}, {"journal": "PLoS ONE", "abstract": [": Couplings between uterine contractions (UC) and fetal heart rate (fHR) provide important information on fetal condition during labor. At present, couplings between UC and fHR are assessed by visual analysis and interpretation of cardiotocography. The application of computerized approaches is restricted due to the non-stationarity of the signal, missing data and noise, typical for fHR. Herein, we propose a novel approach to assess couplings between UC and fHR, based on a signal-processing algorithm termed bivariate phase-rectified signal averaging (BPRSA). Methods: Electrohysterogram (EHG) and fetal electrocardiogram (fECG) were recorded non-invasively by a trans-abdominal device in 73 women at term with uneventful singleton pregnancy during the first stage of labor. Coupling between UC and fHR was analyzed by BPRSA and by conventional cross power spectral density analysis (CPSD). For both methods, degree of coupling was assessed by the maximum coefficient of coherence (CPRSA and CRAW, respectively) in the UC frequency domain. Coherence values greater than 0.50 were consider significant. CPRSA and CRAW were compared by Wilcoxon test. Results: At visual inspection BPRSA analysis identified coupled periodicities in 86.3% (63/73) of the cases. 11/73 (15%) cases were excluded from further analysis because no 30 minutes of fECG recording without signal loss was available for spectral analysis. Significant coupling was found in 90.3% (56/62) of the cases analyzed by BPRSA, and in 24.2% (15/62) of the cases analyzed by CPSD, respectively. The difference between median value of CPRSA and CRAW was highly significant (0.79 [IQR 0.69\u20130.90] and 0.29 [IQR 0.17\u20130.47], respectively; p<0.0001). Conclusion: BPRSA is a novel computer-based approach that can be reliably applied to trans-abdominally acquired EHG-fECG. It allows the assessment of correlations between UC and fHR patterns in the majority of labors, overcoming the limitations of non-stationarity and artifacts. Compared to standard techniques of cross-correlations, such as CPSD, BPRSA is significantly superior. "], "author_display": ["Daniela Casati", "Tamara Stampalija", "Konstantinos Rizas", "Enrico Ferrazzi", "Cristina Mastroianni", "Eleonora Rosti", "Mariachiara Quadrifoglio", "Axel Bauer"], "article_type": "Research Article", "score": 0.4371692, "title_display": "Assessment of Coupling between Trans-Abdominally Acquired Fetal ECG and Uterine Activity by Bivariate Phase-Rectified Signal Averaging Analysis", "publication_date": "2014-04-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0094557"}, {"journal": "PLoS ONE", "abstract": ["\nAlthough accelerometers are extensively used for assessing gait, limited research has evaluated the concurrent validity of these devices on less predictable walking surfaces or the comparability of different methods used for gravitational acceleration compensation. This study evaluated the concurrent validity of trunk accelerations derived from a tri-axial inertial measurement unit while walking on firm, compliant and uneven surfaces and contrasted two methods used to remove gravitational accelerations; i) subtraction of the best linear fit from the data (detrending); and ii) use of orientation information (quaternions) from the inertial measurement unit. Twelve older and twelve younger adults walked at their preferred speed along firm, compliant and uneven walkways. Accelerations were evaluated for the thoracic spine (T12) using a tri-axial inertial measurement unit and an eleven-camera Vicon system. The findings demonstrated excellent agreement between accelerations derived from the inertial measurement unit and motion analysis system, including while walking on uneven surfaces that better approximate a real-world setting (all differences <0.16 m.s\u22122). Detrending produced slightly better agreement between the inertial measurement unit and Vicon system on firm surfaces (delta range: \u22120.05 to 0.06 vs. 0.00 to 0.14 m.s\u22122), whereas the quaternion method performed better when walking on compliant and uneven walkways (delta range: \u22120.16 to \u22120.02 vs. \u22120.07 to 0.07 m.s\u22122). The technique used to compensate for gravitational accelerations requires consideration in future research, particularly when walking on compliant and uneven surfaces. These findings demonstrate trunk accelerations can be accurately measured using a wireless inertial measurement unit and are appropriate for research that evaluates healthy populations in complex environments.\n"], "author_display": ["Michael H. Cole", "Wolbert van den Hoorn", "Justin K. Kavanagh", "Steven Morrison", "Paul W. Hodges", "James E. Smeathers", "Graham K. Kerr"], "article_type": "Research Article", "score": 0.4369288, "title_display": "Concurrent Validity of Accelerations Measured Using a Tri-Axial Inertial Measurement Unit while Walking on Firm, Compliant and Uneven Surfaces", "publication_date": "2014-05-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0098395"}, {"journal": "PLoS ONE", "abstract": ["Background: This study aims to describe the specific characteristics of completed suicides by violent methods and non-violent methods in rural Chinese population, and to explore the related factors for corresponding methods. Methods: Data of this study came from investigation of 199 completed suicide cases and their paired controls of rural areas in three different counties in Shandong, China, by interviewing one informant of each subject using the method of Psychological Autopsy (PA). Results: There were 78 (39.2%) suicides with violent methods and 121 (60.8%) suicides with non-violent methods. Ingesting pesticides, as a non-violent method, appeared to be the most common suicide method (103, 51.8%). Hanging (73 cases, 36.7%) and drowning (5 cases, 2.5%) were the only violent methods observed. Storage of pesticides at home and higher suicide intent score were significantly associated with choice of violent methods while committing suicide. Risk factors related to suicide death included negative life events and hopelessness. Conclusions: Suicide with violent methods has different factors from suicide with non-violent methods. Suicide methods should be considered in suicide prevention and intervention strategies. "], "author_display": ["Shi-Hua Sun", "Cun-Xian Jia"], "article_type": "Research Article", "score": 0.4368699, "title_display": "Completed Suicide with Violent and Non-Violent Methods in Rural Shandong, China: A Psychological Autopsy Study", "publication_date": "2014-08-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0104333"}, {"journal": "PLoS ONE", "abstract": ["\nGenome-wide association study (GWAS) is a promising approach for identifying common genetic variants of the diseases on the basis of millions of single nucleotide polymorphisms (SNPs). In order to avoid low power caused by overmuch correction for multiple comparisons in single locus association study, some methods have been proposed by grouping SNPs together into a SNP set based on genomic features, then testing the joint effect of the SNP set. We compare the performances of principal component analysis (PCA), supervised principal component analysis (SPCA), kernel principal component analysis (KPCA), and sliced inverse regression (SIR). Simulated SNP sets are generated under scenarios of 0, 1 and \u22652 causal SNPs model. Our simulation results show that all of these methods can control the type I error at the nominal significance level. SPCA is always more powerful than the other methods at different settings of linkage disequilibrium structures and minor allele frequency of the simulated datasets. We also apply these four methods to a real GWAS of non-small cell lung cancer (NSCLC) in Han Chinese population\n"], "author_display": ["Min Cai", "Hui Dai", "Yongyong Qiu", "Yang Zhao", "Ruyang Zhang", "Minjie Chu", "Juncheng Dai", "Zhibin Hu", "Hongbing Shen", "Feng Chen"], "article_type": "Research Article", "score": 0.43684125, "title_display": "SNP Set Association Analysis for Genome-Wide Association Studies", "publication_date": "2013-05-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0062495"}, {"journal": "PLoS ONE", "abstract": ["\nA new method based on image matching and frame coupling to handle the problems of object detection caused by a moving camera and object motion is presented in this paper. First, feature points are extracted from each frame. Then, motion parameters can be obtained. Sub-images are extracted from the corresponding frame via these motion parameters. Furthermore, a novel searching method for potential orientations improves efficiency and accuracy. Finally, a method based on frame coupling is adopted, which improves the accuracy of object detection. The results demonstrate the effectiveness and feasibility of our proposed method for a moving object with changing posture and with a moving camera.\n"], "author_display": ["Yong Chen", "Rong hua Zhang", "Lei Shang"], "article_type": "Research Article", "score": 0.4364069, "title_display": "A Novel Method of Object Detection from a Moving Camera Based on Image Matching and Frame Coupling", "publication_date": "2014-10-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0109809"}, {"journal": "PLoS ONE", "abstract": ["\n        Accurate classification of HIV-1 subtypes is essential for studying the dynamic spatial distribution pattern of HIV-1 subtypes and also for developing effective methods of treatment that can be targeted to attack specific subtypes. We propose a classification method based on profile Hidden Markov Model that can accurately identify an unknown strain. We show that a standard method that relies on the construction of a positive training set only, to capture unique features associated with a particular subtype, can accurately classify sequences belonging to all subtypes except B and D. We point out the drawbacks of the standard method; namely, an arbitrary choice of threshold to distinguish between true positives and true negatives, and the inability to discriminate between closely related subtypes. We then propose an improved classification method based on construction of a positive as well as a negative training set to improve discriminating ability between closely related subtypes like B and D. Finally, we show how the improved method can be used to accurately determine the subtype composition of Common Recombinant Forms of the virus that are made up of two or more subtypes. Our method provides a simple and highly accurate alternative to other classification methods and will be useful in accurately annotating newly sequenced HIV-1 strains.\n      "], "author_display": ["Sanjiv K. Dwivedi", "Supratim Sengupta"], "article_type": "Research Article", "score": 0.436369, "title_display": "Classification of HIV-1 Sequences Using Profile Hidden Markov Models", "publication_date": "2012-05-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0036566"}, {"journal": "PLoS ONE", "abstract": ["\n         Assessing conservation/divergence of gene expression across species is important for the understanding of gene regulation evolution. Although advances in microarray technology have provided massive high-dimensional gene expression data, the analysis of such data is still challenging. To date, assessing cross-species conservation of gene expression using microarray data has been mainly based on comparison of expression patterns across corresponding tissues, or comparison of co-expression of a gene with a reference set of genes. Because direct and reliable high-throughput experimental data on conservation of gene expression are often unavailable, the assessment of these two computational models is very challenging and has not been reported yet. In this study, we compared one corresponding tissue based method and three co-expression based methods for assessing conservation of gene expression, in terms of their pair-wise agreements, using a frequently used human-mouse tissue expression dataset. We find that 1) the co-expression based methods are only moderately correlated with the corresponding tissue based methods, 2) the reliability of co-expression based methods is affected by the size of the reference ortholog set, and 3) the corresponding tissue based methods may lose some information for assessing conservation of gene expression. We suggest that the use of either of these two computational models to study the evolution of a gene's expression may be subject to great uncertainty, and the investigation of changes in both gene expression patterns over corresponding tissues and co-expression of the gene with other genes is necessary.\n      "], "author_display": ["Yupeng Wang", "Kelly R. Robbins", "Romdhane Rekaya"], "article_type": "Research Article", "score": 0.43620768, "title_display": "Comparison of Computational Models for Assessing Conservation of Gene Expression across Species", "publication_date": "2010-10-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0013239"}, {"journal": "PLOS ONE", "abstract": ["Background: Non-ionizing radiation imaging assessment has been advocated for the patients with adolescent idiopathic scoliosis (AIS). As one of the radiation-free methods, ultrasound imaging has gained growing attention in scoliosis assessment over the past decade. The center of laminae (COL) method has been proposed to measure the spinal curvature in the coronal plane of ultrasound image. However, the reliability and validity of this ultrasound method have not been validated in the clinical setting. Objectives: To evaluate the reliability and validity of clinical ultrasound imaging on lateral curvature measurements of AIS with their corresponding magnetic resonance imaging (MRI) measurements. Methods: Thirty curves (ranged 10.2\u00b0\u201368.2\u00b0) from sixteen patients with AIS were eligible for this study. The ultrasound scan was performed using a 3-D ultrasound unit within the same morning of MRI examination. Two researchers were involved in data collection of these two examinations. The COL method was used to measure the coronal curvature in ultrasound image, compared with the Cobb method in MRI. The intra- and inter-rater reliability of the COL method was evaluated by intra-class correlation coefficient (ICC). The validity of this method was analyzed by paired Student\u2019s t-test, Bland\u2013Altman statistics and Pearson correlation coefficient. The level of significance was set as 0.05. Results: The COL method showed high intra- and inter-rater reliabilities (both with ICC (2, K) >0.9, p<0.05) to measure the coronal curvature. Compared with Cobb method, COL method showed no significant difference (p<0.05) when measuring coronal curvature. Furthermore, Bland-Altman method demonstrated an agreement between these two methods, and Pearson\u2019s correlation coefficient (r) was high (r>0.9, p<0.05). Conclusion: The ultrasound imaging could provide a reliable and valid measurement of spinal curvature in the coronal plane using the COL method. Further research is needed to validate the proposed ultrasound measurement in larger clinical trial and to optimize the ultrasound scanning and measuring procedure. "], "author_display": ["Q. Wang", "M. Li", "Edmond H. M. Lou", "M. S. Wong"], "article_type": "Research Article", "score": 0.4361809, "title_display": "Reliability and Validity Study of Clinical Ultrasound Imaging on Lateral Curvature of Adolescent Idiopathic Scoliosis", "publication_date": "2015-08-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0135264"}, {"journal": "PLoS ONE", "abstract": ["Objective: To investigate the feasibility, costs and sample representativeness of a recruitment method that used workers with back injuries as the point of entry into diverse working environments. Methods: Workers' compensation claims were used to randomly sample workers from five heavy industries and to recruit their employers for ergonomic assessments of the injured worker and up to 2 co-workers. Results: The final study sample included 54 workers from the workers\u2019 compensation registry and 72 co-workers. This sample of 126 workers was based on an initial random sample of 822 workers with a compensation claim, or a ratio of 1 recruited worker to approximately 7 sampled workers. The average recruitment cost was CND$262/injured worker and CND$240/participating worksite including co-workers. The sample was representative of the heavy industry workforce, and was successful in recruiting the self-employed (8.2%), workers from small employers (<20 workers, 38.7%), and workers from diverse working environments (49 worksites, 29 worksite types, and 51 occupations). Conclusions: The recruitment rate was low but the cost per participant reasonable and the sample representative of workers in small worksites. Small worksites represent a significant portion of the workforce but are typically underrepresented in occupational research despite having distinct working conditions, exposures and health risks worthy of investigation. "], "author_display": ["Mieke Koehoorn", "Catherine M. Trask", "Kay Teschke"], "article_type": "Research Article", "score": 0.43596736, "title_display": "Recruitment for Occupational Research: Using Injured Workers as the Point of Entry into Workplaces", "publication_date": "2013-06-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0068354"}, {"journal": "PLOS ONE", "abstract": ["Background: There is an increasing need to evaluate the production and impact of medical research produced by institutions. Many indicators exist, yet we do not have enough information about their relevance. The objective of this systematic review was (1) to identify all the indicators that could be used to measure the output and outcome of medical research carried out in institutions and (2) enlist their methodology, use, positive and negative points. Methodology: We have searched 3 databases (Pubmed, Scopus, Web of Science) using the following keywords: [Research outcome* OR research output* OR bibliometric* OR scientometric* OR scientific production] AND [indicator* OR index* OR evaluation OR metrics]. We included articles presenting, discussing or evaluating indicators measuring the scientific production of an institution. The search was conducted by two independent authors using a standardised data extraction form. For each indicator we extracted its definition, calculation, its rationale and its positive and negative points. In order to reduce bias, data extraction and analysis was performed by two independent authors. Findings: We included 76 articles. A total of 57 indicators were identified. We have classified those indicators into 6 categories: 9 indicators of research activity, 24 indicators of scientific production and impact, 5 indicators of collaboration, 7 indicators of industrial production, 4 indicators of dissemination, 8 indicators of health service impact. The most widely discussed and described is the h-index with 31 articles discussing it. Discussion: The majority of indicators found are bibliometric indicators of scientific production and impact. Several indicators have been developed to improve the h-index. This indicator has also inspired the creation of two indicators to measure industrial production and collaboration. Several articles propose indicators measuring research impact without detailing a methodology for calculating them. Many bibliometric indicators identified have been created but have not been used or further discussed. "], "author_display": ["Fr\u00e9d\u00e9rique Thonon", "Rym Boulkedid", "Tristan Delory", "Sophie Rousseau", "Mahasti Saghatchian", "Wim van Harten", "Claire O\u2019Neill", "Corinne Alberti"], "article_type": "Research Article", "score": 0.43521062, "title_display": "Measuring the Outcome of Biomedical Research: A Systematic Literature Review", "publication_date": "2015-04-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0122239"}, {"journal": "PLoS ONE", "abstract": ["Background: Brazil has implemented systematic control methods for leishmaniasis for the past 30 years, despite an increase in cases and continued spread of the disease to new regions. A lack high quality evidence from epidemiological observational studies impedes the development of novel control methods to prevent disease transmission among the population. Here, we have evaluated the quality of observational studies on leishmaniasis conducted in Brazil to highlight this issue. Methods/Principal Findings: For this systematic review, all publications on leishmaniasis conducted in Brazil from January 1st, 2002 to December 31st, 2012 were screened via Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) checklist to select observational studies involving human subjects. The 283 included studies, representing only 14.1% of articles screened, were then further evaluated for quality of epidemiological methods and study design based on the STROBE (Strengthening the Reporting of Observational studies in Epidemiology) checklists. Over half of these studies were descriptive or case reports (53.4%, 151), followed by cross-sectional (20.8%, n\u200a=\u200a59), case-control (8.5%, n\u200a=\u200a24), and cohort (6.0%, n\u200a=\u200a17). Study design was not stated in 46.6% (n\u200a=\u200a181) and incorrectly stated in 17.5% (n\u200a=\u200a24). Comparison groups were utilized in just 39.6% (n\u200a=\u200a112) of the publications, and only 13.4% (n\u200a=\u200a38) employed healthy controls. Majority of studies were performed at the city-level (62.9%, n\u200a=\u200a178), in contrast with two (0.7%) studies performed at the national-level. Coauthorship networks showed the number of author collaborations rapidly decreased after three collaborations, with 70.9% (n\u200a=\u200a659/929) of coauthors publishing only one article during the study period. Conclusions/Significance: A review of epidemiological research in Brazil revealed a major lack of quality and evidence. While certain indicators suggested research methods may have improved in the last two years, an emphasis on observational research which employs comparison groups and representative samples is urgently needed. "], "author_display": ["Bruno Trentini", "M\u00e1rio Steindel", "Mariel A. Marlow"], "article_type": "Research Article", "score": 0.4345722, "title_display": "Low Quality Evidence of Epidemiological Observational Studies on Leishmaniasis in Brazil", "publication_date": "2014-09-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0106635"}, {"journal": "PLoS ONE", "abstract": ["Objective: To compare expert assessment with bibliometric indicators as tools to assess the quality and importance of scientific research papers. Methods and Materials: Shortly after their publication in 2005, the quality and importance of a cohort of nearly 700 Wellcome Trust (WT) associated research papers were assessed by expert reviewers; each paper was reviewed by two WT expert reviewers. After 3 years, we compared this initial assessment with other measures of paper impact. Results: Shortly after publication, 62 (9%) of the 687 research papers were determined to describe at least a \u2018major addition to knowledge\u2019 \u20136 were thought to be \u2018landmark\u2019 papers. At an aggregate level, after 3 years, there was a strong positive association between expert assessment and impact as measured by number of citations and F1000 rating. However, there were some important exceptions indicating that bibliometric measures may not be sufficient in isolation as measures of research quality and importance, and especially not for assessing single papers or small groups of research publications. Conclusion: When attempting to assess the quality and importance of research papers, we found that sole reliance on bibliometric indicators would have led us to miss papers containing important results as judged by expert review. In particular, some papers that were highly rated by experts were not highly cited during the first three years after publication. Tools that link expert peer reviews of research paper quality and importance to more quantitative indicators, such as citation analysis would be valuable additions to the field of research assessment and evaluation. "], "author_display": ["Liz Allen", "Ceri Jones", "Kevin Dolby", "David Lynn", "Mark Walport"], "article_type": "Research Article", "score": 0.43434238, "title_display": "Looking for Landmarks: The Role of Expert Review and Bibliometric Analysis in Evaluating Scientific Publication Outputs", "publication_date": "2009-06-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0005910"}, {"journal": "PLoS ONE", "abstract": ["Background: A number of studies have investigated the association between Helicobacter pylori (H. pylori) infection and the prognosis of gastric cancer (GC), with inconsistent and inconclusive results. We performed a meta-analysis to derive a more precise estimation of the association. Methodology/Principal Findings: A systematic search of PubMed, EMBASE, Cochrane and Chinese wanfang databases was performed with the last search updated on February 19, 2013. The hazard ratio (HR) and its 95% confidence interval (95%CI) were used to assess the strength of association. A total of 12 studies including 2454 patients with GC were involved in this meta-analysis. The pooled HR was 0.71 (95%CI: 0.57\u20130.87; P\u200a=\u200a0.001) for OS and 0.60 (95%CI: 0.30\u20131.18; P\u200a=\u200a0.139) for DFS in GC patients, respectively. The protective role of H. pylori infection in the prognosis of GC was also observed among different subgroups stratified by ethnicity, statistical methodology, H. pylori evaluation method and quality assessment. There was no evidence of publication bias. Conclusions/Significance: This meta-analysis suggests a protective role for H. pylori infection in the prognosis of GC. The underlying mechanisms need to be further elucidated, which could provide new therapeutic approaches for GC. "], "author_display": ["Fang Wang", "Guoping Sun", "Yanfeng Zou", "Fei Zhong", "Tai Ma", "Xiaoqiu Li"], "article_type": "Research Article", "score": 0.4342572, "title_display": "Protective Role of <i>Helicobacter pylori</i> Infection in Prognosis of Gastric Cancer: Evidence from 2454 Patients with Gastric Cancer", "publication_date": "2013-05-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0062440"}, {"journal": "PLoS ONE", "abstract": ["\nListeria monocytogenes is the causative bacteria of listeriosis, which has a higher mortality rate than that of other causes of food poisoning. Listeria spp., of which L. monocytogenes is a member, have been isolated from food and manufacturing environments. Several methods have been published for identifying Listeria spp.; however, many of the methods cannot identify newly categorized Listeria spp. Additionally, they are often not suitable for the food industry, owing to their complexity, cost, or time consumption. Recently, high-resolution melting analysis (HRMA), which exploits DNA-sequence differences, has received attention as a simple and quick genomic typing method. In the present study, a new method for the simple, rapid, and low-cost identification of Listeria spp. has been presented using the genes rarA and ldh as targets for HRMA. DNA sequences of 9 Listeria species were first compared, and polymorphisms were identified for each species for primer design. Species specificity of each HRM curve pattern was estimated using type strains of all the species. Among the 9 species, 7 were identified by HRMA using rarA gene, including 3 new species. The remaining 2 species were identified by HRMA of ldh gene. The newly developed HRMA method was then used to assess Listeria isolates from the food industry, and the method efficiency was compared to that of identification by 16S rDNA sequence analysis. The 2 methods were in coherence for 92.6% of the samples, demonstrating the high accuracy of HRMA. The time required for identifying Listeria spp. was substantially low, and the process was considerably simplified, providing a useful and precise method for processing multiple samples per day. Our newly developed method for identifying Listeria spp. is highly valuable; its use is not limited to the food industry, and it can be used for the isolates from the natural environment.\n"], "author_display": ["Chihiro Ohshima", "Hajime Takahashi", "Chirapiphat Phraephaisarn", "Mongkol Vesaratchavest", "Suwimon Keeratipibul", "Takashi Kuda", "Bon Kimura"], "article_type": "Research Article", "score": 0.43401042, "title_display": "Establishment of a Simple and Rapid Identification Method for <i>Listeria</i> spp. by Using High-Resolution Melting Analysis, and Its Application in Food Industry", "publication_date": "2014-06-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0099223"}, {"journal": "PLOS ONE", "abstract": ["Purpose: To evaluate the intra- and inter-observer variability of the North American Symptomatic Carotid Endarterectomy Trial (NASCET) and Warfarin-Aspirin Symptomatic Intracranial Disease (WASID) criteria for the evaluation of middle cerebral artery (MCA) stenosis using digital subtraction angiography (DSA). Materials and Methods: DSA images of 114 cases with 131 stenotic MCAs were retrospectively analyzed. Two radiologists and a researcher measured the degree of MCA stenosis independently using both NASCET and WASID methods. To determine intra-observer agreement, all the observers reevaluated the degree of MCA stenosis 4 weeks later. The linear relation and coefficient of variation (CV) between the measurements made by the two methods were assessed by correlation coefficient and multi-factor analysis of variance (ANOVA), respectively. Intra- and inter-observer variability of the two methods was evaluated by intraclass correlation coefficient (ICC), Spearman\u2019s R value, Pearson correlation coefficient and Bland-Altman plots. Results: Despite the fact that the degree of MCA stenosis measured by NASCET was lower than measured using the WASID method, there was good linear correlation between the measurements made by the two methods (for the mean measurements of the 3 observers, NASCET% = 0.891 \u00d7 WASID% - 1.89%; ICC, Spearman\u2019s R value and Pearson correlation were 0.874, 0.855, and 0.874, respectively). The CVs of both intra- and inter-observer measurements of MCA stenosis using WASID were significantly lower than that using NASCET confirmed by the multi-factor ANOVA results, which showed only the measurement methods of MCA stenosis had significant effects on the CVs both in intra- and inter-observer measurements (both P values < 0.001). Intra-observer measurements showed good or excellent agreement with respect to WASID and NASCET evaluation (ICC, 0.656 to 0.817 and 0.635 to 0.761, respectively). Good agreement for the WASID evaluation (ICC, 0.592 to 0.628) and for the NASCET evaluation (ICC, 0.529 to 0.568) was observed for inter-observer measurements. Bland-Altman plots demonstrated that the WASID method had better reproducibility and intra-observer agreement than NASCET method for evaluating MCA stenosis. Conclusion: Both NASCET and WASID methods have an acceptable level of agreement; however, the WASID method had better reproducibility for the evaluation of MCA stenosis, and thus the WASID method may serve as a standard for measuring the degree of MCA stenosis. "], "author_display": ["Luguang Chen", "Qian Zhan", "Chao Ma", "Qi Liu", "Xuefeng Zhang", "Xia Tian", "Yuanliang Jiang", "Yinmei Dong", "Shiyue Chen", "Jianping Lu"], "article_type": "Research Article", "score": 0.4339167, "title_display": "Reproducibility of Middle Cerebral Artery Stenosis Measurements by DSA: Comparison of the NASCET and WASID Methods", "publication_date": "2015-06-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0130991"}, {"journal": "PLoS ONE", "abstract": ["\n        Recently emerged deep sequencing technologies offer new high-throughput methods to quantify gene expression, epigenetic modifications and DNA-protein binding. From a computational point of view, the data is very different from that produced by the already established microarray technology, providing a new perspective on the samples under study and complementing microarray gene expression data. Software offering the integrated analysis of data from different technologies is of growing importance as new data emerge in systems biology studies. Mayday is an extensible platform for visual data exploration and interactive analysis and provides many methods for dissecting complex transcriptome datasets. We present Mayday SeaSight, an extension that allows to integrate data from different platforms such as deep sequencing and microarrays. It offers methods for computing expression values from mapped reads and raw microarray data, background correction and normalization and linking microarray probes to genomic coordinates. It is now possible to use Mayday's wealth of methods to analyze sequencing data and to combine data from different technologies in one analysis.\n      "], "author_display": ["Florian Battke", "Kay Nieselt"], "article_type": "Research Article", "score": 0.43380567, "title_display": "Mayday SeaSight: Combined Analysis of Deep Sequencing and Microarray Data", "publication_date": "2011-01-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0016345"}, {"journal": "PLoS ONE", "abstract": ["\n        A large number of genome-wide association studies have been performed during the past five years to identify associations between SNPs and human complex diseases and traits. The assignment of a functional role for the identified disease-associated SNP is not straight-forward. Genome-wide expression quantitative trait locus (eQTL) analysis is frequently used as the initial step to define a function while allele-specific gene expression (ASE) analysis has not yet gained a wide-spread use in disease mapping studies. We compared the power to identify cis-acting regulatory SNPs (cis-rSNPs) by genome-wide allele-specific gene expression (ASE) analysis with that of traditional expression quantitative trait locus (eQTL) mapping. Our study included 395 healthy blood donors for whom global gene expression profiles in circulating monocytes were determined by Illumina BeadArrays. ASE was assessed in a subset of these monocytes from 188 donors by quantitative genotyping of mRNA using a genome-wide panel of SNP markers. The performance of the two methods for detecting cis-rSNPs was evaluated by comparing associations between SNP genotypes and gene expression levels in sample sets of varying size. We found that up to 8-fold more samples are required for eQTL mapping to reach the same statistical power as that obtained by ASE analysis for the same rSNPs. The performance of ASE is insensitive to SNPs with low minor allele frequencies and detects a larger number of significantly associated rSNPs using the same sample size as eQTL mapping. An unequivocal conclusion from our comparison is that ASE analysis is more sensitive for detecting cis-rSNPs than standard eQTL mapping. Our study shows the potential of ASE mapping in tissue samples and primary cells which are difficult to obtain in large numbers.\n      "], "author_display": ["Jonas Carlsson Alml\u00f6f", "Per Lundmark", "Anders Lundmark", "Bing Ge", "Seraya Maouche", "Harald H. H. G\u00f6ring", "Ulrika Liljedahl", "Camilla Enstr\u00f6m", "Jessy Brocheton", "Carole Proust", "Tiphaine Godefroy", "Jennifer G. Sambrook", "Jennifer Jolley", "Abigail Crisp-Hihn", "Nicola Foad", "Heather Lloyd-Jones", "Jonathan Stephens", "Rhian Gwilliam", "Catherine M. Rice", "Christian Hengstenberg", "Nilesh J. Samani", "Jeanette Erdmann", "Heribert Schunkert", "Tomi Pastinen", "Panos Deloukas", "Alison H. Goodall", "Willem H. Ouwehand", "Fran\u00e7ois Cambien", "Ann-Christine Syv\u00e4nen"], "article_type": "Research Article", "score": 0.43374974, "title_display": "Powerful Identification of <i>Cis</i>-regulatory SNPs in Human Primary Monocytes Using Allele-Specific Gene Expression", "publication_date": "2012-12-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0052260"}, {"journal": "PLoS ONE", "abstract": ["Background: Effective management depends upon accurately estimating trends in abundance of bird populations over time, and in some cases estimating abundance. Two population estimation methods, double observer (DO) and double sampling (DS), have been advocated for avian population studies and the relative merits and short-comings of these methods remain an area of debate. Methodology/Principal Findings: We used simulations to evaluate the performances of these two population estimation methods under a range of realistic scenarios. For three hypothetical populations with different levels of clustering, we generated DO and DS population size estimates for a range of detection probabilities and survey proportions. Population estimates for both methods were centered on the true population size for all levels of population clustering and survey proportions when detection probabilities were greater than 20%. The DO method underestimated the population at detection probabilities less than 30% whereas the DS method remained essentially unbiased. The coverage probability of 95% confidence intervals for population estimates was slightly less than the nominal level for the DS method but was substantially below the nominal level for the DO method at high detection probabilities. Differences in observer detection probabilities did not affect the accuracy and precision of population estimates of the DO method. Population estimates for the DS method remained unbiased as the proportion of units intensively surveyed changed, but the variance of the estimates decreased with increasing proportion intensively surveyed. Conclusions/Significance: The DO and DS methods can be applied in many different settings and our evaluations provide important information on the performance of these two methods that can assist researchers in selecting the method most appropriate for their particular needs. "], "author_display": ["Sandra L. Taylor", "Katherine S. Pollard"], "article_type": "Research Article", "score": 0.43368477, "title_display": "Evaluation of Two Methods to Estimate and Monitor Bird Populations", "publication_date": "2008-08-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0003047"}, {"journal": "PLoS ONE", "abstract": ["Objectives: As computing technology and image analysis techniques have advanced, the practice of histology has grown from a purely qualitative method to one that is highly quantified. Current image analysis software is imprecise and prone to wide variation due to common artifacts and histological limitations. In order to minimize the impact of these artifacts, a more robust method for quantitative image analysis is required. Methods and Results: Here we present a novel image analysis software, based on the hue saturation value color space, to be applied to a wide variety of histological stains and tissue types. By using hue, saturation, and value variables instead of the more common red, green, and blue variables, our software offers some distinct advantages over other commercially available programs. We tested the program by analyzing several common histological stains, performed on tissue sections that ranged from 4 \u00b5m to 10 \u00b5m in thickness, using both a red green blue color space and a hue saturation value color space. Conclusion: We demonstrated that our new software is a simple method for quantitative analysis of histological sections, which is highly robust to variations in section thickness, sectioning artifacts, and stain quality, eliminating sample-to-sample variation. "], "author_display": ["Katsumi Yabusaki", "Tyler Faits", "Eri McMullen", "Jose Luiz Figueiredo", "Masanori Aikawa", "Elena Aikawa"], "article_type": "Research Article", "score": 0.43263078, "title_display": "A Novel Quantitative Approach for Eliminating Sample-To-Sample Variation Using a Hue Saturation Value Analysis Program", "publication_date": "2014-03-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0089627"}, {"journal": "PLOS ONE", "abstract": ["Objectives: To establish the views of research volunteers on the consent process; to explore their views on the consent process in different research scenarios; to inform debate on emerging models of consent for participation in research. Design, Setting and Participants: 2,308 adult volunteers from the TwinsUK Registry (www.twinsuk.ac.uk) completed an online survey about their views on the consent process for use of their DNA and medical information in research. Their views on the re-consenting process in different scenarios were assessed. Results: The majority of volunteers preferred to be informed of the identity of the main researcher of a study in which they are participating, which is contrary to current practice. Over 80% were willing to complete the consent process online instead of face to face. On the whole, respondents did not view their DNA differently from their medical information with regard to the consent process. Research participants were more willing to give broad consent to cover future research if their DNA was to be used by the original researcher than by another researcher, even if the disease under investigation varied, in contrast to the traditional \u2018gold standard\u2019 whereby specific consent is required for all new research projects. Discussion: In some scenarios, research participants reported that they would be comfortable with not signing a new consent form for future research uses of their data and DNA, and are comfortable with secure, online consent processes rather than traditional face-to-face consent processes. Our findings indicate that the perceived relationship between research participants and researchers plays an important role in shaping preferences regarding the consent process and suggest that this relationship is not captured by traditional consent processes. We argue that the development of new formats of consent should be informed by empirical research on volunteers\u2019 perceptions and preferences regarding the consent process. "], "author_display": ["Susan E. Kelly", "Timothy D. Spector", "Lynn F. Cherkas", "Barbara Prainsack", "Juliette M. Harris"], "article_type": "Research Article", "score": 0.43244034, "title_display": "Evaluating the Consent Preferences of UK Research Volunteers for Genetic and Clinical Studies", "publication_date": "2015-03-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0118027"}, {"journal": "PLoS ONE", "abstract": ["Objectives: Scientific knowledge can help develop interventions that improve public health. The objectives of this review are (1) to describe the status of research on knowledge transfer strategies in the field of complex social interventions in public health and (2) to identify priorities for future research in this field. Method: A scoping study is an exploratory study. After searching databases of bibliographic references and specialized periodicals, we summarized the relevant studies using a predetermined assessment framework. In-depth analysis focused on the following items: types of knowledge transfer strategies, fields of public health, types of publics, types of utilization, and types of research specifications. Results: From the 1,374 references identified, we selected 26 studies. The strategies targeted mostly administrators of organizations and practitioners. The articles generally dealt with instrumental utilization and most often used qualitative methods. In general, the bias risk for the studies is high. Conclusion: Researchers need to consider the methodological challenges in this field of research in order to improve assessment of more complex knowledge transfer strategies (when they exist), not just diffusion/dissemination strategies and conceptual and persuasive utilization. "], "author_display": ["Christian Dagenais", "Marie Malo", "\u00c9milie Robert", "Mathieu Ouimet", "Diane Berthelette", "Val\u00e9ry Ridde"], "article_type": "Research Article", "score": 0.43197486, "title_display": "Knowledge Transfer on Complex Social Interventions in Public Health: A Scoping Study", "publication_date": "2013-12-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0080233"}, {"journal": "PLoS ONE", "abstract": ["\n        Many spatial interpolation methods perform well for gentle terrains when producing spatially continuous surfaces based on ground point data. However, few interpolation methods perform satisfactorily for complex terrains. Our objective in the present study was to analyze the suitability of several popular interpolation methods for complex terrains and propose an optimal method. A data set of 153 soil water profiles (1 m) from the semiarid hilly gully Loess Plateau of China was used, generated under a wide range of land use types, vegetation types and topographic positions. Four spatial interpolation methods, including ordinary kriging, inverse distance weighting, linear regression and regression kriging were used for modeling, randomly partitioning the data set into 2/3 for model fit and 1/3 for independent testing. The performance of each method was assessed quantitatively in terms of mean-absolute-percentage-error, root-mean-square-error, and goodness-of-prediction statistic. The results showed that the prediction accuracy differed significantly between each method in complex terrain. The ordinary kriging and inverse distance weighted methods performed poorly due to the poor spatial autocorrelation of soil moisture at small catchment scale with complex terrain, where the environmental impact factors were discontinuous in space. The linear regression model was much more suitable to the complex terrain than the former two distance-based methods, but the predicted soil moisture changed too sharply near the boundary of the land use types and junction of the sunny (southern) and shady (northern) slopes, which was inconsistent with reality because soil moisture should change gradually in short distance due to its mobility in soil. The most optimal interpolation method in this study for the complex terrain was the hybrid regression kriging, which produced a detailed, reasonable prediction map with better accuracy and prediction effectiveness.\n      "], "author_display": ["Xueling Yao", "Bojie Fu", "Yihe L\u00fc", "Feixiang Sun", "Shuai Wang", "Min Liu"], "article_type": "Research Article", "score": 0.43188387, "title_display": "Comparison of Four Spatial Interpolation Methods for Estimating Soil Moisture in a Complex Terrain Catchment", "publication_date": "2013-01-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0054660"}, {"journal": "PLoS ONE", "abstract": ["\n        The retinal vessel width relationship at vessel branch points in fundus images is an important biomarker of retinal and systemic disease. We propose a fully automatic method to measure the vessel widths at branch points in fundus images. The method is a graph-based method, in which a graph construction method based on electric field theory is applied which specifically deals with complex branching patterns. The vessel centerline image is used as the initial segmentation of the graph. Branching points are detected on the vessel centerline image using a set of detection kernels. Crossing points are distinguished from branch points and excluded. The electric field based graph method is applied to construct the graph. This method is inspired by the non-intersecting force lines in an electric field. At last, the method is further improved to give a consistent vessel width measurement for the whole vessel tree. The algorithm was validated on 100 artery branchings and 100 vein branchings selected from 50 fundus images by comparing with vessel width measurements from two human experts.\n      "], "author_display": ["Xiayu Xu", "Joseph M. Reinhardt", "Qiao Hu", "Benjamin Bakall", "Paul S. Tlucek", "Geir Bertelsen", "Michael D. Abr\u00e0moff"], "article_type": "Research Article", "score": 0.4318245, "title_display": "Retinal Vessel Width Measurement at Branchings Using an Improved Electric Field Theory-Based Graph Approach", "publication_date": "2012-11-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0049668"}, {"journal": "PLoS ONE", "abstract": ["\nIt is thought that the science of ecology has experienced conceptual shifts in recent decades, chiefly from viewing nature as static and balanced to a conception of constantly changing, unpredictable, complex ecosystems. Here, we ask if these changes are reflected in actual ecological research over the last 30 years. We surveyed 750 articles from the entire pool of ecological literature and 750 articles from eight leading journals. Each article was characterized according to its type, ecological domain, and applicability, and major topics. We found that, in contrast to its common image, ecology is still mostly a study of single species (70% of the studies); while ecosystem and community studies together comprise only a quarter of ecological research. Ecological science is somewhat conservative in its topics of research (about a third of all topics changed significantly through time), as well as in its basic methodologies and approaches. However, the growing proportion of problem-solving studies (from 9% in the 1980s to 20% in the 2000 s) may represent a major transition in ecological science in the long run.\n"], "author_display": ["Yohay Carmel", "Rafi Kent", "Avi Bar-Massada", "Lior Blank", "Jonathan Liberzon", "Oded Nezer", "Gill Sapir", "Roy Federman"], "article_type": "Research Article", "score": 0.4316219, "title_display": "Trends in Ecological Research during the Last Three Decades \u2013 A Systematic Review", "publication_date": "2013-04-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0059813"}, {"journal": "PLOS ONE", "abstract": ["Background: The World Health Organization recommends use of multiple approaches to control malaria. The integrated approach to malaria prevention advocates the use of several malaria prevention methods in a holistic manner. This study assessed perceptions and practices on integrated malaria prevention in Wakiso district, Uganda. Methods: A clustered cross-sectional survey was conducted among 727 households from 29 villages using both quantitative and qualitative methods. Assessment was done on awareness of various malaria prevention methods, potential for use of the methods in a holistic manner, and reasons for dislike of certain methods. Households were classified as using integrated malaria prevention if they used at least two methods. Logistic regression was used to test for factors associated with the use of integrated malaria prevention while adjusting for clustering within villages. Results: Participants knew of the various malaria prevention methods in the integrated approach including use of insecticide treated nets (97.5%), removing mosquito breeding sites (89.1%), clearing overgrown vegetation near houses (97.9%), and closing windows and doors early in the evenings (96.4%). If trained, most participants (68.6%) would use all the suggested malaria prevention methods of the integrated approach. Among those who would not use all methods, the main reasons given were there being too many (70.2%) and cost (32.0%). Only 33.0% households were using the integrated approach to prevent malaria. Use of integrated malaria prevention by households was associated with reading newspapers (AOR 0.34; 95% CI 0.22 \u20130.53) and ownership of a motorcycle/car (AOR 1.75; 95% CI 1.03 \u2013 2.98). Conclusion: Although knowledge of malaria prevention methods was high and perceptions on the integrated approach promising, practices on integrated malaria prevention was relatively low. The use of the integrated approach can be improved by promoting use of multiple malaria prevention methods through various communication channels such as mass media. "], "author_display": ["David Musoke", "George Miiro", "George Karani", "Keith Morris", "Simon Kasasa", "Rawlance Ndejjo", "Jessica Nakiyingi-Miiro", "David Guwatudde", "Miph Boses Musoke"], "article_type": "Research Article", "score": 0.43162066, "title_display": "Promising Perceptions, Divergent Practices and Barriers to Integrated Malaria Prevention in Wakiso District, Uganda: A Mixed Methods Study", "publication_date": "2015-04-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0122699"}, {"journal": "PLoS ONE", "abstract": ["Background: We devised and implemented an innovative Location-Based Household Coding System (LBHCS) appropriate to a densely populated informal settlement in Mumbai, India. Methods and Findings: LBHCS codes were designed to double as unique household identifiers and as walking directions; when an entire community is enumerated, LBHCS codes can be used to identify the number of households located per road (or lane) segment. LBHCS was used in community-wide biometric, mental health, diarrheal disease, and water poverty studies. It also facilitated targeted health interventions by a research team of youth from Mumbai, including intensive door-to-door education of residents, targeted follow-up meetings, and a full census. In addition, LBHCS permitted rapid and low-cost preparation of GIS mapping of all households in the slum, and spatial summation and spatial analysis of survey data. Conclusion: LBHCS was an effective, easy-to-use, affordable approach to household enumeration and re-identification in a densely populated informal settlement where alternative satellite imagery and GPS technologies could not be used. "], "author_display": ["Dana R. Thomson", "Shrutika Shitole", "Tejal Shitole", "Kiran Sawant", "Ramnath Subbaraman", "David E. Bloom", "Anita Patil-Deshmukh"], "article_type": "Research Article", "score": 0.43069237, "title_display": "A System for Household Enumeration and Re-identification in Densely Populated Slums to Facilitate Community Research, Education, and Advocacy", "publication_date": "2014-04-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0093925"}, {"journal": "PLoS ONE", "abstract": ["Background: NKT cell is a population of unconventional T cells that mediate both innate and adaptive T cell responses. Since NKT cells are most abundant in the liver, much of NKT biology has been learnt from studies of NKT cells isolated from liver. This is a cumbersome procedure with variations in cell yield. Results: Based on recent evidence that NKT cells reside in liver vascular compartment, we developed a simple method to isolate NKT cells by perfusion with PBS-containing 10 mM of EDTA. The number and cell surface phenotype of liver NKT cells recovered by perfusion and by the traditional method were comparable. The yield of other lymphocytes was also comparable. Conclusion/Significance: Our data demonstrated that liver lymphocytes can be efficiently isolated by simple perfusion. These data provide a convenient method to isolate liver lymphocyte while preserving liver tissue for other analysis. "], "author_display": ["Xianfeng Fang", "Peishuang Du", "Yang Liu", "Jie Tang"], "article_type": "Research Article", "score": 0.43057242, "title_display": "Efficient Isolation of Mouse Liver NKT Cells by Perfusion", "publication_date": "2010-04-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0010288"}, {"abstract": ["Background: The composition vector (CV) method has been proved to be a reliable and fast alignment-free method to analyze large COI barcoding data. In this study, we modify this method for analyzing multi-gene datasets for plant DNA barcoding. The modified method includes an adjustable-weighted algorithm for the vector distance according to the ratio in sequence length of the candidate genes for each pair of taxa. Methodology/Principal Findings: Three datasets, matK+rbcL dataset with 2,083 sequences, matK+rbcL dataset with 397 sequences and matK+rbcL+trnH-psbA dataset with 397 sequences, were tested. We showed that the success rates of grouping sequences at the genus/species level based on this modified CV approach are always higher than those based on the traditional K2P/NJ method. For the matK+rbcL datasets, the modified CV approach outperformed the K2P-NJ approach by 7.9% in both the 2,083-sequence and 397-sequence datasets, and for the matK+rbcL+trnH-psbA dataset, the CV approach outperformed the traditional approach by 16.7%. Conclusions: We conclude that the modified CV approach is an efficient method for analyzing large multi-gene datasets for plant DNA barcoding. Source code, implemented in C++ and supported on MS Windows, is freely available for download at http://math.xtu.edu.cn/myphp/math/research/source/Barcode_source_codes.zip. "], "author_display": ["Chi Pang Li", "Zu Guo Yu", "Guo Sheng Han", "Ka Hou Chu"], "article_type": "Research Article", "score": 0.43036896, "title_display": "Analyzing Multi-locus Plant Barcoding Datasets with a Composition Vector Method Based on Adjustable Weighted Distance", "publication_date": "2012-07-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0042154"}, {"journal": "PLoS ONE", "abstract": ["\nMinimum squared error based classification (MSEC) method establishes a unique classification model for all the test samples. However, this classification model may be not optimal for each test sample. This paper proposes an improved MSEC (IMSEC) method, which is tailored for each test sample. The proposed method first roughly identifies the possible classes of the test sample, and then establishes a minimum squared error (MSE) model based on the training samples from these possible classes of the test sample. We apply our method to face recognition. The experimental results on several datasets show that IMSEC outperforms MSEC and the other state-of-the-art methods in terms of accuracy.\n"], "author_display": ["Qi Zhu", "Zhengming Li", "Jinxing Liu", "Zizhu Fan", "Lei Yu", "Yan Chen"], "article_type": "Research Article", "score": 0.42988205, "title_display": "Improved Minimum Squared Error Algorithm with Applications to Face Recognition", "publication_date": "2013-08-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0070370"}, {"journal": "PLoS ONE", "abstract": ["Background: Plasma fatty acid (FA) composition reflects dietary intake and endogenous turnover and is associated with health outcomes on a short and long term basis. The total plasma FA pool represents the composition of all FA containing lipid fractions. We developed a simplified and affordable high-throughput method for the analysis of total plasma FA composition, suitable for large studies. Methodology/Principal Findings: The total lipid FA from 100 \u00b5l plasma is transferred in situ into methyl esters, avoiding initial extraction and drying steps. The fatty acid methyl esters are extracted once and analyzed by gas chromatography. For the new direct in situ transesterification method optimal, reaction parameters were determined. Intra-assay analysis (n\u200a=\u200a8) revealed coefficients of variation below 4% for FA contributing more than 1% to total FA. Conclusions/Significance: The results show good agreement with FA concentrations obtained by a reference method. The new direct in situ transesterification method is robust and simple. Sample preparation time and analysis costs are reduced to a minimum. This method is an economically and ecologically superior alternative to conventional methods for assessing plasma FA status in large studies. "], "author_display": ["Claudia Glaser", "Hans Demmelmair", "Berthold Koletzko"], "article_type": "Research Article", "score": 0.42970178, "title_display": "High-Throughput Analysis of Total Plasma Fatty Acid Composition with Direct <i>In Situ</i> Transesterification", "publication_date": "2010-08-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0012045"}, {"journal": "PLoS Neglected Tropical Diseases", "abstract": ["Background: Currently there is increasing recognition of the need for research in developing countries where disease burden is high. Understanding the role of local factors is important for undertaking ethical research in developing countries. We explored factors relating to information and communication during the process of informed consent, and the approach that should be followed for gaining consent. The study was conducted prior to a family-based genetic study among people with podoconiosis (non-filarial elephantiasis) in southern Ethiopia. Methodology/Principal Findings: We adapted a method of rapid assessment validated in The Gambia. The methodology was entirely qualitative, involving focus-group discussions and in-depth interviews. Discussions were conducted with podoconiosis patients and non-patients in the community, fieldworkers, researchers, staff of the local non-governmental organisation (NGO) working on prevention and treatment of podoconiosis, and community leaders. We found that the extent of use of everyday language, the degree to which expectations of potential participants were addressed, and the techniques of presentation of information had considerable impact on comprehension of information provided about research. Approaching podoconiosis patients via locally trusted individuals and preceding individual consent with community sensitization were considered the optimal means of communication. Prevailing poverty among podoconiosis patients, the absence of alternative treatment facilities, and participants' trust in the local NGO were identified as potential barriers for obtaining genuine informed consent. Conclusions: Researchers should evaluate the effectiveness of consent processes in providing appropriate information in a comprehensible manner and in supporting voluntary decision-making on a study-by-study basis. Author Summary: Informed consent to biomedical research in developing countries is a highly topical issue. When consent forms and processes are simply borrowed from developed countries, obtaining genuine informed consent becomes extremely challenging. This paper examines how a quick and relatively simple intervention (Rapid Assessment) can influence the design and implementation of informed consent processes in the context of biomedical research involving poor, socially stigmatized and illiterate communities in a developing country. The paper goes on to discuss the effect of social, cultural, and economic factors identified by the intervention in a particular context and demonstrates how knowledge of these influences helped to develop a socially relevant and practical consent process prior to conducting a programme of community-based genetic research. The paper concludes that this intervention is an effective and economical means by which to ensure the efficacy and ethical integrity of consent processes when recruiting participants to new research sites, even within countries with which researchers are already acquainted. "], "author_display": ["Fasil Tekola", "Susan J. Bull", "Bobbie Farsides", "Melanie J. Newport", "Adebowale Adeyemo", "Charles N. Rotimi", "Gail Davey"], "article_type": "Research Article", "score": 0.42958772, "title_display": "Tailoring Consent to Context: Designing an Appropriate Consent Process for a Biomedical Study in a Low Income Setting", "publication_date": "2009-07-21T00:00:00Z", "eissn": "1935-2735", "id": "10.1371/journal.pntd.0000482"}, {"journal": "PLOS ONE", "abstract": ["\nAs a large producer and consumer of wood building materials, China suffers product formaldehyde emissions (PFE) but lacks systematic investigations and basic data on Chinese standard emission tests (CST), so this paper presented a first effort on this issue. The PFE of fiberboards, particleboards, blockboards, floorings, and parquets manufactured in Beijing region were characterized by the perforator extraction method (PE), 9\u201311 L and 40 L desiccator methods (D9, D40), and environmental chamber method (EC) of the Chinese national standard GB 18580; based on statistics of PFE data, measurement uncertainties in CST were evaluated by the Monte Carlo method; moreover, PFE data correlations between tests were established. Results showed: (1) Different tests may give slightly different evaluations on product quality. In PE and D9 tests, blockboards and parquets reached E1 grade for PFE, which can be directly used in indoor environment; but in D40 and EC tests, floorings and parquets achieved E1. (2) In multiple tests, PFE data characterized by PE, D9, and D40 complied with Gaussian distributions, while those characterized by EC followed log-normal distributions. Uncertainties in CST were overall low, with uncertainties for 20 material-method combinations all below 7.5%, and the average uncertainty for each method under 3.5%, thus being acceptable in engineering application. A more complicated material structure and a larger test scale caused higher uncertainties. (3) Conventional linear models applied to correlating PFE values between PE, D9, and EC, with R2 all over 0.840, while novel logarithmic (exponential) models can work better for correlations involving D40, with R2 all beyond 0.901. This research preliminarily demonstrated the effectiveness of CST, where results for D40 presented greater similarities to EC\u2014the currently most reliable test for PFE, thus highlighting the potential of Chinese D40 as a more practical approach in production control and risk assessment.\n"], "author_display": ["Wei Song", "Yang Cao", "Dandan Wang", "Guojun Hou", "Zaihua Shen", "Shuangbao Zhang"], "article_type": "Research Article", "score": 0.4294904, "title_display": "An Investigation on Formaldehyde Emission Characteristics of Wood Building Materials in Chinese Standard Tests: Product Emission Levels, Measurement Uncertainties, and Data Correlations between Various Tests", "publication_date": "2015-12-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0144374"}, {"journal": "PLoS ONE", "abstract": ["\n        Intuitive visualization of data and results is very important in genomics, especially when many conditions are to be analyzed and compared. Heat-maps have proven very useful for the representation of biological data. Here we present Gitools (http://www.gitools.org), an open-source tool to perform analyses and visualize data and results as interactive heat-maps. Gitools contains data import systems from several sources (i.e. IntOGen, Biomart, KEGG, Gene Ontology), which facilitate the integration of novel data with previous knowledge.\n      "], "author_display": ["Christian Perez-Llamas", "Nuria Lopez-Bigas"], "article_type": "Research Article", "score": 0.42910662, "title_display": "Gitools: Analysis and Visualisation of Genomic Data Using Interactive Heat-Maps", "publication_date": "2011-05-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0019541"}, {"journal": "PLoS ONE", "abstract": ["\nWith the advent of high-throughput technologies for measuring genome-wide expression profiles, a large number of methods have been proposed for discovering diagnostic markers that can accurately discriminate between different classes of a disease. However, factors such as the small sample size of typical clinical data, the inherent noise in high-throughput measurements, and the heterogeneity across different samples, often make it difficult to find reliable gene markers. To overcome this problem, several studies have proposed the use of pathway-based markers, instead of individual gene markers, for building the classifier. Given a set of known pathways, these methods estimate the activity level of each pathway by summarizing the expression values of its member genes, and use the pathway activities for classification. It has been shown that pathway-based classifiers typically yield more reliable results compared to traditional gene-based classifiers. In this paper, we propose a new classification method based on probabilistic inference of pathway activities. For a given sample, we compute the log-likelihood ratio between different disease phenotypes based on the expression level of each gene. The activity of a given pathway is then inferred by combining the log-likelihood ratios of the constituent genes. We apply the proposed method to the classification of breast cancer metastasis, and show that it achieves higher accuracy and identifies more reproducible pathway markers compared to several existing pathway activity inference methods.\n"], "author_display": ["Junjie Su", "Byung-Jun Yoon", "Edward R. Dougherty"], "article_type": "Research Article", "score": 0.42841822, "title_display": "Accurate and Reliable Cancer Classification Based on Probabilistic Inference of Pathway Activity", "publication_date": "2009-12-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0008161"}, {"journal": "PLoS ONE", "abstract": ["Purpose: To compare the reproducibilities of manual and semiautomatic segmentation method for the measurement of normalized cerebral blood volume (nCBV) using dynamic susceptibility contrast-enhanced (DSC) perfusion MR imaging in glioblastomas. Materials and Methods: Twenty-two patients (11 male, 11 female; 27 tumors) with histologically confirmed glioblastoma (WHO grade IV) were examined with conventional MR imaging and DSC imaging at 3T before surgery or biopsy. Then nCBV (means and standard deviations) in each mass was measured using two DSC MR perfusion analysis methods including manual and semiautomatic segmentation method, in which contrast-enhanced (CE)-T1WI and T2WI were used as structural imaging. Intraobserver and interobserver reproducibility were assessed according to each perfusion analysis method or each structural imaging. Interclass correlation coefficient (ICC), Bland-Altman plot, and coefficient of variation (CV) were used to evaluate reproducibility. Results: Intraobserver reproducibilities on CE-T1WI and T2WI were ICC of 0.74\u20130.89 and CV of 20.39\u201336.83% in manual segmentation method, and ICC of 0.95\u20130.99 and CV of 8.53\u201316.19% in semiautomatic segmentation method, repectively. Interobserver reproducibilites on CE-T1WI and T2WI were ICC of 0.86\u20130.94 and CV of 19.67\u201335.15% in manual segmentation method, and ICC of 0.74\u20131.0 and CV of 5.48\u201349.38% in semiautomatic segmentation method, respectively. Bland-Altman plots showed a good correlation with ICC or CV in each method. The semiautomatic segmentation method showed higher intraobserver and interobserver reproducibilities at CE-T1WI-based study than other methods. Conclusion: The best reproducibility was found using the semiautomatic segmentation method based on CE-T1WI for structural imaging in the measurement of the nCBV of glioblastomas. "], "author_display": ["Seung Chai Jung", "Seung Hong Choi", "Jeong A. Yeom", "Ji-Hoon Kim", "Inseon Ryoo", "Soo Chin Kim", "Hwaseon Shin", "A. Leum Lee", "Tae Jin Yun", "Chul-Kee Park", "Chul-Ho Sohn", "Sung-Hye Park"], "article_type": "Research Article", "score": 0.428374, "title_display": "Cerebral Blood Volume Analysis in Glioblastomas Using Dynamic Susceptibility Contrast-Enhanced Perfusion MRI: A Comparison of Manual and Semiautomatic Segmentation Methods", "publication_date": "2013-08-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0069323"}, {"journal": "PLoS ONE", "abstract": ["Background: Patient-reported outcomes (PROs), such as health-related quality of life (HRQL) are increasingly used to evaluate treatment effectiveness in clinical trials, are valued by patients, and may inform important decisions in the clinical setting. It is of concern, therefore, that preliminary evidence, gained from group discussions at UK-wide Medical Research Council (MRC) quality of life training days, suggests there are inconsistent standards of HRQL data collection in trials and appropriate training and education is often lacking. Our objective was to investigate these reports, to determine if they represented isolated experiences, or were indicative of a potentially wider problem. Methods And Findings: We undertook a qualitative study, conducting 26 semi-structured interviews with research nurses, data managers, trial coordinators and research facilitators involved in the collection and entry of HRQL data in clinical trials, across one primary care NHS trust, two secondary care NHS trusts and two clinical trials units in the UK. We used conventional content analysis to analyze and interpret our data. Our study participants reported (1) inconsistent standards in HRQL measurement, both between, and within, trials, which appeared to risk the introduction of bias; (2), difficulties in dealing with HRQL data that raised concern for the well-being of the trial participant, which in some instances led to the delivery of non-protocol driven co-interventions, (3), a frequent lack of HRQL protocol content and appropriate training and education of trial staff, and (4) that HRQL data collection could be associated with emotional and/or ethical burden. Conclusions: Our findings suggest there are inconsistencies in the standards of HRQL data collection in some trials resulting from a general lack of HRQL-specific protocol content, training and education. These inconsistencies could lead to biased HRQL trial results. Future research should aim to develop HRQL guidelines and training programmes aimed at supporting researchers to carry out high quality data collection. "], "author_display": ["Derek Kyte", "Jonathan Ives", "Heather Draper", "Thomas Keeley", "Melanie Calvert"], "article_type": "Research Article", "score": 0.42828226, "title_display": "Inconsistencies in Quality of Life Data Collection in Clinical Trials: A Potential Source of Bias? Interviews with Research Nurses and Trialists", "publication_date": "2013-10-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0076625"}, {"journal": "PLoS ONE", "abstract": ["\nHorizontal gene transfer (HGT) has appeared to be of importance for prokaryotic species evolution. As a consequence numerous parametric methods, using only the information embedded in the genomes, have been designed to detect HGTs. Numerous reports of incongruencies in results of the different methods applied to the same genomes were published. The use of artificial genomes in which all HGT parameters are controlled allows testing different methods in the same conditions. The results of this benchmark concerning 16 representative parametric methods showed a great variety of efficiencies. Some methods work very poorly whatever the type of HGTs and some depend on the conditions or on the metrics used. The best methods in terms of total errors were those using tetranucleotides as criterion for the window methods or those using codon usage for gene based methods and the Kullback-Leibler divergence metric. Window methods are very sensitive but less specific and detect badly lone isolated gene. On the other hand gene based methods are often very specific but lack of sensitivity. We propose using two methods in combination to get the best of each category, a gene based one for specificity and a window based one for sensitivity.\n"], "author_display": ["Jennifer Becq", "C\u00e9cile Churlaud", "Patrick Deschavanne"], "article_type": "Research Article", "score": 0.42827308, "title_display": "A Benchmark of Parametric Methods for Horizontal Transfers Detection", "publication_date": "2010-04-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0009989"}, {"abstract": ["\n        Pattern recognition techniques have been used to automatically recognize the objects, personal identities, predict the function of protein, the category of the cancer, identify lesion, perform product inspection, and so on. In this paper we propose a novel quaternion-based discriminant method. This method represents and classifies color images in a simple and mathematically tractable way. The proposed method is suitable for a large variety of real-world applications such as color face recognition and classification of the ground target shown in multispectrum remote images. This method first uses the quaternion number to denote the pixel in the color image and exploits a quaternion vector to represent the color image. This method then uses the linear discriminant analysis algorithm to transform the quaternion vector into a lower-dimensional quaternion vector and classifies it in this space. The experimental results show that the proposed method can obtain a very high accuracy for color face recognition.\n      "], "author_display": ["Yong Xu"], "article_type": "Research Article", "score": 0.42822725, "title_display": "Quaternion-Based Discriminant Analysis Method for Color Face Recognition", "publication_date": "2012-08-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0043493"}, {"journal": "PLoS Genetics", "abstract": ["\n        The role of rare genetic variation in the etiology of complex disease remains unclear. However, the development of next-generation sequencing technologies offers the experimental opportunity to address this question. Several novel statistical methodologies have been recently proposed to assess the contribution of rare variation to complex disease etiology. Nevertheless, no empirical estimates comparing their relative power are available. We therefore assessed the parameters that influence their statistical power in 1,998 individuals Sanger-sequenced at seven genes by modeling different distributions of effect, proportions of causal variants, and direction of the associations (deleterious, protective, or both) in simulated continuous trait and case/control phenotypes. Our results demonstrate that the power of recently proposed statistical methods depend strongly on the underlying hypotheses concerning the relationship of phenotypes with each of these three factors. No method demonstrates consistently acceptable power despite this large sample size, and the performance of each method depends upon the underlying assumption of the relationship between rare variants and complex traits. Sensitivity analyses are therefore recommended to compare the stability of the results arising from different methods, and promising results should be replicated using the same method in an independent sample. These findings provide guidance in the analysis and interpretation of the role of rare base-pair variation in the etiology of complex traits and diseases.\n      Author Summary: There is now evidence that rare variants can contribute to the etiology of complex disease. Next generation sequencing technologies have enabled their detection in large cohorts, and new statistical methods have been proposed to ascertain their association with complex diseases and traits in order to improve power over single-marker analysis. Each of these new methods assumes a particular nature of the relationship between rare variants and complex disease, yet these hypotheses have been largely unverified. Therefore we sought to compare the power of commonly used and novel statistical methods for rare variants using Sanger sequencing data from 1,998 individuals sequenced at 7 genes by simulating several phenotypes under models spanning a spectrum of the common hypotheses concerning such associations. While all methods perform reasonably well under their own model-specific hypotheses, no single method gives consistently acceptable power when these hypotheses are violated. Unlike GWAS, wherein all variants can often be tested using the same method across the entire genome, the analysis and interpretation of sequencing studies will therefore be considerably more challenging. "], "author_display": ["Martin Ladouceur", "Zari Dastani", "Yurii S. Aulchenko", "Celia M. T. Greenwood", "J. Brent Richards"], "article_type": "Research Article", "score": 0.42787802, "title_display": "The Empirical Power of Rare Variant Association Methods: Results from Sanger Sequencing in 1,998 Individuals", "publication_date": "2012-02-02T00:00:00Z", "eissn": "1553-7404", "id": "10.1371/journal.pgen.1002496"}, {"journal": "PLoS ONE", "abstract": ["Motivation: Predicting the part of speech (POS) tag of an unknown word in a sentence is a significant challenge. This is particularly difficult in biomedicine, where POS tags serve as an input to training sophisticated literature summarization techniques, such as those based on Hidden Markov Models (HMM). Different approaches have been taken to deal with the POS tagger challenge, but with one exception \u2013 the TnT POS tagger - previous publications on POS tagging have omitted details of the suffix analysis used for handling unknown words. The suffix of an English word is a strong predictor of a POS tag for that word. As a pre-requisite for an accurate HMM POS tagger for biomedical publications, we present an efficient suffix prediction method for integration into a POS tagger. Results: We have implemented a fully functional HMM POS tagger using experimentally optimised suffix based prediction. Our simple suffix analysis method, significantly outperformed the probability interpolation based TnT method. We have also shown how important suffix analysis can be for probability estimation of a known word (in the training corpus) with an unseen POS tag; a common scenario with a small training corpus. We then integrated this simple method in our POS tagger and determined an optimised parameter set for both methods, which can help developers to optimise their current algorithm, based on our results. We also introduce the concept of counting methods in maximum likelihood estimation for the first time and show how counting methods can affect the prediction result. Finally, we describe how machine-learning techniques were applied to identify words, for which prediction of POS tags were always incorrect and propose a method to handle words of this type. Availability and Implementation: Java source code, binaries and setup instructions are freely available at http://genomes.sapac.edu.au/text_mining/pos_tagger.zip. "], "author_display": ["Mario Fruzangohar", "Trent A. Kroeger", "David L. Adelson"], "article_type": "Research Article", "score": 0.42702773, "title_display": "Improved Part-of-Speech Prediction in Suffix Analysis", "publication_date": "2013-10-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0076042"}, {"journal": "PLoS ONE", "abstract": ["\nThe arterial input function (AIF) plays a crucial role in the quantification of cerebral perfusion parameters. The traditional method for AIF detection is based on manual operation, which is time-consuming and subjective. Two automatic methods have been reported that are based on two frequently used clustering algorithms: fuzzy c-means (FCM) and K-means. However, it is still not clear which is better for AIF detection. Hence, we compared the performance of these two clustering methods using both simulated and clinical data. The results demonstrate that K-means analysis can yield more accurate and robust AIF results, although it takes longer to execute than the FCM method. We consider that this longer execution time is trivial relative to the total time required for image manipulation in a PACS setting, and is acceptable if an ideal AIF is obtained. Therefore, the K-means method is preferable to FCM in AIF detection.\n"], "author_display": ["Jiandong Yin", "Hongzan Sun", "Jiawen Yang", "Qiyong Guo"], "article_type": "Research Article", "score": 0.42699462, "title_display": "Comparison of <i>K</i>-Means and Fuzzy <i>c</i>-Means Algorithm Performance for Automated Determination of the Arterial Input Function", "publication_date": "2014-02-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0085884"}, {"journal": "PLoS ONE", "abstract": ["\n        The genome-wide association study (GWAS) has become a routine approach for mapping disease risk loci with the advent of large-scale genotyping technologies. Multi-allelic haplotype markers can provide superior power compared with single-SNP markers in mapping disease loci. However, the application of haplotype-based analysis to GWAS is usually bottlenecked by prohibitive time cost for haplotype inference, also known as phasing. In this study, we developed an efficient approach to haplotype-based analysis in GWAS. By using a reference panel, our method accelerated the phasing process and reduced the potential bias generated by unrealistic assumptions in phasing process. The haplotype-based approach delivers great power and no type I error inflation for association studies. With only a medium-size reference panel, phasing error in our method is comparable to the genotyping error afforded by commercial genotyping solutions.\n      "], "author_display": ["Yungang He", "Cong Li", "Christopher I. Amos", "Momiao Xiong", "Hua Ling", "Li Jin"], "article_type": "Research Article", "score": 0.4267468, "title_display": "Accelerating Haplotype-Based Genome-Wide Association Study Using Perfect Phylogeny and Phase-Known Reference Data", "publication_date": "2011-07-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0022097"}, {"journal": "PLOS ONE", "abstract": ["\nWe investigate four different methods for background estimation in calcium imaging of the insect brain and evaluate their performance on six data sets consisting of data recorded from two sites in two species of moths. The calcium fluorescence decay curve outside the potential response is estimated using either a low-pass filter or constant, linear or polynomial regression, and is subsequently used to calculate the magnitude, latency and duration of the response. The magnitude and variance of the responses that are obtained by the different methods are compared, and, by computing the receiver operating characteristics of a classifier based on response magnitude, we evaluate the ability of each method to detect the stimulus type and conclude that a polynomial approximation of the background gives the overall best result.\n"], "author_display": ["Anna Balkenius", "Anders J. Johansson", "Christian Balkenius"], "article_type": "Research Article", "score": 0.42672533, "title_display": "Comparing Analysis Methods in Functional Calcium Imaging of the Insect Brain", "publication_date": "2015-06-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0129614"}, {"journal": "PLoS ONE", "abstract": ["\nContemporary high dimensional biological assays, such as mRNA expression microarrays, regularly involve multiple data processing steps, such as experimental processing, computational processing, sample selection, or feature selection (i.e. gene selection), prior to deriving any biological conclusions. These steps can dramatically change the interpretation of an experiment. Evaluation of processing steps has received limited attention in the literature. It is not straightforward to evaluate different processing methods and investigators are often unsure of the best method. We present a simple statistical tool, Standardized WithIn class Sum of Squares (SWISS), that allows investigators to compare alternate data processing methods, such as different experimental methods, normalizations, or technologies, on a dataset in terms of how well they cluster a priori biological classes. SWISS uses Euclidean distance to determine which method does a better job of clustering the data elements based on a priori classifications. We apply SWISS to three different gene expression applications. The first application uses four different datasets to compare different experimental methods, normalizations, and gene sets. The second application, using data from the MicroArray Quality Control (MAQC) project, compares different microarray platforms. The third application compares different technologies: a single Agilent two-color microarray versus one lane of RNA-Seq. These applications give an indication of the variety of problems that SWISS can be helpful in solving. The SWISS analysis of one-color versus two-color microarrays provides investigators who use two-color arrays the opportunity to review their results in light of a single-channel analysis, with all of the associated benefits offered by this design. Analysis of the MACQ data shows differential intersite reproducibility by array platform. SWISS also shows that one lane of RNA-Seq clusters data by biological phenotypes as well as a single Agilent two-color microarray.\n"], "author_display": ["Christopher R. Cabanski", "Yuan Qi", "Xiaoying Yin", "Eric Bair", "Michele C. Hayward", "Cheng Fan", "Jianying Li", "Matthew D. Wilkerson", "J. S. Marron", "Charles M. Perou", "D. Neil Hayes"], "article_type": "Research Article", "score": 0.4265211, "title_display": "SWISS MADE: Standardized WithIn Class Sum of Squares to Evaluate Methodologies and Dataset Elements", "publication_date": "2010-03-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0009905"}, {"journal": "PLoS ONE", "abstract": ["Background: Studies of natural animal populations reveal widespread evidence for the diffusion of novel behaviour patterns, and for intra- and inter-population variation in behaviour. However, claims that these are manifestations of animal \u2018culture\u2019 remain controversial because alternative explanations to social learning remain difficult to refute. This inability to identify social learning in social settings has also contributed to the failure to test evolutionary hypotheses concerning the social learning strategies that animals deploy. Methodology/Principal Findings: We present a solution to this problem, in the form of a new means of identifying social learning in animal populations. The method is based on the well-established premise of social learning research, that - when ecological and genetic differences are accounted for - social learning will generate greater homogeneity in behaviour between animals than expected in its absence. Our procedure compares the observed level of homogeneity to a sampling distribution generated utilizing randomization and other procedures, allowing claims of social learning to be evaluated according to consensual standards. We illustrate the method on data from groups of monkeys provided with novel two-option extractive foraging tasks, demonstrating that social learning can indeed be distinguished from unlearned processes and asocial learning, and revealing that the monkeys only employed social learning for the more difficult tasks. The method is further validated against published datasets and through simulation, and exhibits higher statistical power than conventional inferential statistics. Conclusions/Significance: The method is potentially a significant technological development, which could prove of considerable value in assessing the validity of claims for culturally transmitted behaviour in animal groups. It will also be of value in enabling investigation of the social learning strategies deployed in captive and natural animal populations. "], "author_display": ["Rachel L. Kendal", "Jeremy R. Kendal", "Will Hoppitt", "Kevin N. Laland"], "article_type": "Research Article", "score": 0.4262709, "title_display": "Identifying Social Learning in Animal Populations: A New \u2018Option-Bias\u2019 Method", "publication_date": "2009-08-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0006541"}, {"journal": "PLoS ONE", "abstract": ["\nIdentification of risk factors in patients with a particular disease can be analyzed in clinical data sets by using feature selection procedures of pattern recognition and data mining methods. The applicability of the relaxed linear separability (RLS) method of feature subset selection was checked for high-dimensional and mixed type (genetic and phenotypic) clinical data of patients with end-stage renal disease. The RLS method allowed for substantial reduction of the dimensionality through omitting redundant features while maintaining the linear separability of data sets of patients with high and low levels of an inflammatory biomarker. The synergy between genetic and phenotypic features in differentiation between these two subgroups was demonstrated.\n"], "author_display": ["Leon Bobrowski", "Tomasz \u0141ukaszuk", "Bengt Lindholm", "Peter Stenvinkel", "Olof Heimburger", "Jonas Axelsson", "Peter B\u00e1r\u00e1ny", "Juan Jesus Carrero", "Abdul Rashid Qureshi", "Karin Luttropp", "Malgorzata Debowska", "Louise Nordfors", "Martin Schalling", "Jacek Waniewski"], "article_type": "Research Article", "score": 0.42622006, "title_display": "Selection of Genetic and Phenotypic Features Associated with Inflammatory Status of Patients on Dialysis Using Relaxed Linear Separability Method", "publication_date": "2014-01-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0086630"}, {"journal": "PLoS ONE", "abstract": ["\nMolecular dynamics (MD) simulations of a double-stranded DNA with explicit water and small ions were performed with the zero-dipole summation (ZD) method, which was recently developed as one of the non-Ewald methods. Double-stranded DNA is highly charged and polar, with phosphate groups in its backbone and their counterions, and thus precise treatment for the long-range electrostatic interactions is always required to maintain the stable and native double-stranded form. A simple truncation method deforms it profoundly. On the contrary, the ZD method, which considers the neutralities of charges and dipoles in a truncated subset, well reproduced the electrostatic energies of the DNA system calculated by the Ewald method. The MD simulations using the ZD method provided a stable DNA system, with similar structures and dynamic properties to those produced by the conventional Particle mesh Ewald method.\n"], "author_display": ["Takamasa Arakawa", "Narutoshi Kamiya", "Haruki Nakamura", "Ikuo Fukuda"], "article_type": "Research Article", "score": 0.42604193, "title_display": "Molecular Dynamics Simulations of Double-Stranded DNA in an Explicit Solvent Model with the Zero-Dipole Summation Method", "publication_date": "2013-10-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0076606"}, {"journal": "PLOS ONE", "abstract": ["\nExemplar-based algorithms are a popular technique for image inpainting. They mainly have two important phases: deciding the filling-in order and selecting good exemplars. Traditional exemplar-based algorithms are to search suitable patches from source regions to fill in the missing parts, but they have to face a problem: improper selection of exemplars. To improve the problem, we introduce an independent strategy through investigating the process of patches propagation in this paper. We first define a new separated priority definition to propagate geometry and then synthesize image textures, aiming to well recover image geometry and textures. In addition, an automatic algorithm is designed to estimate steps for the new separated priority definition. Comparing with some competitive approaches, the new priority definition can recover image geometry and textures well.\n"], "author_display": ["Liang-Jian Deng", "Ting-Zhu Huang", "Xi-Le Zhao"], "article_type": "Research Article", "score": 0.42595407, "title_display": "Exemplar-Based Image Inpainting Using a Modified Priority Definition", "publication_date": "2015-10-22T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0141199"}, {"journal": "PLoS ONE", "abstract": ["\nComputational methods have started playing a significant role in semantic analysis. One particularly accessible area for developing good computational methods for linguistic semantics is in color naming, where perceptual dissimilarity measures provide a geometric setting for the analyses. This setting has been studied first by Berlin & Kay in 1969, and then later on by a large data collection effort: the World Color Survey (WCS). From the WCS, a dataset on color naming by 2 616 speakers of 110 different languages is made available for further research. In the analysis of color naming from WCS, however, the choice of analysis method is an important factor of the analysis. We demonstrate concrete problems with the choice of metrics made in recent analyses of WCS data, and offer approaches for dealing with the problems we can identify. Picking a metric for the space of color naming distributions that ignores perceptual distances between colors assumes a decorrelated system, where strong spatial correlations in fact exist. We can demonstrate that the corresponding issues are significantly improved when using Earth Mover's Distance, or Quadratic -square Distance, and we can approximate these solutions with a kernel-based analysis method.\n"], "author_display": ["Mikael Vejdemo-Johansson", "Susanne Vejdemo", "Carl-Henrik Ek"], "article_type": "Research Article", "score": 0.42549884, "title_display": "Comparing Distributions of Color Words: Pitfalls and Metric Choices", "publication_date": "2014-02-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0089184"}, {"journal": "PLoS ONE", "abstract": ["Background: The legal framework and funding mechanisms of the national health research system were recently reformed in Mexico. A study of the resource allocation for health research is still missing. We identified the health research areas funded by the National Council on Science and Technology (CONACYT) and examined whether research funding has been aligned to national health problems. Methods and Findings: We collected the information to create a database of research grant projects supported through the three main Sectoral Funds managed by CONACYT between 2003 and 2010. The health-related projects were identified and classified according to their methodological approach and research objective. A correlation analysis was carried out to evaluate the association between disease-specific funding and two indicators of disease burden. From 2003 to 2010, research grant funding increased by 32% at a compound annual growth rate of 3.5%. By research objective, the budget fluctuated annually resulting in modest increments or even decrements during the period under analysis. The basic science category received the largest share of funding (29%) while the less funded category was violence and accidents (1.4%). The number of deaths (\u03c1\u200a=\u200a0.51; P<0.001) and disability-adjusted life years (DALYs; \u03c1\u200a=\u200a0.33; P\u200a=\u200a0.004) were weakly correlated with the funding for health research. Considering the two indicators, poisonings and infectious and parasitic diseases were among the most overfunded conditions. In contrast, congenital anomalies, road traffic accidents, cerebrovascular disease, and chronic obstructive pulmonary disease were the most underfunded conditions. Conclusions: Although the health research funding has grown since the creation of CONACYT sectoral funds, the financial effort is still low in comparison to other Latin American countries with similar development. Furthermore, the great diversity of the funded topics compromises the efficacy of the investment. Better mechanisms of research priority-setting are required to adjust the research portfolio to the new health panorama of Mexican population. "], "author_display": ["Eduardo Mart\u00ednez-Mart\u00ednez", "Mar\u00eda Luisa Zaragoza", "Elmer Solano", "Brenda Figueroa", "Patricia Z\u00fa\u00f1iga", "Juan P. Laclette"], "article_type": "Research Article", "score": 0.4254657, "title_display": "Health Research Funding in Mexico: The Need for a Long-Term Agenda", "publication_date": "2012-12-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0051195"}, {"journal": "PLoS ONE", "abstract": ["Background: Myeloproliferative disorders are characterized by clonal expansion of normal mature blood cells. Acquired mutations giving rise to constitutive activation of the JAK2 tyrosine kinase has been shown to be present in the majority of patients. Since the demonstration that the V617F mutation in the exon 14 of the JAK2 gene is present in about 90% of patients with Polycythemia Vera (PV), the detection of this mutation has become a key tool for the diagnosis of these patients. More recently, additional mutations in the exon 12 of the JAK2 gene have been described in 5 to 10% of the patients with erythrocytosis. According to the updated WHO criteria the presence of these mutations should be looked for in PV patients with no JAK2 V617F mutation. Reliable and accurate methods dedicated to the detection of these highly variable mutations are therefore necessary. Methods/Findings: For these reasons we have defined the conditions of a High Resolution DNA Melting curve analysis (HRM) method able to detect JAK2 exon 12 mutations. After having validated that the method was able to detect mutated patients, we have verified that it gave reproducible results in repeated experiments, on DNA extracted from either total blood or purified granulocytes. This HRM assay was further validated using 8 samples bearing different mutant sequences in 4 different laboratories, on 3 different instruments. Conclusion: The assay we have developed is thus a valid method, adapted to routine detection of JAK2 exon 12 mutations with highly reproducible results. "], "author_display": ["Valerie Ugo", "Sylvie Tondeur", "Marie-Laurence Menot", "Nadine Bonnin", "Gerald Le Gac", "Carole Tonetti", "Veronique Mansat-De Mas", "Lydie Lecucq", "Jean-Jacques Kiladjian", "Christine Chomienne", "Christine Dosquet", "Nathalie Parquet", "Luc Darnige", "Marc Porneuf", "Martine Escoffre-Barbe", "Stephane Giraudier", "Eric Delabesse", "Bruno Cassinat", "for the French Intergroup of Myeloproliferative disorders (FIM) "], "article_type": "Research Article", "score": 0.4253626, "title_display": "Interlaboratory Development and Validation of a HRM Method Applied to the Detection of JAK2 Exon 12 Mutations in Polycythemia Vera Patients", "publication_date": "2010-01-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0008893"}, {"journal": "PLOS ONE", "abstract": ["\nThe growth of Brazilian scientific production in recent years is remarkable, which motivates an investigation on the factors, inside and outside the country, that helped shape this wealthy research environment. This article provides a thorough analysis of the education of researchers that constitute the main Brazilian research groups, using data on about 6,000 researchers involved in the country\u2019s National Institutes of Science and Technology (INCT) initiative. Data on the steps taken by each researcher in her education, from the bachelor\u2019s degree to doctorate, including a possible postdoctoral experience, and employment, are extracted from an official curriculum vitae repository. The location and the time at which each career step occurred define spatiotemporal career trajectories. We then analyze such trajectories considering additional data, including the area of knowledge of the INCTs to which each researcher is associated. We found an increasing prevalence of Brazilian institutions in the education of Brazilian scientists, as the number of doctorates earned abroad is decreasing over time. Postdoctoral stages, on the other hand, often take place in Europe or in the United States. Taking an international postdoctoral position after a full education in Brazil suggests a drive towards seeking higher-level exchange and cooperation with foreign groups in a more advanced career stage. Results also show that Brazilian researchers tend to seek employment in regions that are close to the institutions at which they received their bachelor\u2019s degrees, suggesting low mobility within the country. This study can be instrumental in defining public policies for correcting distortions, and can help other developing countries that aim to improve their national science systems.\n"], "author_display": ["Caio Alves Furtado", "Clodoveu A. Davis", "Marcos Andr\u00e9 Gon\u00e7alves", "Jussara Marques de Almeida"], "article_type": "Research Article", "score": 0.42510527, "title_display": "A Spatiotemporal Analysis of Brazilian Science from the Perspective of Researchers\u2019 Career Trajectories", "publication_date": "2015-10-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0141528"}, {"journal": "PLoS ONE", "abstract": ["Purpose: To develop EdgeSelect, a semi-automatic method for the segmentation of retinal layers in spectral domain optical coherence tomography images, and to compare the segmentation results with a manual method. Methods: SD-OCT (Heidelberg Spectralis) scans of 28 eyes (24 patients with diabetic macular edema and 4 normal subjects) were imported into a customized MATLAB application, and were manually segmented by three graders at the layers corresponding to the inner limiting membrane (ILM), the inner segment/ellipsoid interface (ISe), the retinal/retinal pigment epithelium interface (RPE), and the Bruch's membrane (BM). The scans were then segmented independently by the same graders using EdgeSelect, a semi-automated method allowing the graders to guide/correct the layer segmentation interactively. The inter-grader reproducibility and agreement in locating the layer positions between the manual and EdgeSelect methods were assessed and compared using the Wilcoxon signed rank test. Results: The inter-grader reproducibility using the EdgeSelect method for retinal layers varied from 0.15 to 1.21 \u00b5m, smaller than those using the manual method (3.36\u20136.43 \u00b5m). The Wilcoxon test indicated the EdgeSelect method had significantly better reproducibility than the manual method. The agreement between the manual and EdgeSelect methods in locating retinal layers ranged from 0.08 to 1.32 \u00b5m. There were small differences between the two methods in locating the ILM (p\u200a=\u200a0.012) and BM layers (p<0.001), but these were statistically indistinguishable in locating the ISe (p\u200a=\u200a0.896) and RPE layers (p\u200a=\u200a0.771). Conclusions: The EdgeSelect method resulted in better reproducibility and good agreement with a manual method in a set of eyes of normal subjects and with retinal disease, suggesting that this approach is feasible for OCT image analysis in clinical trials. "], "author_display": ["Yijun Huang", "Ronald P. Danis", "Jeong W. Pak", "Shiyu Luo", "James White", "Xian Zhang", "Ashwini Narkar", "Amitha Domalpally"], "article_type": "Research Article", "score": 0.424921, "title_display": "Development of a Semi-Automatic Segmentation Method for Retinal OCT Images Tested in Patients with Diabetic Macular Edema", "publication_date": "2013-12-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0082922"}, {"journal": "PLOS ONE", "abstract": ["Objective: This study informs efforts to improve the discoverability of and access to biomedical datasets by providing a preliminary estimate of the number and type of datasets generated annually by research funded by the U.S. National Institutes of Health (NIH). It focuses on those datasets that are \u201cinvisible\u201d or not deposited in a known repository. Methods: We analyzed NIH-funded journal articles that were published in 2011, cited in PubMed and deposited in PubMed Central (PMC) to identify those that indicate data were submitted to a known repository. After excluding those articles, we analyzed a random sample of the remaining articles to estimate how many and what types of invisible datasets were used in each article. Results: About 12% of the articles explicitly mention deposition of datasets in recognized repositories, leaving 88% that are invisible datasets. Among articles with invisible datasets, we found an average of 2.9 to 3.4 datasets, suggesting there were approximately 200,000 to 235,000 invisible datasets generated from NIH-funded research published in 2011. Approximately 87% of the invisible datasets consist of data newly collected for the research reported; 13% reflect reuse of existing data. More than 50% of the datasets were derived from live human or non-human animal subjects. Conclusion: In addition to providing a rough estimate of the total number of datasets produced per year by NIH-funded researchers, this study identifies additional issues that must be addressed to improve the discoverability of and access to biomedical research data: the definition of a \u201cdataset,\u201d determination of which (if any) data are valuable for archiving and preservation, and better methods for estimating the number of datasets of interest. Lack of consensus amongst annotators about the number of datasets in a given article reinforces the need for a principled way of thinking about how to identify and characterize biomedical datasets. "], "author_display": ["Kevin B. Read", "Jerry R. Sheehan", "Michael F. Huerta", "Lou S. Knecht", "James G. Mork", "Betsy L. Humphreys", "NIH Big Data Annotator Group "], "article_type": "Research Article", "score": 0.42468068, "title_display": "Sizing the Problem of Improving Discovery and Access to NIH-Funded Data: A Preliminary Study", "publication_date": "2015-07-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0132735"}, {"journal": "PLOS ONE", "abstract": ["\nIn face recognition, most appearance-based methods require several images of each person to construct the feature space for recognition. However, in the real world it is difficult to collect multiple images per person, and in many cases there is only a single sample per person (SSPP). In this paper, we propose a method to generate new images with various illuminations from a single image taken under frontal illumination. Motivated by the integral image, which was developed for face detection, we extract the bidirectional integral feature (BIF) to obtain the characteristics of the illumination condition at the time of the picture being taken. The experimental results for various face databases show that the proposed method results in improved recognition performance under illumination variation.\n"], "author_display": ["Yonggeol Lee", "Minsik Lee", "Sang-Il Choi"], "article_type": "Research Article", "score": 0.42465135, "title_display": "Image Generation Using Bidirectional Integral Features for Face Recognition with a Single Sample per Person", "publication_date": "2015-09-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0138859"}, {"journal": "PLOS ONE", "abstract": ["\nLoess shoulder-lines are significant structural lines which divide the complicated loess landform into loess interfluves and gully-slope lands. Existing extraction algorithms for shoulder-lines mainly are based on local maximum of terrain features. These algorithms are sensitive to noise for complicated loess surface and the extraction parameters are difficult to be determined, making the extraction results usually inaccurate. This paper presents a new extraction approach for loess shoulder-lines, in which Marr-Hildreth edge operator is employed to construct initial shoulder-lines. Then the terrain mask for confining the boundary of shoulder-lines is proposed based on slope degree classification and morphology methods, avoiding interference from non-valley area and modify the initial loess shoulder-lines. A case study is conducted in Yijun located in the northern Shanxi Loess Plateau of China. The Digital Elevation Models with a grid size of 5 m is applied as original data. To obtain optimal scale parameters, the Euclidean Distance Offset Percentages between shoulder-lines is calculated by the Marr-Hildreth operator and the manual delineations. The experimental results show that the new method could achieve the highest extraction accuracy when \u03c3 = 5 in Gaussian smoothing. According to the accuracy assessment, the average extraction accuracy is about 88.5%, which indicates that the proposed method is applicable for the extraction of loess shoulder-lines in the loess hilly and gully areas.\n"], "author_display": ["Sheng Jiang", "Guoan Tang", "Kai Liu"], "article_type": "Research Article", "score": 0.4245361, "title_display": "A New Extraction Method of Loess Shoulder-Line Based on Marr-Hildreth Operator and Terrain Mask", "publication_date": "2015-04-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0123804"}, {"journal": "PLoS ONE", "abstract": ["\nUrinary exosomes and microvesicles (EMV) are promising biomarkers for renal diseases. Although the density of EMV is very low in urine, large quantity of urine can be easily obtained. In order to analyze urinary EMV mRNA, a unique filter device to adsorb urinary EMV from 10 mL urine was developed, which is far more convenient than the standard ultracentrifugation protocol. The filter part of the device is detachable and aligned to a 96-well microplate format, therefore multiple samples can be processed simultaneously in a high throughput manner following the isolation step. For EMV mRNA quantification, the EMV on the filter is lysed directly by adding lysis buffer and transferred to an oligo(dT)-immobilized microplate for mRNA isolation followed by cDNA synthesis and real-time PCR. Under the optimized assay condition, our method provided comparable or even superior results to the standard ultracentrifugation method in terms of mRNA assay sensitivity, linearity, intra-assay reproducibility, and ease of use. The assay system was applied to quantification of kidney-specific mRNAs such as NPHN and PDCN (glomerular filtration), SLC12A1 (tubular absorption), UMOD and ALB (tubular secretion), and AQP2 (collecting duct water absorption). 12-hour urine samples were collected from four healthy subjects for two weeks, and day-to-day and individual-to-individual variations were investigated. Kidney-specific genes as well as control genes (GAPDH, ACTB, etc.) were successfully detected and confirmed their stable expressions through the two-week study period. In conclusion, this method is readily available to clinical studies of kidney diseases.\n"], "author_display": ["Taku Murakami", "Melanie Oakes", "Mieko Ogura", "Vivian Tovar", "Cindy Yamamoto", "Masato Mitsuhashi"], "article_type": "Research Article", "score": 0.4244554, "title_display": "Development of Glomerulus-, Tubule-, and Collecting Duct-Specific mRNA Assay in Human Urinary Exosomes and Microvesicles", "publication_date": "2014-10-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0109074"}, {"journal": "PLoS ONE", "abstract": ["\n    \t\t\t\tBackground A method for assessing dental maturity in different populations was first developed in 1973 by Demirjian and has been widely used and accepted since then. While the accuracy for evaluating dental age using Demirjian\u2019s method compared to children\u2019s chronological age has been extensively studied in recent years, the results currently available remain controversial and ambiguous.\n\t\t\t\t\tMethods A literature search of PubMed, Embase, Web of Science, CNKI and CBM databases was conducted to identify all eligible studies published before July 12th, 2013. Weighted mean difference (WMD) with corresponding 95% confidence interval (95% CI) was used to evaluate the applicability of Demirjian\u2019s method for estimating chronological age in children.\n\t\t\t\t\t\n\t\t\t\t\tResults: A meta-analysis was conducted on 26 studies with a total of 11,499 children (5,301 boys and 6,198 girls) aged 3.5 to 16.9 years. Overall, we found that Demirjian\u2019s method overestimated dental age by 0.35 (4.2 months) and 0.39 (4.68 months) years in males and females, respectively. A subgroup analysis by age revealed that boys and girls between the ages of 5 to 14 were given a dental age estimate that was significantly more advanced than their chronological age. Differences between underestimated dental ages and actual chronological ages were lower for male and female 15- and 16-year-old subgroups, though a significant difference was found in the 16-year-old subgroup.\n    \t\t\t\tConclusions Demirjian\u2019s method\u2019s overestimation of actual chronological tooth age reveals the need for population-specific standards to better estimate the rate of human dental maturation.\n\t\t\t\t\n\t\t\t"], "author_display": ["Jin Yan", "Xintian Lou", "Liming Xie", "Dedong Yu", "Guofang Shen", "Yilin Wang"], "article_type": "Research Article", "score": 0.42440766, "title_display": "Assessment of Dental Age of Children Aged 3.5 to 16.9 Years Using Demirjian\u2019s Method: A Meta-Analysis Based on 26 Studies", "publication_date": "2013-12-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0084672"}, {"journal": "PLoS ONE", "abstract": ["Background: Systematic reviews of preclinical studies, in vivo animal experiments in particular, can influence clinical research and thus even clinical care. Dissemination bias, selective dissemination of positive or significant results, is one of the major threats to validity in systematic reviews also in the realm of animal studies. We conducted a systematic review to determine the number of published systematic reviews of animal studies until present, to investigate their methodological features especially with respect to assessment of dissemination bias, and to investigate the citation of preclinical systematic reviews on clinical research. Methods: Eligible studies for this systematic review constitute systematic reviews that summarize in vivo animal experiments whose results could be interpreted as applicable to clinical care. We systematically searched Ovid Medline, Embase, ToxNet, and ScienceDirect from 1st January 2009 to 9th January 2013 for eligible systematic reviews without language restrictions. Furthermore we included articles from two previous systematic reviews by Peters et al. and Korevaar et al. Results: The literature search and screening process resulted in 512 included full text articles. We found an increasing number of published preclinical systematic reviews over time. The methodological quality of preclinical systematic reviews was low. The majority of preclinical systematic reviews did not assess methodological quality of the included studies (71%), nor did they assess heterogeneity (81%) or dissemination bias (87%). Statistics quantifying the importance of clinical research citing systematic reviews of animal studies showed that clinical studies referred to the preclinical research mainly to justify their study or a future study (76%). Discussion: Preclinical systematic reviews may have an influence on clinical research but their methodological quality frequently remains low. Therefore, systematic reviews of animal research should be critically appraised before translating them to a clinical context. "], "author_display": ["Katharina F. Mueller", "Matthias Briel", "Daniel Strech", "Joerg J. Meerpohl", "Britta Lang", "Edith Motschall", "Viktoria Gloy", "Francois Lamontagne", "Dirk Bassler"], "article_type": "Research Article", "score": 0.42435992, "title_display": "Dissemination Bias in Systematic Reviews of Animal Research: A Systematic Review", "publication_date": "2014-12-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0116016"}, {"journal": "PLoS ONE", "abstract": ["\nIn this paper, using the Lie group analysis method, we study the invariance properties of the time fractional fifth-order KdV equation. A systematic research to derive Lie point symmetries to time fractional fifth-order KdV equation is performed. In the sense of point symmetry, all of the vector fields and the symmetry reductions of the fractional fifth-order KdV equation are obtained. At last, by virtue of the sub-equation method, some exact solutions to the fractional fifth-order KdV equation are provided.\n"], "author_display": ["Gang wei Wang", "Tian zhou Xu", "Tao Feng"], "article_type": "Research Article", "score": 0.42433214, "title_display": "Lie Symmetry Analysis and Explicit Solutions of the Time Fractional Fifth-Order KdV Equation", "publication_date": "2014-02-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0088336"}, {"journal": "PLoS ONE", "abstract": ["Background: Kymograph analysis is a method widely used by researchers to analyze particle dynamics in one dimensional (1D) trajectories. Results: Here we provide a Visual Basic-coded algorithm to use as a Microsoft Excel add-in that automatically analyzes particles in 2D trajectories with all the advantages of kymograph analysis. Conclusions: This add-in, which we named SkyPad, leads to significant time saving and higher accuracy of particle analysis. Finally, SkyPad can also be used for 3D trajectories analysis. "], "author_display": ["Bruno Cadot", "Vincent Gache", "Edgar R. Gomes"], "article_type": "Research Article", "score": 0.42413872, "title_display": "Fast, Multi-Dimensional and Simultaneous Kymograph-Like Particle Dynamics (SkyPad) Analysis", "publication_date": "2014-02-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0089073"}, {"journal": "PLoS ONE", "abstract": ["\nMatrix assisted laser desorption/ionization time-of-flight (MALDI-TOF) coupled with stable isotope standards (SIS) has been used to quantify native peptides. This peptide quantification by MALDI-TOF approach has difficulties quantifying samples containing peptides with ion currents in overlapping spectra. In these overlapping spectra the currents sum together, which modify the peak heights and make normal SIS estimation problematic. An approach using Gaussian mixtures based on known physical constants to model the isotopic cluster of a known compound is proposed here. The characteristics of this approach are examined for single and overlapping compounds. The approach is compared to two commonly used SIS quantification methods for single compound, namely Peak Intensity method and Riemann sum area under the curve (AUC) method. For studying the characteristics of the Gaussian mixture method, Angiotensin II, Angiotensin-2-10, and Angiotenisn-1-9 and their associated SIS peptides were used. The findings suggest, Gaussian mixture method has similar characteristics as the two methods compared for estimating the quantity of isolated isotopic clusters for single compounds. All three methods were tested using MALDI-TOF mass spectra collected for peptides of the renin-angiotensin system. The Gaussian mixture method accurately estimated the native to labeled ratio of several isolated angiotensin peptides (5.2% error in ratio estimation) with similar estimation errors to those calculated using peak intensity and Riemann sum AUC methods (5.9% and 7.7%, respectively). For overlapping angiotensin peptides, (where the other two methods are not applicable) the estimation error of the Gaussian mixture was 6.8%, which is within the acceptable range. In summary, for single compounds the Gaussian mixture method is equivalent or marginally superior compared to the existing methods of peptide quantification and is capable of quantifying overlapping (convolved) peptides within the acceptable margin of error.\n"], "author_display": ["John Christian G. Spainhour", "Michael G. Janech", "John H. Schwacke", "Juan Carlos Q. Velez", "Viswanathan Ramakrishnan"], "article_type": "Research Article", "score": 0.4240843, "title_display": "The Application of Gaussian Mixture Models for Signal Quantification in MALDI-ToF Mass Spectrometry of Peptides", "publication_date": "2014-11-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0111016"}, {"journal": "PLoS ONE", "abstract": ["Background: It has been well established that theoretical kernel for recently surging genome-wide association study (GWAS) is statistical inference of linkage disequilibrium (LD) between a tested genetic marker and a putative locus affecting a disease trait. However, LD analysis is vulnerable to several confounding factors of which population stratification is the most prominent. Whilst many methods have been proposed to correct for the influence either through predicting the structure parameters or correcting inflation in the test statistic due to the stratification, these may not be feasible or may impose further statistical problems in practical implementation. Methodology: We propose here a novel statistical method to control spurious LD in GWAS from population structure by incorporating a control marker into testing for significance of genetic association of a polymorphic marker with phenotypic variation of a complex trait. The method avoids the need of structure prediction which may be infeasible or inadequate in practice and accounts properly for a varying effect of population stratification on different regions of the genome under study. Utility and statistical properties of the new method were tested through an intensive computer simulation study and an association-based genome-wide mapping of expression quantitative trait loci in genetically divergent human populations. Results/Conclusions: The analyses show that the new method confers an improved statistical power for detecting genuine genetic association in subpopulations and an effective control of spurious associations stemmed from population structure when compared with other two popularly implemented methods in the literature of GWAS. "], "author_display": ["Ning Jiang", "Minghui Wang", "Tianye Jia", "Lin Wang", "Lindsey Leach", "Christine Hackett", "David Marshall", "Zewei Luo"], "article_type": "Research Article", "score": 0.42407504, "title_display": "A Robust Statistical Method for Association-Based eQTL Analysis", "publication_date": "2011-08-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0023192"}, {"abstract": ["\n        QTL (quantitative trait loci) mapping is commonly used to identify genetic regions responsible to important phenotype variation. A common strategy of QTL mapping is to use recombinant inbred lines (RILs), which are usually established by several generations of inbreeding of an F1 population (usually up to F6 or F7 populations). As this inbreeding process involves a large amount of labor, we are particularly interested in the effect of the number of inbreeding generations on the power of QTL mapping; a part of the labor could be saved if a smaller number of inbreeding provides sufficient power. By using simulations, we investigated the performance of QTL mapping with recombinant inbred lines (RILs). As expected, we found that the power of F4 population could be almost comparable to that of F6 and F7 populations. A potential problem in using F4 population is that a large proportion of RILs are heterozygotes. We here introduced a new method to partly relax this problem. The performance of this method was verified by simulations with a wide range of parameters including the size of the segregation population, recombination rate, genome size and the density of markers. We found our method works better than the commonly used standard method especially when there are a number of heterozygous markers. Our results imply that in most cases, QTL mapping does not necessarily require RILs at F6 or F7 generations; rather, F4 (or even F3) populations would be almost as useful as F6 or F7 populations. Because the cost to establish a number of RILs for many generations is enormous, this finding will cause a reduction in the cost of QTL mapping, thereby accelerating gene mapping in many species.\n      "], "author_display": ["Shohei Takuno", "Ryohei Terauchi", "Hideki Innan"], "article_type": "Research Article", "score": 0.4240389, "title_display": "The Power of QTL Mapping with RILs", "publication_date": "2012-10-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0046545"}, {"journal": "PLOS ONE", "abstract": ["\nDetermining the composition of a bird\u2019s diet and its seasonal shifts are fundamental for understanding the ecology and ecological functions of a species. Various methods have been used to estimate the dietary compositions of birds, which have their own advantages and disadvantages. In this study, we examined the possibility of using long-term volunteer monitoring data as the source of dietary information for 15 resident bird species in Kanagawa Prefecture, Japan. The data were collected from field observations reported by volunteers of regional naturalist groups. Based on these monitoring data, we calculated the monthly dietary composition of each bird species directly, and we also estimated unidentified items within the reported foraging episodes using Bayesian models that contained additional information regarding foraging locations. Next, to examine the validity of the estimated dietary compositions, we compared them with the dietary information for focal birds based on stomach analysis methods, collected from past literatures. The dietary trends estimated from the monitoring data were largely consistent with the general food habits determined from the previous studies of focal birds. Thus, the estimates based on the volunteer monitoring data successfully detected noticeable seasonal shifts in many of the birds from plant materials to animal diets during spring\u2014summer. Comparisons with stomach analysis data supported the qualitative validity of the monitoring-based dietary information and the effectiveness of the Bayesian models for improving the estimates. This comparison suggests that one advantage of using monitoring data is its ability to detect dietary items such as fleshy fruits, flower nectar, and vertebrates. These results emphasize the potential importance of observation data collecting and mining by citizens, especially free descriptive observation data, for use in bird ecology studies.\n"], "author_display": ["Tetsuro Yoshikawa", "Yutaka Osada"], "article_type": "Research Article", "score": 0.42386472, "title_display": "Dietary Compositions and Their Seasonal Shifts in Japanese Resident Birds, Estimated from the Analysis of Volunteer Monitoring Data", "publication_date": "2015-02-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0119324"}, {"journal": "PLoS ONE", "abstract": ["Background: The collection of accurate data on adherence and sexual behaviour is crucial in microbicide (and other HIV-related) research. In the absence of a \u201cgold standard\u201d the collection of such data relies largely on participant self-reporting. After reviewing available methods, this paper describes a mixed method/triangulation model for generating more accurate data on adherence and sexual behaviour in a multi-centre vaginal microbicide clinical trial. In a companion paper some of the results from this model are presented [1]. Methodology/Principal Findings: Data were collected from a random subsample of 725 women (7.7% of the trial population) using structured interviews, coital diaries, in-depth interviews, counting returned gel applicators, focus group discussions, and ethnography. The core of the model was a customised, semi-structured in-depth interview. There were two levels of triangulation: first, discrepancies between data from the questionnaires, diaries, in-depth interviews and applicator returns were identified, discussed with participants and, to a large extent, resolved; second, results from individual participants were related to more general data emerging from the focus group discussions and ethnography. A democratic and equitable collaboration between clinical trialists and qualitative social scientists facilitated the success of the model, as did the preparatory studies preceding the trial. The process revealed some of the underlying assumptions and routinised practices in \u201cclinical trial culture\u201d that are potentially detrimental to the collection of accurate data, as well as some of the shortcomings of large qualitative studies, and pointed to some potential solutions. Conclusions/Significance: The integration of qualitative social science and the use of mixed methods and triangulation in clinical trials are feasible, and can reveal (and resolve) inaccuracies in data on adherence and sensitive behaviours, as well as illuminating aspects of \u201ctrial culture\u201d that may also affect data accuracy. "], "author_display": ["Robert Pool", "Catherine M. Montgomery", "Neetha S. Morar", "Oliver Mweemba", "Agnes Ssali", "Mitzy Gafos", "Shelley Lees", "Jonathan Stadler", "Angela Crook", "Andrew Nunn", "Richard Hayes", "Sheena McCormack"], "article_type": "Research Article", "score": 0.4237774, "title_display": "A Mixed Methods and Triangulation Model for Increasing the Accuracy of Adherence and Sexual Behaviour Data: The Microbicides Development Programme", "publication_date": "2010-07-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0011600"}, {"journal": "PLOS ONE", "abstract": ["\nCalreticulin (CALR) mutations have recently been reported in 70\u201384% of JAK2V617F-negative myeloproliferative neoplasms (MPN), and this detection has become necessary to improve the diagnosis of MPN. In a large single-centre cohort of 298 patients suffering from Essential Thrombocythemia (ET), the JAK2V617F, CALR and MPL mutations were noted in 179 (60%), 56 (18.5%) and 13 (4.5%) respectively. For the detection of the CALR mutations, three methods were compared in parallel: high-resolution melting-curve analysis (HRM), product-sizing analysis and Sanger sequencing. The sensitivity for the HRM, product-sizing analysis and Sanger sequencing was 96.4%, 98.2% and 89.3% respectively, whereas the specificity was 96.3%, 100% and 100%. In our cohort, the product-sizing analysis was the most sensitive method and was the easiest to interpret, while the HRM was sometimes difficult to interpret. In contrast, when large series of samples were tested, HRM provided results more quickly than did the other methods, which required more time. Finally, the sequencing method, which is the reference method, had the lowest sensitivity but can be used to describe the type of mutation precisely. Altogether, our results suggest that in routine laboratory practice, product-sizing analysis is globally similar to HRM for the detection of CALR mutations, and that both may be used as first-line screening tests. If the results are positive, Sanger sequencing can be used to confirm the mutation and to determine its type. Product-sizing analysis provides sensitive and specific results, moreover, with the quantitative measurement of CALR, which might be useful to monitor specific treatments.\n"], "author_display": ["Ji-Hye Park", "Margaux Sevin", "Selim Ramla", "Aur\u00e9lie Truffot", "Tiffany Verrier", "Dominique Bouchot", "Martine Courtois", "Mathilde Bas", "Sonia Benali", "Fran\u00e7ois Bailly", "Bernardine Favre", "Julien Guy", "Laurent Martin", "Marc Maynadi\u00e9", "Serge Carillo", "Fran\u00e7ois Girodon"], "article_type": "Research Article", "score": 0.4236918, "title_display": "Calreticulin Mutations in Myeloproliferative Neoplasms: Comparison of Three Diagnostic Methods", "publication_date": "2015-10-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0141010"}, {"journal": "PLoS ONE", "abstract": ["Background: To evaluate the reports\u2019 qualities which are about randomized controlled trials (RCTs) of acupuncture treatment on Diabetic Peripheral Neuropathy (DPN). Methodology/Principal Findings: Eight databases including The Cochrane Library(1993\u2013Sept.,2011), PubMed (1980\u2013Sept., 2011), EMbase (1980\u2013Sept.,2011), SCI Expanded (1998\u2013Sept.,2011), China Biomedicine Database Disc (CBMdisc, 1978\u2013Sept., 2011), China National Knowledge Infrastructure (CNKI, 1979\u2013Sept., 2011 ), VIP (a full text issues database of China, 1989\u2013Sept., 2011), Wan Fang (another full text issues database of China 1998\u2013Sept., 2011) were searched systematically. Hand search for further references was conducted. Language was limited to Chinese and English. We identified 75 RCTs that used acupuncture as an intervention and assessed the quality of these reports with the Consolidated Standards for Reporting of Trials statement 2010 (CONSORT2010) and Standards for Reporting Interventions Controlled Trials of Acupuncture 2010(STRICTA2010). 24 articles (32%) applied the method of random allocation of sequences. No article gave the description of the mechanism of allocation concealment, no experiment applied the method of blinding. Only one article (1.47%) could be identified directly from its title as about the Randomized Controlled Trials, and only 4 articles gave description of the experimental design. No article mentioned the number of cases lost or eliminated. During one experiment, acupuncture syncope led to temporal interruption of the therapy. Two articles (2.94%) recorded the number of needles, and 8 articles (11.76%) mentioned the depth of needle insertion. None of articles reported the base of calculation of sample size, or has any analysis about the metaphase of an experiment or an explanation of its interruption. One (1.47%) mentioned intentional analysis (ITT). Conclusions/Significance: The quality of the reports on RCTs of acupuncture for Diabetic Peripheral Neuropathy is moderate to low. The CONSORT2010 and STRICTA2010 should be used to standardize the reporting of RCTs of acupuncture in future. "], "author_display": ["Chen Bo", "Zhao Xue", "Guo Yi", "Chen Zelin", "Bai Yang", "Wang Zixu", "Wang Yajun"], "article_type": "Research Article", "score": 0.4234426, "title_display": "Assessing the Quality of Reports about Randomized Controlled Trials of Acupuncture Treatment on Diabetic Peripheral Neuropathy", "publication_date": "2012-07-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0038461"}, {"journal": "PLoS ONE", "abstract": ["\n        The gene regulatory network (GRN) reveals the regulatory relationships among genes and can provide a systematic understanding of molecular mechanisms underlying biological processes. The importance of computer simulations in understanding cellular processes is now widely accepted; a variety of algorithms have been developed to study these biological networks. The goal of this study is to provide a comprehensive evaluation and a practical guide to aid in choosing statistical methods for constructing large scale GRNs. Using both simulation studies and a real application in E. coli data, we compare different methods in terms of sensitivity and specificity in identifying the true connections and the hub genes, the ease of use, and computational speed. Our results show that these algorithms performed reasonably well, and each method has its own advantages: (1) GeneNet, WGCNA (Weighted Correlation Network Analysis), and ARACNE (Algorithm for the Reconstruction of Accurate Cellular Networks) performed well in constructing the global network structure; (2) GeneNet and SPACE (Sparse PArtial Correlation Estimation) performed well in identifying a few connections with high specificity.\n      "], "author_display": ["Jeffrey D. Allen", "Yang Xie", "Min Chen", "Luc Girard", "Guanghua Xiao"], "article_type": "Research Article", "score": 0.42340907, "title_display": "Comparing Statistical Methods for Constructing Large Scale Gene Networks", "publication_date": "2012-01-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0029348"}, {"journal": "PLoS ONE", "abstract": ["\nThe overall control of the quality of botanical drugs starts from the botanical raw material, continues through preparation of the botanical drug substance and culminates with the botanical drug product. Chromatographic and spectroscopic fingerprinting has been widely used as a tool for the quality control of herbal/botanical medicines. However, discussions are still on-going on whether a single technique provides adequate information to control the quality of botanical drugs. In this study, high performance liquid chromatography (HPLC), ultra performance liquid chromatography (UPLC), capillary electrophoresis (CE) and near infrared spectroscopy (NIR) were used to generate fingerprints of different plant parts of Panax notoginseng. The power of these chromatographic and spectroscopic techniques to evaluate the identity of botanical raw materials were further compared and investigated in light of the capability to distinguishing different parts of Panax notoginseng. Principal component analysis (PCA) and clustering results showed that samples were classified better when UPLC- and HPLC-based fingerprints were employed, which suggested that UPLC- and HPLC-based fingerprinting are superior to CE- and NIR-based fingerprinting. The UPLC- and HPLC- based fingerprinting with PCA were able to correctly distinguish between samples sourced from rhizomes and main root. Using chemometrics and its ability to distinguish between different plant parts could be a powerful tool to help assure the identity and quality of the botanical raw materials and to support the safety and efficacy of the botanical drug products.\n"], "author_display": ["Jieqiang Zhu", "Xiaohui Fan", "Yiyu Cheng", "Rajiv Agarwal", "Christine M. V. Moore", "Shaw T. Chen", "Weida Tong"], "article_type": "Research Article", "score": 0.42324772, "title_display": "Chemometric Analysis for Identification of Botanical Raw Materials for Pharmaceutical Use: A Case Study Using <i>Panax notoginseng</i>", "publication_date": "2014-01-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0087462"}, {"abstract": ["\n        Like formalin fixed paraffin embedded (FFPE) tissues, archived bone marrow aspirate slides are an abundant and untapped resource of biospecimens that could enable retrospective molecular studies of disease. Historically, RNA obtained from slides is limited in utility because of their low quality and highly fragmented nature. MicroRNAs are small (\u224822 nt) non-coding RNA that regulate gene expression, and are speculated to preserve well in FFPE tissue. Here we investigate the use of archived bone marrow aspirate slides for miRNA expression analysis in paediatric leukaemia. After determining the optimal method of miRNA extraction, we used TaqMan qRT-PCR to identify reference miRNA for normalisation of other miRNA species. We found hsa-miR-16 and hsa-miR-26b to be the most stably expressed between lymphoblastoid cell lines, primary bone marrow aspirates and archived samples. We found the average fold change in expression of hsa-miR-26b and two miRNA reportedly dysregulated in leukaemia (hsa-miR-128a, hsa-miR-223) was <0.5 between matching archived slide and bone marrow aspirates. Differential expression of hsa-miR-128a and hsa-miR-223 was observed between leukaemic and non-leukaemic bone marrow from archived slides or flash frozen bone marrow. The demonstration that archived bone marrow aspirate slides can be utilized for miRNA expression studies offers tremendous potential for future investigations into the role miRNA play in the development and long term outcome of hematologic, as well as non-hematologic, diseases.\n      "], "author_display": ["Leah Morenos", "Richard Saffery", "Francoise Mechinaud", "David Ashley", "Ngaire Elwood", "Jeffrey M. Craig", "Nicholas C. Wong"], "article_type": "Research Article", "score": 0.42253506, "title_display": "Evaluation of MicroRNA Expression in Patient Bone Marrow Aspirate Slides", "publication_date": "2012-08-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0042951"}, {"journal": "PLoS ONE", "abstract": ["\nA major challenge in current systems biology is the combination and integrative analysis of large data sets obtained from different high-throughput omics platforms, such as mass spectrometry based Metabolomics and Proteomics or DNA microarray or RNA-seq-based Transcriptomics. Especially in the case of non-targeted Metabolomics experiments, where it is often impossible to unambiguously map ion features from mass spectrometry analysis to metabolites, the integration of more reliable omics technologies is highly desirable. A popular method for the knowledge-based interpretation of single data sets is the (Gene) Set Enrichment Analysis. In order to combine the results from different analyses, we introduce a methodical framework for the meta-analysis of p-values obtained from Pathway Enrichment Analysis (Set Enrichment Analysis based on pathways) of multiple dependent or independent data sets from different omics platforms. For dependent data sets, e.g. obtained from the same biological samples, the framework utilizes a covariance estimation procedure based on the nonsignificant pathways in single data set enrichment analysis. The framework is evaluated and applied in the joint analysis of Metabolomics mass spectrometry and Transcriptomics DNA microarray data in the context of plant wounding. In extensive studies of simulated data set dependence, the introduced correlation could be fully reconstructed by means of the covariance estimation based on pathway enrichment. By restricting the range of p-values of pathways considered in the estimation, the overestimation of correlation, which is introduced by the significant pathways, could be reduced. When applying the proposed methods to the real data sets, the meta-analysis was shown not only to be a powerful tool to investigate the correlation between different data sets and summarize the results of multiple analyses but also to distinguish experiment-specific key pathways.\n"], "author_display": ["Alexander Kaever", "Manuel Landesfeind", "Kirstin Feussner", "Burkhard Morgenstern", "Ivo Feussner", "Peter Meinicke"], "article_type": "Research Article", "score": 0.42235988, "title_display": "Meta-Analysis of Pathway Enrichment: Combining Independent and Dependent Omics Data Sets", "publication_date": "2014-02-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0089297"}, {"journal": "PLoS ONE", "abstract": ["Introduction: The application of health economic evaluation (HEE) evidence can play an important role in strategic planning and policy making. This study aimed to assess the scope and quality of existing research, with the goal of elucidating implications for improving the use of HEE evidence in Vietnam. Methods: A comprehensive search strategy was developed to search medical online databases (Medline, Google Scholar, and Vietnam Medical Databases) to select all types of HEE studies except cost-only analyses. Two researchers assessed the quality of selected studies using the Quality of Health Economic Studies (QHES) instrument. Results: We selected 26 studies, including 6 published in Vietnam. The majority of these studies focused on infectious diseases (14 studies), with HIV being the most common topic (5 studies). Most papers were cost-effectiveness studies that measured health outcomes using DALY units. Using QHES, we found that the overall quality of HEE studies published internationally was much higher (mean score 88.7+13.3) than that of those published in Vietnam (mean score 67.3+22.9). Lack of costing perspectives, reliable data sources and sensitivity analysis were the main shortcomings of the reviewed studies. Conclusion: This review indicates that HEE studies published in Vietnam are limited in scope and number, as well as by several important technical errors or omissions. It is necessary to formalize the process of health economic research in Vietnam and to institutionalize the links between researchers and policy-makers. Additionally, the quality of HEE should be enhanced through education about research techniques, and the implementation of standard HEE guidelines. "], "author_display": ["Bach Xuan Tran", "Vuong Minh Nong", "Rachel Marie Maher", "Phuong Khanh Nguyen", "Hoat Ngoc Luu"], "article_type": "Research Article", "score": 0.42207018, "title_display": "A Systematic Review of Scope and Quality of Health Economic Evaluation Studies in Vietnam", "publication_date": "2014-08-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0103825"}, {"journal": "PLoS ONE", "abstract": ["\nMeta-analysis methods that combine -values into a single unified -value are frequently employed to improve confidence in hypothesis testing. An assumption made by most meta-analysis methods is that the -values to be combined are independent, which may not always be true. To investigate the accuracy of the unified -value from combining correlated -values, we have evaluated a family of statistical methods that combine: independent, weighted independent, correlated, and weighted correlated -values. Statistical accuracy evaluation by combining simulated correlated -values showed that correlation among -values can have a significant effect on the accuracy of the combined -value obtained. Among the statistical methods evaluated those that weight -values compute more accurate combined -values than those that do not. Also, statistical methods that utilize the correlation information have the best performance, producing significantly more accurate combined -values. In our study we have demonstrated that statistical methods that combine -values based on the assumption of independence can produce inaccurate -values when combining correlated -values, even when the -values are only weakly correlated. Therefore, to prevent from drawing false conclusions during hypothesis testing, our study advises caution be used when interpreting the -value obtained from combining -values of unknown correlation. However, when the correlation information is available, the weighting-capable statistical method, first introduced by Brown and recently modified by Hou, seems to perform the best amongst the methods investigated.\n"], "author_display": ["Gelio Alves", "Yi-Kuo Yu"], "article_type": "Research Article", "score": 0.4219916, "title_display": "Accuracy Evaluation of the Unified <i>P</i>-Value from Combining Correlated <i>P</i>-Values", "publication_date": "2014-03-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0091225"}, {"journal": "PLoS ONE", "abstract": ["Background: In microarray data analysis, hierarchical clustering (HC) is often used to group samples or genes according to their gene expression profiles to study their associations. In a typical HC, nested clustering structures can be quickly identified in a tree. The relationship between objects is lost, however, because clusters rather than individual objects are compared. This results in a tree that is hard to interpret. Methodology/Principal Findings: This study proposes an ordering method, HC-SYM, which minimizes bilateral symmetric distance of two adjacent clusters in a tree so that similar objects in the clusters are located in the cluster boundaries. The performance of HC-SYM was evaluated by both supervised and unsupervised approaches and compared favourably with other ordering methods. Conclusions/Significance: The intuitive relationship between objects and flexibility of the HC-SYM method can be very helpful in the exploratory analysis of not only microarray data but also similar high-dimensional data. "], "author_display": ["Minho Chae", "James J. Chen"], "article_type": "Research Article", "score": 0.42182755, "title_display": "Reordering Hierarchical Tree Based on Bilateral Symmetric Distance", "publication_date": "2011-08-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0022546"}, {"abstract": ["\n        Statistical methods to test for differential expression traditionally assume that each gene's expression summaries are independent across arrays. When certain preprocessing methods are used to obtain those summaries, this assumption is not necessarily true. In general, the erroneous assumption of dependence results in a loss of statistical power. We introduce a diagnostic measure of numerical dependence for gene expression summaries from any preprocessing method and discuss the relative performance of several common preprocessing methods with respect to this measure. Some common preprocessing methods introduce non-trivial levels of numerical dependence. The issue of (between-array) dependence has received little if any attention in the literature, and researchers working with gene expression data should not take such properties for granted, or they risk unnecessarily losing statistical power.\n      "], "author_display": ["John R. Stevens", "Gabriel Nicholas"], "article_type": "Research Article", "score": 0.42155564, "title_display": "Assessing Numerical Dependence in Gene Expression Summaries with the Jackknife Expression Difference", "publication_date": "2012-08-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0039570"}, {"journal": "PLoS ONE", "abstract": ["\n        Genome-wide association studies have been able to identify disease associations with many common variants; however most of the estimated genetic contribution explained by these variants appears to be very modest. Rare variants are thought to have larger effect sizes compared to common SNPs but effects of rare variants cannot be tested in the GWAS setting. Here we propose a novel method to test for association of rare variants obtained by sequencing in family-based samples by collapsing the standard family-based association test (FBAT) statistic over a region of interest. We also propose a suitable weighting scheme so that low frequency SNPs that may be enriched in functional variants can be upweighted compared to common variants. Using simulations we show that the family-based methods perform at par with the population-based methods under no population stratification. By construction, family-based tests are completely robust to population stratification; we show that our proposed methods remain valid even when population stratification is present.\n      "], "author_display": ["Gourab De", "Wai-Ki Yip", "Iuliana Ionita-Laza", "Nan Laird"], "article_type": "Research Article", "score": 0.4213818, "title_display": "Rare Variant Analysis for Family-Based Design", "publication_date": "2013-01-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0048495"}, {"journal": "PLoS ONE", "abstract": ["\nMicroarrays are widely used for examining differential gene expression, identifying single nucleotide polymorphisms, and detecting methylation loci. Multiple testing methods in microarray data analysis aim at controlling both Type I and Type II error rates; however, real microarray data do not always fit their distribution assumptions. Smyth's ubiquitous parametric method, for example, inadequately accommodates violations of normality assumptions, resulting in inflated Type I error rates. The Significance Analysis of Microarrays, another widely used microarray data analysis method, is based on a permutation test and is robust to non-normally distributed data; however, the Significance Analysis of Microarrays method fold change criteria are problematic, and can critically alter the conclusion of a study, as a result of compositional changes of the control data set in the analysis. We propose a novel approach, combining resampling with empirical Bayes methods: the Resampling-based empirical Bayes Methods. This approach not only reduces false discovery rates for non-normally distributed microarray data, but it is also impervious to fold change threshold since no control data set selection is needed. Through simulation studies, sensitivities, specificities, total rejections, and false discovery rates are compared across the Smyth's parametric method, the Significance Analysis of Microarrays, and the Resampling-based empirical Bayes Methods. Differences in false discovery rates controls between each approach are illustrated through a preterm delivery methylation study. The results show that the Resampling-based empirical Bayes Methods offer significantly higher specificity and lower false discovery rates compared to Smyth's parametric method when data are not normally distributed. The Resampling-based empirical Bayes Methods also offers higher statistical power than the Significance Analysis of Microarrays method when the proportion of significantly differentially expressed genes is large for both normally and non-normally distributed data. Finally, the Resampling-based empirical Bayes Methods are generalizable to next generation sequencing RNA-seq data analysis.\n"], "author_display": ["Dongmei Li", "Marc A. Le Pape", "Nisha I. Parikh", "Will X. Chen", "Timothy D. Dye"], "article_type": "Research Article", "score": 0.4211222, "title_display": "Assessing Differential Expression in Two-Color Microarrays: A Resampling-Based Empirical Bayes Approach", "publication_date": "2013-11-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0080099"}, {"journal": "PLoS ONE", "abstract": ["Background: Consortia of microorganisms, commonly known as biofilms, are attracting much attention from the scientific community due to their impact in human activity. As biofilm research grows to be a data-intensive discipline, the need for suitable bioinformatics approaches becomes compelling to manage and validate individual experiments, and also execute inter-laboratory large-scale comparisons. However, biofilm data is widespread across ad hoc, non-standardized individual files and, thus, data interchange among researchers, or any attempt of cross-laboratory experimentation or analysis, is hardly possible or even attempted. Methodology/Principal Findings: This paper presents BiofOmics, the first publicly accessible Web platform specialized in the management and analysis of data derived from biofilm high-throughput studies. The aim is to promote data interchange across laboratories, implementing collaborative experiments, and enable the development of bioinformatics tools in support of the processing and analysis of the increasing volumes of experimental biofilm data that are being generated. BiofOmics\u2019 data deposition facility enforces data structuring and standardization, supported by controlled vocabulary. Researchers are responsible for the description of the experiments, their results and conclusions. BiofOmics\u2019 curators interact with submitters only to enforce data structuring and the use of controlled vocabulary. Then, BiofOmics\u2019 search facility makes publicly available the profile and data associated with a submitted study so that any researcher can profit from these standardization efforts to compare similar studies, generate new hypotheses to be tested or even extend the conditions experimented in the study. Significance: BiofOmics\u2019 novelty lies in its support to standardized data deposition, the availability of computerizable data files and the free-of-charge dissemination of biofilm studies across the community. Hopefully, this will open promising research possibilities, namely the comparison of results between different laboratories, the reproducibility of methods within and between laboratories, and the development of guidelines and standardized protocols for biofilm formation operating procedures and analytical methods. "], "author_display": ["An\u00e1lia Louren\u00e7o", "Andreia Ferreira", "Nuno Veiga", "Idalina Machado", "Maria Olivia Pereira", "Nuno F. Azevedo"], "article_type": "Research Article", "score": 0.42094094, "title_display": "BiofOmics: A Web Platform for the Systematic and Standardized Collection of High-Throughput Biofilm Data", "publication_date": "2012-06-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0039960"}, {"journal": "PLoS ONE", "abstract": ["Background: We sought to analyze the growing worldwide trends of intracranial aneurysm research, investigate China's recent contribution, and compare the contributions of mainland China, Taiwan, and Hong Kong. Methods: Global and China intracranial aneurysm-related publications were retrieved from the Web of Science database from 1991 to 2012. Excel 2007, Matlab, and Thomson Data Analyzer (TDA) software were used to analyze the search results for number of publications, cited frequency, h-index, and organization contributions. Results: 16468 global papers were identified that were cited 273500 times until 2013-08-15. The United States accounted for 31.497% of the articles, 58.64% of the citations, and the highest h-index (127). Japan and Germany followed in frequency. China's articles ranked eighth (third in 2012) in total number, with most of the contributions occurring since 2002 (91.33%). China was at the early stage of the logic growth curve (exponential growth), with the citation frequency and h-index per year increasing. The quality of the publications was low. The main research centers were located in Beijing, Shanghai, Taiwan, and Hong Kong. The main Asian funding body was the National Natural Science Foundation of China. The number of publications and frequency of citations of papers from mainland China was greater than that of Taiwan or Hong Kong. Conclusion: Global intracranial aneurysm research has been developing swiftly since 1991, with the United States making the largest contribution. Research in China started later, in 2002. Since then, China has increased its rate of publication, and became the third largest contributor by 2012. "], "author_display": ["Ze-jun Jia", "Bo Hong", "Da-ming Chen", "Qing-hai Huang", "Zhi-gang Yang", "Cha Yin", "Xiao-qun Deng", "Jian-min Liu"], "article_type": "Research Article", "score": 0.42092338, "title_display": "China's Growing Contribution to Global Intracranial Aneurysm Research (1991\u20132012): A Bibliometric Study", "publication_date": "2014-03-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0091594"}, {"journal": "PLoS ONE", "abstract": ["\n\t\t\t\tFraming, the effect of context on cognitive processes, is a prominent topic of research in psychology and public opinion research. Research on framing has traditionally relied on controlled experiments and manually annotated document collections. In this paper we present a method that allows for quantifying the relative strengths of competing linguistic frames based on corpus analysis. This method requires little human intervention and can therefore be efficiently applied to large bodies of text. We demonstrate its effectiveness by tracking changes in the framing of terror over time and comparing the framing of abortion by Democrats and Republicans in the U.S.\n\t\t\t"], "author_display": ["Eyal Sagi", "Daniel Diermeier", "Stefan Kaufmann"], "article_type": "Research Article", "score": 0.42080405, "title_display": "Identifying Issue Frames in Text", "publication_date": "2013-07-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0069185"}, {"journal": "PLoS ONE", "abstract": ["Background: Recombination is one of the major mechanisms underlying the generation of HIV-1 variability. Currently 61 circulating recombinant forms of HIV-1 have been identified. With the development of recombination detection techniques and accumulation of HIV-1 reference stains, more accurate mosaic structures of circulating recombinant forms (CRFs), like CRF04 and CRF06, have undergone repeated analysis and upgrades. Such revisions may also be necessary for other CRFs. Unlike previous studies, whose results are based primarily on a single recombination detection program, the current study was based on multiple recombination analysis, which may have produced more impartial results. Methods: Representative references of 3 categories of intersubtype recombinants were selected, including BC recombinants (CRF07 and CRF08), BG recombinants (CRF23 and CRF24), and BF recombinants (CRF38 and CRF44). They were reanalyzed in detail using both the jumping profile hidden Markov model and RDP3. Results: The results indicate that revisions and upgrades are very necessary and the entire re-analysis suggested 2 types of revision: (i) length of inserted fragments; and (ii) number of inserted fragments. The reanalysis also indicated that determination of small regions of about 200 bases or fewer should be performed with more caution. Conclusion: Results indicated that the involvement of multiple recombination detection programs is very necessary. Additionally, results suggested two major challenges, one involving the difficulty of accurately determining the locations of breakpoints and the second involving identification of small regions of about 200 bases or fewer with greater caution. Both indicate the complexity of HIV-1 recombination. The resolution would depend critically on development of a recombination analysis algorithm, accumulation of HIV-1 stains, and a higher sequencing quality. With the changes in recombination pattern, phylogenetic relationships of some CRFs may also change. All these results may be critical to understand the role of recombination in a complex and dynamic HIV evolution. "], "author_display": ["Lei Jia", "Lin Li", "Hanping Li", "Siyang Liu", "Xiaolin Wang", "Zuoyi Bao", "Tianyi Li", "Daomin Zhuang", "Yongjian Liu", "Jingyun Li"], "article_type": "Research Article", "score": 0.42073426, "title_display": "Recombination Pattern Reanalysis of Some HIV-1 Circulating Recombination Forms Suggest the Necessity and Difficulty of Revision", "publication_date": "2014-09-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0107349"}, {"journal": "PLOS ONE", "abstract": ["\nOrbital volume evaluation is an important part of pre-operative assessments in orbital trauma and congenital deformity patients. The availability of the affordable, open-source software, OsiriX, as a tool for preoperative planning increased the popularity of radiological assessments by the surgeon. A volume calculation method based on 3D volume rendering-assisted region-of-interest computation was used to determine the normal orbital volume in Taiwanese patients after reorientation to the Frankfurt plane. Method one utilized 3D points for intuitive orbital rim outlining. The mean normal orbital volume for left and right orbits was 24.3\u00b11.51 ml and 24.7\u00b11.17 ml in male and 21.0\u00b11.21 ml and 21.1\u00b11.30 ml in female subjects. Another method (method two) based on the bilateral orbital lateral rim was also used to calculate orbital volume and compared with method one. The mean normal orbital volume for left and right orbits was 19.0\u00b11.68 ml and 19.1\u00b11.45 ml in male and 16.0\u00b11.01 ml and 16.1\u00b10.92 ml in female subjects. The inter-rater reliability and intra-rater measurement accuracy between users for both methods was found to be acceptable for orbital volume calculations. 3D-assisted quantification of orbital volume is a feasible technique for orbital volume assessment. The normal orbital volume can be used as controls in cases of unilateral orbital reconstruction with a mean size discrepancy of less than 3.1\u00b12.03% in females and 2.7\u00b11.32% in males. The OsiriX software can be used reliably by the individual surgeon as a comprehensive preoperative planning and imaging tool for orbital volume measurement and computed tomography reorientation.\n"], "author_display": ["Victor Bong-Hang Shyu", "Chung-En Hsu", "Chih-hao Chen", "Chien-Tzung Chen"], "article_type": "Research Article", "score": 0.420391, "title_display": "3D-Assisted Quantitative Assessment of Orbital Volume Using an Open-Source Software Platform in a Taiwanese Population", "publication_date": "2015-03-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0119589"}, {"journal": "PLoS ONE", "abstract": ["Background: HIV diversity may be a useful biomarker for discriminating between recent and non-recent HIV infection. The high resolution melting (HRM) diversity assay was developed to quantify HIV diversity in viral populations without sequencing. In this assay, HIV diversity is expressed as a single numeric HRM score that represents the width of a melting peak. HRM scores are highly associated with diversity measures obtained with next generation sequencing. In this report, a software package, the HRM Diversity Assay Analysis Tool (DivMelt), was developed to automate calculation of HRM scores from melting curve data. Methods: DivMelt uses computational algorithms to calculate HRM scores by identifying the start (T1) and end (T2) melting temperatures for a DNA sample and subtracting them (T2\u2013T1\u200a=\u200aHRM score). DivMelt contains many user-supplied analysis parameters to allow analyses to be tailored to different contexts. DivMelt analysis options were optimized to discriminate between recent and non-recent HIV infection and to maximize HRM score reproducibility. HRM scores calculated using DivMelt were compared to HRM scores obtained using a manual method that is based on visual inspection of DNA melting curves. Results: HRM scores generated with DivMelt agreed with manually generated HRM scores obtained from the same DNA melting data. Optimal parameters for discriminating between recent and non-recent HIV infection were identified. DivMelt provided greater discrimination between recent and non-recent HIV infection than the manual method. Conclusion: DivMelt provides a rapid, accurate method of determining HRM scores from melting curve data, facilitating use of the HRM diversity assay for large-scale studies. "], "author_display": ["Matthew M. Cousins", "David Swan", "Craig A. Magaret", "Donald R. Hoover", "Susan H. Eshleman"], "article_type": "Research Article", "score": 0.4202171, "title_display": "Analysis of HIV Using a High Resolution Melting (HRM) Diversity Assay: Automation of HRM Data Analysis Enhances the Utility of the Assay for Analysis of HIV Incidence", "publication_date": "2012-12-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0051359"}, {"journal": "PLoS Medicine", "abstract": ["\nn/a\nBackground: There has been increasing interest in measuring under-five mortality as a health indicator and as a critical measure of human development. In countries with complete vital registration systems that capture all births and deaths, under-five mortality can be directly calculated. In the absence of a complete vital registration system, however, child mortality must be estimated using surveys that ask women to report the births and deaths of their children. Two survey methods exist for capturing this information: summary birth histories and complete birth histories. A summary birth history requires a minimum of only two questions: how many live births has each mother had and how many of them have survived. Indirect methods are then applied using the information from these two questions and the age of the mother to estimate under-five mortality going back in time prior to the survey. Estimates generated from complete birth histories are viewed as the most accurate when surveys are required to estimate under-five mortality, especially for the most recent time periods. However, it is much more costly and labor intensive to collect these detailed data, especially for the purpose of generating small area estimates. As a result, there is a demand for improvement of the methods employing summary birth history data to produce more accurate as well as subnational estimates of child mortality. Methods and Findings: We used data from 166 Demographic and Health Surveys (DHS) to develop new empirically based methods of estimating under-five mortality using children ever born and children dead data. We then validated them using both in- and out-of-sample analyses. We developed a range of methods on the basis of three dimensions of the problem: (1) approximating the average length of exposure to mortality from a mother's set of children using either maternal age or time since first birth; (2) using cohort and period measures of the fraction of children ever born that are dead; and (3) capturing country and regional variation in the age pattern of fertility and mortality. We focused on improving estimates in the most recent time periods prior to a survey where the traditional indirect methods fail. In addition, all of our methods incorporated uncertainty. Validated against under-five estimates generated from complete birth histories, our methods outperformed the standard indirect method by an average of 43.7% (95% confidence interval [CI] 41.2\u201345.2). In the 5 y prior to the survey, the new methods resulted in a 53.3% (95% CI 51.3\u201355.2) improvement. To illustrate the value of this method for local area estimation, we applied our new methods to an analysis of summary birth histories in the 1990, 2000, and 2005 Mexican censuses, generating subnational estimates of under-five mortality for each of 233 jurisdictions. Conclusions: The new methods significantly improve the estimation of under-five mortality using summary birth history data. In areas without vital registration data, summary birth histories can provide accurate estimates of child mortality. Because only two questions are required of a female respondent to generate these data, they can easily be included in existing survey programs as well as routine censuses of the population. With the wider application of these methods to census data, countries now have the means to generate estimates for subnational areas and population subgroups, important for measuring and addressing health inequalities and developing local policy to improve child survival. : Please see later in the article for the Editors' Summary Background: Every year, more than 8 million children die before their fifth birthdays. Most of these deaths occur in developing countries, and most are the result of diseases or combinations of diseases that could have been prevented or treated. Measles, for example, is a major killer in low-income countries and undernutrition contributes to one-third of childhood deaths. Faced with this largely avoidable loss of young lives, in 1990, the United Nations' World Summit for Children pledged to improve the survival of children. Later, in 2000, world leaders set a target of reducing child mortality to one-third of its 1990 level by 2015 as Millennium Development Goal 4. This goal, together with seven others, is designed to alleviate extreme poverty by 2015. In 2006, for the first time since mortality records began, annual deaths among children under five fell below 10 million as a result of public-health programs such as the Measles Initiative, which has reduced global measles mortality by more than two-thirds by vaccinating 500 million children, and the Nothing but Nets campaign, which distributed insecticide-treated antimalaria nets in Africa. Why Was This Study Done?: Although global under-five mortality is declining, it is unlikely that Millennium Development Goal 4 will be reached by 2015. Indeed, in some countries, little or no progress is being made toward this goal. To improve progress and to monitor the effects of public-health interventions, accurate, up-to-date estimates of national and subnational child mortality rates are essential. In developed countries, vital registration systems\u2014records of all births and deaths\u2014mean that under-five mortality rates can be directly calculated. But many developing countries lack vital registration systems, and child mortality has to be estimated using data collected in surveys. In \u201ccomplete birth history\u201d surveys, mothers are asked numerous questions about each living child and each dead child. Such surveys can be used to estimate under-five mortality accurately for recent time periods but they are expensive and time-consuming. By contrast, in \u201csummary birth history\u201d surveys, each mother is simply asked how many live births she had and how many of her children have survived. Under-five mortality can be indirectly calculated from this information and the age of the mother, but the current methods for making this calculation cannot provide reliable estimates of under-five mortality more recently than 3 years before the survey. In this study, therefore, the researchers develop methods for estimating more recent under-five mortality rates from summary birth histories. What Did the Researchers Do and Find?: The researchers used data about all children born and dead children extracted from 169 Demographic and Health Surveys (DHS; a project started in 1984 to help developing countries collect data on health and population trends) covering 70 countries to develop four new methods to estimate under-five mortality. They tested these new methods and a method that combined all four approaches by comparing the estimates of under-five mortality provided by these methods and the standard indirect method to the estimates obtained from an analysis of the complete birth data in the DHS. The new methods all outperformed the standard indirect method, particularly for the most recent 5 years. The researchers also used their new methods to generate estimates of under-five mortality for each of the 233 jurisdictions in Mexico from summary birth histories collected in the 1990, 2000, and 2005 Mexico censuses. The overall trends of these subnational estimates, they report, mirrored those obtained from vital registration data. What Do These Findings Mean?: These findings suggest that application of the new methods developed by the researchers could significantly improve the accuracy of estimates of under-five mortality based on summary birth history data. The researchers warn that although their methods can provide accurate estimates of recent under-five mortality, they might not capture rapid fluctuations in mortality such as those that occur during wars. However, they suggest, the two questions needed to generate the data required to apply these new methods could easily be included in existing survey programs and in routine censuses. Consequently, systematic application of the methods proposed in this study should provide policy makers with the information about levels, recent trends, and inequalities in child mortality that they need to accelerate efforts to reduce the global toll of childhood deaths. Additional Information: Please access these Web sites via the online version of this summary at http://dx.doi.org/10.1371/journal.pmed.1000253. "], "author_display": ["Julie Knoll Rajaratnam", "Linda N. Tran", "Alan D. Lopez", "Christopher J. L. Murray"], "article_type": "Research Article", "score": 0.42013347, "title_display": "Measuring Under-Five Mortality: Validation of New Low-Cost Methods", "publication_date": "2010-04-13T00:00:00Z", "eissn": "1549-1676", "id": "10.1371/journal.pmed.1000253"}, {"journal": "PLoS ONE", "abstract": ["\nThe marine bacterium Vibrio parahaemolyticus (V. parahaemolyticus) causes gastroenteritis in humans via the ingestion of raw or undercooked contaminated seafood, and early diagnosis and prompt treatment are important for the prevention of V. parahaemolyticus-related diseases. In this study, a real-time resistance measurement based on loop-mediated isothermal amplification (LAMP), electrochemical ion bonding (Crystal violet and Mg2+), real-time monitoring, and derivative analysis was developed. V. parahaemolyticus DNA was first amplified by LAMP, and the products (DNA and pyrophosphate) represented two types of negative ions that could combine with a positive dye (Crystal violet) and positive ions (Mg2+) to increase the resistance of the reaction liquid. This resistance was measured in real-time using a specially designed resistance electrode, thus permitting the quantitative detection of V. parahaemolyticus. The results were obtained in 1\u20132 hours, with a minimum bacterial density of 10 CFU.mL\u22121 and high levels of accuracy (97%), sensitivity (96.08%), and specificity (97.96%) when compared to cultivation methods. Therefore, this simple and rapid method has a potential application in the detection of V. parahaemolyticus on a gene chip or in point-of-care testing.\n"], "author_display": ["Guiming Xiang", "Xiaoyun Pu", "Dongneng Jiang", "Linlin Liu", "Chang Liu", "Xiaobo Liu"], "article_type": "Research Article", "score": 0.4200752, "title_display": "Development of a Real-Time Resistance Measurement for <i>Vibrio parahaemolyticus</i> Detection by the Lecithin-Dependent Hemolysin Gene", "publication_date": "2013-08-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0072342"}, {"journal": "PLOS ONE", "abstract": ["\nAlternative splicing is an important biological process in the generation of multiple functional transcripts from the same genomic sequences. Differential analysis of splice junctions (SJs) and intron retentions (IRs) is helpful in the detection of alternative splicing events. In this study, we conducted differential analysis of SJs and IRs by use of DEXSeq, a Bioconductor package originally designed for differential exon usage analysis in RNA-seq data analysis. We set up an analysis pipeline including mapping of RNA-seq reads, the preparation of count tables of SJs and IRs as the input files, and the differential analysis in DEXSeq. We analyzed the public RNA-seq datasets generated from RNAi experiments on Drosophila melanogaster S2-DRSC cells to deplete RNA-binding proteins (GSE18508). The analysis confirmed previous findings on the alternative splicing of the trol and Ant2 (sesB) genes in the CG8144 (ps)-depletion experiment and identified some new alternative splicing events in other RNAi experiments. We also identified IRs that were confirmed in our SJ analysis. The proposed method used in our study can output the genomic coordinates of differentially used SJs and thus enable sequence motif search. Sequence motif search and gene function annotation analysis helped us infer the underlying mechanism in alternative splicing events. To further evaluate this method, we also applied the method to public RNA-seq data from human breast cancer (GSE45419) and the plant Arabidopsis (SRP008262). In conclusion, our study showed that DEXSeq can be adapted to differential analysis of SJs and IRs, which will facilitate the identification of alternative splicing events and provide insights into the molecular mechanisms of transcription processes and disease development.\n"], "author_display": ["Yafang Li", "Xiayu Rao", "William W. Mattox", "Christopher I. Amos", "Bin Liu"], "article_type": "Research Article", "score": 0.41961098, "title_display": "RNA-Seq Analysis of Differential Splice Junction Usage and Intron Retentions by DEXSeq", "publication_date": "2015-09-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0136653"}, {"journal": "PLoS ONE", "abstract": ["\nGenome-wide association studies (GWAS) led to the identification of numerous novel loci for a number of complex diseases. Pathway-based approaches using genotypic data provide tangible leads which cannot be identified by single marker approaches as implemented in GWAS. The available pathway analysis approaches mainly differ in the employed databases and in the applied statistics for determining the significance of the associated disease markers.\nSo far, pathway-based approaches using GWAS data failed to consider the overlapping of genes among different pathways or the influence of protein\u2013interactions. We performed a multistage integrative pathway (MIP) analysis on three common diseases - Crohn's disease (CD), rheumatoid arthritis (RA) and type 1 diabetes (T1D) - incorporating genotypic, pathway, protein- and domain-interaction data to identify novel associations between these diseases and pathways. Additionally, we assessed the sensitivity of our method by studying the influence of the most significant SNPs on the pathway analysis by removing those and comparing the corresponding pathway analysis results. Apart from confirming many previously published associations between pathways and RA, CD and T1D, our MIP approach was able to identify three new associations between disease phenotypes and pathways. This includes a relation between the influenza-A pathway and RA, as well as a relation between T1D and the phagosome and toxoplasmosis pathways. These results provide new leads to understand the molecular underpinnings of these diseases.\nThe developed software herein used is available at http://www.cogsys.cs.uni-tuebingen.de/software/GWASPathwayIdentifier/index.htm.\n"], "author_display": ["Finja B\u00fcchel", "Florian Mittag", "Clemens Wrzodek", "Andreas Zell", "Thomas Gasser", "Manu Sharma"], "article_type": "Research Article", "score": 0.41957664, "title_display": "Integrative Pathway-Based Approach for Genome-Wide Association Studies: Identification of New Pathways for Rheumatoid Arthritis and Type 1 Diabetes", "publication_date": "2013-10-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0078577"}, {"journal": "PLOS ONE", "abstract": ["Background: Although researchers have worked in collaboration since the origins of modern science and the publication of the first scientific journals in the eighteenth century, this phenomenon has acquired exceptional importance in the last several decades. Since the mid-twentieth century, new knowledge has been generated from within an ever-growing network of investigators, working cooperatively in research groups across countries and institutions. Cooperation is a crucial determinant of academic success. Objective: The aim of the present paper is to analyze the evolution of scientific collaboration at the micro level, with regard to the scientific production generated on psoriasis research. Methods: A bibliographic search in the Medline database containing the MeSH terms \u201cpsoriasis\u201d or \u201cpsoriatic arthritis\u201d was carried out. The search results were limited to articles, reviews and letters. After identifying the co-authorships of documents on psoriasis indexed in the Medline database (1942\u20132013), various bibliometric indicators were obtained, including the average number of authors per document and degree of multi-authorship over time. In addition, we performed a network analysis to study the evolution of certain features of the co-authorship network as a whole: average degree, size of the largest component, clustering coefficient, density and average distance. We also analyzed the evolution of the giant component to characterize the changing research patterns in the field, and we calculated social network indicators for the nodes, namely betweenness and closeness. Results: The main active research clusters in the area were identified, along with their authors of reference. Our analysis of 28,670 documents sheds light on different aspects related to the evolution of scientific collaboration in the field, including the progressive increase in the mean number of co-authors (which stood at 5.17 in the 2004\u20132013 decade), and the rise in multi-authored papers signed by many different authors (in the same decade, 25.77% of the documents had between 6 and 9 co-authors, and 10.28% had 10 or more). With regard to the network indicators, the average degree gradually increased up to 10.97 in the study period. The percentage of authors pertaining to the largest component also rose to 73.02% of the authors. The clustering coefficient, on the other hand, remained stable throughout the entire 70-year period, with values hovering around 0.9. Finally, the average distance peaked in the decades 1974\u20131983 (8.29) and 1984\u20132003 (8.12) then fell over the next two decades, down to 5.25 in 2004\u20132013. The construction of the co-authorship network (threshold of collaboration \u2265 10 co-authored works) revealed a giant component of 161 researchers, containing 6 highly cohesive sub-components. Conclusions: Our study reveals the existence of a growing research community in which collaboration is increasingly important. We can highlight an essential feature associated with scientific collaboration: multi-authored papers, with growing numbers of collaborators contributing to them, are becoming more and more common, therefore the formation of research groups of increasing depth (specialization) and breadth (multidisciplinarity) is now a cornerstone of research success. "], "author_display": ["Gregorio Gonz\u00e1lez-Alcaide", "Jinseo Park", "Charles Huaman\u00ed", "Isabel Belinch\u00f3n", "Jos\u00e9 M. Ramos"], "article_type": "Research Article", "score": 0.41950342, "title_display": "Evolution of Cooperation Patterns in Psoriasis Research: Co-Authorship Network Analysis of Papers in Medline (1942\u20132013)", "publication_date": "2015-12-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0144837"}, {"journal": "PLoS ONE", "abstract": ["\n            We examined the publication records of a cohort of 168 life scientists in the field of ecology and evolutionary biology to assess gender differences in research performance. Clear discrepancies in publication rate between men and women appear very early in their careers and this has consequences for the subsequent citation of their work. We show that a recently proposed index designed to rank scientists fairly is in fact strongly biased against female researchers, and advocate a modified index to assess men and women on a more equitable basis.\n         "], "author_display": ["Matthew R.E. Symonds", "Neil J. Gemmell", "Tamsin L. Braisher", "Kylie L. Gorringe", "Mark A. Elgar"], "article_type": "Research Article", "score": 0.41945386, "title_display": "Gender Differences in Publication Output: Towards an Unbiased Metric of Research Performance", "publication_date": "2006-12-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0000127"}, {"journal": "PLoS ONE", "abstract": ["Background: De Winter and Happee [1] examined whether science based on selective publishing of significant results may be effective in accurate estimation of population effects, and whether this is even more effective than a science in which all results are published (i.e., a science without publication bias). Based on their simulation study they concluded that \u201cselective publishing yields a more accurate meta-analytic estimation of the true effect than publishing everything, (and that) publishing nonreplicable results while placing null results in the file drawer can be beneficial for the scientific collective\u201d (p.4). Methods and Findings: Using their scenario with a small to medium population effect size, we show that publishing everything is more effective for the scientific collective than selective publishing of significant results. Additionally, we examined a scenario with a null effect, which provides a more dramatic illustration of the superiority of publishing everything over selective publishing. Conclusion: Publishing everything is more effective than only reporting significant outcomes. "], "author_display": ["Marcel A. L. M. van Assen", "Robbie C. M. van Aert", "Mich\u00e8le B. Nuijten", "Jelte M. Wicherts"], "article_type": "Research Article", "score": 0.41918027, "title_display": "Why Publishing Everything Is More Effective than Selective Publishing of Statistically Significant Results", "publication_date": "2014-01-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0084896"}, {"journal": "PLoS ONE", "abstract": ["Background: The use of research evidence to underpin public health policy is strongly promoted. However, its implementation has not been straightforward. The objectives of this systematic review were to synthesise empirical evidence on the use of research evidence by public health decision makers in settings with universal health care systems. Methods: To locate eligible studies, 13 bibliographic databases were screened, organisational websites were scanned, key informants were contacted and bibliographies of included studies were scrutinised. Two reviewers independently assessed studies for inclusion, extracted data and assessed methodological quality. Data were synthesised as a narrative review. Findings: 18 studies were included: 15 qualitative studies, and three surveys. Their methodological quality was mixed. They were set in a range of country and decision making settings. Study participants included 1063 public health decision makers, 72 researchers, and 174 with overlapping roles. Decision making processes varied widely between settings, and were viewed differently by key players. A range of research evidence was accessed. However, there was no reliable evidence on the extent of its use. Its impact was often indirect, competing with other influences. Barriers to the use of research evidence included: decision makers' perceptions of research evidence; the gulf between researchers and decision makers; the culture of decision making; competing influences on decision making; and practical constraints. Suggested (but largely untested) ways of overcoming these barriers included: research targeted at the needs of decision makers; research clearly highlighting key messages; and capacity building. There was little evidence on the role of research evidence in decision making to reduce inequalities. Conclusions: To more effectively implement research informed public health policy, action is required by decision makers and researchers to address the barriers identified in this systematic review. There is an urgent need for evidence to support the use of research evidence to inform public health decision making to reduce inequalities. "], "author_display": ["Lois Orton", "Ffion Lloyd-Williams", "David Taylor-Robinson", "Martin O'Flaherty", "Simon Capewell"], "article_type": "Research Article", "score": 0.41889578, "title_display": "The Use of Research Evidence in Public Health Decision Making Processes: Systematic Review", "publication_date": "2011-07-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0021704"}, {"journal": "PLOS Genetics", "abstract": ["\nMapping expression quantitative trait loci (eQTLs) has been shown as a powerful tool to uncover the genetic underpinnings of many complex traits at molecular level. In this paper, we present an integrative analysis approach that leverages eQTL data collected from multiple population groups. In particular, our approach effectively identifies multiple independent cis-eQTL signals that are consistent across populations, accounting for population heterogeneity in allele frequencies and linkage disequilibrium patterns. Furthermore, by integrating genomic annotations, our analysis framework enables high-resolution functional analysis of eQTLs. We applied our statistical approach to analyze the GEUVADIS data consisting of samples from five population groups. From this analysis, we concluded that i) jointly analysis across population groups greatly improves the power of eQTL discovery and the resolution of fine mapping of causal eQTL ii) many genes harbor multiple independent eQTLs in their cis regions iii) genetic variants that disrupt transcription factor binding are significantly enriched in eQTLs (p-value = 4.93 \u00d7 10-22).\nAuthor Summary: Expression quantitative trait loci (eQTLs) are genetic variants associated with gene expression phenotypes. Mapping eQTLs enables us to study the genetic basis of gene expression variation across individuals. In this study, we introduce a statistical framework for analyzing genotype-expression data collected from multiple population groups. We show that our approach is particularly effective in identifying multiple independent eQTL signals that are consistently presented across populations in the proximity of a gene. In addition, our analysis framework allows effective integration of genomic annotations into eQTL analysis, which is helpful in dissecting the functional basis of eQTLs. "], "author_display": ["Xiaoquan Wen", "Francesca Luca", "Roger Pique-Regi"], "article_type": "Research Article", "score": 0.4187274, "title_display": "Cross-Population Joint Analysis of eQTLs: Fine Mapping and Functional Annotation", "publication_date": "2015-04-23T00:00:00Z", "eissn": "1553-7404", "id": "10.1371/journal.pgen.1005176"}, {"journal": "PLoS ONE", "abstract": ["Background: For several immune-mediated diseases, immunological analysis will become more complex in the future with datasets in which cytokine and gene expression data play a major role. These data have certain characteristics that require sophisticated statistical analysis such as strategies for non-normal distribution and censoring. Additionally, complex and multiple immunological relationships need to be adjusted for potential confounding and interaction effects. Objective: We aimed to introduce and apply different methods for statistical analysis of non-normal censored cytokine and gene expression data. Furthermore, we assessed the performance and accuracy of a novel regression approach in order to allow adjusting for covariates and potential confounding. Methods: For non-normally distributed censored data traditional means such as the Kaplan-Meier method or the generalized Wilcoxon test are described. In order to adjust for covariates the novel approach named Tobit regression on ranks was introduced. Its performance and accuracy for analysis of non-normal censored cytokine/gene expression data was evaluated by a simulation study and a statistical experiment applying permutation and bootstrapping. Results: If adjustment for covariates is not necessary traditional statistical methods are adequate for non-normal censored data. Comparable with these and appropriate if additional adjustment is required, Tobit regression on ranks is a valid method. Its power, type-I error rate and accuracy were comparable to the classical Tobit regression. Conclusion: Non-normally distributed censored immunological data require appropriate statistical methods. Tobit regression on ranks meets these requirements and can be used for adjustment for covariates and potential confounding in large and complex immunological datasets. "], "author_display": ["Nikolaus Ballenberger", "Anna Lluis", "Erika von Mutius", "Sabina Illi", "Bianca Schaub"], "article_type": "Research Article", "score": 0.4186697, "title_display": "Novel Statistical Approaches for Non-Normal Censored Immunological Data: Analysis of Cytokine and Gene Expression Data", "publication_date": "2012-10-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0046423"}, {"abstract": ["\n        Tumour cellularity, the relative proportion of tumour and normal cells in a sample, affects the sensitivity of mutation detection, copy number analysis, cancer gene expression and methylation profiling. Tumour cellularity is traditionally estimated by pathological review of sectioned specimens; however this method is both subjective and prone to error due to heterogeneity within lesions and cellularity differences between the sample viewed during pathological review and tissue used for research purposes. In this paper we describe a statistical model to estimate tumour cellularity from SNP array profiles of paired tumour and normal samples using shifts in SNP allele frequency at regions of loss of heterozygosity (LOH) in the tumour. We also provide qpure, a software implementation of the method. Our experiments showed that there is a medium correlation 0.42 (-value\u200a=\u200a0.0001) between tumor cellularity estimated by qpure and pathology review. Interestingly there is a high correlation 0.87 (-value  2.2e-16) between cellularity estimates by qpure and deep Ion Torrent sequencing of known somatic KRAS mutations; and a weaker correlation 0.32 (-value\u200a=\u200a0.004) between IonTorrent sequencing and pathology review. This suggests that qpure may be a more accurate predictor of tumour cellularity than pathology review. qpure can be downloaded from https://sourceforge.net/projects/qpure/.\n      "], "author_display": ["Sarah Song", "Katia Nones", "David Miller", "Ivon Harliwong", "Karin S. Kassahn", "Mark Pinese", "Marina Pajic", "Anthony J. Gill", "Amber L. Johns", "Matthew Anderson", "Oliver Holmes", "Conrad Leonard", "Darrin Taylor", "Scott Wood", "Qinying Xu", "Felicity Newell", "Mark J. Cowley", "Jianmin Wu", "Peter Wilson", "Lynn Fink", "Andrew V. Biankin", "Nic Waddell", "Sean M. Grimmond", "John V. Pearson"], "article_type": "Research Article", "score": 0.41859496, "title_display": "qpure: A Tool to Estimate Tumor Cellularity from Genome-Wide Single-Nucleotide Polymorphism Profiles", "publication_date": "2012-09-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0045835"}, {"journal": "PLOS ONE", "abstract": ["\nThe discrete data structure and large sequencing depth of RNA sequencing (RNA-seq) experiments can often generate outlier read counts in one or more RNA samples within a homogeneous group. Thus, how to identify and manage outlier observations in RNA-seq data is an emerging topic of interest. One of the main objectives in these research efforts is to develop statistical methodology that effectively balances the impact of outlier observations and achieves maximal power for statistical testing. To reach that goal, strengthening the accuracy of outlier detection is an important precursor. Current outlier detection algorithms for RNA-seq data are executed within a testing framework and may be sensitive to sparse data and heavy-tailed distributions. Therefore, we propose a univariate algorithm that utilizes a probabilistic approach to measure the deviation between an observation and the distribution generating the remaining data and implement it within in an iterative leave-one-out design strategy. Analyses of real and simulated RNA-seq data show that the proposed methodology has higher outlier detection rates for both non-normalized and normalized negative binomial distributed data.\n"], "author_display": ["Nysia I. George", "John F. Bowyer", "Nathaniel M. Crabtree", "Ching-Wei Chang"], "article_type": "Research Article", "score": 0.41844493, "title_display": "An Iterative Leave-One-Out Approach to Outlier Detection in RNA-Seq Data", "publication_date": "2015-06-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0125224"}, {"journal": "PLoS ONE", "abstract": ["Introduction: We examined the design, analysis and reporting in multi-reader multi-case (MRMC) research studies using the area under the receiver-operating curve (ROC AUC) as a measure of diagnostic performance. Methods: We performed a systematic literature review from 2005 to 2013 inclusive to identify a minimum 50 studies. Articles of diagnostic test accuracy in humans were identified via their citation of key methodological articles dealing with MRMC ROC AUC. Two researchers in consensus then extracted information from primary articles relating to study characteristics and design, methods for reporting study outcomes, model fitting, model assumptions, presentation of results, and interpretation of findings. Results were summarized and presented with a descriptive analysis. Results: Sixty-four full papers were retrieved from 475 identified citations and ultimately 49 articles describing 51 studies were reviewed and extracted. Radiological imaging was the index test in all. Most studies focused on lesion detection vs. characterization and used less than 10 readers. Only 6 (12%) studies trained readers in advance to use the confidence scale used to build the ROC curve. Overall, description of confidence scores, the ROC curve and its analysis was often incomplete. For example, 21 (41%) studies presented no ROC curve and only 3 (6%) described the distribution of confidence scores. Of 30 studies presenting curves, only 4 (13%) presented the data points underlying the curve, thereby allowing assessment of extrapolation. The mean change in AUC was 0.05 (\u22120.05 to 0.28). Non-significant change in AUC was attributed to underpowering rather than the diagnostic test failing to improve diagnostic accuracy. Conclusions: Data reporting in MRMC studies using ROC AUC as an outcome measure is frequently incomplete, hampering understanding of methods and the reliability of results and study conclusions. Authors using this analysis should be encouraged to provide a full description of their methods and results. "], "author_display": ["Thaworn Dendumrongsup", "Andrew A. Plumb", "Steve Halligan", "Thomas R. Fanshawe", "Douglas G. Altman", "Susan Mallett"], "article_type": "Research Article", "score": 0.41806954, "title_display": "Multi-Reader Multi-Case Studies Using the Area under the Receiver Operator Characteristic Curve as a Measure of Diagnostic Accuracy: Systematic Review with a Focus on Quality of Data Reporting", "publication_date": "2014-12-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0116018"}, {"journal": "PLoS ONE", "abstract": ["Background: The quest to understand the neurobiology of schizophrenia and bipolar disorder is ongoing with multiple lines of evidence indicating abnormalities of glia, mitochondria, and glutamate in both disorders. Despite high heritability estimates of 81% for schizophrenia and 75% for bipolar disorder, compelling links between findings from neurobiological studies, and findings from large-scale genetic analyses, are only beginning to emerge. Method: Ten publically available gene sets (pathways) related to glia, mitochondria, and glutamate were tested for association to schizophrenia and bipolar disorder using MAGENTA as the primary analysis method. To determine the robustness of associations, secondary analyses were performed with: ALIGATOR, INRICH, and Set Screen. Data from the Psychiatric Genomics Consortium (PGC) were used for all analyses. There were 1,068,286 SNP-level p-values for schizophrenia (9,394 cases/12,462 controls), and 2,088,878 SNP-level p-values for bipolar disorder (7,481 cases/9,250 controls). Results: The Glia-Oligodendrocyte pathway was associated with schizophrenia, after correction for multiple tests, according to primary analysis (MAGENTA p\u200a=\u200a0.0005, 75% requirement for individual gene significance) and also achieved nominal levels of significance with INRICH (p\u200a=\u200a0.0057) and ALIGATOR (p\u200a=\u200a0.022). For bipolar disorder, Set Screen yielded nominally and method-wide significant associations to all three glial pathways, with strongest association to the Glia-Astrocyte pathway (p\u200a=\u200a0.002). Conclusions: Consistent with findings of white matter abnormalities in schizophrenia by other methods of study, the Glia-Oligodendrocyte pathway was associated with schizophrenia in our genomic study. These findings suggest that the abnormalities of myelination observed in schizophrenia are at least in part due to inherited factors, contrasted with the alternative of purely environmental causes (e.g. medication effects or lifestyle). While not the primary purpose of our study, our results also highlight the consequential nature of alternative choices regarding pathway analysis, in that results varied somewhat across methods, despite application to identical datasets and pathways. "], "author_display": ["Laramie E. Duncan", "Peter A. Holmans", "Phil H. Lee", "Colm T. O'Dushlaine", "Andrew W. Kirby", "Jordan W. Smoller", "Dost \u00d6ng\u00fcr", "Bruce M. Cohen"], "article_type": "Research Article", "score": 0.41784942, "title_display": "Pathway Analyses Implicate Glial Cells in Schizophrenia", "publication_date": "2014-02-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0089441"}, {"journal": "PLoS ONE", "abstract": ["Objective: Detection of meningococcal carriers is key to understanding the epidemiology of Neisseria meningitidis, yet no gold standard has been established. Here, we directly compare two methods for collecting pharyngeal swabs to identify meningococcal carriers. Methods: We conducted cross-sectional surveys of schoolchildren at multiple sites in Africa to compare swabbing the posterior pharynx behind the uvula (U) to swabbing the posterior pharynx behind the uvula plus one tonsil (T). Swabs were cultured immediately and analyzed using molecular methods. Results: One thousand and six paired swab samples collected from schoolchildren in four countries were analyzed. Prevalence of meningococcal carriage was 6.9% (95% CI: 5.4-8.6%) based on the results from both swabs, but the observed prevalence was lower based on one swab type alone. Prevalence based on the T swab or the U swab alone was similar (5.2% (95% CI: 3.8-6.7%) versus 4.9% (95% CI: 3.6-6.4%) respectively (p=0.6)). The concordance between the two methods was 96.3% and the kappa was 0.61 (95% CI: 0.50-0.73), indicating good agreement. Conclusions: These two commonly used methods for collecting pharyngeal swabs provide consistent estimates of the prevalence of carriage, but both methods misclassified carriers to some degree, leading to underestimates of the prevalence. "], "author_display": ["Nicole E. Basta", "James M. Stuart", "Maria C. Nascimento", "Olivier Manigart", "Caroline Trotter", "Musa Hassan-King", "Daniel Chandramohan", "Samba O. Sow", "Abdoulaye Berthe", "Ahmed Bedru", "Yenenesh K. Tekletsion", "Jean-Marc Collard", "Jean-Fran\u00e7ois Jusot", "Aldiouma Diallo", "Hubert Bass\u00e9ne", "Doumagoum M. Daugla", "Khadidja Gamougam", "Abraham Hodgson", "Abudulai A. Forgor", "Babatunji A. Omotara", "Galadima B. Gadzama", "Eleanor R. Watkins", "Lisa S. Rebbetts", "Kanny Diallo", "Noel S. Weiss", "M. Elizabeth Halloran", "Martin C. J. Maiden", "Brian Greenwood"], "article_type": "Research Article", "score": 0.4172685, "title_display": "Methods for Identifying <i>Neisseria meningitidis</i> Carriers: A Multi-Center Study in the African Meningitis Belt ", "publication_date": "2013-10-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0078336"}, {"journal": "PLoS ONE", "abstract": ["\nHigh dimensionality and small sample sizes, and their inherent risk of overfitting, pose great challenges for constructing efficient classifiers in microarray data classification. Therefore a feature selection technique should be conducted prior to data classification to enhance prediction performance. In general, filter methods can be considered as principal or auxiliary selection mechanism because of their simplicity, scalability, and low computational complexity. However, a series of trivial examples show that filter methods result in less accurate performance because they ignore the dependencies of features. Although few publications have devoted their attention to reveal the relationship of features by multivariate-based methods, these methods describe relationships among features only by linear methods. While simple linear combination relationship restrict the improvement in performance. In this paper, we used kernel method to discover inherent nonlinear correlations among features as well as between feature and target. Moreover, the number of orthogonal components was determined by kernel Fishers linear discriminant analysis (FLDA) in a self-adaptive manner rather than by manual parameter settings. In order to reveal the effectiveness of our method we performed several experiments and compared the results between our method and other competitive multivariate-based features selectors. In our comparison, we used two classifiers (support vector machine, -nearest neighbor) on two group datasets, namely two-class and multi-class datasets. Experimental results demonstrate that the performance of our method is better than others, especially on three hard-classify datasets, namely Wang's Breast Cancer, Gordon's Lung Adenocarcinoma and Pomeroy's Medulloblastoma.\n"], "author_display": ["Shiquan Sun", "Qinke Peng", "Adnan Shakoor"], "article_type": "Research Article", "score": 0.4172573, "title_display": "A Kernel-Based Multivariate Feature Selection Method for Microarray Data Classification", "publication_date": "2014-07-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0102541"}, {"journal": "PLOS ONE", "abstract": ["\nRetinal fundus images are widely used in diagnosing and providing treatment for several eye diseases. Prior works using retinal fundus images detected the presence of exudation with the aid of publicly available dataset using extensive segmentation process. Though it was proved to be computationally efficient, it failed to create a diabetic retinopathy feature selection system for transparently diagnosing the disease state. Also the diagnosis of diseases did not employ machine learning methods to categorize candidate fundus images into true positive and true negative ratio. Several candidate fundus images did not include more detailed feature selection technique for diabetic retinopathy. To apply machine learning methods and classify the candidate fundus images on the basis of sliding window a method called, Diabetic Fundus Image Recuperation (DFIR) is designed in this paper. The initial phase of DFIR method select the feature of optic cup in digital retinal fundus images based on Sliding Window Approach. With this, the disease state for diabetic retinopathy is assessed. The feature selection in DFIR method uses collection of sliding windows to obtain the features based on the histogram value. The histogram based feature selection with the aid of Group Sparsity Non-overlapping function provides more detailed information of features. Using Support Vector Model in the second phase, the DFIR method based on Spiral Basis Function effectively ranks the diabetic retinopathy diseases. The ranking of disease level for each candidate set provides a much promising result for developing practically automated diabetic retinopathy diagnosis system. Experimental work on digital fundus images using the DFIR method performs research on the factors such as sensitivity, specificity rate, ranking efficiency and feature selection time.\n"], "author_display": ["Somasundaram Krishnamoorthy", "Alli P"], "article_type": "Research Article", "score": 0.41725197, "title_display": "A Novel Image Recuperation Approach for Diagnosing and Ranking Retinopathy Disease Level Using Diabetic Fundus Image", "publication_date": "2015-05-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0125542"}, {"journal": "PLOS ONE", "abstract": ["\nIn ecology, as in other research fields, efficient sampling for population estimation often drives sample designs toward unequal probability sampling, such as in stratified sampling. Design based statistical analysis tools are appropriate for seamless integration of sample design into the statistical analysis. However, it is also common and necessary, after a sampling design has been implemented, to use datasets to address questions that, in many cases, were not considered during the sampling design phase. Questions may arise requiring the use of model based statistical tools such as multiple regression, quantile regression, or regression tree analysis. However, such model based tools may require, for ensuring unbiased estimation, data from simple random samples, which can be problematic when analyzing data from unequal probability designs. Despite numerous method specific tools available to properly account for sampling design, too often in the analysis of ecological data, sample design is ignored and consequences are not properly considered. We demonstrate here that violation of this assumption can lead to biased parameter estimates in ecological research. In addition, to the set of tools available for researchers to properly account for sampling design in model based analysis, we introduce inverse probability bootstrapping (IPB). Inverse probability bootstrapping is an easily implemented method for obtaining equal probability re-samples from a probability sample, from which unbiased model based estimates can be made. We demonstrate the potential for bias in model-based analyses that ignore sample inclusion probabilities, and the effectiveness of IPB sampling in eliminating this bias, using both simulated and actual ecological data. For illustration, we considered three model based analysis tools\u2014linear regression, quantile regression, and boosted regression tree analysis. In all models, using both simulated and actual ecological data, we found inferences to be biased, sometimes severely, when sample inclusion probabilities were ignored, while IPB sampling effectively produced unbiased parameter estimates.\n"], "author_display": ["Matthew Nahorniak", "David P. Larsen", "Carol Volk", "Chris E. Jordan"], "article_type": "Research Article", "score": 0.41710958, "title_display": "Using Inverse Probability Bootstrap Sampling to Eliminate Sample Induced Bias in Model Based Analysis of Unequal Probability Samples", "publication_date": "2015-06-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0131765"}, {"journal": "PLoS ONE", "abstract": ["\n        Accurate identification of cell nuclei and their tracking using three dimensional (3D) microscopic images is a demanding task in many biological studies. Manual identification of nuclei centroids from images is an error-prone task, sometimes impossible to accomplish due to low contrast and the presence of noise. Nonetheless, only a few methods are available for 3D bioimaging applications, which sharply contrast with 2D analysis, where many methods already exist. In addition, most methods essentially adopt segmentation for which a reliable solution is still unknown, especially for 3D bio-images having juxtaposed cells. In this work, we propose a new method that can directly extract nuclei centroids from fluorescence microscopy images. This method involves three steps: (i) Pre-processing, (ii) Local enhancement, and (iii) Centroid extraction. The first step includes two variations: first variation (Variant-1) uses the whole 3D pre-processed image, whereas the second one (Variant-2) modifies the preprocessed image to the candidate regions or the candidate hybrid image for further processing. At the second step, a multiscale cube filtering is employed in order to locally enhance the pre-processed image. Centroid extraction in the third step consists of three stages. In Stage-1, we compute a local characteristic ratio at every voxel and extract local maxima regions as candidate centroids using a ratio threshold. Stage-2 processing removes spurious centroids from Stage-1 results by analyzing shapes of intensity profiles from the enhanced image. An iterative procedure based on the nearest neighborhood principle is then proposed to combine if there are fragmented nuclei. Both qualitative and quantitative analyses on a set of 100 images of 3D mouse embryo are performed. Investigations reveal a promising achievement of the technique presented in terms of average sensitivity and precision (i.e., 88.04% and 91.30% for Variant-1; 86.19% and 95.00% for Variant-2), when compared with an existing method (86.06% and 90.11%), originally developed for analyzing C. elegans images.\n      "], "author_display": ["Md. Khayrul Bashar", "Koji Komatsu", "Toshihiko Fujimori", "Tetsuya J. Kobayashi"], "article_type": "Research Article", "score": 0.41694376, "title_display": "Automatic Extraction of Nuclei Centroids of Mouse Embryonic Cells from Fluorescence Microscopy Images", "publication_date": "2012-05-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0035550"}, {"journal": "PLOS ONE", "abstract": ["\nExtensive genetic studies have identified a large number of causal genetic variations in many human phenotypes; however, these could not completely explain heritability in complex diseases. Some researchers have proposed that the \u201cmissing heritability\u201d may be attributable to gene\u2013gene and gene\u2013environment interactions. Because there are billions of potential interaction combinations, the statistical power of a single study is often ineffective in detecting these interactions. Meta-analysis is a common method of increasing detection power; however, accessing individual data could be difficult. This study presents a simple method that employs aggregated summary values from a \u201ccase\u201d group to detect these specific interactions that based on rare disease and independence assumptions. However, these assumptions, particularly the rare disease assumption, may be violated in real situations; therefore, this study further investigated the robustness of our proposed method when it violates the assumptions. In conclusion, we observed that the rare disease assumption is relatively nonessential, whereas the independence assumption is an essential component. Because single nucleotide polymorphisms (SNPs) are often unrelated to environmental factors and SNPs on other chromosomes, researchers should use this method to investigate gene\u2013gene and gene\u2013environment interactions when they are unable to obtain detailed individual patient data.\n"], "author_display": ["Chin Lin", "Chi-Ming Chu", "John Lin", "Hsin-Yi Yang", "Sui-Lung Su"], "article_type": "Research Article", "score": 0.41690433, "title_display": "Gene-Gene and Gene-Environment Interactions in Meta-Analysis of Genetic Association Studies", "publication_date": "2015-04-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0124967"}, {"journal": "PLoS ONE", "abstract": ["\nThe case-cohort study design combines the advantages of a cohort study with the efficiency of a nested case-control study. However, unlike more standard observational study designs, there are currently no guidelines for reporting results from case-cohort studies. Our aim was to review recent practice in reporting these studies, and develop recommendations for the future. By searching papers published in 24 major medical and epidemiological journals between January 2010 and March 2013 using PubMed, Scopus and Web of Knowledge, we identified 32 papers reporting case-cohort studies. The median subcohort sampling fraction was 4.1% (interquartile range 3.7% to 9.1%). The papers varied in their approaches to describing the numbers of individuals in the original cohort and the subcohort, presenting descriptive data, and in the level of detail provided about the statistical methods used, so it was not always possible to be sure that appropriate analyses had been conducted. Based on the findings of our review, we make recommendations about reporting of the study design, subcohort definition, numbers of participants, descriptive information and statistical methods, which could be used alongside existing STROBE guidelines for reporting observational studies.\n"], "author_display": ["Stephen J. Sharp", "Manon Poulaliou", "Simon G. Thompson", "Ian R. White", "Angela M. Wood"], "article_type": "Research Article", "score": 0.41680092, "title_display": "A Review of Published Analyses of Case-Cohort Studies and Recommendations for Future Reporting", "publication_date": "2014-06-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0101176"}, {"journal": "PLOS ONE", "abstract": ["Introduction: After several decades\u2019 development, meta-analysis has become the pillar of evidence-based medicine. However, heterogeneity is still the threat to the validity and quality of such studies. Currently, Q and its descendant I2 (I square) tests are widely used as the tools for heterogeneity evaluation. The core mission of this kind of test is to identify data sets from similar populations and exclude those are from different populations. Although Q and I2 are used as the default tool for heterogeneity testing, the work we present here demonstrates that the robustness of these two tools is questionable. Methods and Findings: We simulated a strictly normalized population S. The simulation successfully represents randomized control trial data sets, which fits perfectly with the theoretical distribution (experimental group: p = 0.37, control group: p = 0.88). And we randomly generate research samples Si that fits the population with tiny distributions. In short, these data sets are perfect and can be seen as completely homogeneous data from the exactly same population. If Q and I2 are truly robust tools, the Q and I2 testing results on our simulated data sets should not be positive. We then synthesized these trials by using fixed model. Pooled results indicated that the mean difference (MD) corresponds highly with the true values, and the 95% confidence interval (CI) is narrow. But, when the number of trials and sample size of trials enrolled in the meta-analysis are substantially increased; the Q and I2 values also increase steadily. This result indicates that I2 and Q are only suitable for testing heterogeneity amongst small sample size trials, and are not adoptable when the sample sizes and the number of trials increase substantially. Conclusions: Every day, meta-analysis studies which contain flawed data analysis are emerging and passed on to clinical practitioners as \u201cupdated evidence\u201d. Using this kind of evidence that contain heterogeneous data sets leads to wrong conclusion, makes chaos in clinical practice and weakens the foundation of evidence-based medicine. We suggest more strict applications of meta-analysis: it should only be applied to those synthesized trials with small sample sizes. We call upon that the tools of evidence-based medicine should keep up-to-dated with the cutting-edge technologies in data science. Clinical research data should be made available publicly when there is any relevant article published so the research community could conduct in-depth data mining, which is a better alternative for meta-analysis in many instances. "], "author_display": ["Shi-jun Li", "Hua Jiang", "Hao Yang", "Wei Chen", "Jin Peng", "Ming-wei Sun", "Charles Damien Lu", "Xi Peng", "Jun Zeng"], "article_type": "Research Article", "score": 0.41677472, "title_display": "The Dilemma of Heterogeneity Tests in Meta-Analysis: A Challenge from a Simulation Study", "publication_date": "2015-05-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0127538"}, {"journal": "PLOS ONE", "abstract": ["Introduction: Research on complex health conditions such as neurodevelopmental disorders increasingly relies on large-scale research and clinical studies that would benefit from data sharing initiatives. Organizations that share data stand to maximize the efficiency of invested research dollars, expedite research findings, minimize the burden on the patient community, and increase citation rates of publications associated with the data. Objective: This study examined ethics and governance information on websites of databases involving neurodevelopmental disorders to determine the availability of information on key factors crucial for comprehension of, and trust and participation in such initiatives. Methods: We identified relevant databases identified using online keyword searches. Two researchers reviewed each of the websites and identified thematic content using principles from grounded theory. The content for each organization was interrogated using the gap analysis method. Results: Sixteen websites from data sharing organizations met our inclusion criteria. Information about types of data and tissues stored, data access requirements and procedures, and protections for confidentiality were significantly addressed by data sharing organizations. However, special considerations for minors (absent from 63%), controls to check if data and tissues are being submitted (absent from 81%), disaster recovery plans (absent from 81%), and discussions of incidental findings (absent from 88%) emerged as major gaps in thematic website content. When present, content pertaining to special considerations for youth, along with other ethics guidelines and requirements, were scattered throughout the websites or available only from associated documents accessed through live links. Conclusion: The complexities of sharing data acquired from children and adolescents will only increase with advances in genomic and neuro science. Our findings suggest that there is a need to improve the consistency, depth and accessibility of governance and policies on which these collaborations can lean specifically for vulnerable young populations. "], "author_display": ["Holly Longstaff", "Vera Khramova", "Elodie Portales-Casamar", "Judy Illes"], "article_type": "Research Article", "score": 0.41674197, "title_display": "Sharing with More Caring: Coordinating and Improving the Ethical Governance of Data and Biomaterials Obtained from Children", "publication_date": "2015-07-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0130527"}, {"journal": "PLoS ONE", "abstract": ["\nAccurate reconstruction of ancestral character states on a phylogeny is crucial in many genomics studies. We study how to select species to achieve the best reconstruction of ancestral character states on a phylogeny. We first show that the marginal maximum likelihood has the monotonicity property that more taxa give better reconstruction, but the Fitch method does not have it even on an ultrametric phylogeny. We further validate a greedy approach for species selection using simulation. The validation tests indicate that backward greedy selection outperforms forward greedy selection. In addition, by applying our selection strategy, we obtain a set of the ten most informative species for the reconstruction of the genomic sequence of the so-called boreoeutherian ancestor of placental mammals. This study has broad relevance in comparative genomics and paleogenomics since limited research resources do not allow researchers to sequence the large number of descendant species required to reconstruct an ancestral sequence.\n"], "author_display": ["Guoliang Li", "Jian Ma", "Louxin Zhang"], "article_type": "Research Article", "score": 0.41671336, "title_display": "Greedy Selection of Species for Ancestral State Reconstruction on Phylogenies: Elimination Is Better than Insertion", "publication_date": "2010-02-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0008985"}, {"journal": "PLoS ONE", "abstract": ["\nFew studies that examine the neurogenesis\u2013behaviour relationship formally establish covariation between neurogenesis and behaviour or rule out competing explanations. The behavioural relevance of neurogenesis might therefore be overestimated if other mechanisms account for some, or even all, of the experimental effects. A systematic review of the literature was conducted and the data reanalysed using causal mediation analysis, which can estimate the behavioural contribution of new hippocampal neurons separately from other mechanisms that might be operating. Results from eleven eligible individual studies were then combined in a meta-analysis to increase precision (representing data from 215 animals) and showed that neurogenesis made a negligible contribution to behaviour (standarised effect \u200a=\u200a0.15; 95% CI \u200a=\u200a\u22120.04 to 0.34; p\u200a=\u200a0.128); other mechanisms accounted for the majority of experimental effects (standardised effect \u200a=\u200a1.06; 95% CI \u200a=\u200a0.74 to 1.38; p\u200a=\u200a1.7\u00d710\u221211).\n"], "author_display": ["Stanley E. Lazic", "Johannes Fuss", "Peter Gass"], "article_type": "Research Article", "score": 0.41657487, "title_display": "Quantifying the Behavioural Relevance of Hippocampal Neurogenesis", "publication_date": "2014-11-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0113855"}, {"journal": "PLOS ONE", "abstract": ["\nPCR is the most widely applied technique for large scale screening of bacterial clones, mouse genotypes, virus genomes etc. A drawback of large PCR screening is that amplicon analysis is usually performed using gel electrophoresis, a step that is very labor intensive, tedious and chemical waste generating. Single genome amplification (SGA) is used to characterize the diversity and evolutionary dynamics of virus populations within infected hosts. SGA is based on the isolation of single template molecule using limiting dilution followed by nested PCR amplification and requires the analysis of hundreds of reactions per sample, making large scale SGA studies very challenging. Here we present a novel approach entitled Long Amplicon Melt Profiling (LAMP) based on the analysis of the melting profile of the PCR reactions using SYBR Green and/or EvaGreen fluorescent dyes. The LAMP method represents an attractive alternative to gel electrophoresis and enables the quick discrimination of positive reactions. We validate LAMP for SIV and HIV env-SGA, in 96- and 384-well plate formats. Because the melt profiling allows the screening of several thousands of PCR reactions in a cost-effective, rapid and robust way, we believe it will greatly facilitate any large scale PCR screening.\n"], "author_display": ["Laurent Houzet", "Claire Deleage", "Anne-Pascale Satie", "Laetitia Merlande", "Dominique Mahe", "Nathalie Dejucq-Rainsford"], "article_type": "Research Article", "score": 0.41650197, "title_display": "A New Method for Rapid Screening of End-Point PCR Products: Application to Single Genome Amplified HIV and SIV Envelope Amplicons", "publication_date": "2015-06-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0128188"}, {"journal": "PLoS ONE", "abstract": ["Background: Network communities help the functional organization and evolution of complex networks. However, the development of a method, which is both fast and accurate, provides modular overlaps and partitions of a heterogeneous network, has proven to be rather difficult. Methodology/Principal Findings: Here we introduce the novel concept of ModuLand, an integrative method family determining overlapping network modules as hills of an influence function-based, centrality-type community landscape, and including several widely used modularization methods as special cases. As various adaptations of the method family, we developed several algorithms, which provide an efficient analysis of weighted and directed networks, and (1) determine pervasively overlapping modules with high resolution; (2) uncover a detailed hierarchical network structure allowing an efficient, zoom-in analysis of large networks; (3) allow the determination of key network nodes and (4) help to predict network dynamics. Conclusions/Significance: The concept opens a wide range of possibilities to develop new approaches and applications including network routing, classification, comparison and prediction. "], "author_display": ["Istv\u00e1n A. Kov\u00e1cs", "Robin Palotai", "M\u00e1t\u00e9 S. Szalay", "Peter Csermely"], "article_type": "Research Article", "score": 0.41635486, "title_display": "Community Landscapes: An Integrative Approach to Determine Overlapping Network Module Hierarchy, Identify Key Nodes and Predict Network Dynamics", "publication_date": "2010-09-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0012528"}, {"abstract": ["\n        Jill Fisher and Corey Kalbaugh describe their findings from a qualitative research study evaluating the motivations of private-sector physicians conducting contract research for the pharmaceutical industry.\n      Background: There have been dramatic increases over the past 20 years in the number of nonacademic, private-sector physicians who serve as principal investigators on US clinical trials sponsored by the pharmaceutical industry. However, there has been little research on the implications of these investigators' role in clinical investigation. Our objective was to study private-sector clinics involved in US pharmaceutical clinical trials to understand the contract research arrangements supporting drug development, and specifically how private-sector physicians engaged in contract research describe their professional identities. Methods and Findings: We conducted a qualitative study in 2003\u20132004 combining observation at 25 private-sector research organizations in the southwestern United States and 63 semi-structured interviews with physicians, research staff, and research participants at those clinics. We used grounded theory to analyze and interpret our data. The 11 private-sector physicians who participated in our study reported becoming principal investigators on industry clinical trials primarily because contract research provides an additional revenue stream. The physicians reported that they saw themselves as trial practitioners and as businesspeople rather than as scientists or researchers. Conclusions: Our findings suggest that in addition to having financial motivation to participate in contract research, these US private-sector physicians have a professional identity aligned with an industry-based approach to research ethics. The generalizability of these findings and whether they have changed in the intervening years should be addressed in future studies. Background: Before a new drug can be used routinely by physicians, it must be investigated in clinical trials\u2014studies that test the drug's safety and effectiveness in people. In the past, clinical trials were usually undertaken in academic medical centers (institutes where physicians provide clinical care, do research, and teach), but increasingly, clinical trials are being conducted in the private sector as part of a growing contract research system. In the US, for example, most clinical trials completed in the 1980s took place in academic medical centers, but nowadays, more than 70% of trials are conducted by nonacademic (community) physicians working under contract to pharmaceutical companies. The number of private-sector nonacademic physicians serving as principal investigators (PIs) for US clinical trials (the PI takes direct responsibility for completion of the trial) increased from 4,000 in 1990 to 20,250 in 2010, and research contracts for clinical trials are now worth more than US\u1e6911 billion annually. Why Was This Study Done?: To date, there has been little research on the implications of this change in the conduct of clinical trials. Academic PIs are often involved in both laboratory and clinical research and are therefore likely to identify closely with the science of trials. By contrast, nonacademic PIs may see clinical trials more as a business opportunity\u2014pharmaceutical contract research is profitable to US physicians because they get paid for every step of the trial process. As a result, pharmaceutical companies may now have more control over clinical trial data and more opportunities to suppress negative data through selective publication of study results than previously. In this qualitative study, the researchers explore the outsourcing of clinical trials to private-sector research clinics through observations of, and in-depth interviews with, physicians and other research staff involved in the US clinical trials industry. A qualitative study collects non-quantitative data such as how physicians feel about doing contract research and about their responsibilities to their patients. What Did the Researchers Do and Find?: Between October 2003 and September 2004, the researchers observed the interactions between PIs, trial coordinators (individuals who undertake many of the trial activities such as blood collection), and trial participants at 25 US research organizations in the southwestern US and interviewed 63 informants (including 12 PIs) about the trials they were involved in and their reasons for becoming involved. The researchers found that private-sector physicians became PIs on industry-sponsored clinical trials primarily because contract research was financially lucrative. The physicians perceived their roles in terms of business rather than science and claimed that they offered something to the pharmaceutical industry that academics do not\u2014the ability to carry out a diverse range of trials quickly and effectively, regardless of their medical specialty. Finally, the physicians saw their primary ethical responsibility as providing accurate data to the companies that hired them and did not explicitly refer to their ethical responsibility to trial participants. One possible reason for this shift in ethical concerns is the belief among private-sector physicians that pharmaceutical companies must be making scientifically and ethically sound decisions when designing trials because of the amount of money they invest in them. What Do These Findings Mean?: These findings suggest that private-sector physicians participate as PIs in pharmaceutical clinical trials primarily for financial reasons and see themselves as trial practitioners and businesspeople rather than as scientists. The accuracy of these findings is likely to be limited by the small number of PIs interviewed and by the time that has elapsed since the researchers collected their qualitative data. Moreover, these findings may not be generalizable to other regions of the US or to other countries. Nevertheless, they have potentially troubling implications for drug development. By hiring private-sector physicians who see themselves as involved more with the business than the science of contract research, pharmaceutical companies may be able to exert more control over the conduct of clinical trials and the publication of trial results than previously. Compared to the traditional investigatorinitiated system of clinical research, this new system of contract research means that clinical trials now lack the independence that is at the heart of best science practices, a development that casts doubt on the robustness of the knowledge being produced about the safety and effectiveness of new drugs. Additional Information: Please access these websites via the online version of this summary at http://dx.doi.org/10.1371/journal.pmed.1001271. "], "author_display": ["Jill A. Fisher", "Corey A. Kalbaugh"], "article_type": "Research Article", "score": 0.41632557, "title_display": "United States Private-Sector Physicians and Pharmaceutical Contract Research: A Qualitative Study", "publication_date": "2012-07-24T00:00:00Z", "eissn": "1549-1676", "id": "10.1371/journal.pmed.1001271"}, {"journal": "PLOS ONE", "abstract": ["\nA rapid, cost effective method of metagenomic DNA extraction from soil is a useful tool for environmental microbiology. The present work describes an improved method of DNA extraction namely \u201cpowdered glass method\u201d from diverse soils. The method involves the use of sterile glass powder for cell lysis followed by addition of 1% powdered activated charcoal (PAC) as purifying agent to remove humic substances. The method yielded substantial DNA (5.87 \u00b1 0.04 \u03bcg/g of soil) with high purity (A260/280: 1.76 \u00b1 0.05) and reduced humic substances (A340: 0.047 \u00b1 0.03). The quality of the extracted DNA was compared against five different methods based on 16S rDNA PCR amplification, BamHI digestion and validated using quantitative PCR. The digested DNA was used for a metagenomic library construction with the transformation efficiency of 4 X 106 CFU mL-1. Besides providing rapid, efficient and economical extraction of metgenomic DNA from diverse soils, this method\u2019s applicability is also demonstrated for cultivated organisms (Gram positive B. subtilis NRRL-B-201, Gram negative E. coli MTCC40, and a microalgae C. sorokiniana UTEX#1666).\n"], "author_display": ["Selvaraju Gayathri Devi", "Anwar Aliya Fathima", "Sudhakar Radha", "Rex Arunraj", "Wayne R. Curtis", "Mohandass Ramya"], "article_type": "Research Article", "score": 0.41603264, "title_display": "A Rapid and Economical Method for Efficient DNA Extraction from Diverse Soils Suitable for Metagenomic Applications", "publication_date": "2015-07-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0132441"}, {"journal": "PLoS ONE", "abstract": ["\nWith advances in brain-computer interface (BCI) research, a portable few- or single-channel BCI system has become necessary. Most recent BCI studies have demonstrated that the common spatial pattern (CSP) algorithm is a powerful tool in extracting features for multiple-class motor imagery. However, since the CSP algorithm requires multi-channel information, it is not suitable for a few- or single-channel system. In this study, we applied a short-time Fourier transform to decompose a single-channel electroencephalography signal into the time-frequency domain and construct multi-channel information. Using the reconstructed data, the CSP was combined with a support vector machine to obtain high classification accuracies from channels of both the sensorimotor and forehead areas. These results suggest that motor imagery can be detected with a single channel not only from the traditional sensorimotor area but also from the forehead area.\n"], "author_display": ["Sheng Ge", "Ruimin Wang", "Dongchuan Yu"], "article_type": "Research Article", "score": 0.4159817, "title_display": "Classification of Four-Class Motor Imagery Employing Single-Channel Electroencephalography", "publication_date": "2014-06-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0098019"}, {"journal": "PLoS ONE", "abstract": ["\nSegmentation-free direct methods are quite efficient for automated nuclei extraction from high dimensional images. A few such methods do exist but most of them do not ensure algorithmic robustness to parameter and noise variations. In this research, we propose a method based on multiscale adaptive filtering for efficient and robust detection of nuclei centroids from four dimensional (4D) fluorescence images. A temporal feedback mechanism is employed between the enhancement and the initial detection steps of a typical direct method. We estimate the minimum and maximum nuclei diameters from the previous frame and feed back them as filter lengths for multiscale enhancement of the current frame. A radial intensity-gradient function is optimized at positions of initial centroids to estimate all nuclei diameters. This procedure continues for processing subsequent images in the sequence. Above mechanism thus ensures proper enhancement by automated estimation of major parameters. This brings robustness and safeguards the system against additive noises and effects from wrong parameters. Later, the method and its single-scale variant are simplified for further reduction of parameters. The proposed method is then extended for nuclei volume segmentation. The same optimization technique is applied to final centroid positions of the enhanced image and the estimated diameters are projected onto the binary candidate regions to segment nuclei volumes.Our method is finally integrated with a simple sequential tracking approach to establish nuclear trajectories in the 4D space. Experimental evaluations with five image-sequences (each having 271 3D sequential images) corresponding to five different mouse embryos show promising performances of our methods in terms of nuclear detection, segmentation, and tracking. A detail analysis with a sub-sequence of 101 3D images from an embryo reveals that the proposed method can improve the nuclei detection accuracy by 9  over the previous methods, which used inappropriate large valued parameters. Results also confirm that the proposed method and its variants achieve high detection accuracies ( 98 mean F-measure) irrespective of the large variations of filter parameters and noise levels.\n"], "author_display": ["Md. Khayrul Bashar", "Kazuo Yamagata", "Tetsuya J. Kobayashi"], "article_type": "Research Article", "score": 0.41574076, "title_display": "Improved and Robust Detection of Cell Nuclei from Four Dimensional Fluorescence Images", "publication_date": "2014-07-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0101891"}, {"journal": "PLoS ONE", "abstract": ["\nThis paper presents a novel object detection method using a single instance from the object category. Our method uses biologically inspired global scene context criteria to check whether every individual location of the image can be naturally replaced by the query instance, which indicates whether there is a similar object at this location. Different from the traditional detection methods that only look at individual locations for the desired objects, our method evaluates the consistency of the entire scene. It is therefore robust to large intra-class variations, occlusions, a minor variety of poses, low-revolution conditions, background clutter etc., and there is no off-line training. The experimental results on four datasets and two video sequences clearly show the superior robustness of the proposed method, suggesting that global scene context is important for visual detection/localization.\n"], "author_display": ["Changxin Gao", "Nong Sang", "Rui Huang"], "article_type": "Research Article", "score": 0.41572255, "title_display": "Biologically Inspired Scene Context for Object Detection Using a Single Instance", "publication_date": "2014-05-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0098447"}, {"journal": "PLoS ONE", "abstract": ["\n        We investigated the association of the intensity of newspaper reporting of charcoal burning suicide with the incidence of such deaths in Taiwan during 1998\u20132002. A counting process approach was used to estimate the incidence of suicides and intensity of news reporting. Conditional Poisson generalized linear autoregressive models were performed to assess the association of the intensity of newspaper reporting of charcoal burning and non-charcoal burning suicides with the actual number of charcoal burning and non-charcoal burning suicides the following day. We found that increases in the reporting of charcoal burning suicide were associated with increases in the incidence of charcoal burning suicide on the following day, with each reported charcoal burning news item being associated with a 16% increase in next day charcoal burning suicide (p<.0001). However, the reporting of other methods of suicide was not related to their incidence. We conclude that extensive media reporting of charcoal burning suicides appears to have contributed to the rapid rise in the incidence of the novel method in Taiwan during the initial stage of the suicide epidemic. Regulating media reporting of novel suicide methods may prevent an epidemic spread of such new methods.\n      "], "author_display": ["Ying-Yeh Chen", "Feng Chen", "David Gunnell", "Paul S. F. Yip"], "article_type": "Research Article", "score": 0.4154965, "title_display": "The Impact of Media Reporting on the Emergence of Charcoal Burning Suicide in Taiwan", "publication_date": "2013-01-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0055000"}, {"journal": "PLoS ONE", "abstract": ["\n        Demand for high quality gene expression data has driven the development of revolutionary microarray technologies. The quality of the data is affected by the performance of the microarray platform as well as how the nucleic acid targets are prepared. The most common method for target nucleic acid preparation includes in vitro transcription amplification of the sample RNA. Although this method requires a small amount of starting material and is reported to have high reproducibility, there are also technical disadvantages such as amplification bias and the long, laborious protocol. Using RNA derived from human brain, breast and colon, we demonstrate that a non-amplification method, which was previously shown to be inferior, could be transformed to a highly quantitative method with a dynamic range of five orders of magnitude. Furthermore, the correlation coefficient calculated by comparing microarray assays using non-amplified samples with qRT-PCR assays was approximately 0.9, a value much higher than when samples were prepared using amplification methods. Our results were also compared with data from various microarray platforms studied in the MicroArray Quality Control (MAQC) project. In combination with micro-columnar 3D-Gene\u2122 microarray, this non-amplification method is applicable to a variety of genetic analyses, including biomarker screening and diagnostic tests for cancer.\n      "], "author_display": ["Hiroko Sudo", "Atsuko Mizoguchi", "Junpei Kawauchi", "Hideo Akiyama", "Satoko Takizawa"], "article_type": "Research Article", "score": 0.4154417, "title_display": "Use of Non-Amplified RNA Samples for Microarray Analysis of Gene Expression", "publication_date": "2012-02-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0031397"}, {"journal": "PLoS ONE", "abstract": ["\nCoral reef calcification is predicted to decline as a result of ocean acidification and other anthropogenic stressors. The majority of studies predicting declines based on in situ relationships between environmental parameters and net community calcification rate have been location-specific, preventing accurate predictions for coral reefs globally. In this study, net community calcification and production were measured on a coral reef flat at One Tree Island, Great Barrier Reef, using Lagrangian flow respirometry and slack water methods. Net community calcification, daytime net photosynthesis and nighttime respiration were higher under the flow respirometry method, likely due to increased water flow relative to the slack water method. The two methods also varied in the degrees to which they were influenced by potential measurement uncertainties. The difference in the results from these two commonly used methods implies that some of the location-specific differences in coral reef community metabolism may be due to differences in measurement methods.\n"], "author_display": ["Emily C. Shaw", "Stuart R. Phinn", "Bronte Tilbrook", "Andy Steven"], "article_type": "Research Article", "score": 0.4154194, "title_display": "Comparability of Slack Water and Lagrangian Flow Respirometry Methods for Community Metabolic Measurements", "publication_date": "2014-11-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0112161"}, {"journal": "PLoS ONE", "abstract": ["\nSex identification in ancient human remains is a common problem especially if the skeletons are sub-adult, incomplete or damaged. In this paper we propose a new method to identify sex, based on real-time PCR amplification of small fragments (61 and 64 bp) of the third exon within the amelogenin gene covering a 3-bp deletion on the AMELX-allele, followed by a High Resolution Melting analysis (HRM). HRM is based on the melting curves of amplified fragments. The amelogenin gene is located on both chromosomes X and Y, showing dimorphism in length. This molecular tool is rapid, sensitive and reduces the risk of contamination from exogenous genetic material when used for ancient DNA studies. The accuracy of the new method described here has been corroborated by using control samples of known sex and by contrasting our results with those obtained with other methods. Our method has proven to be useful even in heavily degraded samples, where other previously published methods failed. Stochastic problems such as the random allele drop-out phenomenon are expected to occur in a less severe form, due to the smaller fragment size to be amplified. Thus, their negative effect could be easier to overcome by a proper experimental design.\n"], "author_display": ["Brenda A. \u00c1lvarez-Sandoval", "Linda R. Manzanilla", "Rafael Montiel"], "article_type": "Research Article", "score": 0.41536754, "title_display": "Sex Determination in Highly Fragmented Human DNA by High-Resolution Melting (HRM) Analysis", "publication_date": "2014-08-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0104629"}, {"journal": "PLoS ONE", "abstract": ["\nA key challenge in the data analysis of biological high-throughput experiments is to handle the often low number of samples in the experiments compared to the number of biomolecules that are simultaneously measured. Combining experimental data using independent technologies to illuminate the same biological trends, as well as complementing each other in a larger perspective, is one natural way to overcome this challenge. In this work we investigated if integrating proteomics and transcriptomics data from a brain cancer animal model using gene set based analysis methodology, could enhance the biological interpretation of the data relative to more traditional analysis of the two datasets individually. The brain cancer model used is based on serial passaging of transplanted human brain tumor material (glioblastoma - GBM) through several generations in rats. These serial transplantations lead over time to genotypic and phenotypic changes in the tumors and represent a medically relevant model with a rare access to samples and where consequent analyses of individual datasets have revealed relatively few significant findings on their own. We found that the integrated analysis both performed better in terms of significance measure of its findings compared to individual analyses, as well as providing independent verification of the individual results. Thus a better context for overall biological interpretation of the data can be achieved.\n"], "author_display": ["Kjell Petersen", "Uros Rajcevic", "Siti Aminah Abdul Rahim", "Inge Jonassen", "Karl-Henning Kalland", "Connie R. Jimenez", "Rolf Bjerkvig", "Simone P. Niclou"], "article_type": "Research Article", "score": 0.4151145, "title_display": "Gene Set Based Integrated Data Analysis Reveals Phenotypic Differences in a Brain Cancer Model", "publication_date": "2013-07-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0068288"}, {"journal": "PLoS ONE", "abstract": ["\nThe existing empirical research exploring the impact of threat appeals on driver behavior has reported inconsistent findings. In an effort to provide an up-to-date synthesis of the experimental findings, meta-analytic techniques were employed to examine the impact of threat-based messages on fear arousal and on lab-based indices of driving behavior. Experimental studies (k\u200a=\u200a13, N\u200a=\u200a3044), conducted between 1990 and 2011, were included in the analyses. The aims of the current analysis were (a) to examine whether or not the experimental manipulations had a significant impact on evoked fear, (b) to examine the impact of threat appeals on three distinct indices of driving, and (c) to identify moderators and mediators of the relationship between fear and driving outcomes. Large effects emerged for the level of fear evoked, with experimental groups reporting increased fear arousal in comparison to control groups (r\u200a=\u200a.64, n\u200a=\u200a619, p<.01). The effect of threat appeals on driving outcomes, however, was not significant (r\u200a=\u200a.03, p\u200a=\u200a.17). This analysis of the experimental literature indicates that threat appeals can lead to increased fear arousal, but do not appear to have the desired impact on driving behavior. We discuss these findings in the context of threat-based road safety campaigns and future directions for experimental research in this area.\n"], "author_display": ["Rachel N. Carey", "Daragh T. McDermott", "Kiran M. Sarma"], "article_type": "Research Article", "score": 0.41511235, "title_display": "The Impact of Threat Appeals on Fear Arousal and Driver Behavior: A Meta-Analysis of Experimental Research 1990\u20132011", "publication_date": "2013-05-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0062821"}, {"journal": "PLoS ONE", "abstract": ["\nUncovering community structures is important for understanding networks. Currently, several nonnegative matrix factorization algorithms have been proposed for discovering community structure in complex networks. However, these algorithms exhibit some drawbacks, such as unstable results and inefficient running times. In view of the problems, a novel approach that utilizes an initialized Bayesian nonnegative matrix factorization model for determining community membership is proposed. First, based on singular value decomposition, we obtain simple initialized matrix factorizations from approximate decompositions of the complex network\u2019s adjacency matrix. Then, within a few iterations, the final matrix factorizations are achieved by the Bayesian nonnegative matrix factorization method with the initialized matrix factorizations. Thus, the network\u2019s community structure can be determined by judging the classification of nodes with a final matrix factor. Experimental results show that the proposed method is highly accurate and offers competitive performance to that of the state-of-the-art methods even though it is not designed for the purpose of modularity maximization.\n"], "author_display": ["Xianchao Tang", "Tao Xu", "Xia Feng", "Guoqing Yang"], "article_type": "Research Article", "score": 0.41489318, "title_display": "Uncovering Community Structures with Initialized Bayesian Nonnegative Matrix Factorization", "publication_date": "2014-09-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0107884"}, {"journal": "PLOS ONE", "abstract": ["\nConsumers prefer to pay low prices and increase animal welfare; however consumers are typically forced to make tradeoffs between price and animal welfare. Campaign advertising (i.e., advertising used during the 2008 vote on Proposition 2 in California) may affect how consumers make tradeoffs between price and animal welfare. Neuroimaging data was used to determine the effects of brain activation in dorsolateral prefrontal cortex (dlPFC) on choices making a tradeoff between price and animal welfare and responsiveness to campaign advertising. Results indicated that activation in the dlPFC was greater when making choices that forced a tradeoff between price and animal welfare, compared to choices that varied only by price or animal welfare. Furthermore, greater activation differences in right dlPFC between choices that forced a tradeoff and choices that did not, indicated greater responsiveness to campaign advertising.\n"], "author_display": ["Brandon R. McFadden", "Jayson L. Lusk", "John M. Crespi", "J. Bradley C. Cherry", "Laura E. Martin", "Robin L. Aupperle", "Amanda S. Bruce"], "article_type": "Research Article", "score": 0.41486403, "title_display": "Can Neural Activation in Dorsolateral Prefrontal Cortex Predict Responsiveness to Information? An Application to Egg Production Systems and Campaign Advertising", "publication_date": "2015-05-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0125243"}, {"journal": "PLoS Genetics", "abstract": ["Plasmode is a term coined several years ago to describe data sets that are derived from real data but for which some truth is known. Omic techniques, most especially microarray and genomewide association studies, have catalyzed a new zeitgeist of data sharing that is making data and data sets publicly available on an unprecedented scale. Coupling such data resources with a science of plasmode use would allow statistical methodologists to vet proposed techniques empirically (as opposed to only theoretically) and with data that are by definition realistic and representative. We illustrate the technique of empirical statistics by consideration of a common task when analyzing high dimensional data: the simultaneous testing of hundreds or thousands of hypotheses to determine which, if any, show statistical significance warranting follow-on research. The now-common practice of multiple testing in high dimensional experiment (HDE) settings has generated new methods for detecting statistically significant results. Although such methods have heretofore been subject to comparative performance analysis using simulated data, simulating data that realistically reflect data from an actual HDE remains a challenge. We describe a simulation procedure using actual data from an HDE where some truth regarding parameters of interest is known. We use the procedure to compare estimates for the proportion of true null hypotheses, the false discovery rate (FDR), and a local version of FDR obtained from 15 different statistical methods.Author Summary: Plasmode is a term used to describe a data set that has been derived from real data but for which some truth is known. Statistical methods that analyze data from high dimensional experiments (HDEs) seek to estimate quantities that are of interest to scientists, such as mean differences in gene expression levels and false discovery rates. The ability of statistical methods to accurately estimate these quantities depends on theoretical derivations or computer simulations. In computer simulations, data for which the true value of a quantity is known are often simulated from statistical models, and the ability of a statistical method to estimate this quantity is evaluated on the simulated data. However, in HDEs there are many possible statistical models to use, and which models appropriately produce data that reflect properties of real data is an open question. We propose the use of plasmodes as one answer to this question. If done carefully, plasmodes can produce data that reflect reality while maintaining the benefits of simulated data. We show one method of generating plasmodes and illustrate their use by comparing the performance of 15 statistical methods for estimating the false discovery rate in data from an HDE. "], "author_display": ["Gary L. Gadbury", "Qinfang Xiang", "Lin Yang", "Stephen Barnes", "Grier P. Page", "David B. Allison"], "article_type": "Research Article", "score": 0.41479173, "title_display": "Evaluating Statistical Methods Using Plasmode Data Sets in the Age of Massive Public Databases: An Illustration Using False Discovery Rates", "publication_date": "2008-06-20T00:00:00Z", "eissn": "1553-7404", "id": "10.1371/journal.pgen.1000098"}, {"journal": "PLoS Computational Biology", "abstract": ["\n\t\t\t\tGenomic DNA copy-number alterations (CNAs) are associated with complex diseases, including cancer: CNAs are indeed related to tumoral grade, metastasis, and patient survival. CNAs discovered from array-based comparative genomic hybridization (aCGH) data have been instrumental in identifying disease-related genes and potential therapeutic targets. To be immediately useful in both clinical and basic research scenarios, aCGH data analysis requires accurate methods that do not impose unrealistic biological assumptions and that provide direct answers to the key question, \u201cWhat is the probability that this gene/region has CNAs?\u201d Current approaches fail, however, to meet these requirements. Here, we introduce reversible jump aCGH (RJaCGH), a new method for identifying CNAs from aCGH; we use a nonhomogeneous hidden Markov model fitted via reversible jump Markov chain Monte Carlo; and we incorporate model uncertainty through Bayesian model averaging. RJaCGH provides an estimate of the probability that a gene/region has CNAs while incorporating interprobe distance and the capability to analyze data on a chromosome or genome-wide basis. RJaCGH outperforms alternative methods, and the performance difference is even larger with noisy data and highly variable interprobe distance, both commonly found features in aCGH data. Furthermore, our probabilistic method allows us to identify minimal common regions of CNAs among samples and can be extended to incorporate expression data. In summary, we provide a rigorous statistical framework for locating genes and chromosomal regions with CNAs with potential applications to cancer and other complex human diseases.\n\t\t\t: As a consequence of problems during cell division, the number of copies of a gene in a chromosome can either increase or decrease. These copy-number alterations (CNAs) can play a crucial role in the emergence of complex multigenic diseases. For example, in cancer, amplification of oncogenes can drive tumor activation, and CNAs are associated with metastasis development and patient survival. Studies on the relationship between CNAs and disease have been recently fueled by the widespread use of array-based comparative genomic hybridization (aCGH), a technique with much finer resolution than previous experimental approaches. Detection of CNAs from these data depends on methods of analysis that do not impose biologically unrealistic assumptions and that provide direct answers to fundamental research questions. We have developed a statistical method, using a Bayesian approach, that returns estimates of the probabilities of CNAs from aCGH data, the most direct and valuable answer to the key biological question: \u201cWhat is the probability that this gene/region has an altered copy number?\u201d The output of the method can therefore be immediately used in different settings from clinical to basic research scenarios, and is applicable over a wide variety of aCGH technologies. "], "author_display": ["Oscar M Rueda", "Ram\u00f3n D\u00edaz-Uriarte"], "article_type": "Research Article", "score": 0.41475818, "title_display": "Flexible and Accurate Detection of Genomic Copy-Number Changes from aCGH", "publication_date": "2007-06-22T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.0030122"}, {"journal": "PLOS ONE", "abstract": ["Aim: Candida species are known as opportunistic pathogens, and a possible cause of invasive infections. Because of their species-specific antimycotic resistance patterns, reliable techniques for their detection, quantification and identification are needed. We validated a DNA amplification method for direct detection of Candida spp. from clinical samples, namely the ITS2-High Resolution Melting Analysis (direct method), by comparing it with a culture and MALDI-TOF Mass Spectrometry based method (indirect method) to establish the presence of Candida species in three different types of clinical samples. Materials and Methods: A total of 347 clinical samples, i.e. throat swabs, rectal swabs and vaginal swabs, were collected from the gynaecology/obstetrics, intensive care and haematology wards at the Ghent University Hospital, Belgium. For the direct method, ITS2-HRM was preceded by NucliSENS easyMAG DNA extraction, directly on the clinical samples. For the indirect method, clinical samples were cultured on Candida ID and individual colonies were identified by MALDI-TOF. Results: For 83.9% of the samples there was complete concordance between both techniques, i.e. the same Candida species were detected in 31.1% of the samples or no Candida species were detected in 52.8% of the samples. In 16.1% of the clinical samples, discrepant results were obtained, of which only 6.01% were considered as major discrepancies. Discrepancies occurred mostly when overall numbers of Candida cells in the samples were low and/or when multiple species were present in the sample. Discussion: Most of the discrepancies could be decided in the advantage of the direct method. This is due to samples in which no yeast could be cultured whereas low amounts could be detected by the direct method and to samples in which high quantities of Candida robusta according to ITS2-HRM were missed by culture on Candida ID agar. It remains to be decided whether the diagnostic advantages of the direct method compensate for its disadvantages. "], "author_display": ["Hans Duyvejonck", "Piet Cools", "Johan Decruyenaere", "Kristien Roelens", "Lucien Noens", "Stefan Vermeulen", "Geert Claeys", "Ellen Decat", "Els Van Mechelen", "Mario Vaneechoutte"], "article_type": "Research Article", "score": 0.4146906, "title_display": "Validation of High Resolution Melting Analysis (HRM) of the Amplified ITS2 Region for the Detection and Identification of Yeasts from Clinical Samples: Comparison with Culture and MALDI-TOF Based Identification", "publication_date": "2015-08-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0132149"}, {"journal": "PLoS ONE", "abstract": ["Background: We estimated U.S. biomedical research funding across therapeutic areas, determined the association with disease burden, and evaluated new drug approvals that resulted from this investment. Methodology/Principal Findings: We calculated funding from 1995 to 2005 and totaled Food and Drug Administration approvals in eight therapeutic areas (cardiovascular, endocrine, gastrointestinal, genitourinary, HIV/AIDS, infectious disease excluding HIV, oncology, and respiratory) primarily using public data. We then calculated correlations between funding, published estimates of disease burden, and drug approvals. Conclusions/Significance: Across therapeutic areas, biomedical research funding increased substantially, appears aligned with disease burden in high income countries, but is not linked to new drug approvals. The translational gap between funding and new therapies is affecting all of medicine, and remedies must include changes beyond additional financial investment. "], "author_display": ["E. Ray Dorsey", "Joel P. Thompson", "Melisa Carrasco", "Jason de Roulet", "Philip Vitticore", "Sean Nicholson", "S. Claiborne Johnston", "Robert G. Holloway", "Hamilton Moses"], "article_type": "Research Article", "score": 0.41426253, "title_display": "Financing of U.S. Biomedical Research and New Drug Approvals across Therapeutic Areas", "publication_date": "2009-09-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0007015"}, {"journal": "PLoS Computational Biology", "abstract": ["\nPenalized Multiple Regression (PMR) can be used to discover novel disease associations in GWAS datasets. In practice, proposed PMR methods have not been able to identify well-supported associations in GWAS that are undetectable by standard association tests and thus these methods are not widely applied. Here, we present a combined algorithmic and heuristic framework for PUMA (Penalized Unified Multiple-locus Association) analysis that solves the problems of previously proposed methods including computational speed, poor performance on genome-scale simulated data, and identification of too many associations for real data to be biologically plausible. The framework includes a new minorize-maximization (MM) algorithm for generalized linear models (GLM) combined with heuristic model selection and testing methods for identification of robust associations. The PUMA framework implements the penalized maximum likelihood penalties previously proposed for GWAS analysis (i.e. Lasso, Adaptive Lasso, NEG, MCP), as well as a penalty that has not been previously applied to GWAS (i.e. LOG). Using simulations that closely mirror real GWAS data, we show that our framework has high performance and reliably increases power to detect weak associations, while existing PMR methods can perform worse than single marker testing in overall performance. To demonstrate the empirical value of PUMA, we analyzed GWAS data for type 1 diabetes, Crohns's disease, and rheumatoid arthritis, three autoimmune diseases from the original Wellcome Trust Case Control Consortium. Our analysis replicates known associations for these diseases and we discover novel etiologically relevant susceptibility loci that are invisible to standard single marker tests, including six novel associations implicating genes involved in pancreatic function, insulin pathways and immune-cell function in type 1 diabetes; three novel associations implicating genes in pro- and anti-inflammatory pathways in Crohn's disease; and one novel association implicating a gene involved in apoptosis pathways in rheumatoid arthritis. We provide software for applying our PUMA analysis framework.\nAuthor Summary: Genome-wide association studies (GWAS) have identified hundreds of regions of the human genome that are associated with susceptibility to common diseases. Yet many lines of evidence indicate that many susceptibility loci, which cannot be detected by standard statistical methods, remain to be discovered. We have developed PUMA, a framework for applying a family of penalized regression methods that simultaneously consider multiple susceptibility loci in the same statistical model. We demonstrate through simulations that our framework has increased power to detect weak associations compared to both standard GWAS analysis methods and previous applications of penalized methods. We applied PUMA to identify novel susceptibility loci for type 1 diabetes, Crohn's disease and rheumatoid arthritis, where the novel disease loci we identified have been previously associated with similar diseases or are known to function in relevant biological pathways. "], "author_display": ["Gabriel E. Hoffman", "Benjamin A. Logsdon", "Jason G. Mezey"], "article_type": "Research Article", "score": 0.413932, "title_display": "PUMA: A Unified Framework for Penalized Multiple Regression Analysis of GWAS Data", "publication_date": "2013-06-27T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1003101"}, {"journal": "PLoS ONE", "abstract": ["Background: The aim of this study was to test seven previously published image-input methods in state-of-the-art high resolution PET brain images. Images were obtained with a High Resolution Research Tomograph plus a resolution-recovery reconstruction algorithm using two different radioligands with different radiometabolite fractions. Three of the methods required arterial blood samples to scale the image-input, and four were blood-free methods. Methods: All seven methods were tested on twelve scans with [11C](R)-rolipram, which has a low radiometabolite fraction, and on nineteen scans with [11C]PBR28 (high radiometabolite fraction). Logan VT values for both blood and image inputs were calculated using the metabolite-corrected input functions. The agreement of image-derived Logan VT values with the reference blood-derived Logan VT values was quantified using a scoring system. Using the image input methods that gave the most accurate results with Logan analysis, we also performed kinetic modelling with a two-tissue compartment model. Results: For both radioligands the highest scores were obtained with two blood-based methods, while the blood-free methods generally performed poorly. All methods gave higher scores with [11C](R)-rolipram, which has a lower metabolite fraction. Compartment modeling gave less reliable results, especially for the estimation of individual rate constants. Conclusion: Our study shows that: 1) Image input methods that are validated for a specific tracer and a specific machine may not perform equally well in a different setting; 2) despite the use of high resolution PET images, blood samples are still necessary to obtain a reliable image input function; 3) the accuracy of image input may also vary between radioligands depending on the magnitude of the radiometabolite fraction: the higher the metabolite fraction of a given tracer (e.g., [11C]PBR28), the more difficult it is to obtain a reliable image-derived input function; and 4) in association with image inputs, graphical analyses should be preferred over compartmental modelling. "], "author_display": ["Paolo Zanotti-Fregonara", "Jeih-San Liow", "Masahiro Fujita", "Elodie Dusch", "Sami S. Zoghbi", "Elise Luong", "Ronald Boellaard", "Victor W. Pike", "Claude Comtat", "Robert B. Innis"], "article_type": "Research Article", "score": 0.41392553, "title_display": "Image-Derived Input Function for Human Brain Using High Resolution PET Imaging with [<sup>11</sup>C](<i>R</i>)-rolipram and [<sup>11</sup>C]PBR28", "publication_date": "2011-02-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0017056"}, {"journal": "PLoS ONE", "abstract": ["\n\t\t\t\tIn the last decade, an extensive effort has been made to characterize the human microbiota, due to its clinical and economic interests. However, a metagenomic approach to the skin microbiota is hampered by the high proportion of host DNA that is recovered. In contrast with the burgeoning field of gut metagenomics, skin metagenomics has been hindered by the absence of an efficient method to avoid sequencing the host DNA. We present here a method for recovering microbial DNA from skin samples, based on a combination of molecular techniques. We have applied this method to mouse skin, and have validated it by standard, quantitative PCR and amplicon sequencing of 16S rRNA. The taxonomic diversity recovered was not altered by this new method, as proved by comparing the phylogenetic structure revealed by 16S rRNA sequencing in untreated vs. treated samples. As proof of concept, we also present the first two mouse skin metagenomes, which allowed discovering new taxa (not only prokaryotes but also viruses and eukaryots) not reachable by 16S rRNA sequencing, as well as to characterize the skin microbiome functional landscape. Our method paves the way for the development of skin metagenomics, which will allow a much deeper knowledge of the skin microbiome and its relationship with the host, both in a healthy state and in relation to disease. \n\t\t\t"], "author_display": ["Marc Garcia-Garcer\u00e0", "Koldo Garcia-Etxebarria", "Mireia Coscoll\u00e0", "Amparo Latorre", "Francesc Calafell"], "article_type": "Research Article", "score": 0.4138807, "title_display": "A New Method for Extracting Skin Microbes Allows Metagenomic Analysis of Whole-Deep Skin", "publication_date": "2013-09-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0074914"}, {"abstract": ["\n        The new technology of protein binding microarrays (PBMs) allows simultaneous measurement of the binding intensities of a transcription factor to tens of thousands of synthetic double-stranded DNA probes, covering all possible 10-mers. A key computational challenge is inferring the binding motif from these data. We present a systematic comparison of four methods developed specifically for reconstructing a binding site motif represented as a positional weight matrix from PBM data. The reconstructed motifs were evaluated in terms of three criteria: concordance with reference motifs from the literature and ability to predict in vivo and in vitro bindings. The evaluation encompassed over 200 transcription factors and some 300 assays. The results show a tradeoff between how the methods perform according to the different criteria, and a dichotomy of method types. Algorithms that construct motifs with low information content predict PBM probe ranking more faithfully, while methods that produce highly informative motifs match reference motifs better. Interestingly, in predicting high-affinity binding, all methods give far poorer results for in vivo assays compared to in vitro assays.\n      "], "author_display": ["Yaron Orenstein", "Chaim Linhart", "Ron Shamir"], "article_type": "Research Article", "score": 0.4138508, "title_display": "Assessment of Algorithms for Inferring Positional Weight Matrix Motifs of Transcription Factor Binding Sites Using Protein Binding Microarray Data", "publication_date": "2012-09-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0046145"}, {"journal": "PLoS ONE", "abstract": ["\n        We developed a marker based infinitesimal model for quantitative trait analysis. In contrast to the classical infinitesimal model, we now have new information about the segregation of every individual locus of the entire genome. Under this new model, we propose that the genetic effect of an individual locus is a function of the genome location (a continuous quantity). The overall genetic value of an individual is the weighted integral of the genetic effect function along the genome. Numerical integration is performed to find the integral, which requires partitioning the entire genome into a finite number of bins. Each bin may contain many markers. The integral is approximated by the weighted sum of all the bin effects. We now turn the problem of marker analysis into bin analysis so that the model dimension has decreased from a virtual infinity to a finite number of bins. This new approach can efficiently handle virtually unlimited number of markers without marker selection. The marker based infinitesimal model requires high linkage disequilibrium of all markers within a bin. For populations with low or no linkage disequilibrium, we develop an adaptive infinitesimal model. Both the original and the adaptive models are tested using simulated data as well as beef cattle data. The simulated data analysis shows that there is always an optimal number of bins at which the predictability of the bin model is much greater than the original marker analysis. Result of the beef cattle data analysis indicates that the bin model can increase the predictability from 10% (multiple marker analysis) to 33% (multiple bin analysis). The marker based infinitesimal model paves a way towards the solution of genetic mapping and genomic selection using the whole genome sequence data.\n      "], "author_display": ["Zhiqiu Hu", "Zhiquan Wang", "Shizhong Xu"], "article_type": "Research Article", "score": 0.41379112, "title_display": "An Infinitesimal Model for Quantitative Trait Genomic Value Prediction", "publication_date": "2012-07-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0041336"}, {"journal": "PLoS ONE", "abstract": ["Background: Circulating cell-free (ccf) fetal DNA comprises 3\u201320% of all the cell-free DNA present in maternal plasma. Numerous research and clinical studies have described the analysis of ccf DNA using next generation sequencing for the detection of fetal aneuploidies with high sensitivity and specificity. We sought to extend the utility of this approach by assessing semi-automated library preparation, higher sample multiplexing during sequencing, and improved bioinformatic tools to enable a higher throughput, more efficient assay while maintaining or improving clinical performance. Methods: Whole blood (10mL) was collected from pregnant female donors and plasma separated using centrifugation. Ccf DNA was extracted using column-based methods. Libraries were prepared using an optimized semi-automated library preparation method and sequenced on an Illumina HiSeq2000 sequencer in a 12-plex format. Z-scores were calculated for affected chromosomes using a robust method after normalization and genomic segment filtering. Classification was based upon a standard normal transformed cutoff value of z\u200a=\u200a3 for chromosome 21 and z\u200a=\u200a3.95 for chromosomes 18 and 13. Results: Two parallel assay development studies using a total of more than 1900 ccf DNA samples were performed to evaluate the technical feasibility of automating library preparation and increasing the sample multiplexing level. These processes were subsequently combined and a study of 1587 samples was completed to verify the stability of the process-optimized assay. Finally, an unblinded clinical evaluation of 1269 euploid and aneuploid samples utilizing this high-throughput assay coupled to improved bioinformatic procedures was performed. We were able to correctly detect all aneuploid cases with extremely low false positive rates of 0.09%, <0.01%, and 0.08% for trisomies 21, 18, and 13, respectively. Conclusions: These data suggest that the developed laboratory methods in concert with improved bioinformatic approaches enable higher sample throughput while maintaining high classification accuracy. "], "author_display": ["Taylor J. Jensen", "Tricia Zwiefelhofer", "Roger C. Tim", "\u017deljko D\u017eakula", "Sung K. Kim", "Amin R. Mazloom", "Zhanyang Zhu", "John Tynan", "Tim Lu", "Graham McLennan", "Glenn E. Palomaki", "Jacob A. Canick", "Paul Oeth", "Cosmin Deciu", "Dirk van den Boom", "Mathias Ehrich"], "article_type": "Research Article", "score": 0.41349792, "title_display": "High-Throughput Massively Parallel Sequencing for Fetal Aneuploidy Detection from Maternal Plasma", "publication_date": "2013-03-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0057381"}, {"journal": "PLoS ONE", "abstract": ["\nStructural genomic variations play an important role in human disease and phenotypic diversity. With the rise of high-throughput sequencing tools, mate-pair/paired-end/single-read sequencing has become an important technique for the detection and exploration of structural variation. Several analysis tools exist to handle different parts and aspects of such sequencing based structural variation analyses pipelines. A comprehensive analysis platform to handle all steps, from processing the sequencing data, to the discovery and visualization of structural variants, is missing. The ViVar platform is built to handle the discovery of structural variants, from Depth Of Coverage analysis, aberrant read pair clustering to split read analysis. ViVar provides you with powerful visualization options, enables easy reporting of results and better usability and data management. The platform facilitates the processing, analysis and visualization, of structural variation based on massive parallel sequencing data, enabling the rapid identification of disease loci or genes. ViVar allows you to scale your analysis with your work load over multiple (cloud) servers, has user access control to keep your data safe and is easy expandable as analysis techniques advance. URL: https://www.cmgg.be/vivar/\n"], "author_display": ["Tom Sante", "Sarah Vergult", "Pieter-Jan Volders", "Wigard P. Kloosterman", "Geert Trooskens", "Katleen De Preter", "Annelies Dheedene", "Frank Speleman", "Tim De Meyer", "Bj\u00f6rn Menten"], "article_type": "Research Article", "score": 0.41349742, "title_display": "ViVar: A Comprehensive Platform for the Analysis and Visualization of Structural Genomic Variation", "publication_date": "2014-12-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0113800"}, {"journal": "PLoS ONE", "abstract": ["\nBetween species differences in research effort can lead to biases in our global view of evolution, ecology and conservation. The increase in meta-taxonomic comparative analyses on birds underlines the need to better address how research effort is distributed in this class. Methods have been developed to choose which species should be studied to obtain unbiased comparative data sets, but a precise and global knowledge of research effort is required to be able to properly apply them. We address this issue by providing a data set of research effort (number of papers from 1978 to 2008 in the Zoological Record database) estimates for the 10 064 species of birds. We then test whether research effort is associated with phylogeny, geography and eleven different life history and ecological traits. We show that phylogeny accounts for a large proportion of the variance, while geographic range and all the tested traits are also significant contributors to research effort variance. We identify avian taxa that are under- and overstudied and address the importance of research effort biases in evaluating vulnerability to extinction, with non-threatened species studied twice as much as threatened ones. Our research effort data set covering the entire class Aves provides a tool for researchers to incorporate this potential confounding variable in comparative analyses.\n"], "author_display": ["Simon Ducatez", "Louis Lefebvre"], "article_type": "Research Article", "score": 0.4134533, "title_display": "Patterns of Research Effort in Birds", "publication_date": "2014-02-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0089955"}, {"journal": "PLoS ONE", "abstract": ["Objectives: Phosphorylated AKT (p-AKT), constitutive activation of AKT, is a potentially interesting prognostic marker and therapeutic target in non-small cell lung cancer (NSCLC). However, the available results of p-AKT expression in NSCLC are heterogeneous. Therefore, a meta-analysis of published researches investigating the prognostic relevance of p-AKT expression in patients with NSCLC was performed.  Materials and Methods: A literature search via PubMed, EMBASE and CNKI (China National Knowledge Infrastructure) databases was conducted. Data from eligible studies were extracted and included into meta-analysis using a random effects model.  Results: A total of 1049 patients from nine studies were included in the meta-analysis. Nine studies investigated the relationship between p-AKT expression and overall survival using univariate analysis, and five of these undertook multivariate analysis. The pooled hazard ratio (HR) for overall survival was 1.49 (95% confidence interval (CI): 1.01-2.20) by univariate analysis and 1.02 (95% CI: 0.54-1.95) by multivariate analysis.  Conclusion: Our study shows that positive expression of p-AKT is associated with poor prognosis in patients with NSCLC. However, adequately designed prospective studies need to perform. "], "author_display": ["Zhi-Xin Qiu", "Kui Zhang", "Xue-Song Qiu", "Min Zhou", "Wei-Min Li"], "article_type": "Research Article", "score": 0.41343993, "title_display": "The Prognostic Value of Phosphorylated AKT Expression in Non-Small Cell Lung Cancer: A Meta-Analysis", "publication_date": "2013-12-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0081451"}, {"journal": "PLoS ONE", "abstract": ["\n        Covariation analysis is used to identify those positions with similar patterns of sequence variation in an alignment of RNA sequences. These constraints on the evolution of two positions are usually associated with a base pair in a helix. While mutual information (MI) has been used to accurately predict an RNA secondary structure and a few of its tertiary interactions, early studies revealed that phylogenetic event counting methods are more sensitive and provide extra confidence in the prediction of base pairs. We developed a novel and powerful phylogenetic events counting method (PEC) for quantifying positional covariation with the Gutell lab\u2019s new RNA Comparative Analysis Database (rCAD). The PEC and MI-based methods each identify unique base pairs, and jointly identify many other base pairs. In total, both methods in combination with an N-best and helix-extension strategy identify the maximal number of base pairs. While covariation methods have effectively and accurately predicted RNAs secondary structure, only a few tertiary structure base pairs have been identified. Analysis presented herein and at the Gutell lab\u2019s Comparative RNA Web (CRW) Site reveal that the majority of these latter base pairs do not covary with one another. However, covariation analysis does reveal a weaker although significant covariation between sets of nucleotides that are in proximity in the three-dimensional RNA structure. This reveals that covariation analysis identifies other types of structural constraints beyond the two nucleotides that form a base pair.\n      "], "author_display": ["Lei Shang", "Weijia Xu", "Stuart Ozer", "Robin R. Gutell"], "article_type": "Research Article", "score": 0.4133729, "title_display": "Structural Constraints Identified with Covariation Analysis in Ribosomal RNA", "publication_date": "2012-06-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0039383"}, {"journal": "PLoS ONE", "abstract": ["Objectives: This study provides insights into the validity and acceptability of Discrete Choice Experiment (DCE) and profile-case Best Worst Scaling (BWS) methods for eliciting preferences for health care in a priority-setting context. Methods: An adult sample (N\u200a=\u200a24) undertook a traditional DCE and a BWS choice task as part of a wider survey on Health Technology Assessment decision criteria. A \u2018think aloud\u2019 protocol was applied, whereby participants verbalized their thinking while making choices. Internal validity and acceptability were assessed through a thematic analysis of the decision-making process emerging from the qualitative data and a repeated choice task. Results: A thematic analysis of the decision-making process demonstrated clear evidence of \u2018trading\u2019 between multiple attribute/levels for the DCE, and to a lesser extent for the BWS task. Limited evidence consistent with a sequential decision-making model was observed for the BWS task. For the BWS task, some participants found choosing the worst attribute/level conceptually challenging. A desire to provide a complete ranking from best to worst was observed. The majority (18,75%) of participants indicated a preference for DCE, as they felt this enabled comparison of alternative full profiles. Those preferring BWS were averse to choosing an undesirable characteristic that was part of a \u2018package\u2019, or perceived BWS to be less ethically conflicting or burdensome. In a repeated choice task, more participants were consistent for the DCE (22,92%) than BWS (10,42%) (p\u200a=\u200a0.002). Conclusions: This study supports the validity and acceptability of the traditional DCE format. Findings relating to the application of BWS profile methods are less definitive. Research avenues to further clarify the comparative merits of these preference elicitation methods are identified. "], "author_display": ["Jennifer A. Whitty", "Ruth Walker", "Xanthe Golenko", "Julie Ratcliffe"], "article_type": "Research Article", "score": 0.41331664, "title_display": "A Think Aloud Study Comparing the Validity and Acceptability of Discrete Choice and Best Worst Scaling Methods", "publication_date": "2014-04-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0090635"}, {"journal": "PLoS ONE", "abstract": ["Background: Feasibility of genotyping of hundreds and thousands of single nucleotide polymorphisms (SNPs) in thousands of study subjects have triggered the need for fast, powerful, and reliable methods for genome-wide association analysis. Here we consider a situation when study participants are genetically related (e.g. due to systematic sampling of families or because a study was performed in a genetically isolated population). Of the available methods that account for relatedness, the Measured Genotype (MG) approach is considered the \u2018gold standard\u2019. However, MG is not efficient with respect to time taken for the analysis of genome-wide data. In this context we proposed a fast two-step method called Genome-wide Association using Mixed Model and Regression (GRAMMAR) for the analysis of pedigree-based quantitative traits. This method certainly overcomes the drawback of time limitation of the measured genotype (MG) approach, but pays in power. One of the major drawbacks of both MG and GRAMMAR, is that they crucially depend on the availability of complete and correct pedigree data, which is rarely available. Methodology: In this study we first explore type 1 error and relative power of MG, GRAMMAR, and Genomic Control (GC) approaches for genetic association analysis. Secondly, we propose an extension to GRAMMAR i.e. GRAMMAR-GC. Finally, we propose application of GRAMMAR-GC using the kinship matrix estimated through genomic marker data, instead of (possibly missing and/or incorrect) genealogy. Conclusion: Through simulations we show that MG approach maintains high power across a range of heritabilities and possible pedigree structures, and always outperforms other contemporary methods. We also show that the power of our proposed GRAMMAR-GC approaches to that of the \u2018gold standard\u2019 MG for all models and pedigrees studied. We show that this method is both feasible and powerful and has correct type 1 error in the context of genome-wide association analysis in related individuals. "], "author_display": ["Najaf Amin", "Cornelia M. van Duijn", "Yurii S. Aulchenko"], "article_type": "Research Article", "score": 0.41303968, "title_display": "A Genomic Background Based Method for Association Analysis in Related Individuals", "publication_date": "2007-12-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0001274"}, {"journal": "PLoS ONE", "abstract": ["Context: Although qualitative studies are becoming more appreciated in healthcare, the number of publications of quality studies remains low. Little is known about the frequency and characteristics of citation in qualitative studies. Objective: To compare the academic impact of qualitative studies to that of two quantitative studies: systematic reviews and randomized controlled trials. Methods: Publications in BMJ between 1997 and 2006 (BMJ\u2019s median impact factor was 7.04 during this period) employing qualitative methods were matched to two quantitative studies appearing the same year using PubMed. Using Web of Science, citations within a 24-month publication period were determined. Additionally, three hypotheses were examined: qualitative studies are 1) infrequently cited in original articles or reviews; 2) rarely cited by authors in non-English-speaking countries; and 3) more frequently cited in non-medical disciplines (e.g., psychology or sociology). Results: A total of 121 qualitative studies, 270 systematic reviews, and 515 randomised controlled trials were retrieved. Qualitative studies were cited a total of 1,089 times, with a median of 7.00 times (range, 0\u201334) for each study. Matched systematic reviews and randomized controlled trials were cited 2,411times and 1,600 times, respectively. With respect to citing documents, original articles and reviews exceeded 60% for each study design. Relative to quantitative studies, qualitative studies were cited more often by authors in English-speaking countries. With respect to subject area, medical disciplines were more frequently cited than non-medical disciplines for all three study designs (>80%). Conclusion: The median number of citations for qualitative studies was almost the same as the median of BMJ\u2019s impact factor during the survey period. For a suitable evaluation of qualitative studies in healthcare, it will be necessary to develop a reporting framework and include explicit discussions of clinical implications when reporting findings. Coordination between researchers and editors will be needed to achieve this goal. "], "author_display": ["Hiroko Mori", "Takeo Nakayama"], "article_type": "Research Article", "score": 0.41287285, "title_display": "Academic Impact of Qualitative Studies in Healthcare: Bibliometric Analysis", "publication_date": "2013-03-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0057371"}, {"journal": "PLOS ONE", "abstract": ["Background: To date there is no established consensus of assessment criteria for evaluating research ethics review. Methods: We conducted a scoping review of empirical research assessing ethics review processes in order to identify common elements assessed, research foci, and research gaps to aid in the development of assessment criteria. Electronic searches of Ovid Medline, PsychInfo, and the Cochrane DSR, ACP Journal Club, DARE, CCTR, CMR, HTA, and NHSEED, were conducted. After de-duplication, 4234 titles and abstracts were reviewed. Altogether 4036 articles were excluded following screening of titles, abstracts and full text. A total of 198 articles included for final data extraction. Results: Few studies originated from outside North America and Europe. No study reported using an underlying theory or framework of quality/effectiveness to guide study design or analyses. We did not identify any studies that had involved a controlled trial - randomised or otherwise \u2013 of ethics review procedures or processes. Studies varied substantially with respect to outcomes assessed, although tended to focus on structure and timeliness of ethics review. Discussion: Our findings indicate a lack of consensus on appropriate assessment criteria, exemplified by the varied study outcomes identified, but also a fragmented body of research. To date research has been largely quantitative, with little attention given to stakeholder experiences, and is largely cross sectional. A lack of longitudinal research to date precludes analyses of change or assessment of quality improvement in ethics review. "], "author_display": ["Stuart G. Nicholls", "Tavis P. Hayes", "Jamie C. Brehaut", "Michael McDonald", "Charles Weijer", "Raphael Saginur", "Dean Fergusson"], "article_type": "Research Article", "score": 0.41285002, "title_display": "A Scoping Review of Empirical Research Relating to Quality and Effectiveness of Research Ethics Review", "publication_date": "2015-07-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0133639"}, {"journal": "PLoS ONE", "abstract": ["Background: Planning study benefits and payments for participants in international health research in low- income settings can be a difficult and controversial process, with particular challenges in balancing risks of undue inducement and exploitation and understanding how researchers should take account of background inequities. At an international health research programme in Kenya, this study aimed to map local residents' informed and reasoned views on the effects of different levels of study benefits and payments to inform local policy and wider debates in international research. Methods and Findings: Using a relatively novel two-stage process community consultation approach, five participatory workshops involving 90 local residents from diverse constituencies were followed by 15 small group discussions, with components of information-sharing, deliberation and reflection to situate normative reasoning within debates. Framework Analysis drew inductively and deductively on voice- recorded discussions and field notes supported by Nvivo 10 software, and the international research ethics literature. Community members' views on study benefits and payments were diverse, with complex contextual influences and interplay between risks of giving \u2018too many\u2019 and \u2018too few\u2019 benefits, including the role of cash. While recognising important risks for free choice, research relationships and community values in giving \u2018too many\u2019, the greatest concerns were risks of unfairness in giving \u2018too few\u2019 benefits, given difficulties in assessing indirect costs of participation and the serious consequences for families of underestimation, related to perceptions of researchers' responsibilities. Conclusions: Providing benefits and payments to participants in international research in low-income settings is an essential means by which researchers meet individual-level and structural forms of ethical responsibilities, but understanding how this can be achieved requires a careful account of social realities and local judgment. Concerns about undue inducement in low-income communities may often be misplaced; we argue that greater attention should be placed on avoiding unfairness, particularly for the most-poor. "], "author_display": ["Maureen Njue", "Francis Kombe", "Salim Mwalukore", "Sassy Molyneux", "Vicki Marsh"], "article_type": "Research Article", "score": 0.41278967, "title_display": "What Are Fair Study Benefits in International Health Research? Consulting Community Members in Kenya", "publication_date": "2014-12-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0113112"}, {"journal": "PLOS ONE", "abstract": ["\nThe question when and to what extent academic research can benefit society is of great interest to policy-makers and the academic community. Physicians in university hospitals represent a highly relevant test-group for studying the link between research and practice because they engage in biomedical academic research while also providing medical care of measurable quality. Physicians\u2019 research contribution to medical practice can be driven by either high-volume or high-quality research productivity, as often pursuing one productivity strategy excludes the other. To empirically examine the differential contribution to medical practice of the two strategies, we collected secondary data on departments across three specializations (Cardiology, Oncology and Orthopedics) in 50 U.S.-based university hospitals served by 4,330 physicians. Data on volume and quality of biomedical research at each department was correlated with publicly available ratings of departments\u2019 quality of care, demonstrating that high-quality research has significantly greater contribution to quality of care than high-volume research.\n"], "author_display": ["Anat Tchetchik", "Amir Grinstein", "Eran Manes", "Daniel Shapira", "Ronen Durst"], "article_type": "Research Article", "score": 0.4127416, "title_display": "From Research to Practice: Which Research Strategy Contributes More to Clinical Excellence? Comparing High-Volume versus High-Quality Biomedical Research", "publication_date": "2015-06-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0129259"}, {"journal": "PLoS ONE", "abstract": ["\n        Human functional magnetic resonance imaging (fMRI) informs the understanding of the neural basis of mental function and is a key domain of ethical enquiry. It raises questions about the practice and implications of research, and reflexively informs ethics through the empirical investigation of moral judgments. It is at the centre of debate surrounding the importance of neuroscience findings for concepts such as personhood and free will, and the extent of their practical consequences. Here, we map the landscape of fMRI and neuroethics, using citation analysis to uncover salient topics. We find that this landscape is sparsely populated: despite previous calls for debate, there are few articles that discuss both fMRI and ethical, legal, or social implications (ELSI), and even fewer direct citations between the two literatures. Recognizing that practical barriers exist to integrating ELSI discussion into the research literature, we argue nonetheless that the ethical challenges of fMRI, and controversy over its conceptual and practical implications, make this essential.\n      "], "author_display": ["Alex Garnett", "Louise Whiteley", "Heather Piwowar", "Edie Rasmussen", "Judy Illes"], "article_type": "Research Article", "score": 0.41259223, "title_display": "Neuroethics and fMRI: Mapping a Fledgling Relationship", "publication_date": "2011-04-22T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0018537"}, {"journal": "PLoS ONE", "abstract": ["\nRT-qPCR is the accepted technique for the quantification of microRNA (miR) expression: however, stem-loop RT-PCR, the most frequently used method for quantification of miRs, is time- and reagent-consuming as well as inconvenient for scanning. We established a new method called \u2018universal stem-loop primer\u2019 (USLP) with 8 random nucleotides instead of a specific sequence at the 3\u2032 end of the traditional stem-loop primer (TSLP), for screening miR profile and to semi-quantify expression of miRs. Peripheral blood samples were cultured with phytohaemagglutinin (PHA), and then 87 candidate miRs were scanned in cultured T cells. By USLP, our study revealed that the expression of miR-150-5p (miR-150) decreased nearly 10-fold, and miR-155-5p (miR-155) increased more than 7-fold after treated with PHA. The results of the dissociation curve and gel electrophoresis showed that the PCR production of the USLP and TSLP were specificity. The USLP method has high precision because of its low ICV (ICV<2.5%). The sensitivity of the USLP is up to 103 copies/\u00b5l miR. As compared with the TSLP, USLP saved 75% the cost of primers and 60% of the test time. The USLP method is a simple, rapid, precise, sensitive, and cost-effective approach that is suitable for screening miR profiles.\n"], "author_display": ["Li-hong Yang", "Si-lu Wang", "Li-li Tang", "Biao Liu", "Wen-le Ye", "Ling-ling Wang", "Zhang-yang Wang", "Meng-tao Zhou", "Bi-cheng Chen"], "article_type": "Research Article", "score": 0.41236785, "title_display": "Universal Stem-Loop Primer Method for Screening and Quantification of MicroRNA", "publication_date": "2014-12-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0115293"}, {"journal": "PLoS ONE", "abstract": ["Background: DNA extraction is an essential step in all cultivation-independent approaches to characterize microbial diversity, including that associated with the human body. A fundamental challenge in using these approaches has been to isolate DNA that is representative of the microbial community sampled. Methodology/Principal Findings: In this study, we statistically evaluated six commonly used DNA extraction procedures using eleven human-associated bacterial species and a mock community that contained equal numbers of those eleven species. These methods were compared on the basis of DNA yield, DNA shearing, reproducibility, and most importantly representation of microbial diversity. The analysis of 16S rRNA gene sequences from a mock community showed that the observed species abundances were significantly different from the expected species abundances for all six DNA extraction methods used. Conclusions/Significance: Protocols that included bead beating and/or mutanolysin produced significantly better bacterial community structure representation than methods without both of them. The reproducibility of all six methods was similar, and results from different experimenters and different times were in good agreement. Based on the evaluations done it appears that DNA extraction procedures for bacterial community analysis of human associated samples should include bead beating and/or mutanolysin to effectively lyse cells. "], "author_display": ["Sanqing Yuan", "Dora B. Cohen", "Jacques Ravel", "Zaid Abdo", "Larry J. Forney"], "article_type": "Research Article", "score": 0.41233104, "title_display": "Evaluation of Methods for the Extraction and Purification of DNA from the Human Microbiome", "publication_date": "2012-03-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0033865"}, {"journal": "PLoS ONE", "abstract": ["\nSocial network analysis (SNA) helps us understand patterns of interaction between social entities. A number of SNA studies have shed light on the characteristics of research collaboration networks (RCNs). Especially, in the Clinical Translational Science Award (CTSA) community, SNA provides us a set of effective tools to quantitatively assess research collaborations and the impact of CTSA. However, descriptive network statistics are difficult for non-experts to understand. In this article, we present our experiences of building meaningful network visualizations to facilitate a series of visual analysis tasks. The basis of our design is multidimensional, visual aggregation of network dynamics. The resulting visualizations can help uncover hidden structures in the networks, elicit new observations of the network dynamics, compare different investigators and investigator groups, determine critical factors to the network evolution, and help direct further analyses. We applied our visualization techniques to explore the biomedical RCNs at the University of Arkansas for Medical Sciences \u2013 a CTSA institution. And, we created CollaborationViz, an open-source visual analytical tool to help network researchers and administration apprehend the network dynamics of research collaborations through interactive visualization.\n"], "author_display": ["Jiang Bian", "Mengjun Xie", "Teresa J. Hudson", "Hari Eswaran", "Mathias Brochhausen", "Josh Hanna", "William R. Hogan"], "article_type": "Research Article", "score": 0.4121372, "title_display": "CollaborationViz: Interactive Visual Exploration of Biomedical Research Collaboration Networks", "publication_date": "2014-11-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0111928"}, {"journal": "PLoS ONE", "abstract": ["\nThe vasculature of body tissues is continuously subject to remodeling processes originating at the micro-vascular level. The formation of new blood vessels (angiogenesis) is essential for a number of physiological and pathophysiological processes such as tissue regeneration, tumor development and the integration of artificial tissues. There are currently no time-lapsed in vivo imaging techniques providing information on the vascular network at the capillary level in a non-destructive, three-dimensional and high-resolution fashion. This paper presents a novel imaging framework based on contrast enhanced micro-computed tomography (micro-CT) for hierarchical in vivo quantification of blood vessels in mice, ranging from largest to smallest structures. The framework combines for the first time a standard morphometric approach with densitometric analysis. Validation tests showed that the method is precise and robust. Furthermore, the framework is sensitive in detecting different perfusion levels after the implementation of a murine ischemia-reperfusion model. Correlation with both histological data and micro-CT analysis of vascular corrosion casts confirmed accuracy of the method. The newly developed time-lapsed imaging approach shows high potential for in vivo monitoring of a number of different physiological and pathological conditions in angiogenesis and vascular development.\n"], "author_display": ["Laura Nebuloni", "Gisela A. Kuhn", "Johannes Vogel", "Ralph M\u00fcller"], "article_type": "Research Article", "score": 0.41196707, "title_display": "A Novel <i>In Vivo</i> Vascular Imaging Approach for Hierarchical Quantification of Vasculature Using Contrast Enhanced Micro-Computed Tomography", "publication_date": "2014-01-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0086562"}, {"journal": "PLoS ONE", "abstract": ["\nIn this article, an approximate analytical solution of flow and heat transfer for a viscoelastic fluid in an axisymmetric channel with porous wall is presented. The solution is obtained through the use of a powerful method known as Optimal Homotopy Asymptotic Method (OHAM). We obtained the approximate analytical solution for dimensionless velocity and temperature for various parameters. The influence and effect of different parameters on dimensionless velocity, temperature, friction factor, and rate of heat transfer are presented graphically. We also compared our solution with those obtained by other methods and it is found that OHAM solution is better than the other methods considered. This shows that OHAM is reliable for use to solve strongly nonlinear problems in heat transfer phenomena.\n"], "author_display": ["Fazle Mabood", "Waqar A. Khan", "Ahmad Izani Ismail"], "article_type": "Research Article", "score": 0.41193396, "title_display": "Optimal Homotopy Asymptotic Method for Flow and Heat Transfer of a Viscoelastic Fluid in an Axisymmetric Channel with a Porous Wall", "publication_date": "2013-12-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0083581"}, {"journal": "PLoS ONE", "abstract": ["\nThe quantitative real time polymerase chain reaction (qPCR) has become a key molecular enabling technology with an immense range of research, clinical, forensic as well as diagnostic applications. Its relatively moderate instrumentation and reagent requirements have led to its adoption by numerous laboratories, including those located in the Arabian world, where qPCR, which targets DNA, and reverse transcription qPCR (RT-qPCR), which targets RNA, are widely used for region-specific biotechnology, agricultural and human genetic studies. However, it has become increasingly apparent that there are significant problems with both the quality of qPCR-based data as well as the transparency of reporting. This realisation led to the publication of the Minimum Information for Publication of Quantitative Real-Time PCR Experiments (MIQE) guidelines in 2009 and their more widespread adoption in the last couple of years. An analysis of the performance of biomedical research in the Arabian world between 2001\u20132005 suggests that the Arabian world is producing fewer biomedical publications of lower quality than other Middle Eastern countries. Hence we have analysed specifically the quality of RT-qPCR-based peer-reviewed papers published since 2009 from Arabian researchers using a bespoke iOS/Android app developed by one of the authors. Our results show that compliance with 15 essential MIQE criteria was low (median of 40%, range 0\u201393%) and few details on RNA quality controls (22% compliance), assays design (12%), RT strategies (32%), amplification efficiencies (30%) and the normalisation process (3%). These data indicate that one of the reasons for the poor performance of Arabian world biomedical research may be the low standard of any supporting qPCR experiments and identify which aspects of qPCR experiments require significant improvements.\n"], "author_display": ["Afif M. Abdel Nour", "Esam Azhar", "Ghazi Damanhouri", "Stephen A. Bustin"], "article_type": "Research Article", "score": 0.41191572, "title_display": "Five Years MIQE Guidelines: The Case of the Arabian Countries", "publication_date": "2014-02-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0088266"}, {"journal": "PLOS ONE", "abstract": ["\nHow to quantify the impact of a researcher\u2019s or an institution\u2019s body of work is a matter of increasing importance to scientists, funding agencies, and hiring committees. The use of bibliometric indicators, such as the h-index or the Journal Impact Factor, have become widespread despite their known limitations. We argue that most existing bibliometric indicators are inconsistent, biased, and, worst of all, susceptible to manipulation. Here, we pursue a principled approach to the development of an indicator to quantify the scientific impact of both individual researchers and research institutions grounded on the functional form of the distribution of the asymptotic number of citations. We validate our approach using the publication records of 1,283 researchers from seven scientific and engineering disciplines and the chemistry departments at the 106 U.S. research institutions classified as \u201cvery high research activity\u201d. Our approach has three distinct advantages. First, it accurately captures the overall scientific impact of researchers at all career stages, as measured by asymptotic citation counts. Second, unlike other measures, our indicator is resistant to manipulation and rewards publication quality over quantity. Third, our approach captures the time-evolution of the scientific impact of research institutions.\n"], "author_display": ["Jo\u00e3o A. G. Moreira", "Xiao Han T. Zeng", "Lu\u00eds A. Nunes Amaral"], "article_type": "Research Article", "score": 0.41178572, "title_display": "The Distribution of the Asymptotic Number of Citations to Sets of Publications by a Researcher or from an Academic Department Are Consistent with a Discrete Lognormal Model", "publication_date": "2015-11-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0143108"}, {"journal": "PLoS ONE", "abstract": ["Background: Forthright reporting of financial ties and conflicts of interest of researchers is associated with public trust in and esteem for the scientific enterprise. Methods/Principal Findings: We searched Lexis/Nexis Academic News for the top news stories in science published in 2004 and 2005. We conducted a content analysis of 1152 newspaper stories. Funders of the research were identified in 38% of stories, financial ties of the researchers were reported in 11% of stories, and 5% reported financial ties of sources quoted. Of 73 stories not reporting on financial ties, 27% had financial ties publicly disclosed in scholarly journals. Conclusions/Significance: Because science journalists often did not report conflict of interest information, adherence to gold-standard recommendations for science journalism was low. Journalists work under many different constraints, but nonetheless news reports of scientific research were incomplete, potentially eroding public trust in science. "], "author_display": ["Daniel M. Cook", "Elizabeth A. Boyd", "Claudia Grossmann", "Lisa A. Bero"], "article_type": "Research Article", "score": 0.41167012, "title_display": "Reporting Science and Conflicts of Interest in the Lay Press", "publication_date": "2007-12-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0001266"}, {"journal": "PLoS ONE", "abstract": ["\nThis study was conducted for typing Salmonella enterica subspecies enterica strains in Turkey using pulsed\u2013field gel electrophoresis (PFGE) and plasmid DNA profile analysis. Fourty-two strains were isolated from clinical samples obtained from unrelated patients with acute diarrhea. The samples were collected from state hospitals and public health laboratories located at seven provinces in different regions of Turkey at different times between 2004 and 2010. The strains were determined to belong to 4 different serovars. The Salmonella enterica strains belonged to the serovars Salmonella Enteritidis (n\u200a=\u200a23), Salmonella Infantis (n\u200a=\u200a14), Salmonella Munchen (n\u200a=\u200a2), and Salmonella Typhi (n\u200a=\u200a3). Forty-two Salmonella enterica strains were typed with PFGE methods using XbaI restriction enzyme and plasmid analysis. At the end of typing, 11 different PFGE band profiles were obtained. Four different PFGE profiles (type 1, 4, 9, and 10) were found among serotype S. Enteritidis species, 3 different PFGE profiles (type 3, 5, 6) were found among S. Infantis species, 2 different PFGE profiles were found among S. Typhi species (type 2 and 11), and 2 different PFGE profiles were found among S. Munchen species (type 7, 8). The UPGMA dendrogram was built on the PFGE profiles. In this study, it was determined that 4 strains of 42 Salmonella enterica strains possess no plasmid, while the isolates have 1\u20133 plasmids ranging from 5.0 to 150 kb and making 12 different plasmid profiles (P1\u2013P12). In this study, we have applied the analysis of the PFGE patterns and used bioinformatics methods to identify both inter and intra serotype relationships of 4 frequently encountered serotypes for the first time in Turkey.\n"], "author_display": ["Kerem Ozdemir", "Sumeyra Acar"], "article_type": "Research Article", "score": 0.41151577, "title_display": "Plasmid Profile and Pulsed\u2013Field Gel Electrophoresis Analysis of <i>Salmonella enterica</i> Isolates from Humans in Turkey", "publication_date": "2014-05-22T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0095976"}, {"journal": "PLOS ONE", "abstract": ["\nThe objective of this paper is to provide a detailed evaluation of type 2 diabetes mellitus research output from 1951-2012, using large-scale data analysis, bibliometric indicators and density-equalizing mapping. Data were retrieved from the Science Citation Index Expanded database, one of the seven curated databases within Web of Science. Using Boolean operators \"OR\", \"AND\" and \"NOT\", a search strategy was developed to estimate the total number of published items. Only studies with an English abstract were eligible. Type 1 diabetes and gestational diabetes items were excluded. Specific software developed for the database analysed the data. Information including titles, authors\u2019 affiliations and publication years were extracted from all files and exported to excel. Density-equalizing mapping was conducted as described by Groenberg-Kloft et al, 2008. A total of 24,783 items were published and cited 476,002 times. The greatest number of outputs were published in 2010 (n=2,139). The United States contributed 28.8% to the overall output, followed by the United Kingdom (8.2%) and Japan (7.7%). Bilateral cooperation was most common between the United States and United Kingdom (n=237). Harvard University produced 2% of all publications, followed by the University of California (1.1%). The leading journals were Diabetes, Diabetologia and Diabetes Care and they contributed 9.3%, 7.3% and 4.0% of the research yield, respectively. In conclusion, the volume of research is rising in parallel with the increasing global burden of disease due to type 2 diabetes mellitus. Bibliometrics analysis provides useful information to scientists and funding agencies involved in the development and implementation of research strategies to address global health issues.\n"], "author_display": ["Fiona Geaney", "Cristian Scutaru", "Clare Kelly", "Ronan W. Glynn", "Ivan J. Perry"], "article_type": "Research Article", "score": 0.4114667, "title_display": "Type 2 Diabetes Research Yield, 1951-2012: Bibliometrics Analysis and Density-Equalizing Mapping", "publication_date": "2015-07-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0133009"}, {"journal": "PLoS ONE", "abstract": ["Background: Multi-collaborator research is increasingly becoming the norm in the field of biomedicine. With this trend comes the imperative to award recognition to all those who contribute to a study; however, there is a gap in the current \u201cgold standard\u201d in authorship guidelines with regards to the efforts of those who provide high quality biosamples and data, yet do not play a role in the intellectual development of the final publication. Methods and findings: We carried out interviews with 36 individuals working in, or with links to, biobanks in Switzerland, in order to understand how they interpret, apply and value authorship criteria in studies involving biosamples. The majority of respondents feel that authorship is an important motivating factor in working and publishing collaboratively. However, our findings suggest that in some cases, authorship guidelines are being ignored in favor of departmental standards which recognize \u201cscientific work\u201d as meriting authorship. Conclusions: Our results support the current calls in the literature for an alternative method of crediting biomaterial contributions, in order to ensure appropriate authorship inclusion and promote collaborative research involving biobanks. "], "author_display": ["Flora M. A. Colledge", "Bernice S. Elger", "David M. Shaw"], "article_type": "Research Article", "score": 0.41123164, "title_display": "\u201cConferring Authorship\u201d: Biobank Stakeholders\u2019 Experiences with Publication Credit in Collaborative Research", "publication_date": "2013-09-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0076686"}, {"journal": "PLOS ONE", "abstract": ["\nIn order to characterize the dynamics of adaptation, it is important to be able to quantify how a population\u2019s mean fitness changes over time. Such measurements are especially important in experimental studies of evolution using microbes. The Long-Term Evolution Experiment (LTEE) with Escherichia coli provides one such system in which mean fitness has been measured by competing derived and ancestral populations. The traditional method used to measure fitness in the LTEE and many similar experiments, though, is subject to a potential limitation. As the relative fitness of the two competitors diverges, the measurement error increases because the less-fit population becomes increasingly small and cannot be enumerated as precisely. Here, we present and employ two alternatives to the traditional method. One is based on reducing the fitness differential between the competitors by using a common reference competitor from an intermediate generation that has intermediate fitness; the other alternative increases the initial population size of the less-fit, ancestral competitor. We performed a total of 480 competitions to compare the statistical properties of estimates obtained using these alternative methods with those obtained using the traditional method for samples taken over 50,000 generations from one of the LTEE populations. On balance, neither alternative method yielded measurements that were more precise than the traditional method.\n"], "author_display": ["Michael J. Wiser", "Richard E. Lenski"], "article_type": "Research Article", "score": 0.4111135, "title_display": "A Comparison of Methods to Measure Fitness in <i>Escherichia coli</i>", "publication_date": "2015-05-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0126210"}, {"abstract": ["\n        Nepotistic practices are detrimental for academia. An analysis of shared last names among academics was recently proposed to measure the diffusion of nepotism, the results of which have had a huge resonance. This method was thus proposed to orient the decisions of policy makers concerning cuts and funding. Because of the social relevance of this issue, the validity of this method must be assessed. Thus, we compared results from an analysis of Italian and United Kingdom academic last names, and of Italian last and given names. The results strongly suggest that the analysis of shared last names is not a measure of nepotism, as it is largely affected by social capital, professional networking and demographic effects, whose contribution is difficult to assess. Thus, the analysis of shared last names is not useful for guiding research policy.\n      "], "author_display": ["Fabio Ferlazzo", "Stefano Sdoia"], "article_type": "Research Article", "score": 0.4108032, "title_display": "Measuring Nepotism through Shared Last Names: Are We Really Moving from Opinions to Facts?", "publication_date": "2012-08-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0043574"}, {"journal": "PLOS ONE", "abstract": ["\nHuman biological samples (biosamples) are increasingly important in diagnosing, treating and measuring the prevalence of illnesses. For the gay and bisexual population, biosample research is particularly important for measuring the prevalence of human immunodeficiency virus (HIV). By determining people\u2019s understandings of, and attitudes towards, the donation and use of biosamples, researchers can design studies to maximise acceptability and participation. In this study we examine gay and bisexual men\u2019s attitudes towards donating biosamples for HIV research. Semi-structured telephone interviews were conducted with 46 gay and bisexual men aged between 18 and 63 recruited in commercial gay scene venues in two Scottish cities. Interview transcripts were analysed thematically using the framework approach. Most men interviewed seemed to have given little prior consideration to the issues. Participants were largely supportive of donating tissue for medical research purposes, and often favourable towards samples being stored, reused and shared. Support was often conditional, with common concerns related to: informed consent; the protection of anonymity and confidentiality; the right to withdraw from research; and ownership of samples. Many participants were in favour of the storage and reuse of samples, but expressed concerns related to data security and potential misuse of samples, particularly by commercial organisations. The sensitivity of tissue collection varied between tissue types and collection contexts. Blood, urine, semen and bowel tissue were commonly identified as sensitive, and donating saliva and as unlikely to cause discomfort. To our knowledge, this is the first in-depth study of gay and bisexual men\u2019s attitudes towards donating biosamples for HIV research. While most men in this study were supportive of donating tissue for research, some clear areas of concern were identified. We suggest that these minority concerns should be accounted for to develop inclusive, evidence-informed research protocols that balance collective benefits with individual concerns.\n"], "author_display": ["Chris Patterson", "Lisa M. McDaid", "Shona Hilton"], "article_type": "Research Article", "score": 0.41061753, "title_display": "Gay and Bisexual Men\u2019s Perceptions of the Donation and Use of Human Biological Samples for Research: A Qualitative Study", "publication_date": "2015-06-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0129924"}, {"journal": "PLoS ONE", "abstract": ["Introduction: Accurate analyses of microbiota composition of low-density communities (103\u2013104 bacteria/sample) can be challenging. Background DNA from chemicals and consumables, extraction biases as well as differences in PCR efficiency can significantly interfere with microbiota assessment. This study was aiming to establish protocols for accurate microbiota analysis at low microbial density. Methods: To examine possible effects of bacterial density on microbiota analyses we compared microbiota profiles of serial diluted saliva and low (nares, nasopharynx) and high-density (oropharynx) upper airway communities in four healthy individuals. DNA was extracted with four different extraction methods (Epicentre Masterpure, Qiagen DNeasy, Mobio Powersoil and a phenol bead-beating protocol combined with Agowa-Mag-mini). Bacterial DNA recovery was analysed by 16S qPCR and microbiota profiles through GS-FLX-Titanium-Sequencing of 16S rRNA gene amplicons spanning the V5\u2013V7 regions. Results: Lower template concentrations significantly impacted microbiota profiling results. With higher dilutions, low abundant species were overrepresented. In samples of <105 bacteria per ml, e.g. DNA <1 pg/\u00b5l, microbiota profiling deviated from the original sample and other dilutions showing a significant increase in the taxa Proteobacteria and decrease in Bacteroidetes. In similar low density samples, DNA extraction method determined if DNA levels were below or above 1 pg/\u00b5l and, together with lysis preferences per method, had profound impact on microbiota analyses in both relative abundance as well as representation of species. Conclusion: This study aimed to interpret microbiota analyses of low-density communities. Bacterial density seemed to interfere with microbiota analyses at < than 106 bacteria per ml or DNA <1 pg/\u00b5l. We therefore recommend this threshold for working with low density materials. This study underlines that bias reduction is crucial for adequate profiling of especially low-density bacterial communities. "], "author_display": ["Giske Biesbroek", "Elisabeth A. M. Sanders", "Guus Roeselers", "Xinhui Wang", "Martien P. M. Caspers", "Krzysztof Trzci\u0144ski", "Debby Bogaert", "Bart J. F. Keijser"], "article_type": "Research Article", "score": 0.4106031, "title_display": "Deep Sequencing Analyses of Low Density Microbial Communities: Working at the Boundary of Accurate Microbiota Detection", "publication_date": "2012-03-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0032942"}, {"journal": "PLOS ONE", "abstract": ["Background: Providing benefits and payments to participants in health research, either in cash or in kind, is a common but ethically controversial practice. While much literature has concentrated on appropriate levels of benefits or payments, this paper focuses on less well explored ethical issues around the nature of study benefits, drawing on views of community members living close to an international health research centre in Kenya. Methods: The consultation, including 90 residents purposively chosen to reflect diversity, used a two-stage deliberative process. Five half-day workshops were each followed by between two and four small group discussions, within a two week period (total 16 groups). During workshops and small groups, facilitators used participatory methods to share information, and promote reflection and debate on ethical issues around types of benefits, including cash, goods, medical and community benefits. Data from workshop and field notes, and voice recordings of small group discussions, were managed using Nvivo 10 and analysed using a Framework Analysis approach. Findings and Conclusions: The methods generated in-depth discussion with high levels of engagement. Particularly for the most-poor, under-compensation of time in research carries risks of serious harm. Cash payments may best support compensation of costs experienced; while highly valued, goods and medical benefits may be more appropriate as an \u2018appreciation\u2019 or incentive for participation. Community benefits were seen as important in supporting but not replacing individual-level benefits, and in building trust in researcher-community relations. Cash payments were seen to have higher risks of undue inducement, commercialising relationships and generating family conflicts than other benefits, particularly where payments are high. Researchers should consider and account for burdens families may experience when children are involved in research. Careful context-specific research planning and skilled and consistent communication about study benefits and payments are important, including in mitigating potential negative effects. "], "author_display": ["Maureen Njue", "Sassy Molyneux", "Francis Kombe", "Salim Mwalukore", "Dorcas Kamuya", "Vicki Marsh"], "article_type": "Research Article", "score": 0.41058278, "title_display": "Benefits in Cash or in Kind? A Community Consultation on Types of Benefits in Health Research on the Kenyan Coast", "publication_date": "2015-05-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0127842"}, {"journal": "PLoS ONE", "abstract": ["Background: In a search for an effective anticancer therapy the R&D units from leading universities and institutes reveal numerous technologies in the form of patent documents. The article addressed comparative anticancer patent landscape and technology assessment of Council of Scientific and Industrial Research (CSIR): India\u2019s largest R&D organisation with top twenty international public funded universities and institutes from eight different countries. Methodology/Principal Findings: The methodology include quantitative and qualitative assessment based on the bibliometric parameters and manual technology categorisation to understand the changing patent trends and recent novel technologies. The research finding analysed 25,254 patent documents from the year 1993 to 2013 and reported the insights of latest anticancer technologies and targets through categorisation studies at the level of drug discovery, development and treatment & diagnosis. The article has reported the technology correlation matrix of twelve secondary class technologies with 34 tertiary sub-class research area to identify the leading technologies and scope of future research through whitespaces analysis. In addition, the results have also addressed the target analysis, leading inventor, assignee, collaboration network, geographical distribution, patent trend analysis, citation maps and technology assessment with respect to international patent classification systems such as CPC, IPC and CPI codes. Conclusions/Significance: The result suggested peptide technology as the dominating research area next to gene therapy, vaccine and medical preparation containing organic compounds. The Indian CSIR has ranked itself at seventh position among the top 20 universities. Globally, the anticancer research was focused in the area of genetics and immunology, whereas Indian CSIR reported more patents related to plant extract and organic preparation. The article provided a glimpse of two decade anticancer scenario with respect to top public funded universities worldwide. "], "author_display": ["Ajay Dara", "Abhay T. Sangamwar"], "article_type": "Research Article", "score": 0.410567, "title_display": "Clearing the Fog of Anticancer Patents from 1993\u20132013: Through an In-Depth Technology Landscape & Target Analysis from Pioneer Research Institutes and Universities Worldwide", "publication_date": "2014-08-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0103847"}, {"journal": "PLOS ONE", "abstract": ["\nWe examine effects of government spending on postdoctoral researchers\u2019 (postdocs) productivity in biomedical sciences, the largest population of postdocs in the US. We analyze changes in the productivity of postdocs before and after the US government\u2019s 1997 decision to increase NIH funding. In the first round of analysis, we find that more government spending has resulted in longer postdoc careers. We see no significant changes in researchers\u2019 productivity in terms of publication and conference presentations. However, when the population is segmented by citizenship, we find that the effects are heterogeneous; US citizens stay longer in postdoc positions with no change in publications and, in contrast, international permanent residents (green card holders) produce more conference papers and publications without significant changes in postdoc duration. Possible explanations and policy implications of the analysis are discussed.\n"], "author_display": ["Hyungjo Hur", "Navid Ghaffarzadegan", "Joshua Hawley"], "article_type": "Research Article", "score": 0.4103995, "title_display": "Effects of Government Spending on Research Workforce Development: Evidence from Biomedical Postdoctoral Researchers", "publication_date": "2015-05-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0124928"}, {"abstract": ["Context: Publication bias jeopardizes evidence-based medicine, mainly through biased literature syntheses. Publication bias may also affect laboratory animal research, but evidence is scarce. Objectives: To assess the opinion of laboratory animal researchers on the magnitude, drivers, consequences and potential solutions for publication bias. And to explore the impact of size of the animals used, seniority of the respondent, working in a for-profit organization and type of research (fundamental, pre-clinical, or both) on those opinions. Design: Internet-based survey. Setting: All animal laboratories in The Netherlands. Participants: Laboratory animal researchers. Main Outcome Measure(s): Median (interquartile ranges) strengths of beliefs on 5 and 10-point scales (1: totally unimportant to 5 or 10: extremely important). Results: Overall, 454 researchers participated. They considered publication bias a problem in animal research (7 (5 to 8)) and thought that about 50% (32\u201370) of animal experiments are published. Employees (n\u200a=\u200a21) of for-profit organizations estimated that 10% (5 to 50) are published. Lack of statistical significance (4 (4 to 5)), technical problems (4 (3 to 4)), supervisors (4 (3 to 5)) and peer reviewers (4 (3 to 5)) were considered important reasons for non-publication (all on 5-point scales). Respondents thought that mandatory publication of study protocols and results, or the reasons why no results were obtained, may increase scientific progress but expected increased bureaucracy. These opinions did not depend on size of the animal used, seniority of the respondent or type of research. Conclusions: Non-publication of \u201cnegative\u201d results appears to be prevalent in laboratory animal research. If statistical significance is indeed a main driver of publication, the collective literature on animal experimentation will be biased. This will impede the performance of valid literature syntheses. Effective, yet efficient systems should be explored to counteract selective reporting of laboratory animal research. "], "author_display": ["Gerben ter Riet", "Daniel A. Korevaar", "Marlies Leenaars", "Peter J. Sterk", "Cornelis J. F. Van Noorden", "Lex M. Bouter", "Ren\u00e9 Lutter", "Ronald P. Oude Elferink", "Lotty Hooft"], "article_type": "Research Article", "score": 0.4102868, "title_display": "Publication Bias in Laboratory Animal Research: A Survey on Magnitude, Drivers, Consequences and Potential Solutions", "publication_date": "2012-09-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0043404"}, {"journal": "PLoS ONE", "abstract": ["\n        Biomineralization of the extracellular matrix occurs inappropriately in numerous pathological conditions such as cancer and vascular disease, but during normal mammalian development calcification is restricted to the formation of the skeleton and dentition. The comprehensive study of gene expression in mineralized skeletal tissues has been compromized by the traditional decalcification/fixation methods that result in significant mRNA degradation. In this study we developed a novel RNAlater/EDTA decalcification method that protects the integrity of the mRNA in mature mouse tibial epiphyses. Furthermore, this method preserves the tissue structure to allow histological sectioning and microdissection to determine region-specific gene expression, in addition to immuno- and in situ histology. This method will be widely applicable to the molecular analysis of calcified tissues in various pathological conditions, and will be of particular importance in dissection of the gene expression in mouse bone and joint tissues during development and in important clinical conditions such as arthritis.\n      "], "author_display": ["Daniele Belluoccio", "Lynn Rowley", "Christopher B. Little", "John F. Bateman"], "article_type": "Research Article", "score": 0.41009974, "title_display": "Maintaining mRNA Integrity during Decalcification of Mineralized Tissues", "publication_date": "2013-03-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0058154"}, {"journal": "PLoS ONE", "abstract": ["\n        Expression profiling of restricted neural populations using microarrays can facilitate neuronal classification and provide insight into the molecular bases of cellular phenotypes. Due to the formidable heterogeneity of intermixed cell types that make up the brain, isolating cell types prior to microarray processing poses steep technical challenges that have been met in various ways. These methodological differences have the potential to distort cell-type-specific gene expression profiles insofar as they may insufficiently filter out contaminating mRNAs or induce aberrant cellular responses not normally present in vivo. Thus we have compared the repeatability, susceptibility to contamination from off-target cell-types, and evidence for stress-responsive gene expression of five different purification methods - Laser Capture Microdissection (LCM), Translating Ribosome Affinity Purification (TRAP), Immunopanning (PAN), Fluorescence Activated Cell Sorting (FACS), and manual sorting of fluorescently labeled cells (Manual). We found that all methods obtained comparably high levels of repeatability, however, data from LCM and TRAP showed significantly higher levels of contamination than the other methods. While PAN samples showed higher activation of apoptosis-related, stress-related and immediate early genes, samples from FACS and Manual studies, which also require dissociated cells, did not. Given that TRAP targets actively translated mRNAs, whereas other methods target all transcribed mRNAs, observed differences may also reflect translational regulation.\n      "], "author_display": ["Benjamin W. Okaty", "Ken Sugino", "Sacha B. Nelson"], "article_type": "Research Article", "score": 0.41009238, "title_display": "A Quantitative Comparison of Cell-Type-Specific Microarray Gene Expression Profiling Methods in the Mouse Brain", "publication_date": "2011-01-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0016493"}, {"journal": "PLOS ONE", "abstract": ["Objective: To evaluate the characteristics of the design, analysis, and reporting of crossover trials for inclusion in a meta-analysis of treatment for primary open-angle glaucoma and to provide empirical evidence to inform the development of tools to assess the validity of the results from crossover trials and reporting guidelines. Methods: We searched MEDLINE, EMBASE, and Cochrane\u2019s CENTRAL register for randomized crossover trials for a systematic review and network meta-analysis we are conducting. Two individuals independently screened the search results for eligibility and abstracted data from each included report. Results: We identified 83 crossover trials eligible for inclusion. Issues affecting the risk of bias in crossover trials, such as carryover, period effects and missing data, were often ignored. Some trials failed to accommodate the within-individual differences in the analysis. For a large proportion of the trials, the authors tabulated the results as if they arose from a parallel design. Precision estimates properly accounting for the paired nature of the design were often unavailable from the study reports; consequently, to include trial findings in a meta-analysis would require further manipulation and assumptions. Conclusions: The high proportion of poorly reported analyses and results has the potential to affect whether crossover data should or can be included in a meta-analysis. There is pressing need for reporting guidelines for crossover trials. "], "author_display": ["Tianjing Li", "Tsung Yu", "Barbara S. Hawkins", "Kay Dickersin"], "article_type": "Research Article", "score": 0.40993756, "title_display": "Design, Analysis, and Reporting of Crossover Trials for Inclusion in a Meta-Analysis", "publication_date": "2015-08-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0133023"}, {"journal": "PLoS ONE", "abstract": ["\nManual offline analysis, of a scanning electron microscopy (SEM) image, is a time consuming process and requires continuous human intervention and efforts. This paper presents an image processing based method for automated offline analyses of SEM images. To this end, our strategy relies on a two-stage process, viz. texture analysis and quantification. The method involves a preprocessing step, aimed at the noise removal, in order to avoid false edges. For texture analysis, the proposed method employs a state of the art Curvelet transform followed by segmentation through a combination of entropy filtering, thresholding and mathematical morphology (MM). The quantification is carried out by the application of a box-counting algorithm, for fractal dimension (FD) calculations, with the ultimate goal of measuring the parameters, like surface area and perimeter. The perimeter is estimated indirectly by counting the boundary boxes of the filled shapes. The proposed method, when applied to a representative set of SEM images, not only showed better results in image segmentation but also exhibited a good accuracy in the calculation of surface area and perimeter. The proposed method outperforms the well-known Watershed segmentation algorithm.\n"], "author_display": ["Syed Hamad Shirazi", "Nuhman ul Haq", "Khizar Hayat", "Saeeda Naz", "Ihsan ul Haque"], "article_type": "Research Article", "score": 0.40991828, "title_display": "Curvelet Based Offline Analysis of SEM Images", "publication_date": "2014-08-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0103942"}, {"journal": "PLoS ONE", "abstract": ["\nTelomere length measurement is an essential test for the diagnosis of telomeropathies, which are caused by excessive telomere erosion. Commonly used methods are terminal restriction fragment (TRF) analysis by Southern blot, fluorescence in situ hybridization coupled with flow cytometry (flow-FISH), and quantitative PCR (qPCR). Although these methods have been used in the clinic, they have not been comprehensively compared. Here, we directly compared the performance of flow-FISH and qPCR to measure leukocytes' telomere length of healthy individuals and patients evaluated for telomeropathies, using TRF as standard. TRF and flow-FISH showed good agreement and correlation in the analysis of healthy subjects (R2\u200a=\u200a0.60; p<0.0001) and patients (R2\u200a=\u200a0.51; p<0.0001). In contrast, the comparison between TRF and qPCR yielded modest correlation for the analysis of samples of healthy individuals (R2\u200a=\u200a0.35; p<0.0001) and low correlation for patients (R2\u200a=\u200a0.20; p\u200a=\u200a0.001); Bland-Altman analysis showed poor agreement between the two methods for both patients and controls. Quantitative PCR and flow-FISH modestly correlated in the analysis of healthy individuals (R2\u200a=\u200a0.33; p<0.0001) and did not correlate in the comparison of patients' samples (R2\u200a=\u200a0.1, p\u200a=\u200a0.08). Intra-assay coefficient of variation (CV) was similar for flow-FISH (10.8\u00b17.1%) and qPCR (9.5\u00b17.4%; p\u200a=\u200a0.35), but the inter-assay CV was lower for flow-FISH (9.6\u00b17.6% vs. 16\u00b119.5%; p\u200a=\u200a0.02). Bland-Altman analysis indicated that flow-FISH was more precise and reproducible than qPCR. Flow-FISH and qPCR were sensitive (both 100%) and specific (93% and 89%, respectively) to distinguish very short telomeres. However, qPCR sensitivity (40%) and specificity (63%) to detect telomeres below the tenth percentile were lower compared to flow-FISH (80% sensitivity and 85% specificity). In the clinical setting, flow-FISH was more accurate, reproducible, sensitive, and specific in the measurement of human leukocyte's telomere length in comparison to qPCR. In conclusion, flow-FISH appears to be a more appropriate method for diagnostic purposes.\n"], "author_display": ["Fernanda Gutierrez-Rodrigues", "B\u00e1rbara A. Santana-Lemos", "Priscila S. Scheucher", "Raquel M. Alves-Paiva", "Rodrigo T. Calado"], "article_type": "Research Article", "score": 0.40989047, "title_display": "Direct Comparison of Flow-FISH and qPCR as Diagnostic Tests for Telomere Length Measurement in Humans", "publication_date": "2014-11-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0113747"}, {"journal": "PLoS Genetics", "abstract": ["\nMapping expression Quantitative Trait Loci (eQTLs) represents a powerful and widely adopted approach to identifying putative regulatory variants and linking them to specific genes. Up to now eQTL studies have been conducted in a relatively narrow range of tissues or cell types. However, understanding the biology of organismal phenotypes will involve understanding regulation in multiple tissues, and ongoing studies are collecting eQTL data in dozens of cell types. Here we present a statistical framework for powerfully detecting eQTLs in multiple tissues or cell types (or, more generally, multiple subgroups). The framework explicitly models the potential for each eQTL to be active in some tissues and inactive in others. By modeling the sharing of active eQTLs among tissues, this framework increases power to detect eQTLs that are present in more than one tissue compared with \u201ctissue-by-tissue\u201d analyses that examine each tissue separately. Conversely, by modeling the inactivity of eQTLs in some tissues, the framework allows the proportion of eQTLs shared across different tissues to be formally estimated as parameters of a model, addressing the difficulties of accounting for incomplete power when comparing overlaps of eQTLs identified by tissue-by-tissue analyses. Applying our framework to re-analyze data from transformed B cells, T cells, and fibroblasts, we find that it substantially increases power compared with tissue-by-tissue analysis, identifying 63% more genes with eQTLs (at FDR\u200a=\u200a0.05). Further, the results suggest that, in contrast to previous analyses of the same data, the majority of eQTLs detectable in these data are shared among all three tissues.\nAuthor Summary: Genetic variants that are associated with gene expression are known as expression Quantitative Trait Loci, or eQTLs. Many studies have been conducted to identify eQTLs, and they have proven an effective tool for identifying putative regulatory variants and linking them to specific genes. Up to now most studies have been conducted in a single tissue or cell type, but moving forward this is changing, and ongoing studies are collecting data aimed at mapping eQTLs in dozens of tissues. Current statistical methods are not able to fully exploit the richness of these kinds of data, taking account of both the sharing and differences in eQTLs among tissues. In this paper we develop a statistical framework to address this problem, to improve power to detect eQTLs when they are shared among multiple tissues, and to allow for differences among tissues to be estimated. Applying these methods to data from three tissues suggests that sharing of eQTLs among tissues may be substantially more common than it appeared in previous analyses of the same data. "], "author_display": ["Timoth\u00e9e Flutre", "Xiaoquan Wen", "Jonathan Pritchard", "Matthew Stephens"], "article_type": "Research Article", "score": 0.40984496, "title_display": "A Statistical Framework for Joint eQTL Analysis in Multiple Tissues", "publication_date": "2013-05-09T00:00:00Z", "eissn": "1553-7404", "id": "10.1371/journal.pgen.1003486"}, {"journal": "PLoS ONE", "abstract": ["Background: Available blood assays for venous thromboembolism (VTE) suffer from diminished specificity. Compared with single marker tests, such as D-dimer, a multi-marker strategy may improve diagnostic ability. We used direct mass spectrometry (MS) analysis of serum from patients with VTE to determine whether protein expression profiles would predict diagnosis. Methods and Results: We developed a direct MS and computational approach to the proteomic analysis of serum. Using this new method, we analyzed serum from inpatients undergoing radiographic evaluation for VTE. In a balanced cohort of 76 patients, a neural network-based prediction model was built using a training subset of the cohort to first identify proteomic patterns of VTE. The proteomic patterns were then validated in a separate group of patients within the cohort. The model yielded a sensitivity of 68% and specificity of 89%, which exceeded the specificity of D-dimer assay tested by latex agglutination, ELISA, and immunoturbimetric methods (sensitivity/specificity of 63.2%/60.5%, 97.4%/21.1%, 97.4%/15.8%, respectively). We validated differences in protein expression between patients with and without VTE using more traditional gel-based analysis of the same serum samples. Conclusion: Protein expression analysis of serum using direct MS demonstrates potential diagnostic utility for VTE. This pilot study is the first such direct MS study to be applied to a cardiovascular disease. Differences in protein expression were identified and subsequently validated in a separate group of patients. The findings in this initial cohort can be evaluated in other independent cohorts, including patients with inflammatory conditions and chronic (but not acute) VTE, for the diagnosis of VTE. "], "author_display": ["Santhi K. Ganesh", "Yugal Sharma", "Judith Dayhoff", "Henry M. Fales", "Jennifer Van Eyk", "Thomas S. Kickler", "Eric M. Billings", "Elizabeth G. Nabel"], "article_type": "Research Article", "score": 0.4098343, "title_display": "Detection of Venous Thromboembolism by Proteomic Serum Biomarkers", "publication_date": "2007-06-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0000544"}, {"journal": "PLoS ONE", "abstract": ["\n        Understanding the processes and conditions under which populations diverge to give rise to distinct species is a central question in evolutionary biology. Since recently diverged populations have high levels of shared polymorphisms, it is challenging to distinguish between recent divergence with no (or very low) inter-population gene flow and older splitting events with subsequent gene flow. Recently published methods to infer speciation parameters under the isolation-migration framework are based on summarizing polymorphism data at multiple loci in two species using the joint site-frequency spectrum (JSFS). We have developed two improvements of these methods based on a more extensive use of the JSFS classes of polymorphisms for species with high intra-locus recombination rates. First, using a likelihood based method, we demonstrate that taking into account low-frequency polymorphisms shared between species significantly improves the joint estimation of the divergence time and gene flow between species. Second, we introduce a local linear regression algorithm that considerably reduces the computational time and allows for the estimation of unequal rates of gene flow between species. We also investigate which summary statistics from the JSFS allow the greatest estimation accuracy for divergence time and migration rates for low (around 10) and high (around 100) numbers of loci. Focusing on cases with low numbers of loci and high intra-locus recombination rates we show that our methods for the estimation of divergence time and migration rates are more precise than existing approaches.\n      "], "author_display": ["Aur\u00e9lien Tellier", "Peter Pfaffelhuber", "Bernhard Haubold", "Lisha Naduvilezhath", "Laura E. Rose", "Thomas St\u00e4dler", "Wolfgang Stephan", "Dirk Metzler"], "article_type": "Research Article", "score": 0.4098106, "title_display": "Estimating Parameters of Speciation Models Based on Refined Summaries of the Joint Site-Frequency Spectrum", "publication_date": "2011-05-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0018155"}, {"journal": "PLoS ONE", "abstract": ["Objective: To compare the measurements of glomerular filtration rate (GFR) determined by 99mTc-diethylene triamine pentaacetic acid (99mTc-DTPA) renal dynamic imaging with those estimated by Chronic Kidney Disease Epidemiology Collaboration (CDK-EPI) equation and to identify a more accurate measurement of GFR of chronic kidney disease (CKD) patients in clinical practice. Methods: The GFR was determined simultaneously by 3 methods: (a) dual plasma sample clearance method (tGFR); (b) renal dynamic imaging method (dGFR); (c) CDK-EPI equation (eGFR). The tGFR was employed as the reference method. The correlation, regression, and limit of agreement of dGFR and eGFR were used to demonstrate the validity of the two methods. The comparison of bias, precision, and accuracy between dGFR and eGFR was analyzed to identify the most suitable method. The analysis of bias, precision and accuracy was repeated after stratifying patients by a measured tGFR cutpoint of 60 ml\u00b7min\u22121\u00b7(1.73 m2)\u22121. Results: A total of 149 patients were enrolled. Both dGFR and eGFR correlated well with tGFR and the regression equation of dGFR and eGFR against tGFR was respectively Y\u200a=\u200a\u22124.289+0.962X (r\u200a=\u200a0.919; RMSE\u200a=\u200a14.323 ml.min\u22121. (1.73 m2)\u22121; P<0.001) and Y\u200a=\u200a2.462+0.914X (r\u200a=\u200a0.909; RMSE\u200a=\u200a15.123 ml.min\u22121. (1.73 m2)\u22121; P<0.001). In addition, Bland-Altman analysis showed preferable agreement between the two methods and the reference method. The comparison revealed that eGFR, compared with dGFR, showed better performance on bias and 50% accuracy and similar performance on other indexes in the whole cohort and the lower-GFR subgroup, whereas in the higher-GFR subgroup the difference of the two methods was not significant in all parameters. Conclusions: Although both CDK-EPI equation and renal dynamic imaging can be used to determine the GFR of CKD patients, CDK-EPI equation is more accurate than renal dynamic imaging. As a result, 99mTc-DTPA renal dynamic imaging may be unsuitable to be used as the reference method in investigating the validity of CDK-EPI equation. "], "author_display": ["Peng Xie", "Jian-Min Huang", "Xiao-Mei Liu", "Wei-Jie Wu", "Li-Ping Pan", "Hai-Ying Lin"], "article_type": "Research Article", "score": 0.40972623, "title_display": "<sup>99m</sup>Tc-DTPA Renal Dynamic Imaging Method May Be Unsuitable To Be Used as the Reference Method in Investigating the Validity of CDK-EPI Equation for Determining Glomerular Filtration Rate", "publication_date": "2013-05-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0062328"}, {"journal": "PLOS ONE", "abstract": ["Introduction: Sensitivity analyses refer to investigations of the degree to which the results of a meta-analysis remain stable when conditions of the data or the analysis change. To the extent that results remain stable, one can refer to them as robust. Sensitivity analyses are rarely conducted in the organizational science literature. Despite conscientiousness being a valued predictor in employment selection, sensitivity analyses have not been conducted with respect to meta-analytic estimates of the correlation (i.e., validity) between conscientiousness and job performance. Methods: To address this deficiency, we reanalyzed the largest collection of conscientiousness validity data in the personnel selection literature and conducted a variety of sensitivity analyses. Results: Publication bias analyses demonstrated that the validity of conscientiousness is moderately overestimated (by around 30%; a correlation difference of about .06). The misestimation of the validity appears to be due primarily to suppression of small effects sizes in the journal literature. These inflated validity estimates result in an overestimate of the dollar utility of personnel selection by millions of dollars and should be of considerable concern for organizations. Conclusion: The fields of management and applied psychology seldom conduct sensitivity analyses. Through the use of sensitivity analyses, this paper documents that the existing literature overestimates the validity of conscientiousness in the prediction of job performance. Our data show that effect sizes from journal articles are largely responsible for this overestimation. "], "author_display": ["Sven Kepes", "Michael A. McDaniel"], "article_type": "Research Article", "score": 0.40966403, "title_display": "The Validity of Conscientiousness Is Overestimated in the Prediction of Job Performance", "publication_date": "2015-10-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0141468"}, {"journal": "PLoS ONE", "abstract": ["\n        Small interfering RNA (siRNA) has been used widely to induce gene silencing in cells. To predict the efficacy of an siRNA with respect to inhibition of its target mRNA, we developed a two layer system, siPRED, which is based on various characteristic methods in the first layer and fusion mechanisms in the second layer. Characteristic methods were constructed by support vector regression from three categories of characteristics, namely sequence, features, and rules. Fusion mechanisms considered combinations of characteristic methods in different categories and were implemented by support vector regression and neural networks to yield integrated methods. In siPRED, the prediction of siRNA efficacy through integrated methods was better than through any method that utilized only a single method. Moreover, the weighting of each characteristic method in the context of integrated methods was established by genetic algorithms so that the effect of each characteristic method could be revealed. Using a validation dataset, siPRED performed better than other predictive systems that used the scoring method, neural networks, or linear regression. Finally, siPRED can be improved to achieve a correlation coefficient of 0.777 when the threshold of the whole stacking energy is \u2265\u221234.6 kcal/mol. siPRED is freely available on the web at http://predictor.nchu.edu.tw/siPRED.\n      "], "author_display": ["Wei-Jie Pan", "Chi-Wei Chen", "Yen-Wei Chu"], "article_type": "Research Article", "score": 0.40958735, "title_display": "<i>siPRED</i>: Predicting siRNA Efficacy Using Various Characteristic Methods", "publication_date": "2011-11-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0027602"}, {"abstract": ["Background: While the conserved positions of a multiple sequence alignment (MSA) are clearly of interest, non-conserved positions can also be important because, for example, destabilizing effects at one position can be compensated by stabilizing effects at another position. Different methods have been developed to recognize the evolutionary relationship between amino acid sites, and to disentangle functional/structural dependencies from historical/phylogenetic ones. Methodology/Principal Findings: We have used two complementary approaches to test the efficacy of these methods. In the first approach, we have used a new program, MSAvolve, for the in silico evolution of MSAs, which records a detailed history of all covarying positions, and builds a global coevolution matrix as the accumulated sum of individual matrices for the positions forced to co-vary, the recombinant coevolution, and the stochastic coevolution. We have simulated over 1600 MSAs for 8 protein families, which reflect sequences of different sizes and proteins with widely different functions. The calculated coevolution matrices were compared with the coevolution matrices obtained for the same evolved MSAs with different coevolution detection methods. In a second approach we have evaluated the capacity of the different methods to predict close contacts in the representative X-ray structures of an additional 150 protein families using only experimental MSAs. Conclusions/Significance: Methods based on the identification of global correlations between pairs were found to be generally superior to methods based only on local correlations in their capacity to identify coevolving residues using either simulated or experimental MSAs. However, the significant variability in the performance of different methods with different proteins suggests that the simulation of MSAs that replicate the statistical properties of the experimental MSA can be a valuable tool to identify the coevolution detection method that is most effective in each case. "], "author_display": ["Sharon H. Ackerman", "Elisabeth R. Tillier", "Domenico L. Gatti"], "article_type": "Research Article", "score": 0.4094919, "title_display": "Accurate Simulation and Detection of Coevolution Signals in Multiple Sequence Alignments", "publication_date": "2012-10-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0047108"}, {"journal": "PLOS ONE", "abstract": ["\nData \u201cpublication\u201d seeks to appropriate the prestige of authorship in the peer-reviewed literature to reward researchers who create useful and well-documented datasets. The scholarly communication community has embraced data publication as an incentive to document and share data. But, numerous new and ongoing experiments in implementation have not yet resolved what a data publication should be, when data should be peer-reviewed, or how data peer review should work. While researchers have been surveyed extensively regarding data management and sharing, their perceptions and expectations of data publication are largely unknown. To bring this important yet neglected perspective into the conversation, we surveyed \u223c 250 researchers across the sciences and social sciences\u2013 asking what expectations\u201cdata publication\u201d raises and what features would be useful to evaluate the trustworthiness, evaluate the impact, and enhance the prestige of a data publication. We found that researcher expectations of data publication center on availability, generally through an open database or repository. Few respondents expected published data to be peer-reviewed, but peer-reviewed data enjoyed much greater trust and prestige. The importance of adequate metadata was acknowledged, in that almost all respondents expected data peer review to include evaluation of the data\u2019s documentation. Formal citation in the reference list was affirmed by most respondents as the proper way to credit dataset creators. Citation count was viewed as the most useful measure of impact, but download count was seen as nearly as valuable. These results offer practical guidance for data publishers seeking to meet researcher expectations and enhance the value of published data.\n"], "author_display": ["John Ernest Kratz", "Carly Strasser"], "article_type": "Research Article", "score": 0.40921813, "title_display": "Researcher Perspectives on Publication and Peer Review of Data", "publication_date": "2015-02-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0117619"}, {"journal": "PLoS ONE", "abstract": ["\n\t\t\t\tIn the face of demands for researchers to engage more actively with a wider range of publics and to capture different kinds of research impacts and engagements, we explored the ways a small number of environmental researchers use traditional and social media to disseminate research. A questionnaire was developed to investigate the impact of different media as a tool to broker contact between researchers and a variety of different stakeholders (for example, publics, other researchers, policymakers, journalists) as well as how researchers perceive that their use of these media has changed over the past five years. The questionnaire was sent to 504 researchers whose work had featured in a policy-oriented e-news service. 149 valid responses were received (29%). Coverage in traditional media (newspapers, broadcast) not only brokers contact with other journalists, but is a good source of contact from other researchers (n=47, 62%) and members of the public (n=36, 26%). Although the use of social media was limited amongst our sample, it did broker contact with other researchers (n=17, 47%) and the public (n=10, 28%). Nevertheless, few environmental researchers were actively using social media to disseminate their research findings, with many continuing to rely on academic journals and face-to-face communication to reach both academic and public audiences. \n\t\t\t"], "author_display": ["Clare Wilkinson", "Emma Weitkamp"], "article_type": "Research Article", "score": 0.40904292, "title_display": "A Case Study in Serendipity: Environmental Researchers Use of Traditional and Social Media for Dissemination", "publication_date": "2013-12-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0084339"}, {"journal": "PLoS ONE", "abstract": ["Background: Publication records and citation indices often are used to evaluate academic performance. For this reason, obtaining or computing them accurately is important. This can be difficult, largely due to a lack of complete knowledge of an individual's publication list and/or lack of time available to manually obtain or construct the publication-citation record. While online publication search engines have somewhat addressed these problems, using raw search results can yield inaccurate estimates of publication-citation records and citation indices. Methodology: In this paper, we present a new, automated method that produces estimates of an individual's publication-citation record from an individual's name and a set of domain-specific vocabulary that may occur in the individual's publication titles. Because this vocabulary can be harvested directly from a research web page or online (partial) publication list, our method delivers an easy way to obtain estimates of a publication-citation record and the relevant citation indices. Our method works by applying a series of stringent name and content filters to the raw publication search results returned by an online publication search engine. In this paper, our method is run using Google Scholar, but the underlying filters can be easily applied to any existing publication search engine. When compared against a manually constructed data set of individuals and their publication-citation records, our method provides significant improvements over raw search results. The estimated publication-citation records returned by our method have an average sensitivity of  and specificity of  (in contrast to raw search result specificity of less than 10%). When citation indices are computed using these records, the estimated indices are within  of the true value, compared to raw search results which have overestimates of, on average, . Conclusions: These results confirm that our method provides significantly improved estimates over raw search results, and these can either be used directly for large-scale (departmental or university) analysis or further refined manually to quickly give accurate publication-citation records. "], "author_display": ["Derek Ruths", "Faiyaz Al Zamal"], "article_type": "Research Article", "score": 0.40897024, "title_display": "A Method for the Automated, Reliable Retrieval of Publication-Citation Records", "publication_date": "2010-08-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0012133"}, {"journal": "PLoS ONE", "abstract": ["Background: A central problem in systems biology research is the identification and extension of biological modules\u2013groups of genes or proteins participating in a common cellular process or physical complex. As a result, there is a persistent need for practical, principled methods to infer the modular organization of genes from genome-scale data. Results: We introduce a novel approach for the identification of modules based on the persistence of isolated gene groups within an evolving graph process. First, the underlying genomic data is summarized in the form of ranked gene\u2013gene relationships, thereby accommodating studies that quantify the relevant biological relationship directly or indirectly. Then, the observed gene\u2013gene relationship ranks are viewed as the outcome of a random graph process and candidate modules are given by the identifiable subgraphs that arise during this process. An isolation index is computed for each module, which quantifies the statistical significance of its survival time. Conclusions: The Miso (module isolation) method predicts gene modules from genomic data and the associated isolation index provides a module-specific measure of confidence. Improving on existing alternative, such as graph clustering and the global pruning of dendrograms, this index offers two intuitively appealing features: (1) the score is module-specific; and (2) different choices of threshold correlate logically with the resulting performance, i.e. a stringent cutoff yields high quality predictions, but low sensitivity. Through the analysis of yeast phenotype data, the Miso method is shown to outperform existing alternatives, in terms of the specificity and sensitivity of its predictions. "], "author_display": ["Jochen Brumm", "Elizabeth Conibear", "Wyeth W. Wasserman", "Jennifer Bryan"], "article_type": "Research Article", "score": 0.40886146, "title_display": "Discovery and Expansion of Gene Modules by Seeking Isolated Groups in a Random Graph Process", "publication_date": "2008-10-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0003358"}, {"journal": "PLoS ONE", "abstract": ["\n        Boolean-based method, despite of its simplicity, would be a more attractive approach for inferring a network from high-throughput expression data if its effectiveness has not been limited by high false positive prediction. In this study, we explored factors that could simply be adjusted to improve the accuracy of inferring networks. Our work focused on the analysis of the effects of discretisation methods, biological constraints, and stringency of Boolean function assignment on the performance of Boolean network, including accuracy, precision, specificity and sensitivity, using three sets of microarray time-series data. The study showed that biological constraints have pivotal influence on the network performance over the other factors. It can reduce the variation in network performance resulting from the arbitrary selection of discretisation methods and stringency settings. We also presented the master Boolean network as an approach to establish the unique solution for Boolean analysis. The information acquired from the analysis was summarised and deployed as a general guideline for an efficient use of Boolean-based method in the network inference. In the end, we provided an example of the use of such a guideline in the study of Arabidopsis circadian clock genetic network from which much interesting biological information can be inferred.\n      "], "author_display": ["Treenut Saithong", "Somkid Bumee", "Chalothorn Liamwirat", "Asawin Meechai"], "article_type": "Research Article", "score": 0.40878892, "title_display": "Analysis and Practical Guideline of Constraint-Based Boolean Method in Genetic Network Inference", "publication_date": "2012-01-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0030232"}, {"journal": "PLoS ONE", "abstract": ["\n        Antimicrobial peptides (AMPs) represent a class of natural peptides that form a part of the innate immune system, and this kind of \u2018nature's antibiotics\u2019 is quite promising for solving the problem of increasing antibiotic resistance. In view of this, it is highly desired to develop an effective computational method for accurately predicting novel AMPs because it can provide us with more candidates and useful insights for drug design. In this study, a new method for predicting AMPs was implemented by integrating the sequence alignment method and the feature selection method. It was observed that, the overall jackknife success rate by the new predictor on a newly constructed benchmark dataset was over 80.23%, and the Mathews correlation coefficient is 0.73, indicating a good prediction. Moreover, it is indicated by an in-depth feature analysis that the results are quite consistent with the previously known knowledge that some amino acids are preferential in AMPs and that these amino acids do play an important role for the antimicrobial activity. For the convenience of most experimental scientists who want to use the prediction method without the interest to follow the mathematical details, a user-friendly web-server is provided at http://amp.biosino.org/.\n      "], "author_display": ["Ping Wang", "Lele Hu", "Guiyou Liu", "Nan Jiang", "Xiaoyun Chen", "Jianyong Xu", "Wen Zheng", "Li Li", "Ming Tan", "Zugen Chen", "Hui Song", "Yu-Dong Cai", "Kuo-Chen Chou"], "article_type": "Research Article", "score": 0.40869126, "title_display": "Prediction of Antimicrobial Peptides Based on Sequence Alignment and Feature Selection Methods", "publication_date": "2011-04-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0018476"}, {"journal": "PLOS ONE", "abstract": ["\nEmergency evacuation aims to transport people from dangerous places to safe shelters as quickly as possible. Police play an important role in the evacuation process, as they can handle traffic accidents immediately and help people move smoothly on roads. This paper investigates an evacuation routing problem that involves police resource allocation. We propose a novel k-th-shortest-path-based technique that uses explicit congestion control to optimize evacuation routing and police resource allocation. A nonlinear mixed-integer programming model is presented to formulate the problem. The model\u2019s objective is to minimize the overall evacuation clearance time. Two algorithms are given to solve the problem. The first one linearizes the original model and solves the linearized problem with CPLEX. The second one is a heuristic algorithm that uses a police resource utilization efficiency index to directly solve the original model. This police resource utilization efficiency index significantly aids in the evaluation of road links from an evacuation throughput perspective. The proposed algorithms are tested with a number of examples based on real data from cities of different sizes. The computational results show that the police resource utilization efficiency index is very helpful in finding near-optimal solutions. Additionally, comparing the performance of the heuristic algorithm and the linearization method by using randomly generated examples indicates that the efficiency of the heuristic algorithm is superior.\n"], "author_display": ["Yunyue He", "Zhong Liu", "Jianmai Shi", "Yishan Wang", "Jiaming Zhang", "Jinyuan Liu"], "article_type": "Research Article", "score": 0.40851063, "title_display": "<i>K</i>-Shortest-Path-Based Evacuation Routing with Police Resource Allocation in City Transportation Networks", "publication_date": "2015-07-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0131962"}, {"journal": "PLoS ONE", "abstract": ["Background: Authors of randomized trial reports seem to hold a variety of views regarding the relationship between missing outcome data (MOD) and intention to treat (ITT). The objectives of this study were to systematically investigate how authors of methodology articles define ITT in the presence of MOD, how they recommend handling MOD under ITT, and to make a proposal for potential improvement in the definition and use of ITT in relation to MOD. Methods and Findings: We systematically searched MEDLINE in February 2009 for methodological articles written in English that devoted at least one paragraph to ITT and two other paragraphs to either ITT or MOD. We excluded original trial reports, observational studies, and clinical systematic reviews. Working in teams of two, we independently extracted relevant information from each eligible article. Of 1007 titles and abstracts reviewed, 66 articles met eligibility criteria. Five (8%) did not provide a definition of ITT; 25 (38%) mentioned MOD but did not discuss its relationship to ITT; and 36 (55%) discussed the relationship of MOD with ITT. These 36 articles described one or more of three statements: complete follow-up is required for ITT (58%); ITT and MOD are separate issues (17%); and ITT requires a specific strategy for handling MOD (78%); 17 (47%) endorsed more than one relationship. The most frequently mentioned strategies for handling MOD within ITT were: using the last outcome carried forward (50%); sensitivity analysis (50%); and use of available data to impute missing data (46%). Conclusion: We found that there is no consensus on the definition of ITT in relation to MOD. For conceptual clarity, we suggest that both reports of randomized trials and systematic reviews separately consider and describe how they deal with participants with complete data and those with MOD. "], "author_display": ["Mohamad Alshurafa", "Matthias Briel", "Elie A. Akl", "Ted Haines", "Paul Moayyedi", "Stephen J. Gentles", "Lorena Rios", "Chau Tran", "Neera Bhatnagar", "Francois Lamontagne", "Stephen D. Walter", "Gordon H. Guyatt"], "article_type": "Research Article", "score": 0.4084956, "title_display": "Inconsistent Definitions for Intention-To-Treat in Relation to Missing Outcome Data: Systematic Review of the Methods Literature", "publication_date": "2012-11-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0049163"}, {"journal": "PLoS ONE", "abstract": ["\nThis study examines the simultaneous effects of heat and mass transfer on the three-dimensional boundary layer flow of viscous fluid between two infinite parallel plates. Magnetohydrodynamic (MHD) and thermal radiation effects are present. The governing problems are first modeled and then solved by homotopy analysis method (HAM). Influence of several embedded parameters on the velocity, concentration and temperature fields are described.\n"], "author_display": ["T. Hayat", "M. Awais", "A. Alsaedi", "Ambreen Safdar"], "article_type": "Research Article", "score": 0.40842605, "title_display": "On Computations for Thermal Radiation in MHD Channel Flow with Heat and Mass Transfer", "publication_date": "2014-01-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0086695"}, {"journal": "PLoS ONE", "abstract": ["\nSimultaneous electroencephalography (EEG) and functional magnetic resonance imaging (fMRI) allow for a non-invasive investigation of cerebral functions with high temporal and spatial resolution. The main challenge of such integration is the removal of the pulse artefact (PA) that affects EEG signals recorded in the magnetic resonance (MR) scanner. Often applied techniques for this purpose are Optimal Basis Set (OBS) and Independent Component Analysis (ICA). The combination of OBS and ICA is increasingly used, since it can potentially improve the correction performed by each technique separately. The present study is focused on the OBS-ICA combination and is aimed at providing the optimal ICA parameters for PA correction in resting-state EEG data, where the information of interest is not specified in latency and amplitude as in, for example, evoked potential. A comparison between two intervals for ICA calculation and four methods for marking artefactual components was performed. The performance of the methods was discussed in terms of their capability to 1) remove the artefact and 2) preserve the information of interest. The analysis included 12 subjects and two resting-state datasets for each of them. The results showed that none of the signal lengths for the ICA calculation was highly preferable to the other. Among the methods for the identification of PA-related components, the one based on the wavelets transform of each component emerged as the best compromise between the effectiveness in removing PA and the conservation of the physiological neuronal content.\n"], "author_display": ["Eleonora Maggioni", "Jorge Arrubla", "Tracy Warbrick", "J\u00fcrgen Dammers", "Anna M. Bianchi", "Gianluigi Reni", "Michela Tosetti", "Irene Neuner", "N. Jon Shah"], "article_type": "Research Article", "score": 0.40837127, "title_display": "Removal of Pulse Artefact from EEG Data Recorded in MR Environment at 3T. Setting of ICA Parameters for Marking Artefactual Components: Application to Resting-State Data", "publication_date": "2014-11-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0112147"}, {"journal": "PLoS ONE", "abstract": ["\nA central goal of RNA sequencing (RNA-seq) experiments is to detect differentially expressed genes. In the ubiquitous negative binomial model for RNA-seq data, each gene is given a dispersion parameter, and correctly estimating these dispersion parameters is vital to detecting differential expression. Since the dispersions control the variances of the gene counts, underestimation may lead to false discovery, while overestimation may lower the rate of true detection. After briefly reviewing several popular dispersion estimation methods, this article describes a simulation study that compares them in terms of point estimation and the effect on the performance of tests for differential expression. The methods that maximize the test performance are the ones that use a moderate degree of dispersion shrinkage: the DSS, Tagwise wqCML, and Tagwise APL. In practical RNA-seq data analysis, we recommend using one of these moderate-shrinkage methods with the QLShrink test in QuasiSeq R package.\n"], "author_display": ["William Michael Landau", "Peng Liu"], "article_type": "Research Article", "score": 0.40829805, "title_display": "Dispersion Estimation and Its Effect on Test Performance in RNA-seq Data Analysis: A Simulation-Based Comparison of Methods", "publication_date": "2013-12-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0081415"}, {"journal": "PLoS ONE", "abstract": ["\nMetabolomics is concerned with characterizing the large number of metabolites present in a biological system using nuclear magnetic resonance (NMR) and HPLC/MS (high-performance liquid chromatography with mass spectrometry). Multivariate analysis is one of the most important tools for metabolic biomarker identification in metabolomic studies. However, analyzing the large-scale data sets acquired during metabolic fingerprinting is a major challenge. As a posterior probability that the features of interest are not affected, the local false discovery rate (LFDR) is a good interpretable measure. However, it is rarely used to when interrogating metabolic data to identify biomarkers. In this study, we employed the LFDR method to analyze HPLC/MS data acquired from a metabolomic study of metabolic changes in rat urine during hepatotoxicity induced by Genkwa flos (GF) treatment. The LFDR approach was successfully used to identify important rat urine metabolites altered by GF-stimulated hepatotoxicity. Compared with principle component analysis (PCA), LFDR is an interpretable measure and discovers more important metabolites in an HPLC/MS-based metabolomic study.\n"], "author_display": ["Zuojing Li", "Qing Li", "Lulu Geng", "Xiaohui Chen", "Kaishun Bi"], "article_type": "Research Article", "score": 0.40809408, "title_display": "Use of the Local False Discovery Rate for Identification of Metabolic Biomarkers in Rat Urine Following Genkwa Flos-Induced Hepatotoxicity", "publication_date": "2013-07-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0067451"}, {"journal": "PLoS ONE", "abstract": ["Background: Functional magnetic resonance imaging (fMRI) studies have reported multiple activation foci associated with a variety of conditions, stimuli or tasks. However, most of these studies used fewer than 40 participants. Methodology: After extracting data (number of subjects, condition studied, number of foci identified and threshold) from 94 brain fMRI meta-analyses (k\u200a=\u200a1,788 unique datasets) published through December of 2011, we analyzed the correlation between individual study sample sizes and number of significant foci reported. We also performed an analysis where we evaluated each meta-analysis to test whether there was a correlation between the sample size of the meta-analysis and the number of foci that it had identified. Correlation coefficients were then combined across all meta-analyses to obtain a summary correlation coefficient with a fixed effects model and we combine correlation coefficients, using a Fisher\u2019s z transformation. Principal Findings: There was no correlation between sample size and the number of foci reported in single studies (r\u200a=\u200a0.0050) but there was a strong correlation between sample size and number of foci in meta-analyses (r\u200a=\u200a0.62, p<0.001). Only studies with sample sizes <45 identified larger (>40) numbers of foci and claimed as many discovered foci as studies with sample sizes \u226545, whereas meta-analyses yielded a limited number of foci relative to the yield that would be anticipated from smaller single studies. Conclusions: These results are consistent with possible reporting biases affecting small fMRI studies and suggest the need to promote standardized large-scale evidence in this field. It may also be that small studies may be analyzed and reported in ways that may generate a larger number of claimed foci or that small fMRI studies with inconclusive, null, or not very promising results may not be published at all. "], "author_display": ["Sean P. David", "Jennifer J. Ware", "Isabella M. Chu", "Pooja D. Loftus", "Paolo Fusar-Poli", "Joaquim Radua", "Marcus R. Munaf\u00f2", "John P. A. Ioannidis"], "article_type": "Research Article", "score": 0.40801513, "title_display": "Potential Reporting Bias in fMRI Studies of the Brain", "publication_date": "2013-07-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0070104"}, {"journal": "PLoS ONE", "abstract": ["\nIn the process of new cancer drug development, as the first step of their assessment, their activities are usually studied in vitro against a panel of cancer cell lines. The results of these in vitro drug screening assays are commonly expressed as inhibitory concentration 50% (IC50): the concentration of the tested agent that inhibits the proliferation of the cancer cell population to 50% of the theoretically possible effect (absolute IC50) or maximum effect practically achieved by the drug (relative IC50). The currently available software for calculating IC50 values requires manual data entry, is time consuming, and is prone to calculation errors. Thus, we have developed open source, free, easy-to-use software for performing standardized data evaluations and automatically calculating the IC50. This software eliminates the laborious and error-prone manual entry of data, substantially reduces the amount of time spent for data analysis. It has been extensively used in our department as the main tool for in vitro data processing during the past several years and can be useful for other research groups working in the area of anticancer drug discovery, either alone or combined with other software packages. The current version of our program, Cheburator, together with sample data, source code, and documentation, is freely available at the following URL: http://www.cheburator.nevozhay.com (it is free for academic use, but a license is required for commercial use).\n"], "author_display": ["Dmitry Nevozhay"], "article_type": "Research Article", "score": 0.40793067, "title_display": "Cheburator Software for Automatically Calculating Drug Inhibitory Concentrations from <i>In Vitro</i> Screening Assays", "publication_date": "2014-09-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0106186"}, {"journal": "PLoS ONE", "abstract": ["\nNetwork meta-analysis (NMA) \u2013 a statistical technique that allows comparison of multiple treatments in the same meta-analysis simultaneously \u2013 has become increasingly popular in the medical literature in recent years. The statistical methodology underpinning this technique and software tools for implementing the methods are evolving. Both commercial and freely available statistical software packages have been developed to facilitate the statistical computations using NMA with varying degrees of functionality and ease of use. This paper aims to introduce the reader to three R packages, namely, gemtc, pcnetmeta, and netmeta, which are freely available software tools implemented in R. Each automates the process of performing NMA so that users can perform the analysis with minimal computational effort. We present, compare and contrast the availability and functionality of different important features of NMA in these three packages so that clinical investigators and researchers can determine which R packages to implement depending on their analysis needs. Four summary tables detailing (i) data input and network plotting, (ii) modeling options, (iii) assumption checking and diagnostic testing, and (iv) inference and reporting tools, are provided, along with an analysis of a previously published dataset to illustrate the outputs available from each package. We demonstrate that each of the three packages provides a useful set of tools, and combined provide users with nearly all functionality that might be desired when conducting a NMA.\n"], "author_display": ["Binod Neupane", "Danielle Richer", "Ashley Joel Bonner", "Taddele Kibret", "Joseph Beyene"], "article_type": "Research Article", "score": 0.40765184, "title_display": "Network Meta-Analysis Using R: A Review of Currently Available Automated Packages", "publication_date": "2014-12-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0115065"}, {"journal": "PLoS ONE", "abstract": ["\nAgencies that fund scientific research must choose: is it more effective to give large grants to a few elite researchers, or small grants to many researchers? Large grants would be more effective only if scientific impact increases as an accelerating function of grant size. Here, we examine the scientific impact of individual university-based researchers in three disciplines funded by the Natural Sciences and Engineering Research Council of Canada (NSERC). We considered four indices of scientific impact: numbers of articles published, numbers of citations to those articles, the most cited article, and the number of highly cited articles, each measured over a four-year period. We related these to the amount of NSERC funding received. Impact is positively, but only weakly, related to funding. Researchers who received additional funds from a second federal granting council, the Canadian Institutes for Health Research, were not more productive than those who received only NSERC funding. Impact was generally a decelerating function of funding. Impact per dollar was therefore lower for large grant-holders. This is inconsistent with the hypothesis that larger grants lead to larger discoveries. Further, the impact of researchers who received increases in funding did not predictably increase. We conclude that scientific impact (as reflected by publications) is only weakly limited by funding. We suggest that funding strategies that target diversity, rather than \u201cexcellence\u201d, are likely to prove to be more productive.\n"], "author_display": ["Jean-Michel Fortin", "David J. Currie"], "article_type": "Research Article", "score": 0.40751728, "title_display": "Big Science vs. Little Science: How Scientific Impact Scales with Funding", "publication_date": "2013-06-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0065263"}, {"journal": "PLoS ONE", "abstract": ["\n        In time-resolved spectroscopy, composite signal sequences representing energy transfer in fluorescence materials are measured, and the physical characteristics of the materials are analyzed. Each signal sequence is represented by a sum of non-negative signal components, which are expressed by model functions. For analyzing the physical characteristics of a measured signal sequence, the parameters of the model functions are estimated. Furthermore, in order to quantitatively analyze real measurement data and to reduce the risk of improper decisions, it is necessary to obtain the statistical characteristics from several sequences rather than just a single sequence. In the present paper, we propose an automatic method by which to analyze composite signals using non-negative factorization and an information criterion. The proposed method decomposes the composite signal sequences using non-negative factorization subjected to parametric base functions. The number of components (i.e., rank) is also estimated using Akaike's information criterion. Experiments using simulated and real data reveal that the proposed method automatically estimates the acceptable ranks and parameters.\n      "], "author_display": ["Kenji Watanabe", "Akinori Hidaka", "Nobuyuki Otsu", "Takio Kurita"], "article_type": "Research Article", "score": 0.4074549, "title_display": "Automatic Analysis of Composite Physical Signals Using Non-Negative Factorization and Information Criterion", "publication_date": "2012-03-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0032352"}, {"journal": "PLoS ONE", "abstract": ["Background: This paper presents the first meta-analysis for the inter-rater reliability (IRR) of journal peer reviews. IRR is defined as the extent to which two or more independent reviews of the same scientific document agree. Methodology/Principal Findings: Altogether, 70 reliability coefficients (Cohen's Kappa, intra-class correlation [ICC], and Pearson product-moment correlation [r]) from 48 studies were taken into account in the meta-analysis. The studies were based on a total of 19,443 manuscripts; on average, each study had a sample size of 311 manuscripts (minimum: 28, maximum: 1983). The results of the meta-analysis confirmed the findings of the narrative literature reviews published to date: The level of IRR (mean ICC/r2\u200a=\u200a.34, mean Cohen's Kappa\u200a=\u200a.17) was low. To explain the study-to-study variation of the IRR coefficients, meta-regression analyses were calculated using seven covariates. Two covariates that emerged in the meta-regression analyses as statistically significant to gain an approximate homogeneity of the intra-class correlations indicated that, firstly, the more manuscripts that a study is based on, the smaller the reported IRR coefficients are. Secondly, if the information of the rating system for reviewers was reported in a study, then this was associated with a smaller IRR coefficient than if the information was not conveyed. Conclusions/Significance: Studies that report a high level of IRR are to be considered less credible than those with a low level of IRR. According to our meta-analysis the IRR of peer assessments is quite limited and needs improvement (e.g., reader system). "], "author_display": ["Lutz Bornmann", "R\u00fcdiger Mutz", "Hans-Dieter Daniel"], "article_type": "Research Article", "score": 0.4073706, "title_display": "A Reliability-Generalization Study of Journal Peer Reviews: A Multilevel Meta-Analysis of Inter-Rater Reliability and Its Determinants", "publication_date": "2010-12-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0014331"}, {"journal": "PLoS ONE", "abstract": ["\n        Characterization of tissues like brain by using magnetic resonance (MR) images and colorization of the gray scale image has been reported in the literature, along with the advantages and drawbacks. Here, we present two independent methods; (i) a novel colorization method to underscore the variability in brain MR images, indicative of the underlying physical density of bio tissue, (ii) a segmentation method (both hard and soft segmentation) to characterize gray brain MR images. The segmented images are then transformed into color using the above-mentioned colorization method, yielding promising results for manual tracing. Our color transformation incorporates the voxel classification by matching the luminance of voxels of the source MR image and provided color image by measuring the distance between them. The segmentation method is based on single-phase clustering for 2D and 3D image segmentation with a new auto centroid selection method, which divides the image into three distinct regions (gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF) using prior anatomical knowledge). Results have been successfully validated on human T2-weighted (T2) brain MR images. The proposed method can be potentially applied to gray-scale images from other imaging modalities, in bringing out additional diagnostic tissue information contained in the colorized image processing approach as described.\n      "], "author_display": ["Muhammad Attique", "Ghulam Gilanie", "Hafeez-Ullah", "Malik S. Mehmood", "Muhammad S. Naweed", "Masroor Ikram", "Javed A. Kamran", "Alex Vitkin"], "article_type": "Research Article", "score": 0.4071783, "title_display": "Colorization and Automated Segmentation of Human T2 MR Brain Images for Characterization of Soft Tissues", "publication_date": "2012-03-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0033616"}, {"journal": "PLoS ONE", "abstract": ["Background: Most existing methods for phylogenetic analysis involve developing an evolutionary model and then using some type of computational algorithm to perform multiple sequence alignment. There are two problems with this approach: (1) different evolutionary models can lead to different results, and (2) the computation time required for multiple alignments makes it impossible to analyse the phylogeny of a whole genome. This motivates us to create a new approach to characterize genetic sequences. Methodology: To each DNA sequence, we associate a natural vector based on the distributions of nucleotides. This produces a one-to-one correspondence between the DNA sequence and its natural vector. We define the distance between two DNA sequences to be the distance between their associated natural vectors. This creates a genome space with a biological distance which makes global comparison of genomes with same topology possible. We use our proposed method to analyze the genomes of the new influenza A (H1N1) virus, human rhinoviruses (HRV) and mammalian mitochondrial. The result shows that a triple-reassortant swine virus circulating in North America and the Eurasian swine virus belong to the lineage of the influenza A (H1N1) virus. For the HRV and mammalian mitochondrial genomes, the results coincide with biologists' analyses. Conclusions: Our approach provides a powerful new tool for analyzing and annotating genomes and their phylogenetic relationships. Whole or partial genomes can be handled more easily and more quickly than using multiple alignment methods. Once a genome space has been constructed, it can be stored in a database. There is no need to reconstruct the genome space for subsequent applications, whereas in multiple alignment methods, realignment is needed to add new sequences. Furthermore, one can make a global comparison of all genomes simultaneously, which no other existing method can achieve. "], "author_display": ["Mo Deng", "Chenglong Yu", "Qian Liang", "Rong L. He", "Stephen S.-T. Yau"], "article_type": "Research Article", "score": 0.40717453, "title_display": "A Novel Method of Characterizing Genetic Sequences: Genome Space with Biological Distance and Applications", "publication_date": "2011-03-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0017293"}, {"journal": "PLoS ONE", "abstract": ["\nIn anatomic pathology, immunohistochemistry (IHC) serves as a diagnostic and prognostic method for identification of disease markers in tissue samples that directly influences classification and grading the disease, influencing patient management. However, till today over most of the world, pathological analysis of tissue samples remained a time-consuming and subjective procedure, wherein the intensity of antibody staining is manually judged and thus scoring decision is directly influenced by visual bias. This instigated us to design a simple method of automated digital IHC image analysis algorithm for an unbiased, quantitative assessment of antibody staining intensity in tissue sections. As a first step, we adopted the spectral deconvolution method of DAB/hematoxylin color spectra by using optimized optical density vectors of the color deconvolution plugin for proper separation of the DAB color spectra. Then the DAB stained image is displayed in a new window wherein it undergoes pixel-by-pixel analysis, and displays the full profile along with its scoring decision. Based on the mathematical formula conceptualized, the algorithm is thoroughly tested by analyzing scores assigned to thousands (n\u200a=\u200a1703) of DAB stained IHC images including sample images taken from human protein atlas web resource. The IHC Profiler plugin developed is compatible with the open resource digital image analysis software, ImageJ, which creates a pixel-by-pixel analysis profile of a digital IHC image and further assigns a score in a four tier system. A comparison study between manual pathological analysis and IHC Profiler resolved in a match of 88.6% (P<0.0001, CI\u200a=\u200a95%). This new tool developed for clinical histopathological sample analysis can be adopted globally for scoring most protein targets where the marker protein expression is of cytoplasmic and/or nuclear type. We foresee that this method will minimize the problem of inter-observer variations across labs and further help in worldwide patient stratification potentially benefitting various multinational clinical trial initiatives.\n"], "author_display": ["Frency Varghese", "Amirali B. Bukhari", "Renu Malhotra", "Abhijit De"], "article_type": "Research Article", "score": 0.40717193, "title_display": "IHC Profiler: An Open Source Plugin for the Quantitative Evaluation and Automated Scoring of Immunohistochemistry Images of Human Tissue Samples", "publication_date": "2014-05-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0096801"}, {"journal": "PLOS ONE", "abstract": ["\nNitrate, the most oxidized form of nitrogen, is regulated to protect people and animals from harmful levels as there is a large over abundance due to anthropogenic factors. Widespread field testing for nitrate could begin to address the nitrate pollution problem, however, the Cadmium Reduction Method, the leading certified method to detect and quantify nitrate, demands the use of a toxic heavy metal. An alternative, the recently proposed Environmental Protection Agency Nitrate Reductase Nitrate-Nitrogen Analysis Method, eliminates this problem but requires an expensive proprietary spectrophotometer. The development of an inexpensive portable, handheld photometer will greatly expedite field nitrate analysis to combat pollution. To accomplish this goal, a methodology for the design, development, and technical validation of an improved open-source water testing platform capable of performing Nitrate Reductase Nitrate-Nitrogen Analysis Method. This approach is evaluated for its potential to i) eliminate the need for toxic chemicals in water testing for nitrate and nitrite, ii) reduce the cost of equipment to perform this method for measurement for water quality, and iii) make the method easier to carryout in the field. The device is able to perform as well as commercial proprietary systems for less than 15% of the cost for materials. This allows for greater access to the technology and the new, safer nitrate testing technique.\n"], "author_display": ["B. T. Wittbrodt", "D. A. Squires", "J. Walbeck", "E. Campbell", "W. H. Campbell", "J. M. Pearce"], "article_type": "Research Article", "score": 0.40697223, "title_display": "Open-Source Photometric System for Enzymatic Nitrate Quantification", "publication_date": "2015-08-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0134989"}, {"journal": "PLoS ONE", "abstract": ["\nBurn scar extraction using remote sensing data is an efficient way to precisely evaluate burn area and measure vegetation recovery. Traditional burn scar extraction methodologies have no well effect on burn scar image with blurred and irregular edges. To address these issues, this paper proposes an automatic method to extract burn scar based on Level Set Method (LSM). This method utilizes the advantages of the different features in remote sensing images, as well as considers the practical needs of extracting the burn scar rapidly and automatically. This approach integrates Change Vector Analysis (CVA), Normalized Difference Vegetation Index (NDVI) and the Normalized Burn Ratio (NBR) to obtain difference image and modifies conventional Level Set Method Chan-Vese (C-V) model with a new initial curve which results from a binary image applying K-means method on fitting errors of two near-infrared band images. Landsat 5 TM and Landsat 8 OLI data sets are used to validate the proposed method. Comparison with conventional C-V model, OSTU algorithm, Fuzzy C-mean (FCM) algorithm are made to show that the proposed approach can extract the outline curve of fire burn scar effectively and exactly. The method has higher extraction accuracy and less algorithm complexity than that of the conventional C-V model.\n"], "author_display": ["Yang Liu", "Qin Dai", "JianBo Liu", "ShiBin Liu", "Jin Yang"], "article_type": "Research Article", "score": 0.40685573, "title_display": "Study of Burn Scar Extraction Automatically Based on Level Set Method using Remote Sensing Data", "publication_date": "2014-02-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0087480"}, {"journal": "PLOS ONE", "abstract": ["\nMeasurement and calibration of an analog-to-digital converter (ADC) using a histogram-based method requires a large volume of data and a long test duration, especially for a high resolution ADC. A fast and accurate calibration method for pipelined ADCs is proposed in this research. The proposed calibration method composes histograms through the outputs of each stage and calculates error sources. The digitized outputs of a stage are influenced directly by the operation of the prior stage, so the results of the histogram provide the information of errors in the prior stage. The composed histograms reduce the required samples and thus calibration time being implemented by simple modules. For 14-bit resolution pipelined ADC, the measured maximum integral non-linearity (INL) is improved from 6.78 to 0.52 LSB, and the spurious-free dynamic range (SFDR) and signal-to-noise-and-distortion ratio (SNDR) are improved from 67.0 to 106.2dB and from 65.6 to 84.8dB, respectively.\n"], "author_display": ["Hyeonuk Son", "Jaewon Jang", "Heetae Kim", "Sungho Kang"], "article_type": "Research Article", "score": 0.4068172, "title_display": "Histogram-Based Calibration Method for Pipeline ADCs", "publication_date": "2015-06-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0129736"}, {"journal": "PLoS ONE", "abstract": [": Risk factors for cardiovascular disease including diabetes have seen a large rise in prevalence in recent years. This has prompted interest in prevention through the identifying individuals at risk of both diabetes and cardiovascular disease and has seen increased investment in screening interventions taking place in primary care. Community pharmacies have become increasingly involved in the provision of such interventions and this systematic review and meta-analysis aims to gather and analyse the existing literature assessing community pharmacy based screening for risk factors for diabetes and those with a high cardiovascular disease risk. Methods: We conducted systematic searches of electronic databases using MeSH and free text terms from 1950 to March 2012. For our analysis two outcomes were assessed. They were the percentage of those screened who were referred for further assessment by primary care and the uptake of this referral. Results: Sixteen studies fulfilled our inclusion criteria comprising 108,414 participants screened. There was significant heterogeneity for all included outcomes. Consequently we have not presented summary statistics and present forest plots with I2 and p values to describe heterogeneity. We found that all included studies suffered from high rates of attrition between pharmacy screening and follow up. We have also identified a strong trend towards higher rates for referral in more recent studies. Conclusions: Our results show that pharmacies are feasible sites for screening for diabetes and those at risk of cardiovascular disease. A significant number of previously unknown cases of cardiovascular disease risk factors such as hypertension, hypercholesterolemia and diabetes are identified, however a significant number of referred participants at high risk do not attend their practitioner for follow up. Research priorities should include methods of increasing uptake to follow up testing and early intervention, to maximise the efficacy of screening interventions based in community pharmacies. "], "author_display": ["Andrew Willis", "Peter Rivers", "Laura J. Gray", "Melanie Davies", "Kamlesh Khunti"], "article_type": "Research Article", "score": 0.40665677, "title_display": "The Effectiveness of Screening for Diabetes and Cardiovascular Disease Risk Factors in a Community Pharmacy Setting", "publication_date": "2014-04-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0091157"}, {"journal": "PLoS ONE", "abstract": ["Background: Collaborative care is a complex intervention based on chronic disease management models and is effective in the management of depression. However, there is still uncertainty about which components of collaborative care are effective. We used meta-regression to identify factors in collaborative care associated with improvement in patient outcomes (depressive symptoms) and the process of care (use of anti-depressant medication). Methods and Findings: Systematic review with meta-regression. The Cochrane Collaboration Depression, Anxiety and Neurosis Group trials registers were searched from inception to 9th February 2012. An update was run in the CENTRAL trials database on 29th December 2013. Inclusion criteria were: randomised controlled trials of collaborative care for adults \u226518 years with a primary diagnosis of depression or mixed anxiety and depressive disorder. Random effects meta-regression was used to estimate regression coefficients with 95% confidence intervals (CIs) between study level covariates and depressive symptoms and relative risk (95% CI) and anti-depressant use. The association between anti-depressant use and improvement in depression was also explored. Seventy four trials were identified (85 comparisons, across 21,345 participants). Collaborative care that included psychological interventions predicted improvement in depression (\u03b2 coefficient \u22120.11, 95% CI \u22120.20 to \u22120.01, p\u200a=\u200a0.03). Systematic identification of patients (relative risk 1.43, 95% CI 1.12 to 1.81, p\u200a=\u200a0.004) and the presence of a chronic physical condition (relative risk 1.32, 95% CI 1.05 to 1.65, p\u200a=\u200a0.02) predicted use of anti-depressant medication. Conclusion: Trials of collaborative care that included psychological treatment, with or without anti-depressant medication, appeared to improve depression more than those without psychological treatment. Trials that used systematic methods to identify patients with depression and also trials that included patients with a chronic physical condition reported improved use of anti-depressant medication. However, these findings are limited by the observational nature of meta-regression, incomplete data reporting, and the use of study aggregates. "], "author_display": ["Peter A. Coventry", "Joanna L. Hudson", "Evangelos Kontopantelis", "Janine Archer", "David A. Richards", "Simon Gilbody", "Karina Lovell", "Chris Dickens", "Linda Gask", "Waquas Waheed", "Peter Bower"], "article_type": "Research Article", "score": 0.40665388, "title_display": "Characteristics of Effective Collaborative Care for Treatment of Depression: A Systematic Review and Meta-Regression of 74 Randomised Controlled Trials", "publication_date": "2014-09-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0108114"}, {"journal": "PLoS ONE", "abstract": ["\nHigh Throughput Biological Data (HTBD) requires detailed analysis methods and from a life science perspective, these analysis results make most sense when interpreted within the context of biological pathways. Bayesian Networks (BNs) capture both linear and nonlinear interactions and handle stochastic events in a probabilistic framework accounting for noise making them viable candidates for HTBD analysis. We have recently proposed an approach, called Bayesian Pathway Analysis (BPA), for analyzing HTBD using BNs in which known biological pathways are modeled as BNs and pathways that best explain the given HTBD are found. BPA uses the fold change information to obtain an input matrix to score each pathway modeled as a BN. Scoring is achieved using the Bayesian-Dirichlet Equivalent method and significance is assessed by randomization via bootstrapping of the columns of the input matrix. In this study, we improve on the BPA system by optimizing the steps involved in \u201cData Preprocessing and Discretization\u201d, \u201cScoring\u201d, \u201cSignificance Assessment\u201d, and \u201cSoftware and Web Application\u201d. We tested the improved system on synthetic data sets and achieved over 98% accuracy in identifying the active pathways. The overall approach was applied on real cancer microarray data sets in order to investigate the pathways that are commonly active in different cancer types. We compared our findings on the real data sets with a relevant approach called the Signaling Pathway Impact Analysis (SPIA).\n"], "author_display": ["Melike Korucuoglu", "Senol Isci", "Arzucan Ozgur", "Hasan H. Otu"], "article_type": "Research Article", "score": 0.4064191, "title_display": "Bayesian Pathway Analysis of Cancer Microarray Data", "publication_date": "2014-07-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0102803"}, {"journal": "PLoS ONE", "abstract": ["Background: Diabetic nephropathy (DN) is a complex and chronic metabolic disease that evolves into a progressive fibrosing renal disorder. Effective transcriptomic profiling of slowly evolving disease processes such as DN can be problematic. The changes that occur are often subtle and can escape detection by conventional oligonucleotide DNA array analyses. Methodology/Principal Findings: We examined microdissected human renal tissue with or without DN using Affymetrix oligonucleotide microarrays (HG-U133A) by standard Robust Multi-array Analysis (RMA). Subsequent gene ontology analysis by Database for Annotation, Visualization and Integrated Discovery (DAVID) showed limited detection of biological processes previously identified as central mechanisms in the development of DN (e.g. inflammation and angiogenesis). This apparent lack of sensitivity may be associated with the gene-oriented averaging of oligonucleotide probe signals, as this includes signals from cross-hybridizing probes and gene annotation that is based on out of date genomic data. We then examined the same CEL file data using a different methodology to determine how well it could correlate transcriptomic data with observed biology. ChipInspector (CI) is based on single probe analysis and de novo gene annotation that bypasses probe set definitions. Both methods, RMA and CI, used at default settings yielded comparable numbers of differentially regulated genes. However, when verified by RT-PCR, the single probe based analysis demonstrated reduced background noise with enhanced sensitivity and fewer false positives. Conclusions/Significance: Using a single probe based analysis approach with de novo gene annotation allowed an improved representation of the biological processes linked to the development and progression of DN. The improved analysis was exemplified by the detection of Wnt signaling pathway activation in DN, a process not previously reported to be involved in this disease. "], "author_display": ["Clemens D. Cohen", "Maja T. Lindenmeyer", "Felix Eichinger", "Alexander Hahn", "Martin Seifert", "Anton G. Moll", "Holger Schmid", "Eva Kiss", "Elisabeth Gr\u00f6ne", "Hermann-Josef Gr\u00f6ne", "Matthias Kretzler", "Thomas Werner", "Peter J. Nelson"], "article_type": "Research Article", "score": 0.4062466, "title_display": "Improved Elucidation of Biological Processes Linked to Diabetic Nephropathy by Single Probe-Based Microarray Data Analysis", "publication_date": "2008-08-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0002937"}, {"journal": "PLOS ONE", "abstract": ["\nSelf-thinning is a dynamic equilibrium between forest growth and mortality at full site occupancy. Parameters of the self-thinning lines are often confounded by differences across various stand and site conditions. For overcoming the problem of hierarchical and repeated measures, we used hierarchical Bayesian method to estimate the self-thinning line. The results showed that the self-thinning line for Chinese fir (Cunninghamia lanceolata (Lamb.)Hook.) plantations was not sensitive to the initial planting density. The uncertainty of model predictions was mostly due to within-subject variability. The simulation precision of hierarchical Bayesian method was better than that of stochastic frontier function (SFF). Hierarchical Bayesian method provided a reasonable explanation of the impact of other variables (site quality, soil type, aspect, etc.) on self-thinning line, which gave us the posterior distribution of parameters of self-thinning line. The research of self-thinning relationship could be benefit from the use of hierarchical Bayesian method.\n"], "author_display": ["Xiongqing Zhang", "Jianguo Zhang", "Aiguo Duan"], "article_type": "Research Article", "score": 0.40622476, "title_display": "A Hierarchical Bayesian Model to Predict Self-Thinning Line for Chinese Fir in Southern China", "publication_date": "2015-10-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0139788"}, {"journal": "PLoS ONE", "abstract": ["\nResearchers contribute to the scientific peer review system by providing reviews, and \u201cwithdraw\u201d from it by submitting manuscripts that are subsequently reviewed. So far as we are aware, there has been no quantification of the balance of individual's contributions and withdrawals. We compared the number of reviews provided by individual researchers (i.e., their contribution) to the number required by their submissions (i.e. their withdrawals) in a large and anonymised database provided by the British Ecological Society. The database covered the Journal of Ecology, Journal of Animal Ecology, Journal of Applied Ecology, and Functional Ecology from 2003\u20132010. The majority of researchers (64%) did not have balanced contributions and withdrawals. Depending on assumptions, 12% to 44% contributed more than twice as much as required; 20% to 52% contributed less than half as much as required. Balance, or lack thereof, varied little in relation to the number of years a researcher had been active (reviewing or submitting). Researchers who contributed less than required did not lack the opportunity to review. Researchers who submitted more were more likely to accept invitations to review. These finding suggest overall that peer review of the four analysed journals is not in crisis, but only due to the favourable balance of over- and under-contributing researchers. These findings are limited to the four journals analysed, and therefore cannot include researcher's other peer review activities, which if included might change the proportions reported. Relatively low effort was required to assemble, check, and analyse the data. Broader analyses of individual researcher's peer review activities would contribute to greater quality, efficiency, and fairness in the peer review system.\n"], "author_display": ["Owen L. Petchey", "Jeremy W. Fox", "Lindsay Haddon"], "article_type": "Research Article", "score": 0.40615302, "title_display": "Imbalance in Individual Researcher's Peer Review Activities Quantified for Four British Ecological Society Journals, 2003-2010", "publication_date": "2014-03-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0092896"}, {"journal": "PLoS ONE", "abstract": ["\n        The quantity of circulating reticulocytes is an important indicator of erythropoietic activity in response to a wide range of haematological pathologies. While most modern laboratories use flow cytometry to quantify reticulocytes, most field laboratories still rely on \u2018subvital\u2019 staining. The specialist \u2018subvital\u2019 stains, New Methylene Blue (NMB) and Brilliant Cr\u00e9syl Blue are often difficult to procure, toxic, and show inconsistencies between batches. Here we demonstrate the utility of Giemsa's stain (commonly used microbiology and parasitology) in a \u2018subvital\u2019 manner to provide an accurate method to visualize and count reticulocytes in blood samples from normal and malaria-infected individuals.\n      "], "author_display": ["Wenn-Chyau Lee", "Bruce Russell", "Yee-Ling Lau", "Mun-Yik Fong", "Cindy Chu", "Kanlaya Sriprawat", "Rossarin Suwanarusk", "Francois Nosten", "Laurent Renia"], "article_type": "Research Article", "score": 0.40599456, "title_display": "Giemsa-Stained Wet Mount Based Method for Reticulocyte Quantification: A Viable Alternative in Resource Limited or Malaria Endemic Settings", "publication_date": "2013-04-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0060303"}, {"journal": "PLoS ONE", "abstract": ["\nIn terms of making genes expression data more interpretable and comprehensible, there exists a significant superiority on sparse methods. Many sparse methods, such as penalized matrix decomposition (PMD) and sparse principal component analysis (SPCA), have been applied to extract plants core genes. Supervised algorithms, especially the support vector machine-recursive feature elimination (SVM-RFE) method, always have good performance in gene selection. In this paper, we draw into class information via the total scatter matrix and put forward a class-information-based penalized matrix decomposition (CIPMD) method to improve the gene identification performance of PMD-based method. Firstly, the total scatter matrix is obtained based on different samples of the gene expression data. Secondly, a new data matrix is constructed by decomposing the total scatter matrix. Thirdly, the new data matrix is decomposed by PMD to obtain the sparse eigensamples. Finally, the core genes are identified according to the nonzero entries in eigensamples. The results on simulation data show that CIPMD method can reach higher identification accuracies than the conventional gene identification methods. Moreover, the results on real gene expression data demonstrate that CIPMD method can identify more core genes closely related to the abiotic stresses than the other methods.\n"], "author_display": ["Jin-Xing Liu", "Jian Liu", "Ying-Lian Gao", "Jian-Xun Mi", "Chun-Xia Ma", "Dong Wang"], "article_type": "Research Article", "score": 0.40575013, "title_display": "A Class-Information-Based Penalized Matrix Decomposition for Identifying Plants Core Genes Responding to Abiotic Stresses", "publication_date": "2014-09-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0106097"}, {"journal": "PLoS ONE", "abstract": ["\nDiscovery of communities in complex networks is a fundamental data analysis problem with applications in various domains. While most of the existing approaches have focused on discovering communities of nodes, recent studies have shown the advantages and uses of link community discovery in networks. Generative models provide a promising class of techniques for the identification of modular structures in networks, but most generative models mainly focus on the detection of node communities rather than link communities. In this work, we propose a generative model, which is based on the importance of each node when forming links in each community, to describe the structure of link communities. We proceed to fit the model parameters by taking it as an optimization problem, and solve it using nonnegative matrix factorization. Thereafter, in order to automatically determine the number of communities, we extend the above method by introducing a strategy of iterative bipartition. This extended method not only finds the number of communities all by itself, but also obtains high efficiency, and thus it is more suitable to deal with large and unexplored real networks. We test this approach on both synthetic benchmarks and real-world networks including an application on a large biological network, and compare it with two highly related methods. Results demonstrate the superior performance of our approach over competing methods for the detection of link communities.\n"], "author_display": ["Dongxiao He", "Di Jin", "Carlos Baquero", "Dayou Liu"], "article_type": "Research Article", "score": 0.40571803, "title_display": "Link Community Detection Using Generative Model and Nonnegative Matrix Factorization", "publication_date": "2014-01-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0086899"}, {"journal": "PLOS ONE", "abstract": ["\nAn array of empirical research has emerged related to public participation in health research. To date, few studies have explored the particular perspectives of gay and bisexual men taking part in behavioural surveillance research, which includes the donation of saliva swabs to investigate HIV prevalence and rates of undiagnosed HIV. Semi-structured interviews were conducted with twenty-nine gay and bisexual men in Scotland who had participated in a bar-based survey. Thematic analysis of men\u2019s accounts of their motives for participation and their perceptions of not receiving individual feedback on HIV status suggested a shared understanding of participation in research as a means of contributing to \u2018community\u2019 efforts to prevent the spread of HIV. Most men expressed sophisticated understandings of the purpose of behavioural research and distinguished between this and individual diagnostic testing. Despite calls for feedback on HIV results broadly, for these men feedback on HIV status was not deemed crucial.\n"], "author_display": ["Nicola Boydell", "Gillian May Fergie", "Lisa Margaret McDaid", "Shona Hilton"], "article_type": "Research Article", "score": 0.40560454, "title_display": "Understandings of Participation in Behavioural Research: A Qualitative Study of Gay and Bisexual Men in Scotland", "publication_date": "2015-08-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0135001"}, {"journal": "PLoS ONE", "abstract": ["\nSystematic reviews that collate data about the relative effects of multiple interventions via network meta-analysis are highly informative for decision-making purposes. A network meta-analysis provides two types of findings for a specific outcome: the relative treatment effect for all pairwise comparisons, and a ranking of the treatments. It is important to consider the confidence with which these two types of results can enable clinicians, policy makers and patients to make informed decisions. We propose an approach to determining confidence in the output of a network meta-analysis. Our proposed approach is based on methodology developed by the Grading of Recommendations Assessment, Development and Evaluation (GRADE) Working Group for pairwise meta-analyses. The suggested framework for evaluating a network meta-analysis acknowledges (i) the key role of indirect comparisons (ii) the contributions of each piece of direct evidence to the network meta-analysis estimates of effect size; (iii) the importance of the transitivity assumption to the validity of network meta-analysis; and (iv) the possibility of disagreement between direct evidence and indirect evidence. We apply our proposed strategy to a systematic review comparing topical antibiotics without steroids for chronically discharging ears with underlying eardrum perforations. The proposed framework can be used to determine confidence in the results from a network meta-analysis. Judgements about evidence from a network meta-analysis can be different from those made about evidence from pairwise meta-analyses.\n"], "author_display": ["Georgia Salanti", "Cinzia Del Giovane", "Anna Chaimani", "Deborah M. Caldwell", "Julian P. T. Higgins"], "article_type": "Research Article", "score": 0.4054918, "title_display": "Evaluating the Quality of Evidence from a Network Meta-Analysis", "publication_date": "2014-07-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0099682"}, {"journal": "PLoS ONE", "abstract": ["\nMost studies on global health inequality consider unequal health care and socio-economic conditions but neglect inequality in the production of health knowledge relevant to addressing disease burden. We demonstrate this inequality and identify likely causes. Using disability-adjusted life years (DALYs) for 111 prominent medical conditions, assessed globally and nationally by the World Health Organization, we linked DALYs with MEDLINE articles for each condition to assess the influence of DALY-based global disease burden, compared to the global market for treatment, on the production of relevant MEDLINE articles, systematic reviews, clinical trials and research using animal models vs. humans. We then explored how DALYs, wealth, and the production of research within countries correlate with this global pattern. We show that global DALYs for each condition had a small, significant negative relationship with the production of each type of MEDLINE articles for that condition. Local processes of health research appear to be behind this. Clinical trials and animal studies but not systematic reviews produced within countries were strongly guided by local DALYs. More and less developed countries had very different disease profiles and rich countries publish much more than poor countries. Accordingly, conditions common to developed countries garnered more clinical research than those common to less developed countries. Many of the health needs in less developed countries do not attract attention among developed country researchers who produce the vast majority of global health knowledge\u2014including clinical trials\u2014in response to their own local needs. This raises concern about the amount of knowledge relevant to poor populations deficient in their own research infrastructure. We recommend measures to address this critical dimension of global health inequality.\n"], "author_display": ["James A. Evans", "Jae-Mahn Shim", "John P. A. Ioannidis"], "article_type": "Research Article", "score": 0.40537193, "title_display": "Attention to Local Health Burden and the Global Disparity of Health Research", "publication_date": "2014-04-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0090147"}, {"journal": "PLoS ONE", "abstract": ["\nWe aimed to use the willingness to pay (WTP) method to calculate the cost of traffic injuries in Iran in 2013. We conducted a cross-sectional questionnaire-based study of 846 randomly selected road users. WTP data was collected for four scenarios for vehicle occupants, pedestrians, vehicle drivers, and motorcyclists. Final analysis was carried out using Weibull and maximum likelihood method. Mean WTP was 2,612,050 Iranian rials (IRR). Statistical value of life was estimated according to 20,408 fatalities 402,314,106,073,648 IRR (US$13,410,470,202 based on purchasing power parity at (February 27th, 2014). Injury cost was US$25,637,870,872 (based on 318,802 injured people in 2013, multiple daily traffic volume of 311, and multiple daily payment of 31,030 IRR for 250 working days). The total estimated cost of injury and death cases was 39,048,341,074$. Gross national income of Iran was, US$604,300,000,000 in 2013 and the costs of traffic injuries constituted 6\u00b746% of gross national income. WTP was significantly associated with age, gender, monthly income, daily payment, more payment for time reduction, trip mileage, drivers and occupants from road users. The costs of traffic injuries in Iran in 2013 accounted for 6.64% of gross national income, much higher than the global average. Policymaking and resource allocation to reduce traffic-related death and injury rates have the potential to deliver a huge economic benefit.\n"], "author_display": ["Elaheh Ainy", "Hamid Soori", "Mojtaba Ganjali", "Henry Le", "Taban Baghfalaki"], "article_type": "Research Article", "score": 0.40511304, "title_display": "Estimating Cost of Road Traffic Injuries in Iran Using Willingness to Pay (WTP) Method", "publication_date": "2014-12-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0112721"}, {"journal": "PLoS ONE", "abstract": ["\nThe metagenomic method directly sequences and analyses genome information from microbial communities. The main computational tasks for metagenomic analyses include taxonomical and functional structure analysis for all genomes in a microbial community (also referred to as a metagenomic sample). With the advancement of Next Generation Sequencing (NGS) techniques, the number of metagenomic samples and the data size for each sample are increasing rapidly. Current metagenomic analysis is both data- and computation- intensive, especially when there are many species in a metagenomic sample, and each has a large number of sequences. As such, metagenomic analyses require extensive computational power. The increasing analytical requirements further augment the challenges for computation analysis. In this work, we have proposed Parallel-META 2.0, a metagenomic analysis software package, to cope with such needs for efficient and fast analyses of taxonomical and functional structures for microbial communities. Parallel-META 2.0 is an extended and improved version of Parallel-META 1.0, which enhances the taxonomical analysis using multiple databases, improves computation efficiency by optimized parallel computing, and supports interactive visualization of results in multiple views. Furthermore, it enables functional analysis for metagenomic samples including short-reads assembly, gene prediction and functional annotation. Therefore, it could provide accurate taxonomical and functional analyses of the metagenomic samples in high-throughput manner and on large scale.\n"], "author_display": ["Xiaoquan Su", "Weihua Pan", "Baoxing Song", "Jian Xu", "Kang Ning"], "article_type": "Research Article", "score": 0.4049423, "title_display": "Parallel-META 2.0: Enhanced Metagenomic Data Analysis with Functional Annotation, High Performance Computing and Advanced Visualization", "publication_date": "2014-03-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0089323"}, {"journal": "PLoS ONE", "abstract": ["\n        The cell cycle of the fission yeast, Schizosaccharomyces pombe, does not easily lend itself to analysis by flow cytometry, mainly because cells in G1 and G2 phase contain the same amount of DNA. This occurs because fission yeast cells under standard growth conditions do not complete cytokinesis until after G1 phase. We have devised a flow cytometric method exploiting the fact that cells in G1 phase contain two nuclei, whereas cells in G2 are mononuclear. Measurements of the width as well as the total area of the DNA-associated fluorescence signal allows the discrimination between cells in G1 and in G2 phase and the cell-cycle progression of fission yeast can be followed in detail by flow cytometry. Furthermore, we show how this method can be used to monitor the timing of cell entry into anaphase. Fission yeast cells tend to form multimers, which represents another problem of flow cytometry-based cell-cycle analysis. Here we present a method employing light-scatter measurements to enable the exclusion of cell doublets, thereby further improving the analysis of fission yeast cells by flow cytometry.\n      "], "author_display": ["Jon Halvor Jonsrud Knutsen", "Idun Dale Rein", "Christiane Rothe", "Trond Stokke", "Be\u00e1ta Grallert", "Erik Boye"], "article_type": "Research Article", "score": 0.40483207, "title_display": "Cell-Cycle Analysis of Fission Yeast Cells by Flow Cytometry", "publication_date": "2011-02-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0017175"}, {"abstract": ["\n        It has been recognized that other than habitat loss, degradation and fragmentation, the infection of the roundworm Baylisascaris schroederi (B. schroederi) is one of the major causes of death in wild giant pandas. However, the prevalence and intensity of the parasite infection has been inconsistently reported through a method that uses sedimentation-floatation followed by a microscope examination. This method fails to accurately determine infection because there are many bamboo residues and/or few B. schroederi eggs in the examined fecal samples. In the present study, we adopted a method that uses PCR and capillary electrophoresis combined with a single-strand conformation polymorphism analysis (PCR/CE-SSCP) to detect B. schroederi infection in wild giant pandas at a nature reserve, and compared it to the traditional microscope approach. The PCR specifically amplified a single band of 279-bp from both fecal samples and positive controls, which was confirmed by sequence analysis to correspond to the mitochondrial COII gene of B. schroederi. Moreover, it was demonstrated that the amount of genomic DNA was linearly correlated with the peak area of the CE-SSCP analysis. Thus, our adopted method can reliably detect the infectious prevalence and intensity of B. schroederi in wild giant pandas. The prevalence of B. schroederi was found to be 54% in the 91 fecal samples examined, and 48% in the fecal samples of 31 identified individual giant pandas. Infectious intensities of the 91 fecal samples were detected to range from 2.8 to 959.2 units/gram, and from 4.8 to 959.2 units/gram in the fecal samples of the 31 identified giant pandas. For comparison, by using the traditional microscope method, the prevalence of B. schroederi was found to be only 33% in the 91 fecal samples, 32% in the fecal samples of the 31 identified giant pandas, and no reliable infectious intensity was observed.\n      "], "author_display": ["Wenping Zhang", "Shangmian Yie", "Bisong Yue", "Jielong Zhou", "Renxiong An", "Jiangdong Yang", "Wangli Chen", "Chengdong Wang", "Liang Zhang", "Fujun Shen", "Guangyou Yang", "Rong Hou", "Zhihe Zhang"], "article_type": "Research Article", "score": 0.40467137, "title_display": "Determination of <i>Baylisascaris schroederi</i> Infection in Wild Giant Pandas by an Accurate and Sensitive PCR/CE-SSCP Method", "publication_date": "2012-07-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0041995"}, {"journal": "PLoS Genetics", "abstract": ["\nGene expression data, in conjunction with information on genetic variants, have enabled studies to identify expression quantitative trait loci (eQTLs) or polymorphic locations in the genome that are associated with expression levels. Moreover, recent technological developments and cost decreases have further enabled studies to collect expression data in multiple tissues. One advantage of multiple tissue datasets is that studies can combine results from different tissues to identify eQTLs more accurately than examining each tissue separately. The idea of aggregating results of multiple tissues is closely related to the idea of meta-analysis which aggregates results of multiple genome-wide association studies to improve the power to detect associations. In principle, meta-analysis methods can be used to combine results from multiple tissues. However, eQTLs may have effects in only a single tissue, in all tissues, or in a subset of tissues with possibly different effect sizes. This heterogeneity in terms of effects across multiple tissues presents a key challenge to detect eQTLs. In this paper, we develop a framework that leverages two popular meta-analysis methods that address effect size heterogeneity to detect eQTLs across multiple tissues. We show by using simulations and multiple tissue data from mouse that our approach detects many eQTLs undetected by traditional eQTL methods. Additionally, our method provides an interpretation framework that accurately predicts whether an eQTL has an effect in a particular tissue.\nAuthor Summary: The combination of gene expression and genetic variation data has enabled the identification of genetic variants that affect gene expression levels. It has been shown that some variants influence gene expression in only one tissue while others influence gene expression in multiple tissues. However, an analysis of multiple tissue data using traditional statistical methods typically fails to identify those variants that affect multiple tissues because each tissue is treated independently and due to low statistical power, the effect in a given tissue may be missed. Building on recent advances in statistical methods for meta-analysis and mixed models, we present a novel method that combines information from multiple tissues to identify genetic variation that affects multiple tissues. We show that our method detects more genetic variation that influences multiple tissues than traditional statistical methods both on simulated and real data. "], "author_display": ["Jae Hoon Sul", "Buhm Han", "Chun Ye", "Ted Choi", "Eleazar Eskin"], "article_type": "Research Article", "score": 0.40437114, "title_display": "Effectively Identifying eQTLs from Multiple Tissues by Combining Mixed Model and Meta-analytic Approaches", "publication_date": "2013-06-13T00:00:00Z", "eissn": "1553-7404", "id": "10.1371/journal.pgen.1003491"}, {"journal": "PLOS ONE", "abstract": ["Background: The UK, like some other countries, carries out a periodic review of research quality in universities and the most recent Research Excellence Framework (REF) reported a doubling (103% increase) in its \u201cworld leading\u201d or so-called \u201c4*\u201d research outputs in the areas of life sciences and medicine between 2008 and 2014. This is a remarkable improvement in six years and if validated internationally could have profound implications for health sciences. Methods: We compared the reported changes in 4* quality to bibliometric measures of quality for the 56,639 articles submitted to the RAE 2008 and the 50,044 articles submitted to the REF 2014 to Panel A, which assesses the life sciences, including medicine. Findings: UK research submitted to the RAE and REF was of better quality than worldwide research on average. While we found evidence for some increase in the quality of top UK research articles, a 10-25% increase in the top 10%ile papers, depending upon the metrics used, we could not find evidence to support a 103% increase in quality. Instead we found that as compared to the RAE, the REF results implied a lower citation %ile threshold for declaring a 4*. Interpretation: There is a wide discrepancy between bibliometric indices and peer-review panel judgements between the RAE 2008 and REF 2014. It is possible that the changes in the funding regime between 2008 and 2014 that significantly increased the financial premium for 4* articles may have influenced research quality evaluation. For the advancement of science and health, evaluation of research quality requires consistency and validity \u2013 the discrepancy noted here calls for a closer examination of mass peer-review methods like the REF. "], "author_display": ["Steven Wooding", "Thed N. Van Leeuwen", "Sarah Parks", "Shitij Kapur", "Jonathan Grant"], "article_type": "Research Article", "score": 0.40424562, "title_display": "UK Doubles Its \u201cWorld-Leading\u201d Research in Life Sciences and Medicine in Six Years: Testing the Claim?", "publication_date": "2015-07-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0132990"}, {"journal": "PLOS ONE", "abstract": ["Introduction: The aims of this study were to describe the key features of acute NHS Trusts with different levels of research activity and to investigate associations between research activity and clinical outcomes. Methods: National Institute for Health Research (NIHR) Comprehensive Clinical Research Network (CCRN) funding and number of patients recruited to NIHR Clinical Research Network (CRN) portfolio studies for each NHS Trusts were used as markers of research activity. Patient-level data for adult non-elective admissions were extracted from the English Hospital Episode Statistics (2005-10). Risk-adjusted mortality associations between Trust structures, research activity and, clinical outcomes were investigated. Results: Low mortality Trusts received greater levels of funding and recruited more patients adjusted for size of Trust (n = 35, 2,349 \u00a3/bed [95% CI 1,855\u20132,843], 5.9 patients/bed [2.7\u20139.0]) than Trusts with expected (n = 63, 1,110 \u00a3/bed, [864\u20131,357] p<0.0001, 2.6 patients/bed [1.7\u20133.5] p<0.0169) or, high (n = 42, 930 \u00a3/bed [683\u20131,177] p = 0.0001, 1.8 patients/bed [1.4\u20132.1] p<0.0005) mortality rates. The most research active Trusts were those with more doctors, nurses, critical care beds, operating theatres and, made greater use of radiology. Multifactorial analysis demonstrated better survival in the top funding and patient recruitment tertiles (lowest vs. highest (odds ratio & 95% CI: funding 1.050 [1.033\u20131.068] p<0.0001, recruitment 1.069 [1.052\u20131.086] p<0.0001), middle vs. highest (funding 1.040 [1.024\u20131.055] p<0.0001, recruitment 1.085 [1.070\u20131.100] p<0.0001). Conclusions: Research active Trusts appear to have key differences in composition than less research active Trusts. Research active Trusts had lower risk-adjusted mortality for acute admissions, which persisted after adjustment for staffing and other structural factors. "], "author_display": ["Baris A. Ozdemir", "Alan Karthikesalingam", "Sidhartha Sinha", "Jan D. Poloniecki", "Robert J. Hinchliffe", "Matt M. Thompson", "Jonathan D. Gower", "Annette Boaz", "Peter J. E. Holt"], "article_type": "Research Article", "score": 0.4040431, "title_display": "Research Activity and the Association with Mortality", "publication_date": "2015-02-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0118253"}, {"abstract": ["\n        It is widely believed that both common and rare variants contribute to the risks of common diseases or complex traits and the cumulative effects of multiple rare variants can explain a significant proportion of trait variances. Advances in high-throughput DNA sequencing technologies allow us to genotype rare causal variants and investigate the effects of such rare variants on complex traits. We developed an adaptive ridge regression method to analyze the collective effects of multiple variants in the same gene or the same functional unit. Our model focuses on continuous trait and incorporates covariate factors to remove potential confounding effects. The proposed method estimates and tests multiple rare variants collectively but does not depend on the assumption of same direction of each rare variant effect. Compared with the Bayesian hierarchical generalized linear model approach, the state-of-the-art method of rare variant detection, the proposed new method is easy to implement, yet it has higher statistical power. Application of the new method is demonstrated using the well-known data from the Dallas Heart Study.\n      "], "author_display": ["Haimao Zhan", "Shizhong Xu"], "article_type": "Research Article", "score": 0.40369928, "title_display": "Adaptive Ridge Regression for Rare Variant Detection", "publication_date": "2012-08-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0044173"}, {"journal": "PLOS ONE", "abstract": ["Background: Heart Healthy Lenoir is a transdisciplinary project aimed at creating long-term, sustainable approaches to reduce cardiovascular disease risk disparities in Lenoir County, North Carolina using a design spanning genomic analysis and clinical intervention. We hypothesized that residents of Lenoir County would be unfamiliar and mistrustful of genomic research, and therefore reluctant to participate; additionally, these feelings would be higher in African-Americans. Methodology: To test our hypothesis, we conducted qualitative research using community-based participatory research principles to ensure our genomic research strategies addressed the needs, priorities, and concerns of the community. African-American (n = 19) and White (n = 16) adults in Lenoir County participated in four focus groups exploring perceptions about genomics and cardiovascular disease. Demographic surveys were administered and a semi-structured interview guide was used to facilitate discussions. The discussions were digitally recorded, transcribed verbatim, and analyzed in ATLAS.ti. Results and Significance: From our analysis, key themes emerged: transparent communication, privacy, participation incentives and barriers, knowledge, and the impact of knowing. African-Americans were more concerned about privacy and community impact compared to Whites, however, African-Americans were still eager to participate in our genomic research project. The results from our formative study were used to improve the informed consent and recruitment processes by: 1) reducing misconceptions of genomic studies; and 2) helping to foster participant understanding and trust with the researchers. Our study demonstrates how community-based participatory research principles can be used to gain deeper insight into the community and increase participation in genomic research studies. Due in part to these efforts 80.3% of eligible African-American participants and 86.9% of eligible White participants enrolled in the Heart Healthy Lenoir Genomics study making our overall enrollment 57.8% African-American. Future research will investigate return of genomic results in the Lenoir community. "], "author_display": ["Harlyn G. Skinner", "Larissa Calancie", "Maihan B. Vu", "Beverly Garcia", "Molly DeMarco", "Cam Patterson", "Alice Ammerman", "Jonathan C. Schisler"], "article_type": "Research Article", "score": 0.40363258, "title_display": "Using Community-Based Participatory Research Principles to Develop More Understandable Recruitment and Informed Consent Documents in Genomic Research", "publication_date": "2015-05-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0125466"}, {"journal": "PLOS ONE", "abstract": ["\nChoosing an appropriate kernel is very important and critical when classifying a new problem with Support Vector Machine. So far, more attention has been paid on constructing new kernels and choosing suitable parameter values for a specific kernel function, but less on kernel selection. Furthermore, most of current kernel selection methods focus on seeking a best kernel with the highest classification accuracy via cross-validation, they are time consuming and ignore the differences among the number of support vectors and the CPU time of SVM with different kernels. Considering the tradeoff between classification success ratio and CPU time, there may be multiple kernel functions performing equally well on the same classification problem. Aiming to automatically select those appropriate kernel functions for a given data set, we propose a multi-label learning based kernel recommendation method built on the data characteristics. For each data set, the meta-knowledge data base is first created by extracting the feature vector of data characteristics and identifying the corresponding applicable kernel set. Then the kernel recommendation model is constructed on the generated meta-knowledge data base with the multi-label classification method. Finally, the appropriate kernel functions are recommended to a new data set by the recommendation model according to the characteristics of the new data set. Extensive experiments over 132 UCI benchmark data sets, with five different types of data set characteristics, eleven typical kernels (Linear, Polynomial, Radial Basis Function, Sigmoidal function, Laplace, Multiquadric, Rational Quadratic, Spherical, Spline, Wave and Circular), and five multi-label classification methods demonstrate that, compared with the existing kernel selection methods and the most widely used RBF kernel function, SVM with the kernel function recommended by our proposed method achieved the highest classification performance.\n"], "author_display": ["Xueying Zhang", "Qinbao Song"], "article_type": "Research Article", "score": 0.4033609, "title_display": "A Multi-Label Learning Based Kernel Automatic Recommendation Method for Support Vector Machine", "publication_date": "2015-04-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0120455"}, {"journal": "PLoS ONE", "abstract": ["\n        Recurrent copy number alterations (CNAs) play an important role in cancer genesis. While a number of computational methods have been proposed for identifying such CNAs, their relative merits remain largely unknown in practice since very few efforts have been focused on comparative analysis of the methods. To facilitate studies of recurrent CNA identification in cancer genome, it is imperative to conduct a comprehensive comparison of performance and limitations among existing methods. In this paper, six representative methods proposed in the latest six years are compared. These include one-stage and two-stage approaches, working with raw intensity ratio data and discretized data respectively. They are based on various techniques such as kernel regression, correlation matrix diagonal segmentation, semi-parametric permutation and cyclic permutation schemes. We explore multiple criteria including type I error rate, detection power, Receiver Operating Characteristics (ROC) curve and the area under curve (AUC), and computational complexity, to evaluate performance of the methods under multiple simulation scenarios. We also characterize their abilities on applications to two real datasets obtained from cancers with lung adenocarcinoma and glioblastoma. This comparison study reveals general characteristics of the existing methods for identifying recurrent CNAs, and further provides new insights into their strengths and weaknesses. It is believed helpful to accelerate the development of novel and improved methods.\n      "], "author_display": ["Xiguo Yuan", "Junying Zhang", "Shengli Zhang", "Guoqiang Yu", "Yue Wang"], "article_type": "Research Article", "score": 0.4032452, "title_display": "Comparative Analysis of Methods for Identifying Recurrent Copy Number Alterations in Cancer", "publication_date": "2012-12-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0052516"}, {"journal": "PLOS ONE", "abstract": ["\nMultiple rotating annular reactors were seeded with biofilms flushed from water distribution systems to assess (1) whether biofilms grown in bioreactors are representative of biofilms flushed from the water distribution system in terms of bacterial composition and diversity, and (2) whether the biofilm sampling method affects the population profile of the attached bacterial community. Biofilms were grown in bioreactors until thickness stabilized (9 to 11 weeks) and harvested from reactor coupons by sonication, stomaching, bead-beating, and manual scraping. High-throughput sequencing of 16S rRNA amplicons was used to profile bacterial populations from flushed biofilms seeded into bioreactors as well as biofilms recovered from bioreactor coupons by different methods. \u03b2 diversity between flushed and reactor biofilms was compared to \u03b2 diversity between (i) biofilms harvested from different reactors and (ii) biofilms harvested by different methods from the same reactor. These analyses showed that average diversity between flushed and bioreactor biofilms was double the diversity between biofilms from different reactors operated in parallel. The diversity between bioreactors was larger than the diversity associated with different biofilm recovery methods. Compared to other experimental variables, the method used to recover biofilms had a negligible impact on the outcome of water biofilm analyses based on 16S amplicon sequencing. Results from this study show that biofilms grown in reactors over 9 to 11 weeks are not representative models of the microbial populations flushed from a distribution system. Furthermore, the bacterial population profile of biofilms grown in replicate reactors from the same flushed water are likely to diverge. However, four common sampling protocols, which differ with respect to disruption of bacterial cells, provide similar information with respect to the 16S rRNA population profile of the biofilm community.\n"], "author_display": ["Xia Luo", "Kristen L. Jellison", "Kevin Huynh", "Giovanni Widmer"], "article_type": "Research Article", "score": 0.40324232, "title_display": "Impact of Bioreactor Environment and Recovery Method on the Profile of Bacterial Populations from Water Distribution Systems", "publication_date": "2015-07-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0133427"}, {"journal": "PLOS ONE", "abstract": ["\nIn constructing and visualizing a virtual three-dimensional forest scene, we must first obtain the vegetation distribution, namely, the location of each plant in the forest. Because the forest contains a large number of plants, the distribution of each plant is difficult to obtain from actual measurement methods. Random approaches are used as common solutions to simulate a forest distribution but fail to reflect the specific biological arrangements among types of plants. Observations show that plants in the forest tend to generate particular distribution patterns due to growth competition and specific habitats. This pattern, which represents a local feature in the distribution and occurs repeatedly in the forest, is in line with the \u201clocality\u201d and \u201cstatic\u201d characteristics in the \u201ctexture data\u201d, making it possible to use a sample-based texture synthesis strategy to build the distribution. We propose a vegetation distribution data generation method that uses sample-based vector pattern synthesis. A sample forest stand is obtained first and recorded as a two-dimensional vector-element distribution pattern. Next, the large-scale vegetation distribution pattern is synthesized automatically using the proposed vector pattern synthesis algorithm. The synthesized distribution pattern resembles the sample pattern in the distribution features. The vector pattern synthesis algorithm proposed in this paper adopts a neighborhood comparison technique based on histogram matching, which makes it efficient and easy to implement. Experiments show that the distribution pattern synthesized with this method can sufficiently preserve the features of the sample distribution pattern, making our method meaningful for constructing realistic forest scenes.\n"], "author_display": ["Chanchan Xu", "Gang Yang", "Meng Yang"], "article_type": "Research Article", "score": 0.40312254, "title_display": "Sample-Based Vegetation Distribution Information Synthesis", "publication_date": "2015-08-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0134009"}, {"journal": "PLoS ONE", "abstract": ["\nReverse engineering approaches to constructing gene regulatory networks (GRNs) based on genome-wide mRNA expression data have led to significant biological findings, such as the discovery of novel drug targets. However, the reliability of the reconstructed GRNs needs to be improved. Here, we propose an ensemble-based network aggregation approach to improving the accuracy of network topologies constructed from mRNA expression data. To evaluate the performances of different approaches, we created dozens of simulated networks from combinations of gene-set sizes and sample sizes and also tested our methods on three Escherichia coli datasets. We demonstrate that the ensemble-based network aggregation approach can be used to effectively integrate GRNs constructed from different studies \u2013 producing more accurate networks. We also apply this approach to building a network from epithelial mesenchymal transition (EMT) signature microarray data and identify hub genes that might be potential drug targets. The R code used to perform all of the analyses is available in an R package entitled \u201cENA\u201d, accessible on CRAN (http://cran.r-project.org/web/packages/ENA/).\n"], "author_display": ["Rui Zhong", "Jeffrey D. Allen", "Guanghua Xiao", "Yang Xie"], "article_type": "Research Article", "score": 0.4028714, "title_display": "Ensemble-Based Network Aggregation Improves the Accuracy of Gene Network Reconstruction", "publication_date": "2014-11-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0106319"}, {"journal": "PLoS ONE", "abstract": ["\nThe interaction between segregation distortion loci (SDL) has been often observed in all kinds of mapping populations. However, little has been known about the effect of epistatic SDL on quantitative trait locus (QTL) mapping. Here we proposed a multi-QTL mapping approach using epistatic distorted markers. Using the corrected linkage groups, epistatic SDL was identified. Then, these SDL parameters were used to correct the conditional probabilities of QTL genotypes, and these corrections were further incorporated into the new QTL mapping approach. Finally, a set of simulated datasets and a real data in 304 mouse F2 individuals were used to validate the new method. As compared with the old method, the new one corrects genetic distance between distorted markers, and considers epistasis between two linked SDL. As a result, the power in the detection of QTL is higher for the new method than for the old one, and significant differences for estimates of QTL parameters between the two methods were observed, except for QTL position. Among two QTL for mouse weight, one significant difference for QTL additive effect between the above two methods was observed, because epistatic SDL between markers C66 and T93 exists (P\u200a=\u200a2.94e-4).\n"], "author_display": ["Shang-Qian Xie", "Jia Wen", "Yuan-Ming Zhang"], "article_type": "Research Article", "score": 0.40286812, "title_display": "Multi-QTL Mapping for Quantitative Traits Using Epistatic Distorted Markers", "publication_date": "2013-07-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0068510"}, {"journal": "PLOS ONE", "abstract": ["\nStudies using the placental transcriptome to identify key molecules relevant for preeclampsia are hampered by a relatively small sample size. In addition, they use a variety of bioinformatics and statistical methods, making comparison of findings challenging. To generate a more robust preeclampsia gene expression signature, we performed a meta-analysis on the original data of 11 placenta RNA microarray experiments, representing 139 normotensive and 116 preeclamptic pregnancies. Microarray data were pre-processed and analyzed using standardized bioinformatics and statistical procedures and the effect sizes were combined using an inverse-variance random-effects model. Interactions between genes in the resulting gene expression signature were identified by pathway analysis (Ingenuity Pathway Analysis, Gene Set Enrichment Analysis, Graphite) and protein-protein associations (STRING). This approach has resulted in a comprehensive list of differentially expressed genes that led to a 388-gene meta-signature of preeclamptic placenta. Pathway analysis highlights the involvement of the previously identified hypoxia/HIF1A pathway in the establishment of the preeclamptic gene expression profile, while analysis of protein interaction networks indicates CREBBP/EP300 as a novel element central to the preeclamptic placental transcriptome. In addition, there is an apparent high incidence of preeclampsia in women carrying a child with a mutation in CREBBP/EP300 (Rubinstein-Taybi Syndrome). The 388-gene preeclampsia meta-signature offers a vital starting point for further studies into the relevance of these genes (in particular CREBBP/EP300) and their concomitant pathways as biomarkers or functional molecules in preeclampsia. This will result in a better understanding of the molecular basis of this disease and opens up the opportunity to develop rational therapies targeting the placental dysfunction causal to preeclampsia.\n"], "author_display": ["Miranda van Uitert", "Perry D. Moerland", "Daniel A. Enquobahrie", "Hannele Laivuori", "Joris A. M. van der Post", "Carrie Ris-Stalpers", "Gijs B. Afink"], "article_type": "Research Article", "score": 0.40285623, "title_display": "Meta-Analysis of Placental Transcriptome Data Identifies a Novel Molecular Pathway Related to Preeclampsia", "publication_date": "2015-07-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0132468"}, {"journal": "PLoS ONE", "abstract": ["\n        The prediction of the network of protein-protein interactions (PPI) of an organism is crucial for the understanding of biological processes and for the development of new drugs. Machine learning methods have been successfully applied to the prediction of PPI in yeast by the integration of multiple direct and indirect biological data sources. However, experimental data are not available for most organisms. We propose here an ensemble machine learning approach for the prediction of PPI that depends solely on features independent from experimental data. We developed new estimators of the coevolution between proteins and combined them in an ensemble learning procedure.\n        We applied this method to a dataset of known co-complexed proteins in Escherichia coli and compared it to previously published methods. We show that our method allows prediction of PPI with an unprecedented precision of 95.5% for the first 200 sorted pairs of proteins compared to 28.5% on the same dataset with the previous best method.\n        A close inspection of the best predicted pairs allowed us to detect new or recently discovered interactions between chemotactic components, the flagellar apparatus and RNA polymerase complexes in E. coli.\n      "], "author_display": ["Damien M. de Vienne", "J\u00e9r\u00f4me Az\u00e9"], "article_type": "Research Article", "score": 0.4027778, "title_display": "Efficient Prediction of Co-Complexed Proteins Based on Coevolution", "publication_date": "2012-11-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0048728"}, {"journal": "PLOS ONE", "abstract": ["\nThe modern science has become more complex and interdisciplinary in its nature which might encourage researchers to be more collaborative and get engaged in larger collaboration networks. Various aspects of collaboration networks have been examined so far to detect the most determinant factors in knowledge creation and scientific production. One of the network structures that recently attracted much theoretical attention is called small world. It has been suggested that small world can improve the information transmission among the network actors. In this paper, using the data on 12 periods of journal publications of Canadian researchers in natural sciences and engineering, the co-authorship networks of the researchers are created. Through measuring small world indicators, the small worldiness of the mentioned network and its relation with researchers\u2019 productivity, quality of their publications, and scientific team size are assessed. Our results show that the examined co-authorship network strictly exhibits the small world properties. In addition, it is suggested that in a small world network researchers expand their team size through getting connected to other experts of the field. This team size expansion may result in higher productivity of the whole team as a result of getting access to new resources, benefitting from the internal referring, and exchanging ideas among the team members. Moreover, although small world network is positively correlated with the quality of the articles in terms of both citation count and journal impact factor, it is negatively related with the average productivity of researchers in terms of the number of their publications.\n"], "author_display": ["Ashkan Ebadi", "Andrea Schiffauerova"], "article_type": "Research Article", "score": 0.40276182, "title_display": "On the Relation between the Small World Structure and Scientific Activities", "publication_date": "2015-03-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0121129"}, {"journal": "PLoS ONE", "abstract": ["\n        Malaria is a global health problem responsible for nearly one million deaths every year around 85% of which concern children younger than five years old in Sub-Saharan Africa. In addition, around  million clinical cases are declared every year. The level of infection, expressed as parasite density, is classically defined as the number of asexual parasites relative to a microliter of blood. Microscopy of Giemsa-stained thick blood films is the gold standard for parasite enumeration. Parasite density estimation methods usually involve threshold values; either the number of white blood cells counted or the number of high power fields read. However, the statistical properties of parasite density estimators generated by these methods have largely been overlooked.\n         Here, we studied the statistical properties (mean error, coefficient of variation, false negative rates) of parasite density estimators of commonly used threshold-based counting techniques depending on variable threshold values. We also assessed the influence of the thresholds on the cost-effectiveness of parasite density estimation methods. In addition, we gave more insights on the behavior of measurement errors according to varying threshold values, and on what should be the optimal threshold values that minimize this variability.\n      "], "author_display": ["Imen Hammami", "Gr\u00e9gory Nuel", "Andr\u00e9 Garcia"], "article_type": "Research Article", "score": 0.40272704, "title_display": "Statistical Properties of Parasite Density Estimators in Malaria", "publication_date": "2013-03-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0051987"}, {"journal": "PLoS Computational Biology", "abstract": ["\nFoodborne disease outbreaks of recent years demonstrate that due to increasingly interconnected supply chains these type of crisis situations have the potential to affect thousands of people, leading to significant healthcare costs, loss of revenue for food companies, and\u2014in the worst cases\u2014death. When a disease outbreak is detected, identifying the contaminated food quickly is vital to minimize suffering and limit economic losses. Here we present a likelihood-based approach that has the potential to accelerate the time needed to identify possibly contaminated food products, which is based on exploitation of food products sales data and the distribution of foodborne illness case reports. Using a real world food sales data set and artificially generated outbreak scenarios, we show that this method performs very well for contamination scenarios originating from a single \u201cguilty\u201d food product. As it is neither always possible nor necessary to identify the single offending product, the method has been extended such that it can be used as a binary classifier. With this extension it is possible to generate a set of potentially \u201cguilty\u201d products that contains the real outbreak source with very high accuracy. Furthermore we explore the patterns of food distributions that lead to \u201chard-to-identify\u201d foods, the possibility of identifying these food groups a priori, and the extent to which the likelihood-based method can be used to quantify uncertainty. We find that high spatial correlation of sales data between products may be a useful indicator for \u201chard-to-identify\u201d products.\nAuthor Summary: Response to foodborne disease outbreaks is complicated by globalization of our food supply chains. Rapid identification of contaminated products is essential to limit the damage caused by foodborne disease. Worldwide, foodborne disease outbreaks are responsible for $9B a year in medical costs and over $75B in economic losses. Yet relevant data required to accelerate the identification of suspicious food already exists as part of the inventory control systems used by retailers and distributors today. Combining this retail data with public health case reports has the potential to hasten outbreak investigations and provide public health investigators with better information on suspected products to test. This paper demonstrates the feasibility of the principle and efficiency of this approach. Based on these findings it can be concluded that in foodborne disease outbreaks retail data could be used to speed and target public health investigations and consequently reduce numbers of sick/dead people as well as reduce economic losses to the industry. "], "author_display": ["James Kaufman", "Justin Lessler", "April Harry", "Stefan Edlund", "Kun Hu", "Judith Douglas", "Christian Thoens", "Bernd Appel", "Annemarie K\u00e4sbohrer", "Matthias Filter"], "article_type": "Research Article", "score": 0.4024665, "title_display": "A Likelihood-Based Approach to Identifying Contaminated Food Products Using Sales Data: Performance and Challenges", "publication_date": "2014-07-03T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1003692"}, {"journal": "PLoS ONE", "abstract": ["\nConstruction of confidence intervals or regions is an important part of statistical inference. The usual approach to constructing a confidence interval for a single parameter or confidence region for two or more parameters requires that the distribution of estimated parameters is known or can be assumed. In reality, the sampling distributions of parameters of biological importance are often unknown or difficult to be characterized. Distribution-free nonparametric resampling methods such as bootstrapping and permutation have been widely used to construct the confidence interval for a single parameter. There are also several parametric (ellipse) and nonparametric (convex hull peeling, bagplot and HPDregionplot) methods available for constructing confidence regions for two or more parameters. However, these methods have some key deficiencies including biased estimation of the true coverage rate, failure to account for the shape of the distribution inherent in the data and difficulty to implement. The purpose of this paper is to develop a new distribution-free method for constructing the confidence region that is based only on a few basic geometrical principles and accounts for the actual shape of the distribution inherent in the real data. The new method is implemented in an R package, distfree.cr/R. The statistical properties of the new method are evaluated and compared with those of the other methods through Monte Carlo simulation. Our new method outperforms the other methods regardless of whether the samples are taken from normal or non-normal bivariate distributions. In addition, the superiority of our method is consistent across different sample sizes and different levels of correlation between the two variables. We also analyze three biological data sets to illustrate the use of our new method for genomics and other biological researches.\n"], "author_display": ["Zhiqiu Hu", "Rong-Cai Yang"], "article_type": "Research Article", "score": 0.40240553, "title_display": "A New Distribution-Free Approach to Constructing the Confidence Region for Multiple Parameters", "publication_date": "2013-12-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0081179"}, {"journal": "PLoS ONE", "abstract": ["Background: Audio Computer-Assisted Self Interviewing (ACASI) has improved the reliability and accuracy of self-reported HIV health and risk behavior data, yet few studies account for how participants experience the data collection process. Methodology/Principal Findings: This exploratory qualitative analysis aimed to better understand the experience and implications of using ACASI among HIV-positive women participating in sexual risk reduction interventions in Chicago (n\u200a=\u200a12) and Philadelphia (n\u200a=\u200a18). Strategies of Grounded Theory were used to explore participants' ACASI experiences. Conclusion/Significance: Key themes we identified included themes that could be attributed to the ACASI and other methods of data collection (e.g., paper-based self-administered questionnaire or face-to-face interviews). The key themes were usability; privacy and honesty; socially desirable responses and avoiding judgment; and unintentional discomfort resulting from recalling risky behavior using the ACASI. Despite both positive and negative findings about the ACASI experience, we conclude that ACASI is in general an appropriate method for collecting sensitive data about HIV/AIDS risk behaviors among HIV-positive women because it seemed to ensure privacy in the study population allowing for more honest responses, minimize socially desirable responses, and help participants avoid actual or perceived judgment. "], "author_display": ["Larissa J. Estes", "Linda E. Lloyd", "Michelle Teti", "Sheela Raja", "Lisa Bowleg", "Kristi L. Allgood", "Nancy Glick"], "article_type": "Research Article", "score": 0.40238488, "title_display": "Perceptions of Audio Computer-Assisted Self-Interviewing (ACASI) among Women in an HIV-Positive Prevention Program", "publication_date": "2010-02-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0009149"}, {"journal": "PLoS ONE", "abstract": ["\nIn Australia and increasingly worldwide, methamphetamine is one of the most commonly seized drugs analysed by forensic chemists. The current well-established GC/MS methods used to identify and quantify methamphetamine are lengthy, expensive processes, but often rapid analysis is requested by undercover police leading to an interest in developing this new analytical technique. Ninety six illicit drug seizures containing methamphetamine (0.1%\u201378.6%) were analysed using Fourier Transform Infrared Spectroscopy with an Attenuated Total Reflectance attachment and Chemometrics. Two Partial Least Squares models were developed, one using the principal Infrared Spectroscopy peaks of methamphetamine and the other a Hierarchical Partial Least Squares model. Both of these models were refined to choose the variables that were most closely associated with the methamphetamine % vector. Both of the models were excellent, with the principal peaks in the Partial Least Squares model having Root Mean Square Error of Prediction 3.8, R2 0.9779 and lower limit of quantification 7% methamphetamine. The Hierarchical Partial Least Squares model had lower limit of quantification 0.3% methamphetamine, Root Mean Square Error of Prediction 5.2 and R2 0.9637. Such models offer rapid and effective methods for screening illicit drug samples to determine the percentage of methamphetamine they contain.\n"], "author_display": ["Juanita Hughes", "Godwin Ayoko", "Simon Collett", "Gary Golding"], "article_type": "Research Article", "score": 0.40234274, "title_display": "Rapid Quantification of Methamphetamine: Using Attenuated Total Reflectance Fourier Transform Infrared Spectroscopy (ATR-FTIR) and Chemometrics", "publication_date": "2013-07-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0069609"}, {"journal": "PLoS ONE", "abstract": ["\n        With an adaptive partition procedure, we can partition a \u201ctime\n                    course\u201d into consecutive non-overlapped intervals such that the population\n                    means/proportions of the observations in two adjacent intervals are\n                    significantly different at a given level . However, the\n                    widely used recursive combination or partition procedures do not guarantee a\n                    global optimization. We propose a modified dynamic programming algorithm to\n                    achieve a global optimization. Our method can provide consistent estimation\n                    results. In a comprehensive simulation study, our method shows an improved\n                    performance when it is compared to the recursive combination/partition\n                    procedures. In practice,  can be determined\n                    based on a cross-validation procedure. As an application, we consider the\n                    well-known Pima Indian Diabetes data. We explore the relationship among the\n                    diabetes risk and several important variables including the plasma glucose\n                    concentration, body mass index and age.\n      "], "author_display": ["Yinglei Lai"], "article_type": "Research Article", "score": 0.40233642, "title_display": "On the Adaptive Partition Approach to the Detection of Multiple\n                    Change-Points", "publication_date": "2011-05-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0019754"}, {"journal": "PLoS ONE", "abstract": ["\n1. Because of recent technological improvements in the way computer and digital camera perform, the potential use of imaging for contributing to the study of communities, populations or individuals in laboratory microcosms has risen enormously. However its limited use is due to difficulties in the automation of image analysis. 2. We present an accurate and flexible method of image analysis for detecting, counting and measuring moving particles on a fixed but heterogeneous substrate. This method has been specifically designed to follow individuals, or entire populations, in experimental laboratory microcosms. It can be used in other applications. 3. The method consists in comparing multiple pictures of the same experimental microcosm in order to generate an image of the fixed background. This background is then used to extract, measure and count the moving organisms, leaving out the fixed background and the motionless or dead individuals. 4. We provide different examples (springtails, ants, nematodes, daphnia) to show that this non intrusive method is efficient at detecting organisms under a wide variety of conditions even on faintly contrasted and heterogeneous substrates. 5. The repeatability and reliability of this method has been assessed using experimental populations of the Collembola Folsomia candida. 6. We present an ImageJ plugin to automate the analysis of digital pictures of laboratory microcosms. The plugin automates the successive steps of the analysis and recursively analyses multiple sets of images, rapidly producing measurements from a large number of replicated microcosms.\n"], "author_display": ["Fran\u00e7ois Mallard", "Vincent Le Bourlot", "Thomas Tully"], "article_type": "Research Article", "score": 0.4023081, "title_display": "An Automated Image Analysis System to Measure and Count Organisms in Laboratory Microcosms", "publication_date": "2013-05-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0064387"}, {"journal": "PLoS ONE", "abstract": ["Objectives: To explore scientists' perspectives on the challenges and pressures of translating research findings into clinical practice and public health policy. Methods: We conducted semi-structured interviews with a purposive sample of 20 leading scientists engaged in genetic research on addiction. We asked participants for their views on how their own research translates, how genetic research addresses addiction as a public health problem and how it may affect the public's view of addiction. Results: Most scientists described a direct translational route for their research, positing that their research will have significant societal benefits, leading to advances in treatment and novel prevention strategies. However, scientists also pointed to the inherent pressures they feel to quickly translate their research findings into actual clinical or public health use. They stressed the importance of allowing the scientific process to play out, voicing ambivalence about the recent push to speed translation. Conclusions: High expectations have been raised that biomedical science will lead to new prevention and treatment modalities, exerting pressure on scientists. Our data suggest that scientists feel caught in the push for immediate applications. This overemphasis on rapid translation can lead to technologies and applications being rushed into use without critical evaluation of ethical, policy, and social implications, and without balancing their value compared to public health policies and interventions currently in place. "], "author_display": ["Jenny E. Ostergren", "Rachel R. Hammer", "Molly J. Dingel", "Barbara A. Koenig", "Jennifer B. McCormick"], "article_type": "Research Article", "score": 0.40190113, "title_display": "Challenges in Translational Research: The Views of Addiction Scientists", "publication_date": "2014-04-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0093482"}, {"journal": "PLOS ONE", "abstract": ["\nDue to being derived from linear assumption, most elastic body based non-rigid image registration algorithms are facing challenges for soft tissues with complex nonlinear behavior and with large deformations. To take into account the geometric nonlinearity of soft tissues, we propose a registration algorithm on the basis of Newtonian differential equation. The material behavior of soft tissues is modeled as St. Venant-Kirchhoff elasticity, and the nonlinearity of the continuum represents the quadratic term of the deformation gradient under the Green- St.Venant strain. In our algorithm, the elastic force is formulated as the derivative of the deformation energy with respect to the nodal displacement vectors of the finite element; the external force is determined by the registration similarity gradient flow which drives the floating image deforming to the equilibrium condition. We compared our approach to three other models: 1) the conventional linear elastic finite element model (FEM); 2) the dynamic elastic FEM; 3) the robust block matching (RBM) method. The registration accuracy was measured using three similarities: MSD (Mean Square Difference), NC (Normalized Correlation) and NMI (Normalized Mutual Information), and was also measured using the mean and max distance between the ground seeds and corresponding ones after registration. We validated our method on 60 image pairs including 30 medical image pairs with artificial deformation and 30 clinical image pairs for both the chest chemotherapy treatment in different periods and brain MRI normalization. Our method achieved a distance error of 0.320\u00b10.138 mm in x direction and 0.326\u00b10.111 mm in y direction, MSD of 41.96\u00b113.74, NC of 0.9958\u00b10.0019, NMI of 1.2962\u00b10.0114 for images with large artificial deformations; and average NC of 0.9622\u00b10.008 and NMI of 1.2764\u00b10.0089 for the real clinical cases. Student\u2019s t-test demonstrated that our model statistically outperformed the other methods in comparison (p-values <0.05).\n"], "author_display": ["Jingya Zhang", "Jiajun Wang", "Xiuying Wang", "Xin Gao", "Dagan Feng"], "article_type": "Research Article", "score": 0.4017985, "title_display": "Physical Constraint Finite Element Model for Medical Image Registration", "publication_date": "2015-10-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0140567"}, {"journal": "PLoS ONE", "abstract": ["Introduction: Tackling health inequities both within and between countries remains high on the agenda of international organizations including the World Health Organization and local, regional and national governments. Systematic reviews can be a useful tool to assess effects on equity in health status because they include studies conducted in a variety of settings and populations. This study aims to describe the extent to which the impacts of health interventions on equity in health status are considered in systematic reviews, describe methods used, and assess the implications of their equity related findings for policy, practice and research. Methods: We conducted a methodology study of equity assessment in systematic reviews. Two independent reviewers extracted information on the reporting and analysis of impacts of health interventions on equity in health status in a group of 300 systematic reviews collected from all systematic reviews indexed in one month of MEDLINE, using a pre-tested data collection form. Any differences in data extraction were resolved by discussion. Results: Of the 300 systematic reviews, 224 assessed the effectiveness of interventions on health outcomes. Of these 224 reviews, 29 systematic reviews assessed effects on equity in health status using subgroup analysis or targeted analyses of vulnerable populations. Of these, seven conducted subgroup analyses related to health equity which were reported in insufficient detail to judge their credibility. Of these 29 reviews, 18 described implications for policy and practice based on assessment of effects on health equity. Conclusion: The quality and completeness of reporting should be enhanced as a priority, because without this policymakers and practitioners will continue lack the evidence base they need to inform decision-making about health inequity. Furthermore, there is a need to develop methods to systematically consider impacts on equity in health status that is currently lacking in systematic reviews. "], "author_display": ["Vivian Welch", "Mark Petticrew", "Erin Ueffing", "Maria Benkhalti Jandu", "Kevin Brand", "Bharbhoor Dhaliwal", "Elizabeth Kristjansson", "Janet Smylie", "George Anthony Wells", "Peter Tugwell"], "article_type": "Research Article", "score": 0.40173402, "title_display": "Does Consideration and Assessment of Effects on Health Equity Affect the Conclusions of Systematic Reviews? A Methodology Study", "publication_date": "2012-03-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0031360"}, {"journal": "PLoS Neglected Tropical Diseases", "abstract": ["Background: New approaches and tools were needed to support the strategic planning, implementation and management of a Program launched by the Brazilian Government to fund research, development and capacity building on neglected tropical diseases with strong focus on the North, Northeast and Center-West regions of the country where these diseases are prevalent. Methodology/Principal Findings: Based on demographic, epidemiological and burden of disease data, seven diseases were selected by the Ministry of Health as targets of the initiative. Publications on these diseases by Brazilian researchers were retrieved from international databases, analyzed and processed with text-mining tools in order to standardize author- and institution's names and addresses. Co-authorship networks based on these publications were assembled, visualized and analyzed with social network analysis software packages. Network visualization and analysis generated new information, allowing better design and strategic planning of the Program, enabling decision makers to characterize network components by area of work, identify institutions as well as authors playing major roles as central hubs or located at critical network cut-points and readily detect authors or institutions participating in large international scientific collaborating networks. Conclusions/Significance: Traditional criteria used to monitor and evaluate research proposals or R&D Programs, such as researchers' productivity and impact factor of scientific publications, are of limited value when addressing research areas of low productivity or involving institutions from endemic regions where human resources are limited. Network analysis was found to generate new and valuable information relevant to the strategic planning, implementation and monitoring of the Program. It afforded a more proactive role of the funding agencies in relation to public health and equity goals, to scientific capacity building objectives and a more consistent engagement of institutions and authors from endemic regions based on innovative criteria and parameters anchored on objective scientific data. Author Summary: The selection and prioritization of research proposals is always a challenge, particularly when addressing neglected tropical diseases, as the scientific communities are relatively small, funding is usually limited and the disparity between the science and technology capacity of different countries and regions is enormous. When the Ministry of Health and the Ministry of Science and Technology of Brazil decided to launch an R&D program on neglected diseases for which at least 30% of the Program's resources were supposed to be invested in institutions and authors from the poorest regions of Brazil, it became clear to us that new strategies and approaches would be required. Social network analysis of co-authorship networks is one of the new approaches we are exploring to develop new tools to help policy-/decision-makers and academia jointly plan, implement, monitor and evaluate investments in this area. Publications retrieved from international databases provide the starting material. After standardization of names and addresses of authors and institutions with text mining tools, networks are assembled and visualized using social network analysis software. This study enabled the development of innovative criteria and parameters, allowing better strategic planning, smooth implementation and strong support and endorsement of the Program by key stakeholders. "], "author_display": ["Carlos Medicis Morel", "Suzanne Jacob Serruya", "Gerson Oliveira Penna", "Reinaldo Guimar\u00e3es"], "article_type": "Research Article", "score": 0.40166348, "title_display": "Co-authorship Network Analysis: A Powerful Tool for Strategic Planning of Research, Development and Capacity Building Programs on Neglected Diseases", "publication_date": "2009-08-18T00:00:00Z", "eissn": "1935-2735", "id": "10.1371/journal.pntd.0000501"}, {"journal": "PLoS ONE", "abstract": ["\n        A recent study by Bromenshenk et al., published in PLoS One (2010), used proteomic analysis to identify peptides purportedly of Iridovirus and Nosema origin; however the validity of this finding is controversial. We show here through re-analysis of a subset of this data that many of the spectra identified by Bromenshenk et al. as deriving from Iridovirus and Nosema proteins are actually products from Apis mellifera honey bee proteins. We find no reliable evidence that proteins from Iridovirus and Nosema are present in the samples that were re-analyzed. This article is also intended as a learning exercise for illustrating some of the potential pitfalls of analysis of mass spectrometry proteomic data and to encourage authors to observe MS/MS data reporting guidelines that would facilitate recognition of analysis problems during the review process.\n      "], "author_display": ["Giselle M. Knudsen", "Robert J. Chalkley"], "article_type": "Research Article", "score": 0.40162757, "title_display": "The Effect of Using an Inappropriate Protein Database for Proteomic Data Analysis", "publication_date": "2011-06-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0020873"}, {"journal": "PLoS ONE", "abstract": ["\nThe personal genomics era has attracted a large amount of attention for anti-cancer therapy by patient-specific analysis. Patient-specific analysis enables discovery of individual genomic characteristics for each patient, and thus we can effectively predict individual genetic risk of disease and perform personalized anti-cancer therapy. Although the existing methods for patient-specific analysis have successfully uncovered crucial biomarkers, their performance takes a sudden turn for the worst in the presence of outliers, since the methods are based on non-robust manners. In practice, clinical and genomic alterations datasets usually contain outliers from various sources (e.g., experiment error, coding error, etc.) and the outliers may significantly affect the result of patient-specific analysis. We propose a robust methodology for patient-specific analysis in line with the NetwrokProfiler. In the proposed method, outliers in high dimensional gene expression levels and drug response datasets are simultaneously controlled by robust Mahalanobis distance in robust principal component space. Thus, we can effectively perform for predicting anti-cancer drug sensitivity and identifying sensitivity-specific biomarkers for individual patients. We observe through Monte Carlo simulations that the proposed robust method produces outstanding performances for predicting response variable in the presence of outliers. We also apply the proposed methodology to the Sanger dataset in order to uncover cancer biomarkers and predict anti-cancer drug sensitivity, and show the effectiveness of our method.\n"], "author_display": ["Heewon Park", "Teppei Shimamura", "Satoru Miyano", "Seiya Imoto"], "article_type": "Research Article", "score": 0.40155286, "title_display": "Robust Prediction of Anti-Cancer Drug Sensitivity and Sensitivity-Specific Biomarker", "publication_date": "2014-10-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0108990"}, {"journal": "PLoS ONE", "abstract": ["Objective: By testing the mediating effect of coping strategies on the relationship between social support (SS) and posttraumatic growth (PTG), the aim of this research was to develop a new approach for the study of post-disaster psychological intervention. Methods: A mediating effect model analysis was conducted on 2080 adult survivors selected from 19 of the counties hardest-hit by the 2008 Wenchuan earthquake. The Social Support Rating Scale and the Coping Scale were used to predict the PTG. Results: A bivariate correlation analysis showed that there was a correlation between posttraumatic growth, social support and coping strategies. The mediation analysis revealed that coping strategies played a mediating role between social support and posttraumatic growth in survivors after the earthquake. Conclusion: The results demonstrated that mental health programs for survivors need to focus on the establishment of a good social support network, which was found to be conductive to maintaining and increasing mental health levels. At the same time, adequate social support is able to assist survivors in adopting mature coping strategies, such as problem solving and asking for help. Hence, social support was found to play a vital role in balancing and protecting mental health. "], "author_display": ["Lili He", "Jiuping Xu", "Zhibin Wu"], "article_type": "Research Article", "score": 0.4015115, "title_display": "Coping Strategies as a Mediator of Posttraumatic Growth among Adult Survivors of the Wenchuan Earthquake", "publication_date": "2013-12-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0084164"}, {"journal": "PLOS ONE", "abstract": ["Background: Previous research looking at published systematic reviews has shown that their search strategies are often suboptimal and that librarian involvement, though recommended, is low. Confidence in the results, however, is limited due to poor reporting of search strategies the published articles. Objectives: To more accurately measure the use of recommended search methods in systematic reviews, the levels of librarian involvement, and whether librarian involvement predicts the use of recommended methods. Methods: A survey was sent to all authors of English-language systematic reviews indexed in the Database of Abstracts of Reviews of Effects (DARE) from January 2012 through January 2014. The survey asked about their use of search methods recommended by the Institute of Medicine, Cochrane Collaboration, and the Agency for Healthcare Research and Quality and if and how a librarian was involved in the systematic review. Rates of use of recommended methods and librarian involvement were summarized. The impact of librarian involvement on use of recommended methods was examined using a multivariate logistic regression. Results: 1560 authors completed the survey. Use of recommended search methods ranged widely from 98% for use of keywords to 9% for registration in PROSPERO and were generally higher than in previous studies. 51% of studies involved a librarian, but only 64% acknowledge their assistance. Librarian involvement was significantly associated with the use of 65% of recommended search methods after controlling for other potential predictors. Odds ratios ranged from 1.36 (95% CI 1.06 to 1.75) for including multiple languages to 3.07 (95% CI 2.06 to 4.58) for using controlled vocabulary. Conclusions: Use of recommended search strategies is higher than previously reported, but many methods are still under-utilized. Librarian involvement predicts the use of most methods, but their involvement is under-reported within the published article. "], "author_display": ["Jonathan B. Koffel"], "article_type": "Research Article", "score": 0.40148064, "title_display": "Use of Recommended Search Strategies in Systematic Reviews and the Impact of Librarian Involvement: A Cross-Sectional Survey of Recent Authors", "publication_date": "2015-05-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0125931"}, {"abstract": ["Aims: There is strong evidence that diets high in salt are bad for health and that salt reduction strategies are cost effective. However, whilst it is clear that most people are eating too much salt, obtaining an accurate assessment of population salt intake is not straightforward, particularly in resource poor settings. The objective of this study is to identify what approaches governments are taking to monitoring salt intake, with the ultimate goal of identifying what actions are needed to address challenges to monitoring salt intake, especially in low and middle-income countries. Methods and Results: A written survey was issued to governments to establish the details of their monitoring methods. Of the 30 countries that reported conducting formal government salt monitoring activities, 73% were high income countries. Less than half of the 30 countries, used the most accurate assessment of salt through 24 hour urine, and only two of these were developing countries. The remainder mainly relied on estimates through dietary surveys. Conclusions: The study identified a strong need to establish more practical ways of assessing salt intake as well as technical support and advice to ensure that low and middle income countries can implement salt monitoring activities effectively. "], "author_display": ["Corinna Hawkes", "Jacqui Webster"], "article_type": "Research Article", "score": 0.40146488, "title_display": "National Approaches to Monitoring Population Salt Intake: A Trade-Off between Accuracy and Practicality?", "publication_date": "2012-10-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0046727"}, {"journal": "PLoS ONE", "abstract": ["\n        Three advanced technologies to measure soil carbon (C) density (g C m\u22122) are deployed in the field and the results compared against those obtained by the dry combustion (DC) method. The advanced methods are: a) Laser Induced Breakdown Spectroscopy (LIBS), b) Diffuse Reflectance Fourier Transform Infrared Spectroscopy (DRIFTS), and c) Inelastic Neutron Scattering (INS). The measurements and soil samples were acquired at Beltsville, MD, USA and at Centro International para el Mejoramiento del Ma\u00edz y el Trigo (CIMMYT) at El Bat\u00e1n, Mexico. At Beltsville, soil samples were extracted at three depth intervals (0\u20135, 5\u201315, and 15\u201330 cm) and processed for analysis in the field with the LIBS and DRIFTS instruments. The INS instrument determined soil C density to a depth of 30 cm via scanning and stationary measurements. Subsequently, soil core samples were analyzed in the laboratory for soil bulk density (kg m\u22123), C concentration (g kg\u22121) by DC, and results reported as soil C density (kg m\u22122). Results from each technique were derived independently and contributed to a blind test against results from the reference (DC) method. A similar procedure was employed at CIMMYT in Mexico employing but only with the LIBS and DRIFTS instruments. Following conversion to common units, we found that the LIBS, DRIFTS, and INS results can be compared directly with those obtained by the DC method. The first two methods and the standard DC require soil sampling and need soil bulk density information to convert soil C concentrations to soil C densities while the INS method does not require soil sampling. We conclude that, in comparison with the DC method, the three instruments (a) showed acceptable performances although further work is needed to improve calibration techniques and (b) demonstrated their portability and their capacity to perform under field conditions.\n      "], "author_display": ["Roberto C. Izaurralde", "Charles W. Rice", "Lucian Wielopolski", "Michael H. Ebinger", "James B. Reeves", "Allison M. Thomson", "Ronny Harris", "Barry Francis", "Sudeep Mitra", "Aaron G. Rappaport", "Jorge D. Etchevers", "Kenneth D. Sayre", "Bram Govaerts", "Gregory W. McCarty"], "article_type": "Research Article", "score": 0.40136135, "title_display": "Evaluation of Three Field-Based Methods for Quantifying Soil Carbon", "publication_date": "2013-01-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0055560"}, {"journal": "PLoS Medicine", "abstract": ["Background: Alcohol consumption causes an estimated 4% of the global disease burden, prompting goverments to impose regulations to mitigate the adverse effects of alcohol. To assist public health leaders and policymakers, the authors developed a composite indicator\u2014the Alcohol Policy Index\u2014to gauge the strength of a country's alcohol control policies. Methods and Findings: The Index generates a score based on policies from five regulatory domains\u2014physical availability of alcohol, drinking context, alcohol prices, alcohol advertising, and operation of motor vehicles. The Index was applied to the 30 countries that compose the Organization for Economic Cooperation and Development and regression analysis was used to examine the relationship between policy score and per capita alcohol consumption. Countries attained a median score of 42.4 of a possible 100 points, ranging from 14.5 (Luxembourg) to 67.3 (Norway). The analysis revealed a strong negative correlation between score and consumption (r = \u22120.57; p = 0.001): a 10-point increase in the score was associated with a one-liter decrease in absolute alcohol consumption per person per year (95% confidence interval, 0.4\u20131.5 l). A sensitivity analysis demonstrated the robustness of the Index by showing that countries' scores and ranks remained relatively stable in response to variations in methodological assumptions. Conclusions: The strength of alcohol control policies, as estimated by the Alcohol Policy Index, varied widely among 30 countries located in Europe, Asia, North America, and Australia. The study revealed a clear inverse relationship between policy strength and alcohol consumption. The Index provides a straightforward tool for facilitating international comparisons. In addition, it can help policymakers review and strengthen existing regulations aimed at minimizing alcohol-related harm and estimate the likely impact of policy changes. \n        Using an index that gauges the strength of national alcohol policies, a clear inverse relationship was found between policy strength and alcohol consumption.\n      Background.: Alcohol drinking is now recognized as one of the most important risks to human health. Previous research studies (see the research article by Rodgers et al., linked below) have predicted that around 4% of the burden of disease worldwide comes about as a result of drinking alcohol, which can be a factor in a wide range of health problems. These include chronic diseases such as cirrhosis of the liver and certain cancers, as well as poor health resulting from trauma, violence, and accidental injuries. For these reasons, most governments try to control the consumption of alcohol through laws, although very few countries ban alcohol entirely. Why Was This Study Done?: Although bodies such as the World Health Assembly have recommended that its member countries develop national control policies to prevent excessive alcohol use, there is a huge variation between national policies. It is also very unclear whether there is any link between the strictness of legislation regarding alcohol control in any given country and how much people in that country actually drink. What Did the Researchers Do and Find?: The researchers carrying out this study had two broad goals. First, they wanted to develop an index (or scoring system) that would allow them and others to rate the strength of any given country's alcohol control policy. Second, they wanted to see whether there is any link between the strength of control policies on this index and the amount of alcohol that is drunk by people on average in each country. In order to develop the alcohol control index, the researchers chose five main areas relating to alcohol control. These five areas related to the availability of alcohol, the \u201cdrinking context,\u201d pricing, advertising, and vehicles. Within each policy area, specific policy topics relating to prevention of alcohol consumption and harm were identified. Then, each of 30 countries within the OECD (Organization for Economic Cooperation and Development) were rated on this index using recent data from public reports and databases. The researchers also collected data on alcohol consumption within each country from the World Health Organization and used this to estimate the average amount drunk per person in a year. When the researchers plotted scores on their index against the average amount drunk per person per year, they saw a negative correlation. That is, the stronger the alcohol control policy in any given country, the less people seemed to drink. This worked out at around roughly a 10-point increase on the index equating to a one-liter drop in alcohol consumption per person per year. However, some countries did not seem to fit these predictions very well. What Do These Findings Mean?: The finding that there is a link between the strength of alcohol control policies and amount of alcohol drinking does not necessarily mean that greater government control causes lower drinking rates. The relationship might just mean that some other variable (e.g., some cultural factor) plays a role in determining the amount that people drink as well as affecting national policies for alcohol control. However, the index developed here is a useful method for researchers and policy makers to measure changes in alcohol controls and therefore understand more clearly the factors that affect drinking rates. This study looked only at the connection between control measures and extent of alcohol consumption, and did not examine alcohol-related harm. Future research might focus on the links between controls and the harms caused by alcohol. Additional Information.: Please access these Web sites via the online version of this summary at http://dx.doi.org/10.1371/journal.pmed.0040151. "], "author_display": ["Donald A Brand", "Michaela Saisana", "Lisa A Rynn", "Fulvia Pennoni", "Albert B Lowenfels"], "article_type": "Research Article", "score": 0.40134215, "title_display": "Comparative Analysis of Alcohol Control Policies in 30 Countries", "publication_date": "2007-04-24T00:00:00Z", "eissn": "1549-1676", "id": "10.1371/journal.pmed.0040151"}, {"journal": "PLOS ONE", "abstract": ["Objectives: The mammalian target of rapamycin (mTOR) and phosphorylated mTOR (p-mTOR) are potential prognostic markers and therapeutic targets for non-small cell lung cancer (NSCLC). However, the association between mTOR/p-mTOR expression and NSCLC patients\u2019 prognosis remains controversial. Thus, a meta-analysis of existing studies evaluating the prognostic role of mTOR/p-mTOR expression for NSCLC was conducted. Materials and Methods: A systemically literature search was performed via Pubmed, Embase, Medline as well as CNKI (China National Knowledge Infrastructure). Studies were included that reported the hazard ratio (HR) and 95%CI for the association between mTOR/p-mTOR expression and NSCLC patients\u2019 survival. Random-effects model was used to pool HRs. Results: Ten eligible studies were included in this meta-analysis, with 4 about m-TOR and 7 about p-mTOR. For mTOR, the pooled HR of overall survival (OS) was 1.00 (95%CI 0.5 to 1.99) by univariate analysis and 1.22 (95%CI 0.53 to 2.82) by multivariate analysis. For p-mTOR, the pooled HR was 1.39 (95%CI 0.97 to 1.98) by univariate analysis and 1.42 (95%CI 0.56 to 3.60) by multivariate analysis. Conclusion: The results indicated that no statistically significant association was found between mTOR/p-mTOR expression and NSCLC patients\u2019 prognosis. "], "author_display": ["Lei Li", "Dan Liu", "Zhi-Xin Qiu", "Shuang Zhao", "Li Zhang", "Wei-Min Li"], "article_type": "Research Article", "score": 0.40129703, "title_display": "The Prognostic Role of mTOR and P-mTOR for Survival in Non-Small Cell Lung Cancer: A Systematic Review and Meta-Analysis", "publication_date": "2015-02-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0116771"}, {"journal": "PLOS ONE", "abstract": ["\nA total of 89 trees of Korean pine (Pinus koraiensis) were destructively sampled from the plantations in Heilongjiang Province, P.R. China. The sample trees were measured and calculated for the biomass and carbon stocks of tree components (i.e., stem, branch, foliage and root). Both compatible biomass and carbon stock models were developed with the total biomass and total carbon stocks as the constraints, respectively. Four methods were used to evaluate the carbon stocks of tree components. The first method predicted carbon stocks directly by the compatible carbon stocks models (Method 1). The other three methods indirectly predicted the carbon stocks in two steps: (1) estimating the biomass by the compatible biomass models, and (2) multiplying the estimated biomass by three different carbon conversion factors (i.e., carbon conversion factor 0.5 (Method 2), average carbon concentration of the sample trees (Method 3), and average carbon concentration of each tree component (Method 4)). The prediction errors of estimating the carbon stocks were compared and tested for the differences between the four methods. The results showed that the compatible biomass and carbon models with tree diameter (D) as the sole independent variable performed well so that Method 1 was the best method for predicting the carbon stocks of tree components and total. There were significant differences among the four methods for the carbon stock of stem. Method 2 produced the largest error, especially for stem and total. Methods 3 and Method 4 were slightly worse than Method 1, but the differences were not statistically significant. In practice, the indirect method using the mean carbon concentration of individual trees was sufficient to obtain accurate carbon stocks estimation if carbon stocks models are not available.\n"], "author_display": ["Huilin Gao", "Lihu Dong", "Fengri Li", "Lianjun Zhang"], "article_type": "Research Article", "score": 0.401265, "title_display": "Evaluation of Four Methods for Predicting Carbon Stocks of Korean Pine Plantations in Heilongjiang Province, China", "publication_date": "2015-12-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0145017"}, {"journal": "PLoS ONE", "abstract": ["\nBibliographic analysis has been a very powerful tool in evaluating the effective contributions of a researcher and determining his/her future research potential. The lack of an absolute quantification of the author\u2019s scientific contributions by the existing measurement system hampers the decision-making process. In this paper, a new metric system, Absolute index (Ab-index), has been proposed that allows a more objective comparison of the contributions of a researcher. The Ab-index takes into account the impact of research findings while keeping in mind the physical and intellectual contributions of the author(s) in accomplishing the task. The Ab-index and h-index were calculated for 10 highly cited geneticists and molecular biologist and 10 young researchers of biological sciences and compared for their relationship to the researchers input as a primary author. This is the first report of a measuring method clarifying the contributions of the first author, corresponding author, and other co-authors and the sharing of credit in a logical ratio. A java application has been developed for the easy calculation of the Ab-index. It can be used as a yardstick for comparing the credibility of different scientists competing for the same resources while the Productivity index (Pr-index), which is the rate of change in the Ab-index per year, can be used for comparing scientists of different age groups. The Ab-index has clear advantage over other popular metric systems in comparing scientific credibility of young scientists. The sum of the Ab-indices earned by individual researchers of an institute per year can be referred to as Pr-index of the institute.\n"], "author_display": ["Akshaya Kumar Biswal"], "article_type": "Research Article", "score": 0.40119487, "title_display": "An Absolute Index (<i>Ab-index</i>) to Measure a Researcher\u2019s Useful Contributions and Productivity", "publication_date": "2013-12-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0084334"}, {"journal": "PLoS Medicine", "abstract": [": Selda Ulucanlar and colleagues analyze submissions by two tobacco companies to the UK government consultation on standardized packaging. Background: Standardised packaging (SP) of tobacco products is an innovative tobacco control measure opposed by transnational tobacco companies (TTCs) whose responses to the UK government's public consultation on SP argued that evidence was inadequate to support implementing the measure. The government's initial decision, announced 11 months after the consultation closed, was to wait for \u2018more evidence\u2019, but four months later a second \u2018independent review\u2019 was launched. In view of the centrality of evidence to debates over SP and TTCs' history of denying harms and manufacturing uncertainty about scientific evidence, we analysed their submissions to examine how they used evidence to oppose SP. Methods and Findings: We purposively selected and analysed two TTC submissions using a verification-oriented cross-documentary method to ascertain how published studies were used and interpretive analysis with a constructivist grounded theory approach to examine the conceptual significance of TTC critiques. The companies' overall argument was that the SP evidence base was seriously flawed and did not warrant the introduction of SP. However, this argument was underpinned by three complementary techniques that misrepresented the evidence base. First, published studies were repeatedly misquoted, distorting the main messages. Second, \u2018mimicked scientific critique\u2019 was used to undermine evidence; this form of critique insisted on methodological perfection, rejected methodological pluralism, adopted a litigation (not scientific) model, and was not rigorous. Third, TTCs engaged in \u2018evidential landscaping\u2019, promoting a parallel evidence base to deflect attention from SP and excluding company-held evidence relevant to SP. The study's sample was limited to sub-sections of two out of four submissions, but leaked industry documents suggest at least one other company used a similar approach. Conclusions: The TTCs' claim that SP will not lead to public health benefits is largely without foundation. The tools of Better Regulation, particularly stakeholder consultation, provide an opportunity for highly resourced corporations to slow, weaken, or prevent public health policies. Background: Every year, about 6 million people die from tobacco-related diseases and, if current trends continue, annual tobacco-related deaths will increase to more than 8 million by 2030. To reduce this loss of life, national and international bodies have drawn up various conventions and directives designed to implement tobacco control measures such as the adoption of taxation policies aimed at reducing tobacco consumption and bans on tobacco advertising, promotion, and sponsorship. One innovative but largely unused tobacco control measure is standardised packaging of tobacco products. Standardised packaging aims to prevent the use of packaging as a marketing tool by removing all brand imagery and text (other than name) and by introducing packs of a standard shape and colour that include prominent pictorial health warnings. Standardised packaging was first suggested as a tobacco control measure in 1986 but has been consistently opposed by the tobacco industry. Why Was This Study Done?: The UK is currently considering standardised packaging of tobacco products. In the UK, Better Regulation guidance obliges officials to seek the views of stakeholders, including corporations, on the government's cost and benefit estimates of regulatory measures such as standardised packaging and on the evidence underlying these estimates. In response to a public consultation about standardised packaging in July 2013, which considered submissions from several transnational tobacco companies (TTCs), the UK government announced that it would wait for the results of the standardised packaging legislation that Australia adopted in December 2012 before making its final decision about this tobacco control measure. Parliamentary debates and media statements have suggested that doubt over the adequacy of the evidence was the main reason for this \u2018wait and see\u2019 decision. Notably, TTCs have a history of manufacturing uncertainty about the scientific evidence related to the harms of tobacco. Given the centrality of evidence to the debate about standardised packaging, in this study, the researchers analyse submissions made by two TTCs, British American Tobacco (BAT) and Japan Tobacco International (JTI), to the first UK consultation on standardised packaging (a second review is currently underway and will report shortly) to examine how TTCs used evidence to oppose standardised packaging. What Did the Researchers Do and Find?: The researchers analysed sub-sections of two of the four TTC submissions (those submitted by BAT and JTI) made to the public consultation using verification-oriented cross-documentary analysis, which compared references made to published sources with the original sources to ascertain how these sources had been used, and interpretative analysis to examine the conceptual significance of TTC critiques of the evidence on standardised packaging. The researchers report that the companies' overall argument was that the evidence base in support of standardised packaging was seriously flawed and did not warrant the introduction of such packaging. The researchers identified three ways in which the TTC reports misrepresented the evidence base. First, the TTCs misquoted published studies, thereby distorting the main messages of these studies. For example, the TTCs sometimes omitted important qualifying information when quoting from published studies. Second, the TTCs undermined evidence by employing experts to review published studies for methodological rigor and value in ways that did not conform to normal scientific critique approaches (\u2018mimicked scientific critique\u2019). So, for example, the experts considered each piece of evidence in isolation for its ability to support standardised packaging rather than considering the cumulative weight of the evidence. Finally, the TTCs engaged in \u2018evidential landscaping\u2019. That is, they promoted research that deflected attention from standardised packaging (for example, research into social explanations of smoking behaviour) and omitted internal industry research on the role of packaging in marketing. What Do These Findings Mean?: These findings suggest that the TTC critique of the evidence in favour of standardised packaging that was presented to the UK public consultation on this tobacco control measure is highly misleading. However, because the researchers' analysis only considered subsections of the submissions from two TTCs, these findings may not be applicable to the other submissions or to other TTCs. Moreover, their analysis only considered the efforts made by TTCs to influence public health policy and not the effectiveness of these efforts. Nevertheless, these findings suggest that the claim of TTCs that standardised packaging will not lead to public health benefits is largely without foundation. More generally, these findings highlight the possibility that the tools of Better Regulation, particularly stakeholder consultation, provide an opportunity for wealthy corporations to slow, weaken, or prevent the implementation of public health policies. Additional Information: Please access these websites via the online version of this summary at http://dx.doi.org/10.1371/journal.pmed.1001629. "], "author_display": ["Selda Ulucanlar", "Gary J. Fooks", "Jenny L. Hatchard", "Anna B. Gilmore"], "article_type": "Research Article", "score": 0.400905, "title_display": "Representation and Misrepresentation of Scientific Evidence in Contemporary Tobacco Regulation: A Review of Tobacco Industry Submissions to the UK Government Consultation on Standardised Packaging", "publication_date": "2014-03-25T00:00:00Z", "eissn": "1549-1676", "id": "10.1371/journal.pmed.1001629"}, {"journal": "PLoS ONE", "abstract": ["Background: Metagenomics is a relatively new but fast growing field within environmental biology and medical sciences. It enables researchers to understand the diversity of microbes, their functions, cooperation, and evolution in a particular ecosystem. Traditional methods in genomics and microbiology are not efficient in capturing the structure of the microbial community in an environment. Nowadays, high-throughput next-generation sequencing technologies are powerfully driving the metagenomic studies. However, there is an urgent need to develop efficient statistical methods and computational algorithms to rapidly analyze the massive metagenomic short sequencing data and to accurately detect the features/functions present in the microbial community. Although several issues about functions of metagenomes at pathways or subsystems level have been investigated, there is a lack of studies focusing on functional analysis at a low level of a hierarchical functional tree, such as SEED subsystem tree. Results: A two-step statistical procedure (metaFunction) is proposed to detect all possible functional roles at the low level from a metagenomic sample/community. In the first step a statistical mixture model is proposed at the base of gene codons to estimate the abundances for the candidate functional roles, with sequencing error being considered. As a gene could be involved in multiple biological processes the functional assignment is therefore adjusted by utilizing an error distribution in the second step. The performance of the proposed procedure is evaluated through comprehensive simulation studies. Compared with other existing methods in metagenomic functional analysis the new approach is more accurate in assigning reads to functional roles, and therefore at more general levels. The method is also employed to analyze two real data sets. Conclusions: metaFunction is a powerful tool in accurate profiling functions in a metagenomic sample. "], "author_display": ["Lingling An", "Nauromal Pookhao", "Hongmei Jiang", "Jiannong Xu"], "article_type": "Research Article", "score": 0.4009041, "title_display": "Statistical Approach of Functional Profiling for a Microbial Community", "publication_date": "2014-09-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0106588"}, {"journal": "PLoS ONE", "abstract": ["Background: The analysis of co-authorship network aims at exploring the impact of network structure on the outcome of scientific collaborations and research publications. However, little is known about what network properties are associated with authors who have increased number of joint publications and are being cited highly. Methodology/Principal Findings: Measures of social network analysis, for example network centrality and tie strength, have been utilized extensively in current co-authorship literature to explore different behavioural patterns of co-authorship networks. Using three SNA measures (i.e., degree centrality, closeness centrality and betweenness centrality), we explore scientific collaboration networks to understand factors influencing performance (i.e., citation count) and formation (tie strength between authors) of such networks. A citation count is the number of times an article is cited by other articles. We use co-authorship dataset of the research field of \u2018steel structure\u2019 for the year 2005 to 2009. To measure the strength of scientific collaboration between two authors, we consider the number of articles co-authored by them. In this study, we examine how citation count of a scientific publication is influenced by different centrality measures of its co-author(s) in a co-authorship network. We further analyze the impact of the network positions of authors on the strength of their scientific collaborations. We use both correlation and regression methods for data analysis leading to statistical validation. We identify that citation count of a research article is positively correlated with the degree centrality and betweenness centrality values of its co-author(s). Also, we reveal that degree centrality and betweenness centrality values of authors in a co-authorship network are positively correlated with the strength of their scientific collaborations. Conclusions/Significance: Authors\u2019 network positions in co-authorship networks influence the performance (i.e., citation count) and formation (i.e., tie strength) of scientific collaborations. "], "author_display": ["Shahadat Uddin", "Liaquat Hossain", "Kim Rasmussen"], "article_type": "Research Article", "score": 0.40075085, "title_display": "Network Effects on Scientific Collaborations", "publication_date": "2013-02-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0057546"}, {"journal": "PLoS ONE", "abstract": ["\nDetection of yet unknown subgroups showing differential gene or protein expression is a frequent goal in the analysis of modern molecular data. Applications range from cancer biology over developmental biology to toxicology. Often a control and an experimental group are compared, and subgroups can be characterized by differential expression for only a subgroup-specific set of genes or proteins. Finding such genes and corresponding patient subgroups can help in understanding pathological pathways, diagnosis and defining drug targets. The size of the subgroup and the type of differential expression determine the optimal strategy for subgroup identification. To date, commonly used software packages hardly provide statistical tests and methods for the detection of such subgroups. Different univariate methods for subgroup detection are characterized and compared, both on simulated and on real data. We present an advanced design for simulation studies: Data is simulated under different distributional assumptions for the expression of the subgroup, and performance results are compared against theoretical upper bounds. For each distribution, different degrees of deviation from the majority of observations are considered for the subgroup. We evaluate classical approaches as well as various new suggestions in the context of omics data, including outlier sum, PADGE, and kurtosis. We also propose the new FisherSum score. ROC curve analysis and AUC values are used to quantify the ability of the methods to distinguish between genes or proteins with and without certain subgroup patterns. In general, FisherSum for small subgroups and -test for large subgroups achieve best results. We apply each method to a case-control study on Parkinson's disease and underline the biological benefit of the new method.\n"], "author_display": ["Maike Ahrens", "Michael Turewicz", "Swaantje Casjens", "Caroline May", "Beate Pesch", "Christian Stephan", "Dirk Woitalla", "Ralf Gold", "Thomas Br\u00fcning", "Helmut E. Meyer", "J\u00f6rg Rahnenf\u00fchrer", "Martin Eisenacher"], "article_type": "Research Article", "score": 0.40053356, "title_display": "Detection of Patient Subgroups with Differential Expression in Omics Data: A Comprehensive Comparison of Univariate Measures", "publication_date": "2013-11-22T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0079380"}, {"journal": "PLoS ONE", "abstract": ["Summary: Complex human diseases can show significant heterogeneity between patients with the same phenotypic disorder. An outlier detection strategy was developed to identify variants at the level of gene transcription that are of potential biological and phenotypic importance. Here we describe a graphical software package (z-score outlier detection (ZODET)) that enables identification and visualisation of gross abnormalities in gene expression (outliers) in individuals, using whole genome microarray data. Mean and standard deviation of expression in a healthy control cohort is used to detect both over and under-expressed probes in individual test subjects. We compared the potential of ZODET to detect outlier genes in gene expression datasets with a previously described statistical method, gene tissue index (GTI), using a simulated expression dataset and a publicly available monocyte-derived macrophage microarray dataset. Taken together, these results support ZODET as a novel approach to identify outlier genes of potential pathogenic relevance in complex human diseases. The algorithm is implemented using R packages and Java. Availability: The software is freely available from http://www.ucl.ac.uk/medicine/molecular-medicine/publications/microarray-outlier-analysis. "], "author_display": ["Daniel L. Roden", "Gavin W. Sewell", "Anna Lobley", "Adam P. Levine", "Andrew M. Smith", "Anthony W. Segal"], "article_type": "Research Article", "score": 0.40050757, "title_display": "ZODET: Software for the Identification, Analysis and Visualisation of Outlier Genes in Microarray Expression Data", "publication_date": "2014-01-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0081123"}, {"journal": "PLoS ONE", "abstract": ["\nNon-small cell lung cancer (NSCLC) has two major subtypes: adenocarcinoma (AC) and squamous cell carcinoma (SCC). The diagnosis and treatment of NSCLC are hindered by the limited knowledge about the pathogenesis mechanisms of subtypes of NSCLC. It is necessary to research the molecular mechanisms related with AC and SCC. In this work, we improved the logic analysis algorithm to mine the sufficient and necessary conditions for the presence states (presence or absence) of phenotypes. We applied our method to AC and SCC specimens, and identified  lower and  higher logic relationships between genes and two subtypes of NSCLC. The discovered relationships were independent of specimens selected, and their significance was validated by statistic test. Compared with the two earlier methods (the non-negative matrix factorization method and the relevance analysis method), the current method outperformed these methods in the recall rate and classification accuracy on NSCLC and normal specimens. We obtained  biomarkers. Among  biomarkers,  genes have been used to distinguish AC from SCC in practice, and other six genes were newly discovered biomarkers for distinguishing subtypes. Furthermore, NKX2-1 has been considered as a molecular target for the targeted therapy of AC, and  other genes may be novel molecular targets. By gene ontology analysis, we found that two biological processes (\u2018epidermis development\u2019 and \u2018cell adhesion\u2019) were closely related with the tumorigenesis of subtypes of NSCLC. More generally, the current method could be extended to other complex diseases for distinguishing subtypes and detecting the molecular targets for targeted therapy.\n"], "author_display": ["Yansen Su", "Linqiang Pan"], "article_type": "Research Article", "score": 0.4005029, "title_display": "Identification of Logic Relationships between Genes and Subtypes of Non-Small Cell Lung Cancer", "publication_date": "2014-04-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0094644"}, {"journal": "PLoS ONE", "abstract": ["\nThe inference of a genetic network is a problem in which mutual interactions among genes are inferred from time-series of gene expression levels. While a number of models have been proposed to describe genetic networks, this study focuses on a mathematical model proposed by Vohradsk\u00fd. Because of its advantageous features, several researchers have proposed the inference methods based on Vohradsk\u00fd's model. When trying to analyze large-scale networks consisting of dozens of genes, however, these methods must solve high-dimensional non-linear function optimization problems. In order to resolve the difficulty of estimating the parameters of the Vohradsk\u00fd's model, this study proposes a new method that defines the problem as several two-dimensional function optimization problems. Through numerical experiments on artificial genetic network inference problems, we showed that, although the computation time of the proposed method is not the shortest, the method has the ability to estimate parameters of Vohradsk\u00fd's models more effectively with sufficiently short computation times. This study then applied the proposed method to an actual inference problem of the bacterial SOS DNA repair system, and succeeded in finding several reasonable regulations.\n"], "author_display": ["Shuhei Kimura", "Masanao Sato", "Mariko Okada-Hatakeyama"], "article_type": "Research Article", "score": 0.40044183, "title_display": "Inference of Vohradsk\u00fd's Models of Genetic Networks by Solving Two-Dimensional Function Optimization Problems", "publication_date": "2013-12-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0083308"}, {"journal": "PLOS ONE", "abstract": ["\nThis paper presents a novel voxel-based method for texture analysis of brain images. Texture analysis is a powerful quantitative approach for analyzing voxel intensities and their interrelationships, but has been thus far limited to analyzing regions of interest. The proposed method provides a 3D statistical map comparing texture features on a voxel-by-voxel basis. The validity of the method was examined on artificially generated effects as well as on real MRI data in Alzheimer's Disease (AD). The artificially generated effects included hyperintense and hypointense signals added to T1-weighted brain MRIs from 30 healthy subjects. The AD dataset included 30 patients with AD and 30 age/sex matched healthy control subjects. The proposed method detected artificial effects with high accuracy and revealed statistically significant differences between the AD and control groups. This paper extends the usage of texture analysis beyond the current region of interest analysis to voxel-by-voxel 3D statistical mapping and provides a hypothesis-free analysis tool to study cerebral pathology in neurological diseases.\n"], "author_display": ["Rouzbeh Maani", "Yee Hong Yang", "Sanjay Kalra"], "article_type": "Research Article", "score": 0.40035632, "title_display": "Voxel-Based Texture Analysis of the Brain", "publication_date": "2015-03-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0117759"}, {"journal": "PLoS ONE", "abstract": ["Background: Mathematical modeling in epidemiology (MME) is being used increasingly. However, there are many uncertainties in terms of definitions, uses and quality features of MME. Methodology/Principal Findings: To delineate the current status of these models, a 10-item questionnaire on MME was devised. Proposed via an anonymous internet-based survey, the questionnaire was completed by 189 scientists who had published in the domain of MME. A small minority (18%) of respondents claimed to have in mind a concise definition of MME. Some techniques were identified by the researchers as characterizing MME (e.g. Markov models), while others\u2013at the same level of sophistication in terms of mathematics\u2013were not (e.g. Cox regression). The researchers' opinions were also contrasted about the potential applications of MME, perceived as higly relevant for providing insight into complex mechanisms and less relevant for identifying causal factors. The quality criteria were those of good science and were not related to the size and the nature of the public health problems addressed. Conclusions/Significance: This study shows that perceptions on the nature, uses and quality criteria of MME are contrasted, even among the very community of published authors in this domain. Nevertheless, MME is an emerging discipline in epidemiology and this study underlines that it is associated with specific areas of application and methods. The development of this discipline is likely to deserve a framework providing recommendations and guidance at various steps of the studies, from design to report. "], "author_display": ["Gilles Hejblum", "Michel Setbon", "Laura Temime", "Sophie Lesieur", "Alain-Jacques Valleron"], "article_type": "Research Article", "score": 0.400154, "title_display": "Modelers' Perception of Mathematical Modeling in Epidemiology: A Web-Based Survey", "publication_date": "2011-01-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0016531"}, {"journal": "PLoS ONE", "abstract": ["\nMicroRNAs (miRNAs) are a group of small non-coding RNAs that play important regulatory roles at the post-transcriptional level. Although several computational methods have been developed to compare miRNAs, it is still a challenging and a badly needed task with the availability of various biological data resources. In this study, we proposed a novel graph theoretic property based computational framework and method, called miRFunSim, for quantifying the associations between miRNAs based on miRNAs targeting propensity and proteins connectivity in the integrated protein-protein interaction network. To evaluate the performance of our method, we applied the miRFunSim method to compute functional similarity scores of miRNA pairs between 100 miRNAs whose target genes have been experimentally supported and found that the functional similarity scores of miRNAs in the same family or in the same cluster are significantly higher compared with other miRNAs which are consistent with prior knowledge. Further validation analysis on experimentally verified miRNA-disease associations suggested that miRFunSim can effectively recover the known miRNA pairs associated with the same disease and achieve a higher AUC of 83.1%. In comparison with similar methods, our miRFunSim method can achieve more effective and more reliable performance for measuring the associations of miRNAs. We also conducted the case study examining liver cancer based on our method, and succeeded in uncovering the candidate liver cancer related miRNAs such as miR-34 which also has been proven in the latest study.\n"], "author_display": ["Jie Sun", "Meng Zhou", "Haixiu Yang", "Jiaen Deng", "Letian Wang", "Qianghu Wang"], "article_type": "Research Article", "score": 0.40003988, "title_display": "Inferring Potential microRNA-microRNA Associations Based on Targeting Propensity and Connectivity in the Context of Protein Interaction Network", "publication_date": "2013-07-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0069719"}, {"journal": "PLoS ONE", "abstract": ["Purpose: The ICECAP-A and EQ-5D-5L are two index measures appropriate for use in health research.  Assessment of content validity allows understanding of whether a measure captures the most relevant and important aspects of a concept.  This paper reports a qualitative assessment of the content validity and appropriateness for use of the eq-5D-5L and ICECAP-A measures, using novel methodology. Methods: In-depth semi-structured interviews were conducted with research professionals in the UK and Australia.  Informants were purposively sampled based on their professional role.  Data were analysed in an iterative, thematic and constant comparative manner.  A two stage investigation - the comparative direct approach - was developed to address the methodological challenges of the content validity research and allow rigorous assessment. Results: Informants viewed the ICECAP-A as an assessment of the broader determinants of quality of life, but lacking in assessment of health-related determinants.  The eq-5D-5L was viewed as offering good coverage of health determinants, but as lacking in assessment of these broader determinants.  Informants held some concerns about the content or wording of the Self-care, Pain/Discomfort and Anxiety/Depression items (EQ-5D-5L) and the Enjoyment, Achievement and attachment items (ICECAP-A).   Conclusion: Using rigorous qualitative methodology the results suggest that the ICECAP-A and EQ-5D-5L hold acceptable levels of content validity and are appropriate for use in health research.  This work adds expert opinion to the emerging body of research using patients and public to validate these measures.   "], "author_display": ["Thomas Keeley", "Hareth Al-Janabi", "Paula Lorgelly", "Joanna Coast"], "article_type": "Research Article", "score": 0.3999578, "title_display": "A Qualitative Assessment of the Content Validity of the ICECAP-A and EQ-5D-5L and Their Appropriateness for Use in Health Research", "publication_date": "2013-12-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0085287"}, {"journal": "PLoS ONE", "abstract": ["Background: \u03b2-amyloid (A\u03b2) plaques in brain's grey matter (GM) are one of the pathological hallmarks of Alzheimer's disease (AD), and can be imaged in vivo using Positron Emission Tomography (PET) with 11C or 18F radiotracers. Estimating A\u03b2 burden in cortical GM has been shown to improve diagnosis and monitoring of AD. However, lacking structural information in PET images requires such assessments to be performed with anatomical MRI scans, which may not be available at different clinical settings or being contraindicated for particular reasons. This study aimed to develop an MR-less A\u03b2 imaging quantification method that requires only PET images for reliable A\u03b2 burden estimations. Materials and Methods: The proposed method has been developed using a multi-atlas based approach on 11C-PiB scans from 143 subjects (75 PiB+ and 68 PiB- subjects) in AIBL study. A subset of 20 subjects (PET and MRI) were used as atlases: 1) MRI images were co-registered with tissue segmentation; 2) 3D surface at the GM-WM interfacing was extracted and registered to a canonical space; 3) Mean PiB retention within GM was estimated and mapped to the surface. For other participants, each atlas PET image (and surface) was registered to the subject's PET image for PiB estimation within GM. The results are combined by subject-specific atlas selection and Bayesian fusion to generate estimated surface values. Results: All PiB+ subjects (N\u200a=\u200a75) were highly correlated between the MR-dependent and the PET-only methods with Intraclass Correlation (ICC) of 0.94, and an average relative difference error of 13% (or 0.23 SUVR) per surface vertex. All PiB- subjects (N\u200a=\u200a68) revealed visually akin patterns with a relative difference error of 16% (or 0.19 SUVR) per surface vertex. Conclusion: The demonstrated accuracy suggests that the proposed method could be an effective clinical inspection tool for A\u03b2 imaging scans when MRI images are unavailable. "], "author_display": ["Luping Zhou", "Olivier Salvado", "Vincent Dore", "Pierrick Bourgeat", "Parnesh Raniga", "S. Lance Macaulay", "David Ames", "Colin L. Masters", "Kathryn A. Ellis", "Victor L. Villemagne", "Christopher C. Rowe", "Jurgen Fripp", "AIBL Research Group "], "article_type": "Research Article", "score": 0.39990747, "title_display": "MR-Less Surface-Based Amyloid Assessment Based on <sup>11</sup>C PiB PET", "publication_date": "2014-01-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0084777"}, {"journal": "PLOS ONE", "abstract": ["\nIn complex networks, it is of great theoretical and practical significance to identify a set of critical spreaders which help to control the spreading process. Some classic methods are proposed to identify multiple spreaders. However, they sometimes have limitations for the networks with community structure because many chosen spreaders may be clustered in a community. In this paper, we suggest a novel method to identify multiple spreaders from communities in a balanced way. The network is first divided into a great many super nodes and then k spreaders are selected from these super nodes. Experimental results on real and synthetic networks with community structure show that our method outperforms the classic methods for degree centrality, k-core and ClusterRank in most cases.\n"], "author_display": ["Jia-Lin He", "Yan Fu", "Duan-Bing Chen"], "article_type": "Research Article", "score": 0.39982447, "title_display": "A Novel Top-<i>k</i> Strategy for Influence Maximization in Complex Networks with Community Structure", "publication_date": "2015-12-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0145283"}, {"journal": "PLoS ONE", "abstract": ["Background and Objectives: Randomization, allocation concealment, and blind outcome assessment have been shown to reduce bias in human studies. Authors from the Collaborative Approach to Meta Analysis and Review of Animal Data from Experimental Studies (CAMARADES) collaboration recently found that these features protect against bias in animal stroke studies. We extended the scope the work from CAMARADES to include investigations of treatments for any condition. Methods: We conducted an overview of systematic reviews. We searched Medline and Embase for systematic reviews of animal studies testing any intervention (against any control) and we included any disease area and outcome. We included reviews comparing randomized versus not randomized (but otherwise controlled), concealed versus unconcealed treatment allocation, or blinded versus unblinded outcome assessment. Results: Thirty-one systematic reviews met our inclusion criteria: 20 investigated treatments for experimental stroke, 4 reviews investigated treatments for spinal cord diseases, while 1 review each investigated treatments for bone cancer, intracerebral hemorrhage, glioma, multiple sclerosis, Parkinson's disease, and treatments used in emergency medicine. In our sample 29% of studies reported randomization, 15% of studies reported allocation concealment, and 35% of studies reported blinded outcome assessment. We pooled the results in a meta-analysis, and in our primary analysis found that failure to randomize significantly increased effect sizes, whereas allocation concealment and blinding did not. In our secondary analyses we found that randomization, allocation concealment, and blinding reduced effect sizes, especially where outcomes were subjective. Conclusions: Our study demonstrates the need for randomization, allocation concealment, and blind outcome assessment in animal research across a wide range of outcomes and disease areas. Since human studies are often justified based on results from animal studies, our results suggest that unduly biased animal studies should not be allowed to constitute part of the rationale for human trials. "], "author_display": ["Jennifer A. Hirst", "Jeremy Howick", "Jeffrey K. Aronson", "Nia Roberts", "Rafael Perera", "Constantinos Koshiaris", "Carl Heneghan"], "article_type": "Research Article", "score": 0.39972553, "title_display": "The Need for Randomization in Animal Trials: An Overview of Systematic Reviews", "publication_date": "2014-06-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0098856"}, {"journal": "PLOS ONE", "abstract": ["Background: Spatial normalization is a prerequisite step for analyzing positron emission tomography (PET) images both by using volume-of-interest (VOI) template and voxel-based analysis. Magnetic resonance (MR) or ligand-specific PET templates are currently used for spatial normalization of PET images. We used computed tomography (CT) images acquired with PET/CT scanner for the spatial normalization for [18F]-N-3-fluoropropyl-2-betacarboxymethoxy-3-beta-(4-iodophenyl) nortropane (FP-CIT) PET images and compared target-to-cerebellar standardized uptake value ratio (SUVR) values with those obtained from MR- or PET-guided spatial normalization method in healthy controls and patients with Parkinson\u2019s disease (PD). Methods: We included 71 healthy controls and 56 patients with PD who underwent [18F]-FP-CIT PET scans with a PET/CT scanner and T1-weighted MR scans. Spatial normalization of MR images was done with a conventional spatial normalization tool (cvMR) and with DARTEL toolbox (dtMR) in statistical parametric mapping software. The CT images were modified in two ways, skull-stripping (ssCT) and intensity transformation (itCT). We normalized PET images with cvMR-, dtMR-, ssCT-, itCT-, and PET-guided methods by using specific templates for each modality and measured striatal SUVR with a VOI template. The SUVR values measured with FreeSurfer-generated VOIs (FSVOI) overlaid on original PET images were also used as a gold standard for comparison. Results: The SUVR values derived from all four structure-guided spatial normalization methods were highly correlated with those measured with FSVOI (P < 0.0001). Putaminal SUVR values were highly effective for discriminating PD patients from controls. However, the PET-guided method excessively overestimated striatal SUVR values in the PD patients by more than 30% in caudate and putamen, and thereby spoiled the linearity between the striatal SUVR values in all subjects and showed lower disease discrimination ability. Two CT-guided methods showed comparable capability with the MR-guided methods in separating PD patients from controls and showed better correlation between putaminal SUVR values and the parkinsonian motor severity than the PET-guided method. Conclusion: CT-guided spatial normalization methods provided reliable striatal SUVR values comparable to those obtained with MR-guided methods. CT-guided methods can be useful for analyzing dopamine transporter PET images when MR images are unavailable. "], "author_display": ["Jin Su Kim", "Hanna Cho", "Jae Yong Choi", "Seung Ha Lee", "Young Hoon Ryu", "Chul Hyoung Lyoo", "Myung Sik Lee"], "article_type": "Research Article", "score": 0.3996445, "title_display": "Feasibility of Computed Tomography-Guided Methods for Spatial Normalization of Dopamine Transporter Positron Emission Tomography Image", "publication_date": "2015-07-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0132585"}, {"journal": "PLoS ONE", "abstract": ["\n        Expression quantitative trait loci (eQTL) mapping is a widely used technique to uncover regulatory relationships between genes. A range of methodologies have been developed to map links between expression traits and genotypes. The DREAM (Dialogue on Reverse Engineering Assessments and Methods) initiative is a community project to objectively assess the relative performance of different computational approaches for solving specific systems biology problems. The goal of one of the DREAM5 challenges was to reverse-engineer genetic interaction networks from synthetic genetic variation and gene expression data, which simulates the problem of eQTL mapping. In this framework, we proposed an approach whose originality resides in the use of a combination of existing machine learning algorithms (committee). Although it was not the best performer, this method was by far the most precise on average. After the competition, we continued in this direction by evaluating other committees using the DREAM5 data and developed a method that relies on Random Forests and LASSO. It achieved a much higher average precision than the DREAM best performer at the cost of slightly lower average sensitivity.\n      "], "author_display": ["Marit Ackermann", "Mathieu Cl\u00e9ment-Ziza", "Jacob J. Michaelson", "Andreas Beyer"], "article_type": "Research Article", "score": 0.39956784, "title_display": "Teamwork: Improved eQTL Mapping Using Combinations of Machine Learning Methods", "publication_date": "2012-07-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0040916"}, {"journal": "PLoS ONE", "abstract": ["Background: Patents are one of the most important forms of intellectual property. They grant a time-limited exclusivity on the use of an invention allowing the recuperation of research costs. The use of patents is fiercely debated for medical innovation and especially controversial for publicly funded research, where the patent holder is an institution accountable to public interest. Despite this controversy, for the situation in Germany almost no empirical information exists. The purpose of this study is to examine the amount, types and trends of patent applications for health products submitted by German public research organisations. Methods/Principal Findings: We conducted a systematic search for patent documents using the publicly accessible database search interface of the German Patent and Trademark Office. We defined keywords and search criteria and developed search patterns for the database request. We retrieved documents with application date between 1988 and 2006 and processed the collected data stepwise to compile the most relevant documents in patent families for further analysis. We developed a rationale and present individual steps of a systematic method to request and process patent data from a publicly accessible database. We retrieved and processed 10194 patent documents. Out of these, we identified 1772 relevant patent families, applied for by 193 different universities and non-university public research organisations. 827 (47%) of these patent families contained granted patents. The number of patent applications submitted by universities and university-affiliated institutions more than tripled since the introduction of legal reforms in 2002, constituting almost half of all patent applications and accounting for most of the post-reform increase. Patenting of most non-university public research organisations remained stable. Conclusions: We search, process and analyse patent applications from publicly accessible databases. Internationally mounting evidence questions the viability of policies to increase commercial exploitation of publicly funded research results. To evaluate the outcome of research policies a transparent evidence base for public debate is needed in Germany. "], "author_display": ["Peter Tinnemann", "Jonas \u00d6zbay", "Victoria A. Saint", "Stefan N. Willich"], "article_type": "Research Article", "score": 0.39943278, "title_display": "Patenting of University and Non-University Public Research Organisations in Germany: Evidence from Patent Applications for Medical Research Results", "publication_date": "2010-11-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0014059"}], "response": [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]}