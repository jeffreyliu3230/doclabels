{"docs": [{"journal": "PLoS ONE", "abstract": ["\n        We propose a novel, information-theoretic, characterisation of cascades within the spatiotemporal dynamics of swarms, explicitly measuring the extent of collective communications. This is complemented by dynamic tracing of collective memory, as another element of distributed computation, which represents capacity for swarm coherence. The approach deals with both global and local information dynamics, ultimately discovering diverse ways in which an individual\u2019s spatial position is related to its information processing role. It also allows us to contrast cascades that propagate conflicting information with waves of coordinated motion. Most importantly, our simulation experiments provide the first direct information-theoretic evidence (verified in a simulation setting) for the long-held conjecture that the information cascades occur in waves rippling through the swarm. Our experiments also exemplify how features of swarm dynamics, such as cascades\u2019 wavefronts, can be filtered and predicted. We observed that maximal information transfer tends to follow the stage with maximal collective memory, and principles like this may be generalised in wider biological and social contexts.\n      "], "author_display": ["X. Rosalind Wang", "Jennifer M. Miller", "Joseph T. Lizier", "Mikhail Prokopenko", "Louis F. Rossi"], "article_type": "Research Article", "score": 0.6130036, "title_display": "Quantifying and Tracing Information Cascades in Swarms", "publication_date": "2012-07-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0040084"}, {"journal": "PLoS ONE", "abstract": ["\n        Classical models of computation traditionally resort to halting schemes in order to enquire about the state of a computation. In such schemes, a computational process is responsible for signaling an end of a calculation by setting a halt bit, which needs to be systematically checked by an observer. The capacity of quantum computational models to operate on a superposition of states requires an alternative approach. From a quantum perspective, any measurement of an equivalent halt qubit would have the potential to inherently interfere with the computation by provoking a random collapse amongst the states. This issue is exacerbated by undecidable problems such as the Entscheidungsproblem which require universal computational models, e.g. the classical Turing machine, to be able to proceed indefinitely. In this work we present an alternative view of quantum computation based on production system theory in conjunction with Grover's amplitude amplification scheme that allows for (1) a detection of halt states without interfering with the final result of a computation; (2) the possibility of non-terminating computation and (3) an inherent speedup to occur during computations susceptible of parallelization. We discuss how such a strategy can be employed in order to simulate classical Turing machines.\n      "], "author_display": ["Lu\u00eds Tarrataca", "Andreas Wichert"], "article_type": "Research Article", "score": 0.58521974, "title_display": "Quantum Iterative Deepening with an Application to the Halting Problem", "publication_date": "2013-03-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0057309"}, {"journal": "PLOS ONE", "abstract": ["\nThe introduction of computer-based testing in high-stakes examining in higher education is developing rather slowly due to institutional barriers (the need of extra facilities, ensuring test security) and teacher and student acceptance. From the existing literature it is unclear whether computer-based exams will result in similar results as paper-based exams and whether student acceptance can change as a result of administering computer-based exams. In this study, we compared results from a computer-based and paper-based exam in a sample of psychology students and found no differences in total scores across the two modes. Furthermore, we investigated student acceptance and change in acceptance of computer-based examining. After taking the computer-based exam, fifty percent of the students preferred paper-and-pencil exams over computer-based exams and about a quarter preferred a computer-based exam. We conclude that computer-based exam total scores are similar as paper-based exam scores, but that for the acceptance of high-stakes computer-based exams it is important that students practice and get familiar with this new mode of test administration.\n"], "author_display": ["Anja J. Boev\u00e9", "Rob R. Meijer", "Casper J. Albers", "Yta Beetsma", "Roel J. Bosker"], "article_type": "Research Article", "score": 0.5796909, "title_display": "Introducing Computer-Based Testing in High-Stakes Exams in Higher Education: Results of a Field Experiment", "publication_date": "2015-12-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0143616"}, {"journal": "PLoS ONE", "abstract": ["\nThe latest developments in mobile computing technology have enabled intensive applications on the modern Smartphones. However, such applications are still constrained by limitations in processing potentials, storage capacity and battery lifetime of the Smart Mobile Devices (SMDs). Therefore, Mobile Cloud Computing (MCC) leverages the application processing services of computational clouds for mitigating resources limitations in SMDs. Currently, a number of computational offloading frameworks are proposed for MCC wherein the intensive components of the application are outsourced to computational clouds. Nevertheless, such frameworks focus on runtime partitioning of the application for computational offloading, which is time consuming and resources intensive. The resource constraint nature of SMDs require lightweight procedures for leveraging computational clouds. Therefore, this paper presents a lightweight framework which focuses on minimizing additional resources utilization in computational offloading for MCC. The framework employs features of centralized monitoring, high availability and on demand access services of computational clouds for computational offloading. As a result, the turnaround time and execution cost of the application are reduced. The framework is evaluated by testing prototype application in the real MCC environment. The lightweight nature of the proposed framework is validated by employing computational offloading for the proposed framework and the latest existing frameworks. Analysis shows that by employing the proposed framework for computational offloading, the size of data transmission is reduced by 91%, energy consumption cost is minimized by 81% and turnaround time of the application is decreased by 83.5% as compared to the existing offloading frameworks. Hence, the proposed framework minimizes additional resources utilization and therefore offers lightweight solution for computational offloading in MCC.\n"], "author_display": ["Muhammad Shiraz", "Abdullah Gani", "Raja Wasim Ahmad", "Syed Adeel Ali Shah", "Ahmad Karim", "Zulkanain Abdul Rahman"], "article_type": "Research Article", "score": 0.57651734, "title_display": "A Lightweight Distributed Framework for Computational Offloading in Mobile Cloud Computing", "publication_date": "2014-08-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0102270"}, {"journal": "PLoS ONE", "abstract": ["\nKnowing the information dissemination mechanisms of different media and having an efficient information dissemination plan for disaster pre-warning plays a very important role in reducing losses and ensuring the safety of human beings. In this paper we established models of information dissemination for six typical information media, including short message service (SMS), microblogs, news portals, cell phones, television, and oral communication. Then, the information dissemination capability of each medium concerning individuals of different ages, genders, and residential areas was simulated, and the dissemination characteristics were studied. Finally, radar graphs were used to illustrate comprehensive assessments of the six media; these graphs show directly the information dissemination characteristics of all media. The models and the results are essential for improving the efficiency of information dissemination for the purpose of disaster pre-warning and for formulating emergency plans which help to reduce the possibility of injuries, deaths and other losses in a disaster.\n"], "author_display": ["Nan Zhang", "Hong Huang", "Boni Su", "Jinlong Zhao", "Bo Zhang"], "article_type": "Research Article", "score": 0.5687454, "title_display": "Information Dissemination Analysis of Different Media towards the Application for Disaster Pre-Warning", "publication_date": "2014-05-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0098649"}, {"journal": "PLoS ONE", "abstract": ["\n        The activity of networking neurons is largely characterized by the alternation of synchronous and asynchronous spiking sequences. One of the most relevant challenges that scientists are facing today is, then, relating that evidence with the fundamental mechanisms through which the brain computes and processes information, as well as with the arousal (or progress) of a number of neurological illnesses. In other words, the problem is how to associate an organized dynamics of interacting neural assemblies to a computational task. Here we show that computation can be seen as a feature emerging from the collective dynamics of an ensemble of networking neurons, which interact by means of adaptive dynamical connections. Namely, by associating logical states to synchronous neuron's dynamics, we show how the usual Boolean logics can be fully recovered, and a universal Turing machine can be constructed. Furthermore, we show that, besides the static binary gates, a wider class of logical operations can be efficiently constructed as the fundamental computational elements interact within an adaptive network, each operation being represented by a specific motif. Our approach qualitatively differs from the past attempts to encode information and compute with complex systems, where computation was instead the consequence of the application of control loops enforcing a desired state into the specific system's dynamics. Being the result of an emergent process, the computation mechanism here described is not limited to a binary Boolean logic, but it can involve a much larger number of states. As such, our results can enlighten new concepts for the understanding of the real computing processes taking place in the brain.\n      "], "author_display": ["Massimiliano Zanin", "Francisco Del Pozo", "Stefano Boccaletti"], "article_type": "Research Article", "score": 0.56472826, "title_display": "Computation Emerges from Adaptive Synchronization of Networking Neurons", "publication_date": "2011-11-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0026467"}, {"journal": "PLoS ONE", "abstract": ["\n        Global maps of science can be used as a reference system to chart career trajectories, the location of emerging research frontiers, or the expertise profiles of institutes or nations. This paper details data preparation, analysis, and layout performed when designing and subsequently updating the UCSD map of science and classification system. The original classification and map use 7.2 million papers and their references from Elsevier\u2019s Scopus (about 15,000 source titles, 2001\u20132005) and Thomson Reuters\u2019 Web of Science (WoS) Science, Social Science, Arts & Humanities Citation Indexes (about 9,000 source titles, 2001\u20132004)\u2013about 16,000 unique source titles. The updated map and classification adds six years (2005\u20132010) of WoS data and three years (2006\u20132008) from Scopus to the existing category structure\u2013increasing the number of source titles to about 25,000. To our knowledge, this is the first time that a widely used map of science was updated. A comparison of the original 5-year and the new 10-year maps and classification system show (i) an increase in the total number of journals that can be mapped by 9,409 journals (social sciences had a 80% increase, humanities a 119% increase, medical (32%) and natural science (74%)), (ii) a simplification of the map by assigning all but five highly interdisciplinary journals to exactly one discipline, (iii) a more even distribution of journals over the 554 subdisciplines and 13 disciplines when calculating the coefficient of variation, and (iv) a better reflection of journal clusters when compared with paper-level citation data. When evaluating the map with a listing of desirable features for maps of science, the updated map is shown to have higher mapping accuracy, easier understandability as fewer journals are multiply classified, and higher usability for the generation of data overlays, among others.\n      "], "author_display": ["Katy B\u00f6rner", "Richard Klavans", "Michael Patek", "Angela M. Zoss", "Joseph R. Biberstine", "Robert P. Light", "Vincent Larivi\u00e8re", "Kevin W. Boyack"], "article_type": "Research Article", "score": 0.56180567, "title_display": "Design and Update of a Classification System: The UCSD Map of Science", "publication_date": "2012-07-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0039464"}, {"journal": "PLoS Biology", "abstract": ["\n        Neural oscillations are ubiquitous measurements of cognitive processes and\n                    dynamic routing and gating of information. The fundamental and so far unresolved\n                    problem for neuroscience remains to understand how oscillatory activity in the\n                    brain codes information for human cognition. In a biologically relevant\n                    cognitive task, we instructed six human observers to categorize facial\n                    expressions of emotion while we measured the observers' EEG. We combined\n                    state-of-the-art stimulus control with statistical information theory analysis\n                    to quantify how the three parameters of oscillations (i.e., power, phase, and\n                    frequency) code the visual information relevant for behavior in a cognitive\n                    task. We make three points: First, we demonstrate that phase codes considerably\n                    more information (2.4 times) relating to the cognitive task than power. Second,\n                    we show that the conjunction of power and phase coding reflects detailed visual\n                    features relevant for behavioral response\u2014that is, features of facial\n                    expressions predicted by behavior. Third, we demonstrate, in analogy to\n                    communication technology, that oscillatory frequencies in the brain multiplex\n                    the coding of visual features, increasing coding capacity. Together, our\n                    findings about the fundamental coding properties of neural oscillations will\n                    redirect the research agenda in neuroscience by establishing the differential\n                    role of frequency, phase, and amplitude in coding behaviorally relevant\n                    information in the brain.\n      Author Summary: To recognize visual information rapidly, the brain must continuously code\n                    complex, high-dimensional information impinging on the retina, not all of which\n                    is relevant, because a low-dimensional code can be sufficient for both\n                    recognition and behavior (e.g. a fearful expression can be correctly recognized\n                    only from the wide-opened eyes). The oscillatory networks of the brain\n                    dynamically reduce the high-dimensional information into a low dimensional code,\n                    but it remains unclear which aspects of these oscillations produce the low\n                    dimensional code. Here, we measured the EEG of human observers while we\n                    presented them with samples of visual information from expressive faces (happy,\n                    sad, fear, etc.). Using statistical information theory, we extracted the\n                    low-dimensional code that is most informative for correct recognition of each\n                    expression (e.g. the opened mouth for \u201chappy,\u201d the wide opened eyes\n                    for \u201cfear\u201d). Next, we measured how the three parameters of brain\n                    oscillations (frequency, power and phase) code for low-dimensional features.\n                    Surprisingly, we find that phase codes 2.4 times more task information than\n                    power. We also show that the conjunction of power and phase sufficiently codes\n                    the low-dimensional facial features across brain oscillations. These findings\n                    offer a new way of thinking about the differential role of frequency, phase and\n                    amplitude in coding behaviorally relevant information in the brain. "], "author_display": ["Philippe G. Schyns", "Gregor Thut", "Joachim Gross"], "article_type": "Research Article", "score": 0.56159234, "title_display": "Cracking the Code of Oscillatory Activity", "publication_date": "2011-05-17T00:00:00Z", "eissn": "1545-7885", "id": "10.1371/journal.pbio.1001064"}, {"journal": "PLOS ONE", "abstract": ["\nIt is important to cluster heterogeneous information networks. A fast clustering algorithm based on an approximate commute time embedding for heterogeneous information networks with a star network schema is proposed in this paper by utilizing the sparsity of heterogeneous information networks. First, a heterogeneous information network is transformed into multiple compatible bipartite graphs from the compatible point of view. Second, the approximate commute time embedding of each bipartite graph is computed using random mapping and a linear time solver. All of the indicator subsets in each embedding simultaneously determine the target dataset. Finally, a general model is formulated by these indicator subsets, and a fast algorithm is derived by simultaneously clustering all of the indicator subsets using the sum of the weighted distances for all indicators for an identical target object. The proposed fast algorithm, FctClus, is shown to be efficient and generalizable and exhibits high clustering accuracy and fast computation speed based on a theoretic analysis and experimental verification.\n"], "author_display": ["Jing Yang", "Limin Chen", "Jianpei Zhang"], "article_type": "Research Article", "score": 0.5598738, "title_display": "FctClus: A Fast Clustering Algorithm for Heterogeneous Information Networks", "publication_date": "2015-06-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0130086"}, {"journal": "PLOS ONE", "abstract": ["\nThe use of digital information in geological fields is becoming very important. Thus, informatization in geological surveys should not stagnate as a result of the level of data accumulation. The integration and sharing of distributed, multi-source, heterogeneous geological information is an open problem in geological domains. Applications and services use geological spatial data with many features, including being cross-region and cross-domain and requiring real-time updating. As a result of these features, desktop and web-based geographic information systems (GISs) experience difficulties in meeting the demand for geological spatial information. To facilitate the real-time sharing of data and services in distributed environments, a GIS platform that is open, integrative, reconfigurable, reusable and elastic would represent an indispensable tool. The purpose of this paper is to develop a geological cloud-computing platform for integrating and sharing geological information based on a cloud architecture. Thus, the geological cloud-computing platform defines geological ontology semantics; designs a standard geological information framework and a standard resource integration model; builds a peer-to-peer node management mechanism; achieves the description, organization, discovery, computing and integration of the distributed resources; and provides the distributed spatial meta service, the spatial information catalog service, the multi-mode geological data service and the spatial data interoperation service. The geological survey information cloud-computing platform has been implemented, and based on the platform, some geological data services and geological processing services were developed. Furthermore, an iron mine resource forecast and an evaluation service is introduced in this paper.\n"], "author_display": ["Liang Wu", "Lei Xue", "Chaoling Li", "Xia Lv", "Zhanlong Chen", "Mingqiang Guo", "Zhong Xie"], "article_type": "Research Article", "score": 0.5576588, "title_display": "A Geospatial Information Grid Framework for Geological Survey", "publication_date": "2015-12-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0145312"}, {"journal": "PLoS ONE", "abstract": ["\n        In this article, we tackle a challenging problem in quantitative graph theory. We establish relations between graph entropy measures representing the structural information content of networks. In particular, we prove formal relations between quantitative network measures based on Shannon's entropy to study the relatedness of those measures. In order to establish such information inequalities for graphs, we focus on graph entropy measures based on information functionals. To prove such relations, we use known graph classes whose instances have been proven useful in various scientific areas. Our results extend the foregoing work on information inequalities for graphs.\n      "], "author_display": ["Matthias Dehmer", "Lavanya Sivakumar"], "article_type": "Research Article", "score": 0.54801303, "title_display": "Recent Developments in Quantitative Graph Theory: Information Inequalities for Networks", "publication_date": "2012-02-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0031395"}, {"journal": "PLOS ONE", "abstract": ["\nUnderstanding how information about external stimuli is transformed into behavior is one of the central goals of neuroscience. Here we characterize the information flow through a complete sensorimotor circuit: from stimulus, to sensory neurons, to interneurons, to motor neurons, to muscles, to motion. Specifically, we apply a recently developed framework for quantifying information flow to a previously published ensemble of models of salt klinotaxis in the nematode worm Caenorhabditis elegans. Despite large variations in the neural parameters of individual circuits, we found that the overall information flow architecture circuit is remarkably consistent across the ensemble. This suggests structural connectivity is not necessarily predictive of effective connectivity. It also suggests information flow analysis captures general principles of operation for the klinotaxis circuit. In addition, information flow analysis reveals several key principles underlying how the models operate: (1) Interneuron class AIY is responsible for integrating information about positive and negative changes in concentration, and exhibits a strong left/right information asymmetry. (2) Gap junctions play a crucial role in the transfer of information responsible for the information symmetry observed in interneuron class AIZ. (3) Neck motor neuron class SMB implements an information gating mechanism that underlies the circuit\u2019s state-dependent response. (4) The neck carries more information about small changes in concentration than about large ones, and more information about positive changes in concentration than about negative ones. Thus, not all directions of movement are equally informative for the worm. Each of these findings corresponds to hypotheses that could potentially be tested in the worm. Knowing the results of these experiments would greatly refine our understanding of the neural circuit underlying klinotaxis.\n"], "author_display": ["Eduardo J. Izquierdo", "Paul L. Williams", "Randall D. Beer"], "article_type": "Research Article", "score": 0.5382967, "title_display": "Information Flow through a Model of the <i>C. elegans</i> Klinotaxis Circuit", "publication_date": "2015-10-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0140397"}, {"journal": "PLoS ONE", "abstract": ["\nClassical decision theory predicts that people should be indifferent to information that is not useful for making decisions, but this model often fails to describe human behavior. Here we investigate one such scenario, where people desire information about whether an event (the gain/loss of money) will occur even though there is no obvious decision to be made on the basis of this information. We find a curious dual trend: if information is costless, as the probability of the event increases people want the information more; if information is not costless, people's desire for the information peaks at an intermediate probability. People also want information more as the importance of the event increases, and less as the cost of the information increases. We propose a model that explains these results, based on the assumption that people have limited cognitive resources and obtain information about which events will occur so they can determine whether to expend effort planning for them.\n"], "author_display": ["Emma Pierson", "Noah Goodman"], "article_type": "Research Article", "score": 0.5341929, "title_display": "Uncertainty and Denial: A Resource-Rational Model of the Value of Information", "publication_date": "2014-11-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0113342"}, {"journal": "PLoS ONE", "abstract": ["Objective: This study analyzed cost of implementing computer-assisted Clinical Decision Support System (CDSS) in selected health care centres in Ghana. Methods: A descriptive cross sectional study was conducted in the Kassena-Nankana district (KND). CDSS was deployed in selected health centres in KND as an intervention to manage patients attending antenatal clinics and the labour ward. The CDSS users were mainly nurses who were trained. Activities and associated costs involved in the implementation of CDSS (pre-intervention and intervention) were collected for the period between 2009\u20132013 from the provider perspective. The ingredients approach was used for the cost analysis. Costs were grouped into personnel, trainings, overheads (recurrent costs) and equipment costs (capital cost). We calculated cost without annualizing capital cost to represent financial cost and cost with annualizing capital costs to represent economic cost. Results: Twenty-two trained CDSS users (at least 2 users per health centre) participated in the study. Between April 2012 and March 2013, users managed 5,595 antenatal clients and 872 labour clients using the CDSS. We observed a decrease in the proportion of complications during delivery (pre-intervention 10.74% versus post-intervention 9.64%) and a reduction in the number of maternal deaths (pre-intervention 4 deaths versus post-intervention 1 death). The overall financial cost of CDSS implementation was US$23,316, approximately US$1,060 per CDSS user trained. Of the total cost of implementation, 48% (US$11,272) was pre-intervention cost and intervention cost was 52% (US$12,044). Equipment costs accounted for the largest proportion of financial cost: 34% (US$7,917). When economic cost was considered, total cost of implementation was US$17,128\u2013lower than the financial cost by 26.5%. Conclusions: The study provides useful information in the implementation of CDSS at health facilities to enhance health workers' adherence to practice guidelines and taking accurate decisions to improve maternal health care. "], "author_display": ["Maxwell Ayindenaba Dalaba", "Patricia Akweongo", "John Williams", "Happiness Pius Saronga", "Pencho Tonchev", "Rainer Sauerborn", "Nathan Mensah", "Antje Blank", "Jens Kaltschmidt", "Svetla Loukanova"], "article_type": "Research Article", "score": 0.5322909, "title_display": "Costs Associated with Implementation of Computer-Assisted Clinical Decision Support System for Antenatal and Delivery Care: Case Study of Kassena-Nankana District of Northern Ghana", "publication_date": "2014-09-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0106416"}, {"journal": "PLoS ONE", "abstract": ["\nThe automatic clustering of chemical compounds is an important branch of chemoinformatics. In this paper the Asymmetric Clustering Index (Aci) is proposed to assess how well an automatically created partition reflects the reference. The asymmetry allows for a distinction between the fixed reference and the numerically constructed partition. The introduced index is applied to evaluate the quality of hierarchical clustering procedures for 5-HT1A receptor ligands. We find that the most appropriate combination of parameters for the hierarchical clustering of compounds with a determined activity for this biological target is the Klekota Roth fingerprint combined with the complete linkage function and the Buser similarity metric.\n"], "author_display": ["Marek \u015amieja", "Dawid Warszycki", "Jacek Tabor", "Andrzej J. Bojarski"], "article_type": "Research Article", "score": 0.53104657, "title_display": "Asymmetric Clustering Index in a Case Study of 5-HT<sub>1A</sub> Receptor Ligands", "publication_date": "2014-07-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0102069"}, {"journal": "PLoS ONE", "abstract": ["\n        Computer use draws on linguistic abilities. Using this medium thus presents challenges for young people with Specific Language Impairment (SLI) and raises questions of whether computer-based tasks are appropriate for them. We consider theoretical arguments predicting impaired performance and negative outcomes relative to peers without SLI versus the possibility of positive gains. We examine the relationship between frequency of computer use (for leisure and educational purposes) and educational achievement; in particular examination performance at the end of compulsory education and level of educational progress two years later. Participants were 49 young people with SLI and 56 typically developing (TD) young people. At around age 17, the two groups did not differ in frequency of educational computer use or leisure computer use. There were no associations between computer use and educational outcomes in the TD group. In the SLI group, after PIQ was controlled for, educational computer use at around 17 years of age contributed substantially to the prediction of educational progress at 19 years. The findings suggest that educational uses of computers are conducive to educational progress in young people with SLI.\n      "], "author_display": ["Kevin Durkin", "Gina Conti-Ramsden"], "article_type": "Research Article", "score": 0.52891594, "title_display": "Frequency of Educational Computer Use as a Longitudinal Predictor of Educational Outcome in Young People with Specific Language Impairment", "publication_date": "2012-12-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0052194"}, {"journal": "PLoS ONE", "abstract": ["Introduction: In order to monitor the effectiveness of HPV vaccination in Canada the linkage of multiple data registries may be required. These registries may not always be managed by the same organization and, furthermore, privacy legislation or practices may restrict any data linkages of records that can actually be done among registries. The objective of this study was to develop a secure protocol for linking data from different registries and to allow on-going monitoring of HPV vaccine effectiveness. Methods: A secure linking protocol, using commutative hash functions and secure multi-party computation techniques was developed. This protocol allows for the exact matching of records among registries and the computation of statistics on the linked data while meeting five practical requirements to ensure patient confidentiality and privacy. The statistics considered were: odds ratio and its confidence interval, chi-square test, and relative risk and its confidence interval. Additional statistics on contingency tables, such as other measures of association, can be added using the same principles presented. The computation time performance of this protocol was evaluated. Results: The protocol has acceptable computation time and scales linearly with the size of the data set and the size of the contingency table. The worse case computation time for up to 100,000 patients returned by each query and a 16 cell contingency table is less than 4 hours for basic statistics, and the best case is under 3 hours. Discussion: A computationally practical protocol for the secure linking of data from multiple registries has been demonstrated in the context of HPV vaccine initiative impact assessment. The basic protocol can be generalized to the surveillance of other conditions, diseases, or vaccination programs. "], "author_display": ["Khaled El Emam", "Saeed Samet", "Jun Hu", "Liam Peyton", "Craig Earle", "Gayatri C. Jayaraman", "Tom Wong", "Murat Kantarcioglu", "Fida Dankar", "Aleksander Essex"], "article_type": "Research Article", "score": 0.5288669, "title_display": "A Protocol for the Secure Linking of Registries for HPV Surveillance", "publication_date": "2012-07-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0039915"}, {"journal": "PLoS ONE", "abstract": ["\nVoluntary control of information processing is crucial to allocate resources and prioritize the processes that are most important under a given situation; the algorithms underlying such control, however, are often not clear. We investigated possible algorithms of control for the performance of the majority function, in which participants searched for and identified one of two alternative categories (left or right pointing arrows) as composing the majority in each stimulus set. We manipulated the amount (set size of 1, 3, and 5) and content (ratio of left and right pointing arrows within a set) of the inputs to test competing hypotheses regarding mental operations for information processing. Using a novel measure based on computational load, we found that reaction time was best predicted by a grouping search algorithm as compared to alternative algorithms (i.e., exhaustive or self-terminating search). The grouping search algorithm involves sampling and resampling of the inputs before a decision is reached. These findings highlight the importance of investigating the implications of voluntary control via algorithms of mental operations.\n"], "author_display": ["Jin Fan", "Kevin G. Guise", "Xun Liu", "Hongbin Wang"], "article_type": "Research Article", "score": 0.52456033, "title_display": "Searching for the Majority: Algorithms of Voluntary Control", "publication_date": "2008-10-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0003522"}, {"journal": "PLoS ONE", "abstract": ["\n        In living cells, DNA is packaged along with protein and RNA into chromatin. Chemical modifications to nucleotides and histone proteins are added, removed and recognized by multi-functional molecular complexes. Here I define a new computational model, in which chromatin modifications are information units that can be written onto a one-dimensional string of nucleosomes, analogous to the symbols written onto cells of a Turing machine tape, and chromatin-modifying complexes are modeled as read-write rules that operate on a finite set of adjacent nucleosomes. I illustrate the use of this \u201cchromatin computer\u201d to solve an instance of the Hamiltonian path problem. I prove that chromatin computers are computationally universal \u2013 and therefore more powerful than the logic circuits often used to model transcription factor control of gene expression. Features of biological chromatin provide a rich instruction set for efficient computation of nontrivial algorithms in biological time scales. Modeling chromatin as a computer shifts how we think about chromatin function, suggests new approaches to medical intervention, and lays the groundwork for the engineering of a new class of biological computing machines.\n      "], "author_display": ["Barbara Bryant"], "article_type": "Research Article", "score": 0.5238758, "title_display": "Chromatin Computation", "publication_date": "2012-05-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0035703"}, {"journal": "PLoS ONE", "abstract": ["\n        We present a scheme to use external quantum devices using the universal quantum computer previously constructed. We thereby show how the universal quantum computer can utilize networked quantum information resources to carry out local computations. Such information may come from specialized quantum devices or even from remote universal quantum computers. We show how to accomplish this by devising universal quantum computer programs that implement well known oracle based quantum algorithms, namely the Deutsch, Deutsch-Jozsa, and the Grover algorithms using external black-box quantum oracle devices. In the process, we demonstrate a method to map existing quantum algorithms onto the universal quantum computer.\n      "], "author_display": ["Antonio A. Lagana", "Max A. Lohe", "Lorenz von Smekal"], "article_type": "Research Article", "score": 0.52374387, "title_display": "Interfacing External Quantum Devices to a Universal Quantum Computer", "publication_date": "2011-12-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0029417"}, {"journal": "PLoS ONE", "abstract": ["Background: Explosive growth of next-generation sequencing data has resulted in ultra-large-scale data sets and ensuing computational problems. Cloud computing provides an on-demand and scalable environment for large-scale data analysis. Using a MapReduce framework, data and workload can be distributed via a network to computers in the cloud to substantially reduce computational latency. Hadoop/MapReduce has been successfully adopted in bioinformatics for genome assembly, mapping reads to genomes, and finding single nucleotide polymorphisms. Major cloud providers offer Hadoop cloud services to their users. However, it remains technically challenging to deploy a Hadoop cloud for those who prefer to run MapReduce programs in a cluster without built-in Hadoop/MapReduce. Results: We present CloudDOE, a platform-independent software package implemented in Java. CloudDOE encapsulates technical details behind a user-friendly graphical interface, thus liberating scientists from having to perform complicated operational procedures. Users are guided through the user interface to deploy a Hadoop cloud within in-house computing environments and to run applications specifically targeted for bioinformatics, including CloudBurst, CloudBrush, and CloudRS. One may also use CloudDOE on top of a public cloud. CloudDOE consists of three wizards, i.e., Deploy, Operate, and Extend wizards. Deploy wizard is designed to aid the system administrator to deploy a Hadoop cloud. It installs Java runtime environment version 1.6 and Hadoop version 0.20.203, and initiates the service automatically. Operate wizard allows the user to run a MapReduce application on the dashboard list. To extend the dashboard list, the administrator may install a new MapReduce application using Extend wizard. Conclusions: CloudDOE is a user-friendly tool for deploying a Hadoop cloud. Its smart wizards substantially reduce the complexity and costs of deployment, execution, enhancement, and management. Interested users may collaborate to improve the source code of CloudDOE to further incorporate more MapReduce bioinformatics tools into CloudDOE and support next-generation big data open source tools, e.g., Hadoop BigTop and Spark. Availability: CloudDOE is distributed under Apache License 2.0 and is freely available at http://clouddoe.iis.sinica.edu.tw/. "], "author_display": ["Wei-Chun Chung", "Chien-Chih Chen", "Jan-Ming Ho", "Chung-Yen Lin", "Wen-Lian Hsu", "Yu-Chun Wang", "D. T. Lee", "Feipei Lai", "Chih-Wei Huang", "Yu-Jung Chang"], "article_type": "Research Article", "score": 0.5153015, "title_display": "CloudDOE: A User-Friendly Tool for Deploying Hadoop Clouds and Analyzing High-Throughput Sequencing Data with MapReduce", "publication_date": "2014-06-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0098146"}, {"journal": "PLoS ONE", "abstract": ["\nIn this paper, we introduce the Hosoya-Spectral indices and the Hosoya information content of a graph. The first measure combines structural information captured by partial Hosoya polynomials and graph spectra. The latter is a graph entropy measure which is based on blocks consisting of vertices with the same partial Hosoya polynomial. We evaluate the discrimination power of these quantities by interpreting numerical results.\n"], "author_display": ["Matthias Dehmer", "Abbe Mowshowitz", "Yongtang Shi"], "article_type": "Research Article", "score": 0.51161915, "title_display": "Structural Differentiation of Graphs Using Hosoya-Based Indices", "publication_date": "2014-07-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0102459"}, {"journal": "PLOS ONE", "abstract": ["\nOver the past few years, secure and privacy-preserving user authentication scheme has become an integral part of the applications of the healthcare systems. Recently, Wen has designed an improved user authentication system over the Lee et al.\u2019s scheme for integrated electronic patient record (EPR) information system, which has been analyzed in this study. We have found that Wen\u2019s scheme still has the following inefficiencies: (1) the correctness of identity and password are not verified during the login and password change phases; (2) it is vulnerable to impersonation attack and privileged-insider attack; (3) it is designed without the revocation of lost/stolen smart card; (4) the explicit key confirmation and the no key control properties are absent, and (5) user cannot update his/her password without the help of server and secure channel. Then we aimed to propose an enhanced two-factor user authentication system based on the intractable assumption of the quadratic residue problem (QRP) in the multiplicative group. Our scheme bears more securities and functionalities than other schemes found in the literature.\n"], "author_display": ["SK Hafizul Islam", "Muhammad Khurram Khan", "Xiong Li"], "article_type": "Research Article", "score": 0.51005805, "title_display": "Security Analysis and Improvement of \u2018a More Secure Anonymous User Authentication Scheme for the Integrated EPR Information System\u2019", "publication_date": "2015-08-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0131368"}, {"journal": "PLoS ONE", "abstract": ["\nWe relate different self-reported measures of computer use to individuals' propensity to cooperate in the Prisoner's dilemma. The average cooperation rate is positively related to the self-reported amount participants spend playing computer games. None of the other computer time use variables (including time spent on social media, browsing internet, working etc.) are significantly related to cooperation rates.\n"], "author_display": ["Friederike Mengel"], "article_type": "Research Article", "score": 0.5057733, "title_display": "Computer Games and Prosocial Behaviour", "publication_date": "2014-04-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0094099"}, {"journal": "PLoS ONE", "abstract": ["Background: Web-based, free-text documents on science and technology have been increasing growing on the web. However, most of these documents are not immediately processable by computers slowing down the acquisition of useful information. Computational ontologies might represent a possible solution by enabling semantically machine readable data sets. But, the process of ontology creation, instantiation and maintenance is still based on manual methodologies and thus time and cost intensive. Method: We focused on a large corpus containing information on researchers, research fields, and institutions. We based our strategy on traditional entity recognition, social computing and correlation. We devised a semi automatic approach for the recognition, correlation and extraction of named entities and relations from textual documents which are then used to create, instantiate, and maintain an ontology. Results: We present a prototype demonstrating the applicability of the proposed strategy, along with a case study describing how direct and indirect relations can be extracted from academic and professional activities registered in a database of curriculum vitae in free-text format. We present evidence that this system can identify entities to assist in the process of knowledge extraction and representation to support ontology maintenance. We also demonstrate the extraction of relationships among ontology classes and their instances. Conclusion: We have demonstrated that our system can be used for the conversion of research information in free text format into database with a semantic structure. Future studies should test this system using the growing number of free-text information available at the institutional and national levels. "], "author_display": ["Fl\u00e1vio Ceci", "Ricardo Pietrobon", "Alexandre Leopoldo Gon\u00e7alves"], "article_type": "Research Article", "score": 0.50208807, "title_display": "Turning Text into Research Networks: Information Retrieval and Computational Ontologies in the Creation of Scientific Databases", "publication_date": "2012-01-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0027499"}, {"journal": "PLoS ONE", "abstract": ["\n        To facilitate analysis and understanding of biological systems, large-scale data are often integrated into models using a variety of mathematical and computational approaches. Such models describe the dynamics of the biological system and can be used to study the changes in the state of the system over time. For many model classes, such as discrete or continuous dynamical systems, there exist appropriate frameworks and tools for analyzing system dynamics. However, the heterogeneous information that encodes and bridges molecular and cellular dynamics, inherent to fine-grained molecular simulation models, presents significant challenges to the study of system dynamics. In this paper, we present an algorithmic information theory based approach for the analysis and interpretation of the dynamics of such executable models of biological systems. We apply a normalized compression distance (NCD) analysis to the state representations of a model that simulates the immune decision making and immune cell behavior. We show that this analysis successfully captures the essential information in the dynamics of the system, which results from a variety of events including proliferation, differentiation, or perturbations such as gene knock-outs. We demonstrate that this approach can be used for the analysis of executable models, regardless of the modeling framework, and for making experimentally quantifiable predictions.\n      "], "author_display": ["Avital Sadot", "Septimia Sarbu", "Juha Kesseli", "Hila Amir-Kroll", "Wei Zhang", "Matti Nykter", "Ilya Shmulevich"], "article_type": "Research Article", "score": 0.5003224, "title_display": "Information-Theoretic Analysis of the Dynamics of an Executable Biological Model", "publication_date": "2013-03-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0059303"}, {"journal": "PLoS ONE", "abstract": ["\nSo far, many studies on educational games have been carried out in America and Europe. Very few related empirical studies, however, have been conducted in China. This study, combining both quantitative with qualitative research methods, possibly compensated for this regret. The study compared data collected from two randomly selected classes (out of 13 classes) under computer game-based instruction (CGBI) and non-computer game-based instruction (NCGBI), respectively, in a senior high school located in Nanjing, Capital of Jiangsu Province, in China. The participants were 103 students, composed of 52 boys and 51 girls (aged 17-18 years old). The following conclusion was reached: (1) participants under CGBI obtained significantly greater learning achievement than those under NCGBI; (2) participants were significantly more motivated by CGBI compared with NCGBI; (3) there were no significant differences in learning achievement between boys and girls; although (4) boys were significantly more motivated by CGBI than girls. Both disadvantages and advantages were discussed, together with directions for future research.\n"], "author_display": ["Zhonggen Yu", "Wei Hua Yu", "Xiaohui Fan", "Xiao Wang"], "article_type": "Research Article", "score": 0.4986298, "title_display": "An Exploration of Computer Game-Based Instruction in the \u201cWorld History\u201d Class in Secondary Education: A Comparative Study in China", "publication_date": "2014-05-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0096865"}, {"journal": "PLoS Computational Biology", "abstract": ["\n        One of the hallmarks of biological organisms is their ability to integrate disparate information sources to optimize their behavior in complex environments. How this capability can be quantified and related to the functional complexity of an organism remains a challenging problem, in particular since organismal functional complexity is not well-defined. We present here several candidate measures that quantify information and integration, and study their dependence on fitness as an artificial agent (\u201canimat\u201d) evolves over thousands of generations to solve a navigation task in a simple, simulated environment. We compare the ability of these measures to predict high fitness with more conventional information-theoretic processing measures. As the animat adapts by increasing its \u201cfit\u201d to the world, information integration and processing increase commensurately along the evolutionary line of descent. We suggest that the correlation of fitness with information integration and with processing measures implies that high fitness requires both information processing as well as integration, but that information integration may be a better measure when the task requires memory. A correlation of measures of information integration (but also information processing) and fitness strongly suggests that these measures reflect the functional complexity of the animat, and that such measures can be used to quantify functional complexity even in the absence of fitness data.\n      Author Summary: Intelligent behavior encompasses appropriate navigation in complex environments that is achieved through the integration of sensorial information and memory of past events to create purposeful movement. This behavior is often described as \u201ccomplex\u201d, but universal ways to quantify such a notion do not exist. Promising candidates for measures of functional complexity are based on information theory, but fail to take into account the important role that memory plays in complex navigation. Here, we study a different information-theoretic measure called \u201cintegrated information\u201d, and investigate its ability to reflect the complexity of navigation that uses both sensory data and memory. We suggest that measures based on the integrated-information concept correlate better with fitness than other standard measures when memory evolves as a key element in navigation strategy, but perform as well as more standard information processing measures if the robots navigate using a purely reactive sensor-motor loop. We conclude that the integration of information that emanates from the sensorial data stream with some (short-term) memory of past events is crucial to complex and intelligent behavior and speculate that integrated information\u2013to the extent that it can be measured and computed\u2013might best reflect the complexity of animal behavior, including that of humans. "], "author_display": ["Jeffrey A. Edlund", "Nicolas Chaumont", "Arend Hintze", "Christof Koch", "Giulio Tononi", "Christoph Adami"], "article_type": "Research Article", "score": 0.49828, "title_display": "Integrated Information Increases with Fitness in the Evolution of Animats", "publication_date": "2011-10-20T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1002236"}, {"journal": "PLoS ONE", "abstract": ["\n        The information transfer rate provides an objective and rigorous way to quantify how much information is being transmitted through a communications channel whose input and output consist of time-varying signals. However, current estimators of information content in continuous signals are typically based on assumptions about the system's linearity and signal statistics, or they require prohibitive amounts of data. Here we present a novel information rate estimator without these limitations that is also optimized for computational efficiency. We validate the method with a simulated Gaussian information channel and demonstrate its performance with two example applications. Information transfer between the input and output signals of a nonlinear system is analyzed using a sensory receptor neuron as the model system. Then, a climate data set is analyzed to demonstrate that the method can be applied to a system based on two outputs generated by interrelated random processes. These analyses also demonstrate that the new method offers consistent performance in situations where classical methods fail. In addition to these examples, the method is applicable to a wide range of continuous time series commonly observed in the natural sciences, economics and engineering.\n      "], "author_display": ["Jouni Takalo", "Irina Ignatova", "Matti Weckstr\u00f6m", "Mikko V\u00e4h\u00e4s\u00f6yrinki"], "article_type": "Research Article", "score": 0.49798518, "title_display": "A Novel Estimator for the Rate of Information Transfer by Continuous Signals", "publication_date": "2011-04-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0018792"}, {"journal": "PLOS ONE", "abstract": ["\nWe consider a computing system where a master processor assigns a task for execution to worker processors that may collude. We model the workers\u2019 decision of whether to comply (compute the task) or not (return a bogus result to save the computation cost) as a game among workers. That is, we assume that workers are rational in a game-theoretic sense. We identify analytically the parameter conditions for a unique Nash Equilibrium where the master obtains the correct result. We also evaluate experimentally mixed equilibria aiming to attain better reliability-profit trade-offs. For a wide range of parameter values that may be used in practice, our simulations show that, in fact, both master and workers are better off using a pure equilibrium where no worker cheats, even under collusion, and even for colluding behaviors that involve deviating from the game.\n"], "author_display": ["Antonio Fern\u00e1ndez Anta", "Chryssis Georgiou", "Miguel A. Mosteiro", "Daniel Pareja"], "article_type": "Research Article", "score": 0.4971597, "title_display": "Algorithmic Mechanisms for Reliable Crowdsourcing Computation under Collusion", "publication_date": "2015-03-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0116520"}, {"journal": "PLoS ONE", "abstract": ["\nThis paper aims to investigate information-theoretic network complexity measures which have already been intensely used in mathematical- and medicinal chemistry including drug design. Numerous such measures have been developed so far but many of them lack a meaningful interpretation, e.g., we want to examine which kind of structural information they detect. Therefore, our main contribution is to shed light on the relatedness between some selected information measures for graphs by performing a large scale analysis using chemical networks. Starting from several sets containing real and synthetic chemical structures represented by graphs, we study the relatedness between a classical (partition-based) complexity measure called the topological information content of a graph and some others inferred by a different paradigm leading to partition-independent measures. Moreover, we evaluate the uniqueness of network complexity measures numerically. Generally, a high uniqueness is an important and desirable property when designing novel topological descriptors having the potential to be applied to large chemical databases.\n"], "author_display": ["Matthias Dehmer", "Nicola Barbarini", "Kurt Varmuza", "Armin Graber"], "article_type": "Research Article", "score": 0.4961387, "title_display": "A Large Scale Analysis of Information-Theoretic Network Complexity Measures Using Chemical Structures", "publication_date": "2009-12-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0008057"}, {"journal": "PLoS ONE", "abstract": ["\nSemantic technology plays a key role in various domains, from conversation understanding to algorithm analysis. As the most efficient semantic tool, ontology can represent, process and manage the widespread knowledge. Nowadays, many researchers use ontology to collect and organize data's semantic information in order to maximize research productivity. In this paper, we firstly describe our work on the development of a remote sensing data ontology, with a primary focus on semantic fusion-driven research for big data. Our ontology is made up of 1,264 concepts and 2,030 semantic relationships. However, the growth of big data is straining the capacities of current semantic fusion and reasoning practices. Considering the massive parallelism of DNA strands, we propose a novel DNA-based semantic fusion model. In this model, a parallel strategy is developed to encode the semantic information in DNA for a large volume of remote sensing data. The semantic information is read in a parallel and bit-wise manner and an individual bit is converted to a base. By doing so, a considerable amount of conversion time can be saved, i.e., the cluster-based multi-processes program can reduce the conversion time from 81,536 seconds to 4,937 seconds for 4.34 GB source data files. Moreover, the size of result file recording DNA sequences is 54.51 GB for parallel C program compared with 57.89 GB for sequential Perl. This shows that our parallel method can also reduce the DNA synthesis cost. In addition, data types are encoded in our model, which is a basis for building type system in our future DNA computer. Finally, we describe theoretically an algorithm for DNA-based semantic fusion. This algorithm enables the process of integration of the knowledge from disparate remote sensing data sources into a consistent, accurate, and complete representation. This process depends solely on ligation reaction and screening operations instead of the ontology.\n"], "author_display": ["Heng Sun", "Jian Weng", "Guangchuang Yu", "Richard H. Massawe"], "article_type": "Research Article", "score": 0.49491975, "title_display": "A DNA-Based Semantic Fusion Model for Remote Sensing Data", "publication_date": "2013-10-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0077090"}, {"journal": "PLOS Computational Biology", "abstract": ["\nStimulus dimensionality-reduction methods in neuroscience seek to identify a low-dimensional space of stimulus features that affect a neuron\u2019s probability of spiking. One popular method, known as maximally informative dimensions (MID), uses an information-theoretic quantity known as \u201csingle-spike information\u201d to identify this space. Here we examine MID from a model-based perspective. We show that MID is a maximum-likelihood estimator for the parameters of a linear-nonlinear-Poisson (LNP) model, and that the empirical single-spike information corresponds to the normalized log-likelihood under a Poisson model. This equivalence implies that MID does not necessarily find maximally informative stimulus dimensions when spiking is not well described as Poisson. We provide several examples to illustrate this shortcoming, and derive a lower bound on the information lost when spiking is Bernoulli in discrete time bins. To overcome this limitation, we introduce model-based dimensionality reduction methods for neurons with non-Poisson firing statistics, and show that they can be framed equivalently in likelihood-based or information-theoretic terms. Finally, we show how to overcome practical limitations on the number of stimulus dimensions that MID can estimate by constraining the form of the non-parametric nonlinearity in an LNP model. We illustrate these methods with simulations and data from primate visual cortex.\nAuthor Summary: A popular approach to the neural coding problem is to identify a low-dimensional linear projection of the stimulus space that preserves the aspects of the stimulus that affect a neuron\u2019s probability of spiking. Previous work has focused on both information-theoretic and likelihood-based estimators for finding such projections. Here, we show that these two approaches are in fact equivalent. We show that maximally informative dimensions (MID), a popular information-theoretic method for dimensionality reduction, is identical to the maximum-likelihood estimator for a particular linear-nonlinear encoding model with Poisson spiking. One implication of this equivalence is that MID may not find the information-theoretically optimal stimulus projection when spiking is non-Poisson, which we illustrate with a few simple examples. Using these insights, we propose novel dimensionality-reduction methods that incorporate non-Poisson spiking, and suggest new parametrizations that allow for tractable estimation of high-dimensional subspaces. "], "author_display": ["Ross S. Williamson", "Maneesh Sahani", "Jonathan W. Pillow"], "article_type": "Research Article", "score": 0.49398452, "title_display": "The Equivalence of Information-Theoretic and Likelihood-Based Methods for Neural Dimensionality Reduction", "publication_date": "2015-04-01T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1004141"}, {"journal": "PLoS Computational Biology", "abstract": ["\nA recent measure of \u2018integrated information\u2019, \u03a6DM, quantifies the extent to which a system generates more information than the sum of its parts as it transitions between states, possibly reflecting levels of consciousness generated by neural systems. However, \u03a6DM is defined only for discrete Markov systems, which are unusual in biology; as a result, \u03a6DM can rarely be measured in practice. Here, we describe two new measures, \u03a6E and \u03a6AR, that overcome these limitations and are easy to apply to time-series data. We use simulations to demonstrate the in-practice applicability of our measures, and to explore their properties. Our results provide new opportunities for examining information integration in real and model systems and carry implications for relations between integrated information, consciousness, and other neurocognitive processes. However, our findings pose challenges for theories that ascribe physical meaning to the measured quantities.\nAuthor Summary: A key feature of the human brain is its ability to represent a vast amount of information, and to integrate this information in order to produce specific and selective behaviour, as well as a stream of unified conscious scenes. Attempts have been made to quantify so-called \u2018integrated information\u2019 by formalizing in mathematics the extent to which a system as a whole generates more information than the sum of its parts. However, so far, the resulting measures have turned out to be inapplicable to real neural systems. In this paper we introduce two new measures that can be applied to both realistic neural models and to time-series data garnered from a broad range of neuroimaging and electrophysiological methods. Our work provides new opportunities for examining the role of integrated information in cognition and consciousness, and indeed in the function of any complex biological system. However, our results also pose challenges for theories that ascribe a direct physical meaning to any version of integrated information so far described. "], "author_display": ["Adam B. Barrett", "Anil K. Seth"], "article_type": "Research Article", "score": 0.49385905, "title_display": "Practical Measures of Integrated Information for Time-Series Data", "publication_date": "2011-01-20T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1001052"}, {"journal": "PLoS ONE", "abstract": ["\nThe Voynich manuscript has remained so far as a mystery for linguists and cryptologists. While the text written on medieval parchment -using an unknown script system- shows basic statistical patterns that bear resemblance to those from real languages, there are features that suggested to some researches that the manuscript was a forgery intended as a hoax. Here we analyse the long-range structure of the manuscript using methods from information theory. We show that the Voynich manuscript presents a complex organization in the distribution of words that is compatible with those found in real language sequences. We are also able to extract some of the most significant semantic word-networks in the text. These results together with some previously known statistical features of the Voynich manuscript, give support to the presence of a genuine message inside the book.\n"], "author_display": ["Marcelo A. Montemurro", "Dami\u00e1n H. Zanette"], "article_type": "Research Article", "score": 0.493205, "title_display": "Keywords and Co-Occurrence Patterns in the Voynich Manuscript: An Information-Theoretic Analysis", "publication_date": "2013-06-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0066344"}, {"journal": "PLOS Computational Biology", "abstract": ["\nPrevious explanations of computations performed by recurrent networks have focused on symmetrically connected saturating neurons and their convergence toward attractors. Here we analyze the behavior of asymmetrical connected networks of linear threshold neurons, whose positive response is unbounded. We show that, for a wide range of parameters, this asymmetry brings interesting and computationally useful dynamical properties. When driven by input, the network explores potential solutions through highly unstable \u2018expansion\u2019 dynamics. This expansion is steered and constrained by negative divergence of the dynamics, which ensures that the dimensionality of the solution space continues to reduce until an acceptable solution manifold is reached. Then the system contracts stably on this manifold towards its final solution trajectory. The unstable positive feedback and cross inhibition that underlie expansion and divergence are common motifs in molecular and neuronal networks. Therefore we propose that very simple organizational constraints that combine these motifs can lead to spontaneous computation and so to the spontaneous modification of entropy that is characteristic of living systems.\nAuthor Summary: Biological systems are obviously able to process abstract information on the states of neuronal and molecular networks. However, the concepts and principles of such biological computation are poorly understood by comparison with technological computing. A key concept in models of biological computation has been the attractor of dynamical systems, and much progress has been made in describing the conditions under which attractors exist, and their stability. Instead, we show here for a broad class of asymmetrically connected networks that it is the unstable dynamics of the system that drive its computation, and we develop new analytical tools to describe the kinds of unstable dynamics that support this computation in our model. In particular we explore the conditions under which networks will exhibit unstable expansion of their dynamics, and how these can be steered and constrained so that the trajectory implements a specific computation. Importantly, the underlying computational elements of the network are not themselves stable. Instead, the overall boundedness of the system is provided by the asymmetrical coupling between excitatory and inhibitory elements commonly observed in neuronal and molecular networks. This inherent boundedness permits the network to operate with the unstably high gain necessary to continually switch its states as it searches for a solution. We propose that very simple organizational constraints can lead to spontaneous computation, and thereby to the spontaneous modification of entropy that is characteristic of living systems. "], "author_display": ["Ueli Rutishauser", "Jean-Jacques Slotine", "Rodney Douglas"], "article_type": "Research Article", "score": 0.49304965, "title_display": "Computation in Dynamically Bounded Asymmetric Systems", "publication_date": "2015-01-24T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1004039"}, {"journal": "PLoS ONE", "abstract": ["\nSocial networking services (e.g., Twitter, Facebook) are now major sources of World Wide Web (called \u201cWeb\u201d) dynamics, together with Web search services (e.g., Google). These two types of Web services mutually influence each other but generate different dynamics. In this paper, we distinguish two modes of Web dynamics: the reactive mode and the default mode. It is assumed that Twitter messages (called \u201ctweets\u201d) and Google search queries react to significant social movements and events, but they also demonstrate signs of becoming self-activated, thereby forming a baseline Web activity. We define the former as the reactive mode and the latter as the default mode of the Web. In this paper, we investigate these reactive and default modes of the Web's dynamics using transfer entropy (TE). The amount of information transferred between a time series of 1,000 frequent keywords in Twitter and the same keywords in Google queries is investigated across an 11-month time period. Study of the information flow on Google and Twitter revealed that information is generally transferred from Twitter to Google, indicating that Twitter time series have some preceding information about Google time series. We also studied the information flow among different Twitter keywords time series by taking keywords as nodes and flow directions as edges of a network. An analysis of this network revealed that frequent keywords tend to become an information source and infrequent keywords tend to become sink for other keywords. Based on these findings, we hypothesize that frequent keywords form the Web's default mode, which becomes an information source for infrequent keywords that generally form the Web's reactive mode. We also found that the Web consists of different time resolutions with respect to TE among Twitter keywords, which will be another focal point of this paper.\n"], "author_display": ["Mizuki Oka", "Takashi Ikegami"], "article_type": "Research Article", "score": 0.49285927, "title_display": "Exploring Default Mode and Information Flow on the Web", "publication_date": "2013-04-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0060398"}, {"journal": "PLoS Computational Biology", "abstract": ["\n         The multidimensional computations performed by many biological systems are often characterized with limited information about the correlations between inputs and outputs. Given this limitation, our approach is to construct the maximum noise entropy response function of the system, leading to a closed-form and minimally biased model consistent with a given set of constraints on the input/output moments; the result is equivalent to conditional random field models from machine learning. For systems with binary outputs, such as neurons encoding sensory stimuli, the maximum noise entropy models are logistic functions whose arguments depend on the constraints. A constraint on the average output turns the binary maximum noise entropy models into minimum mutual information models, allowing for the calculation of the information content of the constraints and an information theoretic characterization of the system's computations. We use this approach to analyze the nonlinear input/output functions in macaque retina and thalamus; although these systems have been previously shown to be responsive to two input dimensions, the functional form of the response function in this reduced space had not been unambiguously identified. A second order model based on the logistic function is found to be both necessary and sufficient to accurately describe the neural responses to naturalistic stimuli, accounting for an average of 93% of the mutual information with a small number of parameters. Thus, despite the fact that the stimulus is highly non-Gaussian, the vast majority of the information in the neural responses is related to first and second order correlations. Our results suggest a principled and unbiased way to model multidimensional computations and determine the statistics of the inputs that are being encoded in the outputs.\n      Author Summary: Biological systems across many scales, from molecules to ecosystems, can all be considered information processors, detecting important events in their environment and transforming them into actions. Detecting events of interest in the presence of noise and other overlapping events often necessitates the use of nonlinear transformations of inputs. The nonlinear nature of the relationships between inputs and outputs makes it difficult to characterize them experimentally given the limitations imposed by data collection. Here we discuss how minimal models of the nonlinear input/output relationships of information processing systems can be constructed by maximizing a quantity called the noise entropy. The proposed approach can be used to \u201cfocus\u201d the available data by determining which input/output correlations are important and creating the least-biased model consistent with those correlations. We hope that this method will aid the exploration of the computations carried out by complex biological systems and expand our understanding of basic phenomena in the biological world. "], "author_display": ["Jeffrey D. Fitzgerald", "Lawrence C. Sincich", "Tatyana O. Sharpee"], "article_type": "Research Article", "score": 0.4925964, "title_display": "Minimal Models of Multidimensional Computations", "publication_date": "2011-03-24T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1001111"}, {"journal": "PLoS ONE", "abstract": ["\n        This paper explores relationships between classical and parametric measures of graph (or network) complexity. Classical measures are based on vertex decompositions induced by equivalence relations. Parametric measures, on the other hand, are constructed by using information functions to assign probabilities to the vertices. The inequalities established in this paper relating classical and parametric measures lay a foundation for systematic classification of entropy-based measures of graph complexity.\n      "], "author_display": ["Matthias Dehmer", "Abbe Mowshowitz", "Frank Emmert-Streib"], "article_type": "Research Article", "score": 0.48893064, "title_display": "Connections between Classical and Parametric Network Entropies", "publication_date": "2011-01-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0015733"}, {"journal": "PLoS ONE", "abstract": ["\nThis paper introduces a novel algorithm to compute equivalent latitude by applying regions of interest (ROI). The technique is illustrated using code written in Interactive Data Language (IDL). The ROI method is compared with the \u201cpiecewise-constant\u201d method, the approach commonly used in atmospheric sciences, using global fields of atmospheric potential vorticity. The ROI method is considerably more accurate and computationally faster than the piecewise-constant method, and it also works well with irregular grids. Both the ROI and piecewise-constant IDL codes for equivalent latitude are included as a useful reference for the research community.\n"], "author_display": ["Juan A. A\u00f1el", "Douglas R. Allen", "Guadalupe S\u00e1enz", "Luis Gimeno", "Laura de la Torre"], "article_type": "Research Article", "score": 0.4874568, "title_display": "Equivalent Latitude Computation Using Regions of Interest (ROI)", "publication_date": "2013-09-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0072970"}, {"journal": "PLoS ONE", "abstract": ["\nJournal policy on research data and code availability is an important part of the ongoing shift toward publishing reproducible computational science. This article extends the literature by studying journal data sharing policies by year (for both 2011 and 2012) for a referent set of 170 journals. We make a further contribution by evaluating code sharing policies, supplemental materials policies, and open access status for these 170 journals for each of 2011 and 2012. We build a predictive model of open data and code policy adoption as a function of impact factor and publisher and find higher impact journals more likely to have open data and code policies and scientific societies more likely to have open data and code policies than commercial publishers. We also find open data policies tend to lead open code policies, and we find no relationship between open data and code policies and either supplemental material policies or open access journal status. Of the journals in this study, 38% had a data policy, 22% had a code policy, and 66% had a supplemental materials policy as of June 2012. This reflects a striking one year increase of 16% in the number of data policies, a 30% increase in code policies, and a 7% increase in the number of supplemental materials policies. We introduce a new dataset to the community that categorizes data and code sharing, supplemental materials, and open access policies in 2011 and 2012 for these 170 journals.\n"], "author_display": ["Victoria Stodden", "Peixuan Guo", "Zhaokun Ma"], "article_type": "Research Article", "score": 0.48490942, "title_display": "Toward Reproducible Computational Research: An Empirical Analysis of Data and Code Policy Adoption by Journals", "publication_date": "2013-06-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0067111"}, {"journal": "PLoS ONE", "abstract": ["\nIn this paper we derive entropy bounds for hierarchical networks. More precisely, starting from a recently introduced measure to determine the topological entropy of non-hierarchical networks, we provide bounds for estimating the entropy of hierarchical graphs. Apart from bounds to estimate the entropy of a single hierarchical graph, we see that the derived bounds can also be used for characterizing graph classes. Our contribution is an important extension to previous results about the entropy of non-hierarchical networks because for practical applications hierarchical networks are playing an important role in chemistry and biology. In addition to the derivation of the entropy bounds, we provide a numerical analysis for two special graph classes, rooted trees and generalized trees, and demonstrate hereby not only the computational feasibility of our method but also learn about its characteristics and interpretability with respect to data analysis.\n"], "author_display": ["Matthias Dehmer", "Stephan Borgert", "Frank Emmert-Streib"], "article_type": "Research Article", "score": 0.48440868, "title_display": "Entropy Bounds for Hierarchical Molecular Networks", "publication_date": "2008-08-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0003079"}, {"journal": "PLoS ONE", "abstract": ["\nWe designed a program called MolGridCal that can be used to screen small molecule database in grid computing on basis of JPPF grid environment. Based on MolGridCal program, we proposed an integrated strategy for virtual screening and binding mode investigation by combining molecular docking, molecular dynamics (MD) simulations and free energy calculations. To test the effectiveness of MolGridCal, we screened potential ligands for \u03b22 adrenergic receptor (\u03b22AR) from a database containing 50,000 small molecules. MolGridCal can not only send tasks to the grid server automatically, but also can distribute tasks using the screensaver function. As for the results of virtual screening, the known agonist BI-167107 of \u03b22AR is ranked among the top 2% of the screened candidates, indicating MolGridCal program can give reasonable results. To further study the binding mode and refine the results of MolGridCal, more accurate docking and scoring methods are used to estimate the binding affinity for the top three molecules (agonist BI-167107, neutral antagonist alprenolol and inverse agonist ICI 118,551). The results indicate agonist BI-167107 has the best binding affinity. MD simulation and free energy calculation are employed to investigate the dynamic interaction mechanism between the ligands and \u03b22AR. The results show that the agonist BI-167107 also has the lowest binding free energy. This study can provide a new way to perform virtual screening effectively through integrating molecular docking based on grid computing, MD simulations and free energy calculations. The source codes of MolGridCal are freely available at http://molgridcal.codeplex.com.\n"], "author_display": ["Qifeng Bai", "Yonghua Shao", "Dabo Pan", "Yang Zhang", "Huanxiang Liu", "Xiaojun Yao"], "article_type": "Research Article", "score": 0.48131034, "title_display": "Search for \u03b2<sub>2</sub> Adrenergic Receptor Ligands by Virtual Screening via Grid Computing and Investigation of Binding Modes by Docking and Molecular Dynamics Simulations", "publication_date": "2014-09-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0107837"}, {"journal": "PLoS ONE", "abstract": ["\n        The research blog has become a popular mechanism for the quick discussion of scholarly information. However, unlike peer-reviewed journals, the characteristics of this form of scientific discourse are not well understood, for example in terms of the spread of blogger levels of education, gender and institutional affiliations. In this paper we fill this gap by analyzing a sample of blog posts discussing science via an aggregator called ResearchBlogging.org (RB). ResearchBlogging.org aggregates posts based on peer-reviewed research and allows bloggers to cite their sources in a scholarly manner. We studied the bloggers, blog posts and referenced journals of bloggers who posted at least 20 items. We found that RB bloggers show a preference for papers from high-impact journals and blog mostly about research in the life and behavioral sciences. The most frequently referenced journal sources in the sample were: Science, Nature, PNAS and PLoS One. Most of the bloggers in our sample had active Twitter accounts connected with their blogs, and at least 90% of these accounts connect to at least one other RB-related Twitter account. The average RB blogger in our sample is male, either a graduate student or has been awarded a PhD and blogs under his own name.\n      "], "author_display": ["Hadas Shema", "Judit Bar-Ilan", "Mike Thelwall"], "article_type": "Research Article", "score": 0.48020154, "title_display": "Research Blogs and the Discussion of Scholarly Information", "publication_date": "2012-05-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0035869"}, {"journal": "PLOS ONE", "abstract": ["\nTraditional fact checking by expert journalists cannot keep up with the enormous volume of information that is now generated online. Computational fact checking may significantly enhance our ability to evaluate the veracity of dubious information. Here we show that the complexities of human fact checking can be approximated quite well by finding the shortest path between concept nodes under properly defined semantic proximity metrics on knowledge graphs. Framed as a network problem this approach is feasible with efficient computational techniques. We evaluate this approach by examining tens of thousands of claims related to history, entertainment, geography, and biographical information using a public knowledge graph extracted from Wikipedia. Statements independently known to be true consistently receive higher support via our method than do false ones. These findings represent a significant step toward scalable computational fact-checking methods that may one day mitigate the spread of harmful misinformation.\n"], "author_display": ["Giovanni Luca Ciampaglia", "Prashant Shiralkar", "Luis M. Rocha", "Johan Bollen", "Filippo Menczer", "Alessandro Flammini"], "article_type": "Research Article", "score": 0.4800241, "title_display": "Computational Fact Checking from Knowledge Networks", "publication_date": "2015-06-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0128193"}, {"journal": "PLOS ONE", "abstract": ["\nIn recent years, formalization and reasoning of topological relations have become a hot topic as a means to generate knowledge about the relations between spatial objects at the conceptual and geometrical levels. These mechanisms have been widely used in spatial data query, spatial data mining, evaluation of equivalence and similarity in a spatial scene, as well as for consistency assessment of the topological relations of multi-resolution spatial databases. The concept of computational fuzzy topological space is applied to simple fuzzy regions to efficiently and more accurately solve fuzzy topological relations. Thus, extending the existing research and improving upon the previous work, this paper presents a new method to describe fuzzy topological relations between simple spatial regions in Geographic Information Sciences (GIS) and Artificial Intelligence (AI). Firstly, we propose a new definition for simple fuzzy line segments and simple fuzzy regions based on the computational fuzzy topology. And then, based on the new definitions, we also propose a new combinational reasoning method to compute the topological relations between simple fuzzy regions, moreover, this study has discovered that there are (1) 23 different topological relations between a simple crisp region and a simple fuzzy region; (2) 152 different topological relations between two simple fuzzy regions. In the end, we have discussed some examples to demonstrate the validity of the new method, through comparisons with existing fuzzy models, we showed that the proposed method can compute more than the existing models, as it is more expressive than the existing fuzzy models.\n"], "author_display": ["Bo Liu", "Dajun Li", "Yuanping Xia", "Jian Ruan", "Lili Xu", "Huanyi Wu"], "article_type": "Research Article", "score": 0.47869575, "title_display": "Combinational Reasoning of Quantitative Fuzzy Topological Relations for Simple Fuzzy Regions", "publication_date": "2015-03-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0117379"}, {"journal": "PLoS ONE", "abstract": ["\nWe quantify the notion of pattern and formalize the process of pattern discovery under the framework of binary bipartite networks. Patterns of particular focus are interrelated global interactions between clusters on its row and column axes. A binary bipartite network is built into a thermodynamic system embracing all up-and-down spin configurations defined by product-permutations on rows and columns. This system is equipped with its ferromagnetic energy ground state under Ising model potential. Such a ground state, also called a macrostate, is postulated to congregate all patterns of interest embedded within the network data in a multiscale fashion. A new computing paradigm for indirect searching for such a macrostate, called Data Mechanics, is devised by iteratively building a surrogate geometric system with a pair of nearly optimal marginal ultrametrics on row and column spaces. The coupling measure minimizing the Gromov-Wasserstein distance of these two marginal geometries is also seen to be in the vicinity of the macrostate. This resultant coupling geometry reveals multiscale block pattern information that characterizes multiple layers of interacting relationships between clusters on row and on column axes. It is the nonparametric information content of a binary bipartite network. This coupling geometry is then demonstrated to shed new light and bring resolution to interaction issues in community ecology and in gene-content-based phylogenetics. Its implied global inferences are expected to have high potential in many scientific areas.\n"], "author_display": ["Hsieh Fushing", "Chen Chen"], "article_type": "Research Article", "score": 0.4786353, "title_display": "Data Mechanics and Coupling Geometry on Binary Bipartite Networks", "publication_date": "2014-08-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0106154"}, {"journal": "PLoS ONE", "abstract": ["\nAlthough humans have inevitably interacted with both human and artificial intelligence in real life situations, it is unknown whether the human brain engages homologous neurocognitive strategies to cope with both forms of intelligence. To investigate this, we scanned subjects, using functional MRI, while they inferred the reasoning processes conducted by human agents or by computers. We found that the inference of reasoning processes conducted by human agents but not by computers induced increased activity in the precuneus but decreased activity in the ventral medial prefrontal cortex and enhanced functional connectivity between the two brain areas. The findings provide evidence for distinct neurocognitive strategies of taking others' perspective and inhibiting the process referenced to the self that are specific to the comprehension of human intelligence.\n"], "author_display": ["Jianqiao Ge", "Shihui Han"], "article_type": "Research Article", "score": 0.47643146, "title_display": "Distinct Neurocognitive Strategies for Comprehensions of Human and Artificial Intelligence", "publication_date": "2008-07-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0002797"}, {"journal": "PLOS ONE", "abstract": ["\nIn multi-server environments, user authentication is a very important issue because it provides the authorization that enables users to access their data and services; furthermore, remote user authentication schemes for multi-server environments have solved the problem that has arisen from user\u2019s management of different identities and passwords. For this reason, numerous user authentication schemes that are designed for multi-server environments have been proposed over recent years. In 2015, Lu et al. improved upon Mishra et al.\u2019s scheme, claiming that their remote user authentication scheme is more secure and practical; however, we found that Lu et al.\u2019s scheme is still insecure and incorrect. In this paper, we demonstrate that Lu et al.\u2019s scheme is vulnerable to outsider attack and user impersonation attack, and we propose a new biometrics-based scheme for authentication and key agreement that can be used in multi-server environments; then, we show that our proposed scheme is more secure and supports the required security properties.\n"], "author_display": ["Jongho Moon", "Younsung Choi", "Jaewook Jung", "Dongho Won"], "article_type": "Research Article", "score": 0.47520652, "title_display": "An Improvement of Robust Biometrics-Based Authentication and Key Agreement Scheme for Multi-Server Environments Using Smart Cards", "publication_date": "2015-12-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0145263"}, {"journal": "PLoS ONE", "abstract": ["\nIn this paper I introduce computational techniques to extend qualitative analysis into the study of large textual datasets. I demonstrate these techniques by using probabilistic topic modeling to analyze a broad sample of 14,952 documents published in major American newspapers from 1980 through 2012. I show how computational data mining techniques can identify and evaluate the significance of qualitatively distinct subjects of discussion across a wide range of public discourse. I also show how examining large textual datasets with computational methods can overcome methodological limitations of conventional qualitative methods, such as how to measure the impact of particular cases on broader discourse, how to validate substantive inferences from small samples of textual data, and how to determine if identified cases are part of a consistent temporal pattern.\n"], "author_display": ["Michael S. Evans"], "article_type": "Research Article", "score": 0.47433972, "title_display": "A Computational Approach to Qualitative Analysis in Large Textual Datasets", "publication_date": "2014-02-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0087908"}, {"journal": "PLoS Computational Biology", "abstract": ["\nVarious algorithms have been developed for variant calling using next-generation sequencing data, and various methods have been applied to reduce the associated false positive and false negative rates. Few variant calling programs, however, utilize the pedigree information when the family-based sequencing data are available. Here, we present a program, FamSeq, which reduces both false positive and false negative rates by incorporating the pedigree information from the Mendelian genetic model into variant calling. To accommodate variations in data complexity, FamSeq consists of four distinct implementations of the Mendelian genetic model: the Bayesian network algorithm, a graphics processing unit version of the Bayesian network algorithm, the Elston-Stewart algorithm and the Markov chain Monte Carlo algorithm. To make the software efficient and applicable to large families, we parallelized the Bayesian network algorithm that copes with pedigrees with inbreeding loops without losing calculation precision on an NVIDIA graphics processing unit. In order to compare the difference in the four methods, we applied FamSeq to pedigree sequencing data with family sizes that varied from 7 to 12. When there is no inbreeding loop in the pedigree, the Elston-Stewart algorithm gives analytical results in a short time. If there are inbreeding loops in the pedigree, we recommend the Bayesian network method, which provides exact answers. To improve the computing speed of the Bayesian network method, we parallelized the computation on a graphics processing unit. This allowed the Bayesian network method to process the whole genome sequencing data of a family of 12 individuals within two days, which was a 10-fold time reduction compared to the time required for this computation on a central processing unit.\n"], "author_display": ["Gang Peng", "Yu Fan", "Wenyi Wang"], "article_type": "Research Article", "score": 0.47338164, "title_display": "FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units", "publication_date": "2014-10-30T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1003880"}, {"journal": "PLOS ONE", "abstract": ["\nAn analysis of cardiorespiratory dynamics during mental arithmetic, which induces stress, and sustained attention was conducted using information theory. The information storage and internal information of heart rate variability (HRV) were determined respectively as the self-entropy of the tachogram, and the self-entropy of the tachogram conditioned to the knowledge of respiration. The information transfer and cross information from respiration to HRV were assessed as the transfer and cross-entropy, both measures of cardiorespiratory coupling. These information-theoretic measures identified significant nonlinearities in the cardiorespiratory time series. Additionally, it was shown that, although mental stress is related to a reduction in vagal activity, no difference in cardiorespiratory coupling was found when several mental states (rest, mental stress, sustained attention) are compared. However, the self-entropy of HRV conditioned to respiration was very informative to study the predictability of RR interval series during mental tasks, and showed higher predictability during mental arithmetic compared to sustained attention or rest.\n"], "author_display": ["Devy Widjaja", "Alessandro Montalto", "Elke Vlemincx", "Daniele Marinazzo", "Sabine Van Huffel", "Luca Faes"], "article_type": "Research Article", "score": 0.47089022, "title_display": "Cardiorespiratory Information Dynamics during Mental Arithmetic and Sustained Attention", "publication_date": "2015-06-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0129112"}, {"journal": "PLoS ONE", "abstract": ["\nTopological properties of networks are widely applied to study the link-prediction problem recently. Common Neighbors, for example, is a natural yet efficient framework. Many variants of Common Neighbors have been thus proposed to further boost the discriminative resolution of candidate links. In this paper, we reexamine the role of network topology in predicting missing links from the perspective of information theory, and present a practical approach based on the mutual information of network structures. It not only can improve the prediction accuracy substantially, but also experiences reasonable computing complexity.\n"], "author_display": ["Fei Tan", "Yongxiang Xia", "Boyao Zhu"], "article_type": "Research Article", "score": 0.4705441, "title_display": "Link Prediction in Complex Networks: A Mutual Information Perspective", "publication_date": "2014-09-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0107056"}, {"journal": "PLOS ONE", "abstract": ["\nResearchers are perpetually amassing biological sequence data. The computational approaches employed by ecologists for organizing this data (e.g. alignment, phylogeny, etc.) typically scale nonlinearly in execution time with the size of the dataset. This often serves as a bottleneck for processing experimental data since many molecular studies are characterized by massive datasets. To keep up with experimental data demands, ecologists are forced to choose between continually upgrading expensive in-house computer hardware or outsourcing the most demanding computations to the cloud. Outsourcing is attractive since it is the least expensive option, but does not necessarily allow direct user interaction with the data for exploratory analysis. Desktop analytical tools such as ARB are indispensable for this purpose, but they do not necessarily offer a convenient solution for the coordination and integration of datasets between local and outsourced destinations. Therefore, researchers are currently left with an undesirable tradeoff between computational throughput and analytical capability. To mitigate this tradeoff we introduce a software package to leverage the utility of the interactive exploratory tools offered by ARB with the computational throughput of cloud-based resources. Our pipeline serves as middleware between the desktop and the cloud allowing researchers to form local custom databases containing sequences and metadata from multiple resources and a method for linking data outsourced for computation back to the local database. A tutorial implementation of the toolkit is provided in the supporting information, S1 Tutorial. Availability: http://www.ece.drexel.edu/gailr/EESI/tutorial.php.\n"], "author_display": ["Steven D. Essinger", "Erin Reichenberger", "Calvin Morrison", "Christopher B. Blackwood", "Gail L. Rosen"], "article_type": "Research Article", "score": 0.46964747, "title_display": "A Toolkit for ARB to Integrate Custom Databases and Externally Built Phylogenies", "publication_date": "2015-01-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0109277"}, {"journal": "PLoS ONE", "abstract": [": In computational science literature including, e.g., bioinformatics, computational statistics or machine learning, most published articles are devoted to the development of \u201cnew methods\u201d, while comparison studies are generally appreciated by readers but surprisingly given poor consideration by many journals. This paper stresses the importance of neutral comparison studies for the objective evaluation of existing methods and the establishment of standards by drawing parallels with clinical research. The goal of the paper is twofold. Firstly, we present a survey of recent computational papers on supervised classification published in seven high-ranking computational science journals. The aim is to provide an up-to-date picture of current scientific practice with respect to the comparison of methods in both articles presenting new methods and articles focusing on the comparison study itself. Secondly, based on the results of our survey we critically discuss the necessity, impact and limitations of neutral comparison studies in computational sciences. We define three reasonable criteria a comparison study has to fulfill in order to be considered as neutral, and explicate general considerations on the individual components of a \u201ctidy neutral comparison study\u201d. R codes for completely replicating our statistical analyses and figures are available from the companion website http://www.ibe.med.uni-muenchen.de/organisation/mitarbeiter/020_professuren/boulesteix/plea2013. "], "author_display": ["Anne-Laure Boulesteix", "Sabine Lauer", "Manuel J. A. Eugster"], "article_type": "Research Article", "score": 0.4679045, "title_display": "A Plea for Neutral Comparison Studies in Computational Sciences", "publication_date": "2013-04-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0061562"}, {"journal": "PLoS Computational Biology", "abstract": ["\nSocial animals may share information to obtain a more complete and accurate picture of their surroundings. However, physical constraints on communication limit the flow of information between interacting individuals in a way that can cause an accumulation of errors and deteriorated collective behaviors. Here, we theoretically study a general model of information sharing within animal groups. We take an algorithmic perspective to identify efficient communication schemes that are, nevertheless, economic in terms of communication, memory and individual internal computation. We present a simple and natural algorithm in which each agent compresses all information it has gathered into a single parameter that represents its confidence in its behavior. Confidence is communicated between agents by means of active signaling. We motivate this model by novel and existing empirical evidences for confidence sharing in animal groups. We rigorously show that this algorithm competes extremely well with the best possible algorithm that operates without any computational constraints. We also show that this algorithm is minimal, in the sense that further reduction in communication may significantly reduce performances. Our proofs rely on the Cram\u00e9r-Rao bound and on our definition of a Fisher Channel Capacity. We use these concepts to quantify information flows within the group which are then used to obtain lower bounds on collective performance. The abstract nature of our model makes it rigorously solvable and its conclusions highly general. Indeed, our results suggest confidence sharing as a central notion in the context of animal communication.\nAuthor Summary: Cooperative groups are abundant on all scales of the biological world. Despite much empirical evidence on a wide variety of natural communication schemes, there is still a growing need for rigorous tools to quantify and understand the information flows involved. Here, we borrow techniques from information theory and theoretical distributed computing to study information sharing within animal groups. We consider a group of individuals that integrate personal and social information to obtain improved knowledge of their surroundings. We rigorously show that communication between such individuals can be compressed into simple messages that contain an opinion and a corresponding confidence parameter. While this algorithm is extremely efficient, further reduction in communication capacity may greatly hamper collective performances. "], "author_display": ["Amos Korman", "Efrat Greenwald", "Ofer Feinerman"], "article_type": "Research Article", "score": 0.46727484, "title_display": "Confidence Sharing: An Economic Strategy for Efficient Information Flows in Animal Groups", "publication_date": "2014-10-02T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1003862"}, {"journal": "PLoS ONE", "abstract": ["\n        The HUGO Pan-Asian SNP consortium conducted the largest survey to date of human genetic diversity among Asians by sampling 1,719 unrelated individuals among 71 populations from China, India, Indonesia, Japan, Malaysia, the Philippines, Singapore, South Korea, Taiwan, and Thailand. We have constructed a database (PanSNPdb), which contains these data and various new analyses of them. PanSNPdb is a research resource in the analysis of the population structure of Asian peoples, including linkage disequilibrium patterns, haplotype distributions, and copy number variations. Furthermore, PanSNPdb provides an interactive comparison with other SNP and CNV databases, including HapMap3, JSNP, dbSNP and DGV and thus provides a comprehensive resource of human genetic diversity. The information is accessible via a widely accepted graphical interface used in many genetic variation databases. Unrestricted access to PanSNPdb and any associated files is available at: http://www4a.biotec.or.th/PASNP.\n      "], "author_display": ["Chumpol Ngamphiw", "Anunchai Assawamakin", "Shuhua Xu", "Philip J. Shaw", "Jin Ok Yang", "Ho Ghang", "Jong Bhak", "Edison Liu", "Sissades Tongsima", "and the HUGO Pan-Asian SNP Consortium "], "article_type": "Research Article", "score": 0.4670654, "title_display": "PanSNPdb: The Pan-Asian SNP Genotyping Database", "publication_date": "2011-06-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0021451"}, {"journal": "PLoS ONE", "abstract": ["Background: Intricate maps of science have been created from citation data to visualize the structure of scientific activity. However, most scientific publications are now accessed online. Scholarly web portals record detailed log data at a scale that exceeds the number of all existing citations combined. Such log data is recorded immediately upon publication and keeps track of the sequences of user requests (clickstreams) that are issued by a variety of users across many different domains. Given these advantages of log datasets over citation data, we investigate whether they can produce high-resolution, more current maps of science. Methodology: Over the course of 2007 and 2008, we collected nearly 1 billion user interactions recorded by the scholarly web portals of some of the most significant publishers, aggregators and institutional consortia. The resulting reference data set covers a significant part of world-wide use of scholarly web portals in 2006, and provides a balanced coverage of the humanities, social sciences, and natural sciences. A journal clickstream model, i.e. a first-order Markov chain, was extracted from the sequences of user interactions in the logs. The clickstream model was validated by comparing it to the Getty Research Institute's Architecture and Art Thesaurus. The resulting model was visualized as a journal network that outlines the relationships between various scientific domains and clarifies the connection of the social sciences and humanities to the natural sciences. Conclusions: Maps of science resulting from large-scale clickstream data provide a detailed, contemporary view of scientific activity and correct the underrepresentation of the social sciences and humanities that is commonly found in citation data. "], "author_display": ["Johan Bollen", "Herbert Van de Sompel", "Aric Hagberg", "Luis Bettencourt", "Ryan Chute", "Marko A. Rodriguez", "Lyudmila Balakireva"], "article_type": "Research Article", "score": 0.4660108, "title_display": "Clickstream Data Yields High-Resolution Maps of Science", "publication_date": "2009-03-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0004803"}, {"journal": "PLoS ONE", "abstract": ["\nCloud computing is becoming the new generation computing infrastructure, and many cloud vendors provide different types of cloud services. How to choose the best cloud services for specific applications is very challenging. Addressing this challenge requires balancing multiple factors, such as business demands, technologies, policies and preferences in addition to the computing requirements. This paper recommends a mechanism for selecting the best public cloud service at the levels of Infrastructure as a Service (IaaS) and Platform as a Service (PaaS). A systematic framework and associated workflow include cloud service filtration, solution generation, evaluation, and selection of public cloud services. Specifically, we propose the following: a hierarchical information model for integrating heterogeneous cloud information from different providers and a corresponding cloud information collecting mechanism; a cloud service classification model for categorizing and filtering cloud services and an application requirement schema for providing rules for creating application-specific configuration solutions; and a preference-aware solution evaluation mode for evaluating and recommending solutions according to the preferences of application providers. To test the proposed framework and methodologies, a cloud service advisory tool prototype was developed after which relevant experiments were conducted. The results show that the proposed system collects/updates/records the cloud information from multiple mainstream public cloud services in real-time, generates feasible cloud configuration solutions according to user specifications and acceptable cost predication, assesses solutions from multiple aspects (e.g., computing capability, potential cost and Service Level Agreement, SLA) and offers rational recommendations based on user preferences and practical cloud provisioning; and visually presents and compares solutions through an interactive web Graphical User Interface (GUI).\n"], "author_display": ["Zhipeng Gui", "Chaowei Yang", "Jizhe Xia", "Qunying Huang", "Kai Liu", "Zhenlong Li", "Manzhu Yu", "Min Sun", "Nanyin Zhou", "Baoxuan Jin"], "article_type": "Research Article", "score": 0.46598178, "title_display": "A Service Brokering and Recommendation Mechanism for Better Selecting Cloud Services", "publication_date": "2014-08-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0105297"}, {"journal": "PLoS Computational Biology", "abstract": ["\n        Our understanding of most biological systems is in its infancy. Learning their structure and intricacies is fraught with challenges, and often side-stepped in favour of studying the function of different gene products in isolation from their physiological context. Constructing and inferring global mathematical models from experimental data is, however, central to systems biology. Different experimental setups provide different insights into such systems. Here we show how we can combine concepts from Bayesian inference and information theory in order to identify experiments that maximize the information content of the resulting data. This approach allows us to incorporate preliminary information; it is global and not constrained to some local neighbourhood in parameter space and it readily yields information on parameter robustness and confidence. Here we develop the theoretical framework and apply it to a range of exemplary problems that highlight how we can improve experimental investigations into the structure and dynamics of biological systems and their behavior.\n      Author Summary: For most biological signalling and regulatory systems we still lack reliable mechanistic models. And where such models exist, e.g. in the form of differential equations, we typically have only rough estimates for the parameters that characterize the biochemical reactions. In order to improve our knowledge of such systems we require better estimates for these parameters and here we show how judicious choice of experiments, based on a combination of simulations and information theoretical analysis, can help us. Our approach builds on the available, frequently rudimentary information, and identifies which experimental set-up provides most additional information about all the parameters, or individual parameters. We will also consider the related but subtly different problem of which experiments need to be performed in order to decrease the uncertainty about the behaviour of the system under altered conditions. We develop the theoretical framework in the necessary detail before illustrating its use and applying it to the repressilator model, the regulation of Hes1 and signal transduction in the Akt pathway. "], "author_display": ["Juliane Liepe", "Sarah Filippi", "Micha\u0142 Komorowski", "Michael P. H. Stumpf"], "article_type": "Research Article", "score": 0.4652296, "title_display": "Maximizing the Information Content of Experiments in Systems Biology", "publication_date": "2013-01-31T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1002888"}, {"journal": "PLoS ONE", "abstract": ["Objectives: To assess: 1) the feasibility of electronic information provision; 2) gather evidence on the topics and level of detail of information potential research participant\u2019s accessed; 3) to assess satisfaction and understanding. Design: Observational study with an embedded randomised controlled trial. Setting: Low risk intervention study based in primary care. Participants: White British & Irish, South Asian and African-Caribbean subjects aged between 40-74 years eligible for a blood pressure monitoring study. Interventions: PDF copy of the standard paper participant information sheet (PDF-PIS) and an electronic Interactive Information Sheet (IIS) where participants could choose both the type and level of detail accessed. Main Outcome Measures: 1) Proportion of participants providing an email address and accessing electronic information 2) Willingness to participate in a recruitment clinic. 3) Type and depth of information accessed on the IIS. 4) Participant satisfaction and understanding. Results: 1160 participants were eligible for the study. Of these, 276 (24%) provided an active email address, of whom 84 did not respond to the email. 106 responded to the email but chose not to access any electronic information and were therefore ineligible for randomisation. 42 were randomised to receive the PDF-PIS and 44 to receive the IIS (with consent rates of 48% and 36%, respectively; odds ratio 0.6, 95% confidence interval 0.25 to 1.4). Electronic observation of information accessed by potential participants showed 41% chose to access no information and only 9% accessed the detail presented on the Research Ethics Committee approved participant information sheet before booking to attend a recruitment clinic for the intervention study. 63 of the 106 participants (59%) who chose not to access any electronic information also booked an appointment. Conclusions: Current written information about research may not be read, emphasising the importance of the consent interview and consideration of new ways of presenting complex information. "], "author_display": ["Helen M. Kirkby", "Melanie Calvert", "Richard J. McManus", "Heather Draper"], "article_type": "Research Article", "score": 0.46208403, "title_display": "Informing Potential Participants about Research: Observational Study with an Embedded Randomized Controlled Trial", "publication_date": "2013-10-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0076435"}, {"journal": "PLoS ONE", "abstract": ["Backgrounds: Recent explosion of biological data brings a great challenge for the traditional clustering algorithms. With increasing scale of data sets, much larger memory and longer runtime are required for the cluster identification problems. The affinity propagation algorithm outperforms many other classical clustering algorithms and is widely applied into the biological researches. However, the time and space complexity become a great bottleneck when handling the large-scale data sets. Moreover, the similarity matrix, whose constructing procedure takes long runtime, is required before running the affinity propagation algorithm, since the algorithm clusters data sets based on the similarities between data pairs. Methods: Two types of parallel architectures are proposed in this paper to accelerate the similarity matrix constructing procedure and the affinity propagation algorithm. The memory-shared architecture is used to construct the similarity matrix, and the distributed system is taken for the affinity propagation algorithm, because of its large memory size and great computing capacity. An appropriate way of data partition and reduction is designed in our method, in order to minimize the global communication cost among processes. Result: A speedup of 100 is gained with 128 cores. The runtime is reduced from serval hours to a few seconds, which indicates that parallel algorithm is capable of handling large-scale data sets effectively. The parallel affinity propagation also achieves a good performance when clustering large-scale gene data (microarray) and detecting families in large protein superfamilies. "], "author_display": ["Minchao Wang", "Wu Zhang", "Wang Ding", "Dongbo Dai", "Huiran Zhang", "Hao Xie", "Luonan Chen", "Yike Guo", "Jiang Xie"], "article_type": "Research Article", "score": 0.46050534, "title_display": "Parallel Clustering Algorithm for Large-Scale Biological Data Sets", "publication_date": "2014-04-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0091315"}, {"journal": "PLOS ONE", "abstract": ["\nMany scientific questions are best approached by sharing data\u2014collected by different groups or across large collaborative networks\u2014into a combined analysis. Unfortunately, some of the most interesting and powerful datasets\u2014like health records, genetic data, and drug discovery data\u2014cannot be freely shared because they contain sensitive information. In many situations, knowing if private datasets overlap determines if it is worthwhile to navigate the institutional, ethical, and legal barriers that govern access to sensitive, private data. We report the first method of publicly measuring the overlap between private datasets that is secure under a malicious model without relying on private protocols or message passing. This method uses a publicly shareable summary of a dataset\u2019s contents, its cryptoset, to estimate its overlap with other datasets. Cryptosets approach \u201cinformation-theoretic\u201d security, the strongest type of security possible in cryptography, which is not even crackable with infinite computing power. We empirically and theoretically assess both the accuracy of these estimates and the security of the approach, demonstrating that cryptosets are informative, with a stable accuracy, and secure.\n"], "author_display": ["S. Joshua Swamidass", "Matthew Matlock", "Leon Rozenblit"], "article_type": "Research Article", "score": 0.4589328, "title_display": "Securely Measuring the Overlap between Private Datasets with Cryptosets", "publication_date": "2015-02-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0117898"}, {"journal": "PLoS ONE", "abstract": ["\n        In this article, we discuss the problem of establishing relations between information measures for network structures. Two types of entropy based measures namely, the Shannon entropy and its generalization, the R\u00e9nyi entropy have been considered for this study. Our main results involve establishing formal relationships, by means of inequalities, between these two kinds of measures. Further, we also state and prove inequalities connecting the classical partition-based graph entropies and partition-independent entropy measures. In addition, several explicit inequalities are derived for special classes of graphs.\n      "], "author_display": ["Lavanya Sivakumar", "Matthias Dehmer"], "article_type": "Research Article", "score": 0.45879218, "title_display": "Towards Information Inequalities for Generalized Graph Entropies", "publication_date": "2012-06-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0038159"}, {"journal": "PLOS ONE", "abstract": ["\nAggregate signatures allow anyone to combine different signatures signed by different signers on different messages into a short signature. An ideal aggregate signature scheme is an identity-based aggregate signature (IBAS) scheme that supports full aggregation since it can reduce the total transmitted data by using an identity string as a public key and anyone can freely aggregate different signatures. Constructing a secure IBAS scheme that supports full aggregation in bilinear maps is an important open problem. Recently, Yuan et al. proposed such a scheme and claimed its security in the random oracle model under the computational Diffie-Hellman assumption. In this paper, we show that there is an efficient forgery on their IBAS scheme and that their security proof has a serious flaw.\n"], "author_display": ["Kwangsu Lee", "Dong Hoon Lee"], "article_type": "Research Article", "score": 0.45793346, "title_display": "Security Analysis of the Unrestricted Identity-Based Aggregate Signature Scheme", "publication_date": "2015-05-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0128081"}, {"journal": "PLoS ONE", "abstract": ["Background: Medical devices increasingly depend on computing functions such as wireless communication and Internet connectivity for software-based control of therapies and network-based transmission of patients\u2019 stored medical information. These computing capabilities introduce security and privacy risks, yet little is known about the prevalence of such risks within the clinical setting. Methods: We used three comprehensive, publicly available databases maintained by the Food and Drug Administration (FDA) to evaluate recalls and adverse events related to security and privacy risks of medical devices. Results: Review of weekly enforcement reports identified 1,845 recalls; 605 (32.8%) of these included computers, 35 (1.9%) stored patient data, and 31 (1.7%) were capable of wireless communication. Searches of databases specific to recalls and adverse events identified only one event with a specific connection to security or privacy. Software-related recalls were relatively common, and most (81.8%) mentioned the possibility of upgrades, though only half of these provided specific instructions for the update mechanism. Conclusions: Our review of recalls and adverse events from federal government databases reveals sharp inconsistencies with databases at individual providers with respect to security and privacy risks. Recalls related to software may increase security risks because of unprotected update and correction mechanisms. To detect signals of security and privacy problems that adversely affect public health, federal postmarket surveillance strategies should rethink how to effectively and efficiently collect data on security and privacy problems in devices that increasingly depend on computing systems susceptible to malware. "], "author_display": ["Daniel B. Kramer", "Matthew Baker", "Benjamin Ransford", "Andres Molina-Markham", "Quinn Stewart", "Kevin Fu", "Matthew R. Reynolds"], "article_type": "Research Article", "score": 0.45608938, "title_display": "Security and Privacy Qualities of Medical Devices: An Analysis of FDA Postmarket Surveillance", "publication_date": "2012-07-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0040200"}, {"abstract": ["\n        We present a simple and secure system for encrypting and decrypting information using DNA self-assembly. Binary data is encoded in the geometry of DNA nanostructures with two distinct conformations. Removing or leaving out a single component reduces these structures to an encrypted solution of ssDNA, whereas adding back this missing \u201cdecryption key\u201d causes the spontaneous formation of the message through self-assembly, enabling rapid read out via gel electrophoresis. Applications include authentication, secure messaging, and barcoding.\n      "], "author_display": ["Ken Halvorsen", "Wesley P. Wong"], "article_type": "Research Article", "score": 0.45592827, "title_display": "Binary DNA Nanostructures for Data Encryption", "publication_date": "2012-09-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0044212"}, {"journal": "PLoS Computational Biology", "abstract": ["\n                Recent studies of cellular networks have revealed modular organizations of genes\n                    and proteins. For example, in interactome networks, a module refers to a group\n                    of interacting proteins that form molecular complexes and/or biochemical\n                    pathways and together mediate a biological process. However, it is still poorly\n                    understood how biological information is transmitted between different modules.\n                    We have developed information flow analysis, a new\n                    computational approach that identifies proteins central to the transmission of\n                    biological information throughout the network. In the information flow analysis,\n                    we represent an interactome network as an electrical circuit, where interactions\n                    are modeled as resistors and proteins as interconnecting junctions. Construing\n                    the propagation of biological signals as flow of electrical current, our method\n                    calculates an information flow score for every protein. Unlike\n                    previous metrics of network centrality such as degree or betweenness that only\n                    consider topological features, our approach incorporates confidence scores of\n                    protein\u2013protein interactions and automatically considers all possible\n                    paths in a network when evaluating the importance of each protein. We apply our\n                    method to the interactome networks of Saccharomyces cerevisiae\n                    and Caenorhabditis elegans. We find that the likelihood of\n                    observing lethality and pleiotropy when a protein is eliminated is positively\n                    correlated with the protein's information flow score. Even among\n                    proteins of low degree or low betweenness, high information scores serve as a\n                    strong predictor of loss-of-function lethality or pleiotropy. The correlation\n                    between information flow scores and phenotypes supports our hypothesis that the\n                    proteins of high information flow reside in central positions in interactome\n                    networks. We also show that the ranks of information flow scores are more\n                    consistent than that of betweenness when a large amount of noisy data is added\n                    to an interactome. Finally, we combine gene expression data with interaction\n                    data in C. elegans and construct an interactome network for\n                    muscle-specific genes. We find that genes that rank high in terms of information\n                    flow in the muscle interactome network but not in the entire network tend to\n                    play important roles in muscle function. This framework for studying\n                    tissue-specific networks by the information flow model can be applied to other\n                    tissues and other organisms as well.\n            Author Summary: Protein\u2013protein interactions mediate numerous biological processes. In\n                    the last decade, there have been efforts to comprehensively map\n                    protein\u2013protein interactions occurring in an organism. The interaction\n                    data generated from these high-throughput projects can be represented as\n                    interconnected networks. It has been found that knockouts of proteins residing\n                    in topologically central positions in the networks more likely result in\n                    lethality of the organism than knockouts of peripheral proteins. However, it is\n                    difficult to accurately define topologically central proteins because\n                    high-throughput data is error-prone and some interactions are not as reliable as\n                    others. In addition, the architecture of interaction networks varies in\n                    different tissues for multi-cellular organisms. To this end, we present a novel\n                    computational approach to identify central proteins while considering the\n                    confidence of data and gene expression in tissues. Moreover, our approach takes\n                    into account multiple alternative paths in interaction networks. We apply our\n                    method to yeast and nematode interaction networks. We find that the likelihood\n                    of observing lethality and pleiotropy when a given protein is eliminated\n                    correlates better with our centrality score for that protein than with its\n                    scores based on traditional centrality metrics. Finally, we set up a framework\n                    to identify central proteins in tissue-specific interaction networks. "], "author_display": ["Patrycja Vasilyev Missiuro", "Kesheng Liu", "Lihua Zou", "Brian C. Ross", "Guoyan Zhao", "Jun S. Liu", "Hui Ge"], "article_type": "Research Article", "score": 0.45544523, "title_display": "Information Flow Analysis of Interactome Networks", "publication_date": "2009-04-10T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1000350"}, {"journal": "PLoS Computational Biology", "abstract": ["\nInformation is encoded in neural circuits using both graded and action potentials, converting between them within single neurons and successive processing layers. This conversion is accompanied by information loss and a drop in energy efficiency. We investigate the biophysical causes of this loss of information and efficiency by comparing spiking neuron models, containing stochastic voltage-gated Na+ and K+ channels, with generator potential and graded potential models lacking voltage-gated Na+ channels. We identify three causes of information loss in the generator potential that are the by-product of action potential generation: (1) the voltage-gated Na+ channels necessary for action potential generation increase intrinsic noise and (2) introduce non-linearities, and (3) the finite duration of the action potential creates a \u2018footprint\u2019 in the generator potential that obscures incoming signals. These three processes reduce information rates by \u223c50% in generator potentials, to \u223c3 times that of spike trains. Both generator potentials and graded potentials consume almost an order of magnitude less energy per second than spike trains. Because of the lower information rates of generator potentials they are substantially less energy efficient than graded potentials. However, both are an order of magnitude more efficient than spike trains due to the higher energy costs and low information content of spikes, emphasizing that there is a two-fold cost of converting analogue to digital; information loss and cost inflation.\nAuthor Summary: As in electronics, many of the brain's neural circuits convert continuous time signals into a discrete-time binary code. Although some neurons use only graded voltage signals, most convert these signals into discrete-time action potentials. Yet the costs and benefits associated with such a switch in signalling mechanism are largely unexplored. We investigate why the conversion of graded potentials to action potentials is accompanied by substantial information loss and how this changes energy efficiency. Action potentials are generated by a large cohort of noisy Na+ channels. We show that this channel noise and the added non-linearity of Na+ channels destroy input information provided by graded generator potentials. Furthermore, action potentials themselves cause information loss due to their finite widths because the neuron is oblivious to the input that is arriving during an action potential. Consequently, neurons with high firing rates lose a large amount of the information in their inputs. The additional cost incurred by voltage-gated Na+ channels also means that action potentials can encode less information per unit energy, proving metabolically inefficient, and suggesting penalisation of high firing rates in the nervous system. "], "author_display": ["Biswa Sengupta", "Simon Barry Laughlin", "Jeremy Edward Niven"], "article_type": "Research Article", "score": 0.45435935, "title_display": "Consequences of Converting Graded to Action Potentials upon Neural Information Coding and Energy Efficiency", "publication_date": "2014-01-23T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1003439"}, {"journal": "PLoS ONE", "abstract": ["\nRecommender systems are designed to assist individual users to navigate through the rapidly growing amount of information. One of the most successful recommendation techniques is the collaborative filtering, which has been extensively investigated and has already found wide applications in e-commerce. One of challenges in this algorithm is how to accurately quantify the similarities of user pairs and item pairs. In this paper, we employ the multidimensional scaling (MDS) method to measure the similarities between nodes in user-item bipartite networks. The MDS method can extract the essential similarity information from the networks by smoothing out noise, which provides a graphical display of the structure of the networks. With the similarity measured from MDS, we find that the item-based collaborative filtering algorithm can outperform the diffusion-based recommendation algorithms. Moreover, we show that this method tends to recommend unpopular items and increase the global diversification of the networks in long term.\n"], "author_display": ["Wei Zeng", "An Zeng", "Hao Liu", "Ming-Sheng Shang", "Yi-Cheng Zhang"], "article_type": "Research Article", "score": 0.45413873, "title_display": "Similarity from Multi-Dimensional Scaling: Solving the Accuracy and Diversity Dilemma in Information Filtering", "publication_date": "2014-10-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0111005"}, {"journal": "PLoS ONE", "abstract": ["\nThe increasing public availability of personal complete genome sequencing data has ushered in an era of democratized genomics. However, read mapping and variant calling software is constantly improving and individuals with personal genomic data may prefer to customize and update their variant calls. Here, we describe STORMSeq (Scalable Tools for Open-Source Read Mapping), a graphical interface cloud computing solution that does not require a parallel computing environment or extensive technical experience. This customizable and modular system performs read mapping, read cleaning, and variant calling and annotation. At present, STORMSeq costs approximately $2 and 5\u201310 hours to process a full exome sequence and $30 and 3\u20138 days to process a whole genome sequence. We provide this open-access and open-source resource as a user-friendly interface in Amazon EC2.\n"], "author_display": ["Konrad J. Karczewski", "Guy Haskin Fernald", "Alicia R. Martin", "Michael Snyder", "Nicholas P. Tatonetti", "Joel T. Dudley"], "article_type": "Research Article", "score": 0.4526131, "title_display": "STORMSeq: An Open-Source, User-Friendly Pipeline for Processing Personal Genomics Data in the Cloud", "publication_date": "2014-01-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0084860"}, {"journal": "PLoS ONE", "abstract": ["\n        There has been an increasing interest in the geographic aspects of economic development, exemplified by P. Krugman\u2019s logical analysis. We show in this paper that the geographic aspects of economic development can be modeled using multi-agent systems that incorporate multiple underlying factors. The extent of information sharing is assumed to be a driving force that leads to economic geographic heterogeneity across locations without geographic advantages or disadvantages. We propose an agent-based market model that considers a spectrum of different information-sharing mechanisms: no information sharing, information sharing among friends and pheromone-like information sharing. Finally, we build a unified model that accommodates all three of these information-sharing mechanisms based on the number of friends who can share information. We find that the no information-sharing model does not yield large economic zones, and more information sharing can give rise to a power-law distribution of market size that corresponds to the stylized fact of city size and firm size distributions. The simulations show that this model is robust. This paper provides an alternative approach to studying economic geographic development, and this model could be used as a test bed to validate the detailed assumptions that regulate real economic agglomeration.\n      "], "author_display": ["Qianqian Li", "Tao Yang", "Erbo Zhao", "Xing\u2019ang Xia", "Zhangang Han"], "article_type": "Research Article", "score": 0.45247084, "title_display": "The Impacts of Information-Sharing Mechanisms on Spatial Market Formation Based on Agent-Based Modeling", "publication_date": "2013-03-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0058270"}, {"journal": "PLoS ONE", "abstract": ["\nThis study investigated the interaction between remembered landmark and path integration strategies for estimating current location when walking in an environment without vision. We asked whether observers navigating without vision only rely on path integration information to judge their location, or whether remembered landmarks also influence judgments. Participants estimated their location in a hallway after viewing a target (remembered landmark cue) and then walking blindfolded to the same or a conflicting location (path integration cue). We found that participants averaged remembered landmark and path integration information when they judged that both sources provided congruent information about location, which resulted in more precise estimates compared to estimates made with only path integration. In conclusion, humans integrate remembered landmarks and path integration in a gated fashion, dependent on the congruency of the information. Humans can flexibly combine information about remembered landmarks with path integration cues while navigating without visual information.\n"], "author_display": ["Amy A. Kalia", "Paul R. Schrater", "Gordon E. Legge"], "article_type": "Research Article", "score": 0.45209774, "title_display": "Combining Path Integration and Remembered Landmarks When Navigating without Vision", "publication_date": "2013-09-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0072170"}, {"journal": "PLOS ONE", "abstract": ["\nWe investigate a multi-agent patrolling problem where information is distributed alongside threats in environments with uncertainties. Specifically, the information and threat at each location are independently modelled as multi-state Markov chains, whose states are not observed until the location is visited by an agent. While agents will obtain information at a location, they may also suffer damage from the threat at that location. Therefore, the goal of the agents is to gather as much information as possible while mitigating the damage incurred. To address this challenge, we formulate the single-agent patrolling problem as a Partially Observable Markov Decision Process (POMDP) and propose a computationally efficient algorithm to solve this model. Building upon this, to compute patrols for multiple agents, the single-agent algorithm is extended for each agent with the aim of maximising its marginal contribution to the team. We empirically evaluate our algorithm on problems of multi-agent patrolling and show that it outperforms a baseline algorithm up to 44% for 10 agents and by 21% for 15 agents in large domains.\n"], "author_display": ["Shaofei Chen", "Feng Wu", "Lincheng Shen", "Jing Chen", "Sarvapali D. Ramchurn"], "article_type": "Research Article", "score": 0.45196545, "title_display": "Multi-Agent Patrolling under Uncertainty and Threats", "publication_date": "2015-06-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0130154"}, {"journal": "PLoS ONE", "abstract": ["Psychophysical studies suggest that humans preferentially use a narrow band of low spatial frequencies for face recognition. Here we asked whether artificial face recognition systems have an improved recognition performance at the same spatial frequencies as humans. To this end, we estimated recognition performance over a large database of face images by computing three discriminability measures: Fisher Linear Discriminant Analysis, Non-Parametric Discriminant Analysis, and Mutual Information. In order to address frequency dependence, discriminabilities were measured as a function of (filtered) image size. All three measures revealed a maximum at the same image sizes, where the spatial frequency content corresponds to the psychophysical found frequencies. Our results therefore support the notion that the critical band of spatial frequencies for face recognition in humans and machines follows from inherent properties of face images, and that the use of these frequencies is associated with optimal face recognition performance."], "author_display": ["Matthias S. Keil", "Agata Lapedriza", "David Masip", "Jordi Vitria"], "article_type": "Research Article", "score": 0.45175102, "title_display": "Preferred Spatial Frequencies for Human Face Processing Are Associated with Optimal Class Discrimination in the Machine", "publication_date": "2008-07-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0002590"}, {"journal": "PLOS ONE", "abstract": ["\nFast and computationally less complex feature extraction for moving object detection using aerial images from unmanned aerial vehicles (UAVs) remains as an elusive goal in the field of computer vision research. The types of features used in current studies concerningmoving object detection are typically chosen based on improving detection rate rather than on providing fast and computationally less complex feature extraction methods. Because moving object detection using aerial images from UAVs involves motion as seen from a certain altitude, effective and fast feature extraction is a vital issue for optimum detection performance. This research proposes a two-layer bucket approach based on a new feature extraction algorithm referred to as the moment-based feature extraction algorithm (MFEA). Because a moment represents thecoherent intensity of pixels and motion estimation is a motion pixel intensity measurement, this research used this relation to develop the proposed algorithm. The experimental results reveal the successful performance of the proposed MFEA algorithm and the proposed methodology.\n"], "author_display": ["A. F. M. Saifuddin Saif", "Anton Satria Prabuwono", "Zainal Rasyid Mahayuddin"], "article_type": "Research Article", "score": 0.4516389, "title_display": "Moment Feature Based Fast Feature Extraction Algorithm for Moving Object Detection Using Aerial Images", "publication_date": "2015-06-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0126212"}, {"journal": "PLOS Computational Biology", "abstract": ["\nStochastic fluctuations in signaling and gene expression limit the ability of cells to sense the state of their environment, transfer this information along cellular pathways, and respond to it with high precision. Mutual information is now often used to quantify the fidelity with which information is transmitted along a cellular pathway. Mutual information calculations from experimental data have mostly generated low values, suggesting that cells might have relatively low signal transmission fidelity. In this work, we demonstrate that mutual information calculations might be artificially lowered by cell-to-cell variability in both initial conditions and slowly fluctuating global factors across the population. We carry out our analysis computationally using a simple signaling pathway and demonstrate that in the presence of slow global fluctuations, every cell might have its own high information transmission capacity but that population averaging underestimates this value. We also construct a simple synthetic transcriptional network and demonstrate using experimental measurements coupled to computational modeling that its operation is dominated by slow global variability, and hence that its mutual information is underestimated by a population averaged calculation.\nAuthor Summary: This work demonstrates how different sources of variability within biochemical networks impact the interpretation of information transmission. These sources are the intrinsic noise generated within the pathway of a single cell, variability due to initial conditions and/or global parameters across the population. A theoretical analysis of a simple signaling pathway and experimental exploration of a synthetic circuit are used to discuss the contributions of these sources of variability to information transmission using mutual information as a metric. "], "author_display": ["Michael Chevalier", "Ophelia Venturelli", "Hana El-Samad"], "article_type": "Research Article", "score": 0.45148057, "title_display": "The Impact of Different Sources of Fluctuations on Mutual Information in Biochemical Networks", "publication_date": "2015-10-20T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1004462"}, {"journal": "PLoS Computational Biology", "abstract": ["\nIn many settings, copying, learning from or assigning value to group behavior is rational because such behavior can often act as a proxy for valuable returns. However, such herd behavior can also be pathologically misleading by coaxing individuals into behaviors that are otherwise irrational and it may be one source of the irrational behaviors underlying market bubbles and crashes. Using a two-person tandem investment game, we sought to examine the neural and behavioral responses of herd instincts in situations stripped of the incentive to be influenced by the choices of one's partner. We show that the investments of the two subjects correlate over time if they are made aware of their partner's choices even though these choices have no impact on either player's earnings. We computed an \u201cinterpersonal prediction error\u201d, the difference between the investment decisions of the two subjects after each choice. BOLD responses in the striatum, implicated in valuation and action selection, were highly correlated with this interpersonal prediction error. The revelation of the partner's investment occurred after all useful information about the market had already been revealed. This effect was confirmed in two separate experiments where the impact of the time of revelation of the partner's choice was tested at 2 seconds and 6 seconds after a subject's choice; however, the effect was absent in a control condition with a computer partner. These findings strongly support the existence of mechanisms that drive correlated behavior even in contexts where there is no explicit advantage to do so.\nAuthor Summary: In this study we examine the neural substrates of inter-personal error signals on behavior in an investment task using real historical markets. We show that behaviorally, subjects correlate their investments, despite the fact that another trader has no extra information about how the market may move. These behavioral results are supported by neural data showing large, parametric responses in brain areas related to reward and learning when information about another trader's behavior is revealed, even though this occurs after all useful information about the market has already been shown. These results promise to elucidate some of the subconscious processes that guide people to correlate their behavior in markets and other group environments. "], "author_display": ["Terry Lohrenz", "Meghana Bhatt", "Nathan Apple", "P. Read Montague"], "article_type": "Research Article", "score": 0.45032355, "title_display": "Keeping up with the Joneses: Interpersonal Prediction Errors and the Correlation of Behavior in a Tandem Sequential Choice Task", "publication_date": "2013-10-24T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1003275"}, {"journal": "PLoS ONE", "abstract": ["\nA major bottleneck in biological discovery is now emerging at the computational level. Cloud computing offers a dynamic means whereby small and medium-sized laboratories can rapidly adjust their computational capacity. We benchmarked two established cloud computing services, Amazon Web Services Elastic MapReduce (EMR) on Amazon EC2 instances and Google Compute Engine (GCE), using publicly available genomic datasets (E.coli CC102 strain and a Han Chinese male genome) and a standard bioinformatic pipeline on a Hadoop-based platform. Wall-clock time for complete assembly differed by 52.9% (95% CI: 27.5\u201378.2) for E.coli and 53.5% (95% CI: 34.4\u201372.6) for human genome, with GCE being more efficient than EMR. The cost of running this experiment on EMR and GCE differed significantly, with the costs on EMR being 257.3% (95% CI: 211.5\u2013303.1) and 173.9% (95% CI: 134.6\u2013213.1) more expensive for E.coli and human assemblies respectively. Thus, GCE was found to outperform EMR both in terms of cost and wall-clock time. Our findings confirm that cloud computing is an efficient and potentially cost-effective alternative for analysis of large genomic datasets. In addition to releasing our cost-effectiveness comparison, we present available ready-to-use scripts for establishing Hadoop instances with Ganglia monitoring on EC2 or GCE.\n"], "author_display": ["Seyhan Yazar", "George E. C. Gooden", "David A. Mackey", "Alex W. Hewitt"], "article_type": "Research Article", "score": 0.45004773, "title_display": "Benchmarking Undedicated Cloud Computing Providers for Analysis of Genomic Datasets", "publication_date": "2014-09-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0108490"}, {"journal": "PLoS ONE", "abstract": ["\nRecently, contagion-based (disease, information, etc.) spreading on social networks has been extensively studied. In this paper, other than traditional full interaction, we propose a partial interaction based spreading model, considering that the informed individuals would transmit information to only a certain fraction of their neighbors due to the transmission ability in real-world social networks. Simulation results on three representative networks (BA, ER, WS) indicate that the spreading efficiency is highly correlated with the network heterogeneity. In addition, a special phenomenon, namely Information Blind Areas where the network is separated by several information-unreachable clusters, will emerge from the spreading process. Furthermore, we also find that the size distribution of such information blind areas obeys power-law-like distribution, which has very similar exponent with that of site percolation. Detailed analyses show that the critical value is decreasing along with the network heterogeneity for the spreading process, which is complete the contrary to that of random selection. Moreover, the critical value in the latter process is also larger than that of the former for the same network. Those findings might shed some lights in in-depth understanding the effect of network properties on information spreading.\n"], "author_display": ["Zi-Ke Zhang", "Chu-Xu Zhang", "Xiao-Pu Han", "Chuang Liu"], "article_type": "Research Article", "score": 0.45004213, "title_display": "Emergence of Blind Areas in Information Spreading", "publication_date": "2014-04-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0095785"}, {"journal": "PLoS ONE", "abstract": ["\nAn aggregate signature scheme allows anyone to compress multiple individual signatures from various users into a single compact signature. The main objective of such a scheme is to reduce the costs on storage, communication and computation. However, among existing aggregate signature schemes in the identity-based setting, some of them fail to achieve constant-length aggregate signature or require a large amount of pairing operations which grows linearly with the number of signers, while others have some limitations on the aggregated signatures. The main challenge in building efficient aggregate signature scheme is to compress signatures into a compact, constant-length signature without any restriction. To address the above drawbacks, by using the bilinear pairings, we propose an efficient unrestricted identity-based aggregate signature. Our scheme achieves both full aggregation and constant pairing computation. We prove that our scheme has existential unforgeability under the computational Diffie-Hellman assumption.\n"], "author_display": ["Yumin Yuan", "Qian Zhan", "Hua Huang"], "article_type": "Research Article", "score": 0.448422, "title_display": "Efficient Unrestricted Identity-Based Aggregate Signature Scheme", "publication_date": "2014-10-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0110100"}, {"journal": "PLoS ONE", "abstract": ["\nData summarization and triage is one of the current top challenges in visual analytics. The goal is to let users visually inspect large data sets and examine or request data with particular characteristics. The need for summarization and visual analytics is also felt when dealing with digital representations of DNA sequences. Genomic data sets are growing rapidly, making their analysis increasingly more difficult, and raising the need for new, scalable tools. For example, being able to look at very large DNA sequences while immediately identifying potentially interesting regions would provide the biologist with a flexible exploratory and analytical tool. In this paper we present a new concept, the \u201cinformation profile\u201d, which provides a quantitative measure of the local complexity of a DNA sequence, independently of the direction of processing. The computation of the information profiles is computationally tractable: we show that it can be done in time proportional to the length of the sequence. We also describe a tool to compute the information profiles of a given DNA sequence, and use the genome of the fission yeast Schizosaccharomyces pombe strain 972 h\u2212 and five human chromosomes 22 for illustration. We show that information profiles are useful for detecting large-scale genomic regularities by visual inspection. Several discovery strategies are possible, including the standalone analysis of single sequences, the comparative analysis of sequences from individuals from the same species, and the comparative analysis of sequences from different organisms. The comparison scale can be varied, allowing the users to zoom-in on specific details, or obtain a broad overview of a long segment. Software applications have been made available for non-commercial use at http://bioinformatics.ua.pt/software/dna-at-glance.\n"], "author_display": ["Armando J. Pinho", "Sara P. Garcia", "Diogo Pratas", "Paulo J. S. G. Ferreira"], "article_type": "Research Article", "score": 0.44791776, "title_display": "DNA Sequences at a Glance", "publication_date": "2013-11-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0079922"}, {"journal": "PLOS ONE", "abstract": ["\nScientific collaboration has been studied by researchers for decades. Several approaches have been adopted to address the question of how collaboration has evolved in terms of publication output, numbers of coauthors, and multidisciplinary trends. One particular type of collaboration that has received very little attention concerns advisor and advisee relationships. In this paper, we examine this relationship for the researchers who are involved in the area of Exact and Earth Sciences in Brazil and its eight subareas. These pairs are registered in the Lattes Platform that manages the individual curricula vitae of Brazilian researchers. The individual features of these academic researchers and their coauthoring relationships were investigated. We have found evidence that there exists positive correlation between time of advisor\u2013advisee relationship with the advisee\u2019s productivity. Additionally, there has been a gradual decline in advisor\u2013advisee coauthoring over a number of years as measured by the Kulczynski index, which could be interpreted as decline of the dependence.\n"], "author_display": ["Esteban F. Tuesta", "Karina V. Delgado", "Rog\u00e9rio Mugnaini", "Luciano A. Digiampietri", "Jes\u00fas P. Mena-Chalco", "Jos\u00e9 J. P\u00e9rez-Alc\u00e1zar"], "article_type": "Research Article", "score": 0.44659576, "title_display": "Analysis of an Advisor\u2013Advisee Relationship: An Exploratory Study of the Area of Exact and Earth Sciences in Brazil", "publication_date": "2015-05-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0129065"}, {"journal": "PLoS ONE", "abstract": ["\n         The existing certificateless signcryption schemes were designed mainly based on the traditional public key cryptography, in which the security relies on the hard problems, such as factor decomposition and discrete logarithm. However, these problems will be easily solved by the quantum computing. So the existing certificateless signcryption schemes are vulnerable to the quantum attack. Multivariate public key cryptography (MPKC), which can resist the quantum attack, is one of the alternative solutions to guarantee the security of communications in the post-quantum age. Motivated by these concerns, we proposed a new construction of the certificateless multi-receiver signcryption scheme (CLMSC) based on MPKC. The new scheme inherits the security of MPKC, which can withstand the quantum attack. Multivariate quadratic polynomial operations, which have lower computation complexity than bilinear pairing operations, are employed in signcrypting a message for a certain number of receivers in our scheme. Security analysis shows that our scheme is a secure MPKC-based scheme. We proved its security under the hardness of the Multivariate Quadratic (MQ) problem and its unforgeability under the Isomorphism of Polynomials (IP) assumption in the random oracle model. The analysis results show that our scheme also has the security properties of non-repudiation, perfect forward secrecy, perfect backward secrecy and public verifiability. Compared with the existing schemes in terms of computation complexity and ciphertext length, our scheme is more efficient, which makes it suitable for terminals with low computation capacity like smart cards.\n\n      "], "author_display": ["Huixian Li", "Xubao Chen", "Liaojun Pang", "Weisong Shi"], "article_type": "Research Article", "score": 0.44633618, "title_display": "Quantum Attack-Resistent Certificateless Multi-Receiver Signcryption Scheme", "publication_date": "2013-06-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0049141"}, {"journal": "PLoS ONE", "abstract": ["Background: Visual to auditory conversion systems have been in existence for several decades. Besides being among the front runners in providing visual capabilities to blind users, the auditory cues generated from image sonification systems are still easier to learn and adapt to compared to other similar techniques. Other advantages include low cost, easy customizability, and universality. However, every system developed so far has its own set of strengths and weaknesses. In order to improve these systems further, we propose an automated and quantitative method to measure the performance of such systems. With these quantitative measurements, it is possible to gauge the relative strengths and weaknesses of different systems and rank the systems accordingly. Methodology: Performance is measured by both the interpretability and also the information preservation of visual to auditory conversions. Interpretability is measured by computing the correlation of inter image distance (IID) and inter sound distance (ISD) whereas the information preservation is computed by applying Information Theory to measure the entropy of both visual and corresponding auditory signals. These measurements provide a basis and some insights on how the systems work. Conclusions: With an automated interpretability measure as a standard, more image sonification systems can be developed, compared, and then improved. Even though the measure does not test systems as thoroughly as carefully designed psychological experiments, a quantitative measurement like the one proposed here can compare systems to a certain degree without incurring much cost. Underlying this research is the hope that a major breakthrough in image sonification systems will allow blind users to cost effectively regain enough visual functions to allow them to lead secure and productive lives. "], "author_display": ["Shern Shiou Tan", "Tom\u00e1s Henrique Bode Maul", "Neil Russell Mennie"], "article_type": "Research Article", "score": 0.4463087, "title_display": "Measuring the Performance of Visual to Auditory Information Conversion", "publication_date": "2013-05-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0063042"}, {"journal": "PLoS Computational Biology", "abstract": ["\nBiological sensory systems react to changes in their surroundings. They are characterized by fast response and slow adaptation to varying environmental cues. Insofar as sensory adaptive systems map environmental changes to changes of their internal degrees of freedom, they can be regarded as computational devices manipulating information. Landauer established that information is ultimately physical, and its manipulation subject to the entropic and energetic bounds of thermodynamics. Thus the fundamental costs of biological sensory adaptation can be elucidated by tracking how the information the system has about its environment is altered. These bounds are particularly relevant for small organisms, which unlike everyday computers, operate at very low energies. In this paper, we establish a general framework for the thermodynamics of information processing in sensing. With it, we quantify how during sensory adaptation information about the past is erased, while information about the present is gathered. This process produces entropy larger than the amount of old information erased and has an energetic cost bounded by the amount of new information written to memory. We apply these principles to the E. coli's chemotaxis pathway during binary ligand concentration changes. In this regime, we quantify the amount of information stored by each methyl group and show that receptors consume energy in the range of the information-theoretic minimum. Our work provides a basis for further inquiries into more complex phenomena, such as gradient sensing and frequency response.\nAuthor Summary: The ability to process information is a ubiquitous feature of living organisms. Indeed, in order to survive, every living being, from the smallest bacterium to the biggest mammal, has to gather and process information about its surrounding environment. In the same way as our everyday computers need power to function, biological sensors need energy in order to gather and process this sensory information. How much energy do living organisms have to spend in order to get information about their environment? In this paper, we show that the minimum energy required for a biological sensor to detect a change in some environmental signal is proportional to the amount of information processed during that event. In order to know how far a real biological sensor operates from this minimum, we apply our predictions to chemo-sensing in the bacterium Escherichia Coli and find that the theoretical minimum corresponds to a sizable portion of the energy spent by the bacterium. "], "author_display": ["Pablo Sartori", "L\u00e9o Granger", "Chiu Fan Lee", "Jordan M. Horowitz"], "article_type": "Research Article", "score": 0.4456947, "title_display": "Thermodynamic Costs of Information Processing in Sensory Adaptation", "publication_date": "2014-12-11T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1003974"}, {"journal": "PLoS ONE", "abstract": ["\n        The way information is represented by sequences of action potentials of spiking neurons is determined by the input each neuron receives, but also by its biophysics, and the specifics of the circuit in which it is embedded. Even the \u201ccode\u201d of identified neurons can vary considerably from individual to individual. Here we compared the neural codes of the identified H1 neuron in the visual systems of two families of flies, blow flies and flesh flies, and explored the effect of the sensory environment that the flies were exposed to during development on the H1 code. We found that the two families differed considerably in the temporal structure of the code, its content and energetic efficiency, as well as the temporal delay of neural response. The differences in the environmental conditions during the flies' development had no significant effect. Our results may thus reflect an instance of a family-specific design of the neural code. They may also suggest that individual variability in information processing by this specific neuron, in terms of both form and content, is regulated genetically.\n      "], "author_display": ["Yoav Kfir", "Ittai Renan", "Elad Schneidman", "Ronen Segev"], "article_type": "Research Article", "score": 0.44498792, "title_display": "The Natural Variation of a Neural Code", "publication_date": "2012-03-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0033149"}, {"journal": "PLoS ONE", "abstract": ["Background: Personal health records (PHR) may improve patients' health by providing access to and context for health information. Among patients receiving care at a safety-net HIV/AIDS clinic, we examined the hypothesis that a mental health (MH) or substance use (SU) condition represents a barrier to engagement with web-based health information, as measured by consent to participate in a trial that provided access to personal (PHR) or general (non-PHR) health information portals and by completion of baseline study surveys posted there. Methods: Participants were individually trained to access and navigate individualized online accounts and to complete study surveys. In response to need, during accrual months 4 to 12 we enhanced participant training to encourage survey completion with the help of staff. Using logistic regression models, we estimated odds ratios for study participation and for survey completion by combined MH/SU status, adjusted for levels of computer competency, on-study training, and demographics. Results: Among 2,871 clinic patients, 70% had MH/SU conditions, with depression (38%) and methamphetamine use (17%) most commonly documented. Middle-aged patients and those with a MH/SU condition were over-represented among study participants (N\u200a=\u200a338). Survey completion was statistically independent of MH/SU status (OR, 1.85 [95% CI, 0.93\u20133.66]) but tended to be higher among those with MH/SU conditions. Completion rates were low among beginner computer users, regardless of training level (<50%), but adequate among advanced users (>70%). Conclusions: Among patients attending a safety-net clinic, MH/SU conditions were not barriers to engagement with web-based health information. Instead, level of computer competency was useful for identifying individuals requiring substantial computer training in order to fully participate in the study. Intensive on-study training was insufficient to enable beginner computer users to complete study surveys. "], "author_display": ["Joan F. Hilton", "Lynsey Barkoff", "Olivia Chang", "Lindsay Halperin", "Neda Ratanawongsa", "Urmimala Sarkar", "Yan Leykin", "Ricardo F. Mu\u00f1oz", "David H. Thom", "James S. Kahn"], "article_type": "Research Article", "score": 0.44408357, "title_display": "A Cross-Sectional Study of Barriers to Personal Health Record Use among Patients Attending a Safety-Net Clinic", "publication_date": "2012-02-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0031888"}, {"journal": "PLoS ONE", "abstract": ["Background: In contrast to Newton's well-known aphorism that he had been able \u201cto see further only by standing on the shoulders of giants,\u201d one attributes to the Spanish philosopher Ortega y Gasset the hypothesis saying that top-level research cannot be successful without a mass of medium researchers on which the top rests comparable to an iceberg. Methodology/Principal Findings: The Ortega hypothesis predicts that highly-cited papers and medium-cited (or lowly-cited) papers would equally refer to papers with a medium impact. The Newton hypothesis would be supported if the top-level research more frequently cites previously highly-cited work than that medium-level research cites highly-cited work. Our analysis is based on (i) all articles and proceedings papers which were published in 2003 in the life sciences, health sciences, physical sciences, and social sciences, and (ii) all articles and proceeding papers which were cited within these publications. The results show that highly-cited work in all scientific fields more frequently cites previously highly-cited papers than that medium-cited work cites highly-cited work. Conclusions/Significance: We demonstrate that papers contributing to the scientific progress in a field lean to a larger extent on previously important contributions than papers contributing little. These findings support the Newton hypothesis and call into question the Ortega hypothesis (given our usage of citation counts as a proxy for impact). "], "author_display": ["Lutz Bornmann", "F\u00e9lix de Moya Aneg\u00f3n", "Loet Leydesdorff"], "article_type": "Research Article", "score": 0.444066, "title_display": "Do Scientific Advancements Lean on the Shoulders of Giants? A Bibliometric Investigation of the Ortega Hypothesis", "publication_date": "2010-10-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0013327"}, {"journal": "PLoS ONE", "abstract": ["Availability: The software package is available at https://sites.google.com/site/desheng619/download. "], "author_display": ["Desheng Zheng", "Guowu Yang", "Xiaoyu Li", "Zhicai Wang", "Feng Liu", "Lei He"], "article_type": "Research Article", "score": 0.44381812, "title_display": "An Efficient Algorithm for Computing Attractors of Synchronous And Asynchronous Boolean Networks", "publication_date": "2013-04-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0060593"}, {"journal": "PLoS ONE", "abstract": ["\n        Computational visual attention systems have been constructed in order for robots and other devices to detect and locate regions of interest in their visual world. Such systems often attempt to take account of what is known of the human visual system and employ concepts, such as \u2018active vision\u2019, to gain various perceived advantages. However, despite the potential for gaining insights from such experiments, the computational requirements for visual attention processing are often not clearly presented from a biological perspective. This was the primary objective of this study, attained through two specific phases of investigation: 1) conceptual modeling of a top-down-bottom-up framework through critical analysis of the psychophysical and neurophysiological literature, 2) implementation and validation of the model into robotic hardware (as a representative of an active vision system). Seven computational requirements were identified: 1) transformation of retinotopic to egocentric mappings, 2) spatial memory for the purposes of medium-term inhibition of return, 3) synchronization of \u2018where\u2019 and \u2018what\u2019 information from the two visual streams, 4) convergence of top-down and bottom-up information to a centralized point of information processing, 5) a threshold function to elicit saccade action, 6) a function to represent task relevance as a ratio of excitation and inhibition, and 7) derivation of excitation and inhibition values from object-associated feature classes. The model provides further insight into the nature of data representation and transfer between brain regions associated with the vertebrate \u2018active\u2019 visual attention system. In particular, the model lends strong support to the functional role of the lateral intraparietal region of the brain as a primary area of information consolidation that directs putative action through the use of a \u2018priority map\u2019.\n      "], "author_display": ["Sebastian McBride", "Martin Huelse", "Mark Lee"], "article_type": "Research Article", "score": 0.44366089, "title_display": "Identifying the Computational Requirements of an Integrated Top-Down-Bottom-Up Model for Overt Visual Attention within an Active Vision System", "publication_date": "2013-02-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0054585"}, {"journal": "PLoS ONE", "abstract": ["\nSurprisal analysis is increasingly being applied for the examination of transcription levels in cellular processes, towards revealing inner network structures and predicting response. But to achieve its full potential, surprisal analysis should be integrated into a wider range computational tool. The purposes of this paper are to combine surprisal analysis with other important computation procedures, such as easy manipulation of the analysis results \u2013 e.g. to choose desirable result sub-sets for further inspection \u2013, retrieval and comparison with relevant datasets from public databases, and flexible graphical displays for heuristic thinking. The whole set of computation procedures integrated into a single practical tool is what we call Computational Surprisal Analysis. This combined kind of analysis should facilitate significantly quantitative understanding of different cellular processes for researchers, including applications in proteomics and metabolomics. Beyond that, our vision is that Computational Surprisal Analysis has the potential to reach the status of a routine method of analysis for practitioners. The resolving power of Computational Surprisal Analysis is here demonstrated by its application to a variety of cellular cancer process transcription datasets, ours and from the literature. The results provide a compact biological picture of the thermodynamic significance of the leading gene expression phenotypes in every stage of the disease. For each transcript we characterize both its inherent steady state weight, its correlation with the other transcripts and its variation due to the disease. We present a dedicated website to facilitate the analysis for researchers and practitioners.\n"], "author_display": ["Nataly Kravchenko-Balasha", "Simcha Simon", "R. D. Levine", "F. Remacle", "Iaakov Exman"], "article_type": "Research Article", "score": 0.44336253, "title_display": "Computational Surprisal Analysis Speeds-Up Genomic Characterization of Cancer Processes", "publication_date": "2014-11-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0108549"}, {"journal": "PLoS Computational Biology", "abstract": ["\nTo understand visual cognition, it is imperative to determine when, how and with what information the human brain categorizes the visual input. Visual categorization consistently involves at least an early and a late stage: the occipito-temporal N170 event related potential related to stimulus encoding and the parietal P300 involved in perceptual decisions. Here we sought to understand how the brain globally transforms its representations of face categories from their early encoding to the later decision stage over the 400 ms time window encompassing the N170 and P300 brain events. We applied classification image techniques to the behavioral and electroencephalographic data of three observers who categorized seven facial expressions of emotion and report two main findings: (1) over the 400 ms time course, processing of facial features initially spreads bilaterally across the left and right occipito-temporal regions to dynamically converge onto the centro-parietal region; (2) concurrently, information processing gradually shifts from encoding common face features across all spatial scales (e.g., the eyes) to representing only the finer scales of the diagnostic features that are richer in useful information for behavior (e.g., the wide opened eyes in \u2018fear\u2019; the detailed mouth in \u2018happy\u2019). Our findings suggest that the brain refines its diagnostic representations of visual categories over the first 400 ms of processing by trimming a thorough encoding of features over the N170, to leave only the detailed information important for perceptual decisions over the P300.\nAuthor Summary: How the brain uses visual information to construct representations of categories is a central question of cognitive neuroscience. With our methods we visualize how the brain transforms its representations of facial expressions. Using electroencephalographic data, we analyze how representations change over the first 450 ms of processing both in feature content (e.g., which aspects of the face, such as the eyes or the mouth are represented across time) and level of detail. We show that facial expressions are initially encoded with most of their features (i.e., mouth and eyes) across all levels of details in the occipito-temporal regions. In a later phase, we show that a gradual reorganization of representations occurs, whereby only task relevant face features are kept (e.g., the mouth in \u201chappy\u201d) at only the finest level of details. We describe this elimination of irrelevant and redundant information as \u2018trimming\u2019. We suggest that this may be an example of the brain optimizing categorical representations. "], "author_display": ["Nicola J. van Rijsbergen", "Philippe G. Schyns"], "article_type": "Research Article", "score": 0.44282666, "title_display": "Dynamics of Trimming the Content of Face Representations for Categorization in the Brain", "publication_date": "2009-11-13T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1000561"}, {"journal": "PLoS ONE", "abstract": ["\nWe implement the Ising model on a structural connectivity matrix describing the brain at two different resolutions. Tuning the model temperature to its critical value, i.e. at the susceptibility peak, we find a maximal amount of total information transfer between the spin variables. At this point the amount of information that can be redistributed by some nodes reaches a limit and the net dynamics exhibits signature of the law of diminishing marginal returns, a fundamental principle connected to saturated levels of production. Our results extend the recent analysis of dynamical oscillators models on the connectome structure, taking into account lagged and directional influences, focusing only on the nodes that are more prone to became bottlenecks of information. The ratio between the outgoing and the incoming information at each node is related to the the sum of the weights to that node and to the average time between consecutive time flips of spins. The results for the connectome of 66 nodes and for that of 998 nodes are similar, thus suggesting that these properties are scale-independent. Finally, we also find that the brain dynamics at criticality is organized maximally to a rich-club w.r.t. the network of information flows.\n"], "author_display": ["Daniele Marinazzo", "Mario Pellicoro", "Guorong Wu", "Leonardo Angelini", "Jes\u00fas M. Cort\u00e9s", "Sebastiano Stramaglia"], "article_type": "Research Article", "score": 0.44214097, "title_display": "Information Transfer and Criticality in the Ising Model on the Human Connectome", "publication_date": "2014-04-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0093616"}, {"journal": "PLOS ONE", "abstract": ["\nGeoscience observations and model simulations are generating vast amounts of multi-dimensional data. Effectively analyzing these data are essential for geoscience studies. However, the tasks are challenging for geoscientists because processing the massive amount of data is both computing and data intensive in that data analytics requires complex procedures and multiple tools. To tackle these challenges, a scientific workflow framework is proposed for big geoscience data analytics. In this framework techniques are proposed by leveraging cloud computing, MapReduce, and Service Oriented Architecture (SOA). Specifically, HBase is adopted for storing and managing big geoscience data across distributed computers. MapReduce-based algorithm framework is developed to support parallel processing of geoscience data. And service-oriented workflow architecture is built for supporting on-demand complex data analytics in the cloud environment. A proof-of-concept prototype tests the performance of the framework. Results show that this innovative framework significantly improves the efficiency of big geoscience data analytics by reducing the data processing time as well as simplifying data analytical procedures for geoscientists.\n"], "author_display": ["Zhenlong Li", "Chaowei Yang", "Baoxuan Jin", "Manzhu Yu", "Kai Liu", "Min Sun", "Matthew Zhan"], "article_type": "Research Article", "score": 0.4419242, "title_display": "Enabling Big Geoscience Data Analytics with a Cloud-Based, MapReduce-Enabled and Service-Oriented Workflow Framework", "publication_date": "2015-03-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0116781"}, {"journal": "PLoS ONE", "abstract": ["\nKeyword search on encrypted data allows one to issue the search token and conduct search operations on encrypted data while still preserving keyword privacy. In the present paper, we consider the keyword search problem further and introduce a novel notion called attribute-based proxy re-encryption with keyword search (), which introduces a promising feature: In addition to supporting keyword search on encrypted data, it enables data owners to delegate the keyword search capability to some other data users complying with the specific access control policy. To be specific,  allows (i) the data owner to outsource his encrypted data to the cloud and then ask the cloud to conduct keyword search on outsourced encrypted data with the given search token, and (ii) the data owner to delegate other data users keyword search capability in the fine-grained access control manner through allowing the cloud to re-encrypted stored encrypted data with a re-encrypted data (embedding with some form of access control policy). We formalize the syntax and security definitions for , and propose two concrete constructions for : key-policy  and ciphertext-policy . In the nutshell, our constructions can be treated as the integration of technologies in the fields of attribute-based cryptography and proxy re-encryption cryptography.\n"], "author_display": ["Yanfeng Shi", "Jiqiang Liu", "Zhen Han", "Qingji Zheng", "Rui Zhang", "Shuo Qiu"], "article_type": "Research Article", "score": 0.44170675, "title_display": "Attribute-Based Proxy Re-Encryption with Keyword Search", "publication_date": "2014-12-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0116325"}, {"journal": "PLOS ONE", "abstract": ["\nThe use of citizen science for scientific discovery relies on the acceptance of this method by the scientific community. Using the Web of Science and Scopus as the source of peer reviewed articles, an analysis of all published articles on \u201ccitizen science\u201d confirmed its growth, and found that significant research on methodology and validation techniques preceded the rapid rise of the publications on research outcomes based on citizen science methods. Of considerable interest is the growing number of studies relying on the re-use of collected datasets from past citizen science research projects, which used data from either individual or multiple citizen science projects for new discoveries, such as for climate change research. The extent to which citizen science has been used in scientific discovery demonstrates its importance as a research approach. This broad analysis of peer reviewed papers on citizen science, that included not only citizen science projects, but the theory and methods developed to underpin the research, highlights the breadth and depth of the citizen science approach and encourages cross-fertilization between the different disciplines.\n"], "author_display": ["Ria Follett", "Vladimir Strezov"], "article_type": "Research Article", "score": 0.44066852, "title_display": "An Analysis of Citizen Science Based Research: Usage and Publication Patterns", "publication_date": "2015-11-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0143687"}, {"journal": "PLoS ONE", "abstract": ["\n        During the stationary part of neuronal spiking response, the stimulus can be encoded in the firing rate, but also in the statistical structure of the interspike intervals. We propose and discuss two information-based measures of statistical dispersion of the interspike interval distribution, the entropy-based dispersion and Fisher information-based dispersion. The measures are compared with the frequently used concept of standard deviation. It is shown, that standard deviation is not well suited to quantify some aspects of dispersion that are often expected intuitively, such as the degree of randomness. The proposed dispersion measures are not entirely independent, although each describes the interspike intervals from a different point of view. The new methods are applied to common models of neuronal firing and to both simulated and experimental data.\n      "], "author_display": ["Lubomir Kostal", "Petr Lansky", "Ondrej Pokora"], "article_type": "Research Article", "score": 0.43959573, "title_display": "Variability Measures of Positive Random Variables", "publication_date": "2011-07-22T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0021998"}, {"journal": "PLoS ONE", "abstract": ["\n        In this paper, we evaluate the uniqueness of several information-theoretic measures for graphs based on so-called information functionals and compare the results with other information indices and non-information-theoretic measures such as the well-known Balaban  index. We show that, by employing an information functional based on degree-degree associations, the resulting information index outperforms the Balaban  index tremendously. These results have been obtained by using nearly 12 million exhaustively generated, non-isomorphic and unweighted graphs. Also, we obtain deeper insights on these and other topological descriptors when exploring their uniqueness by using exhaustively generated sets of alkane trees representing connected and acyclic graphs in which the degree of a vertex is at most four.\n      "], "author_display": ["Matthias Dehmer", "Martin Grabner", "Kurt Varmuza"], "article_type": "Research Article", "score": 0.4393803, "title_display": "Information Indices with High Discriminative Power for Graphs", "publication_date": "2012-02-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0031214"}, {"journal": "PLoS ONE", "abstract": ["The advancement of the computational biology field hinges on progress in three fundamental directions \u2013 the development of new computational algorithms, the availability of informatics resource management infrastructures and the capability of tools to interoperate and synergize. There is an explosion in algorithms and tools for computational biology, which makes it difficult for biologists to find, compare and integrate such resources. We describe a new infrastructure, iTools, for managing the query, traversal and comparison of diverse computational biology resources. Specifically, iTools stores information about three types of resources\u2013data, software tools and web-services. The iTools design, implementation and resource meta - data content reflect the broad research, computational, applied and scientific expertise available at the seven National Centers for Biomedical Computing. iTools provides a system for classification, categorization and integration of different computational biology resources across space-and-time scales, biomedical problems, computational infrastructures and mathematical foundations. A large number of resources are already iTools-accessible to the community and this infrastructure is rapidly growing. iTools includes human and machine interfaces to its resource meta-data repository. Investigators or computer programs may utilize these interfaces to search, compare, expand, revise and mine meta-data descriptions of existent computational biology resources. We propose two ways to browse and display the iTools dynamic collection of resources. The first one is based on an ontology of computational biology resources, and the second one is derived from hyperbolic projections of manifolds or complex structures onto planar discs. iTools is an open source project both in terms of the source code development as well as its meta-data content. iTools employs a decentralized, portable, scalable and lightweight framework for long-term resource management. We demonstrate several applications of iTools as a framework for integrated bioinformatics. iTools and the complete details about its specifications, usage and interfaces are available at the iTools web page http://iTools.ccb.ucla.edu."], "author_display": ["Ivo D. Dinov", "Daniel Rubin", "William Lorensen", "Jonathan Dugan", "Jeff Ma", "Shawn Murphy", "Beth Kirschner", "William Bug", "Michael Sherman", "Aris Floratos", "David Kennedy", "H. V. Jagadish", "Jeanette Schmidt", "Brian Athey", "Andrea Califano", "Mark Musen", "Russ Altman", "Ron Kikinis", "Isaac Kohane", "Scott Delp", "D. Stott Parker", "Arthur W. Toga"], "article_type": "Research Article", "score": 0.4388587, "title_display": "<i>iTools</i>: A Framework for Classification, Categorization and Integration of Computational Biology Resources", "publication_date": "2008-05-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0002265"}, {"journal": "PLoS ONE", "abstract": ["\n        With the recent advances in high-throughput RNA sequencing (RNA-Seq), biologists are able to measure transcription with unprecedented precision. One problem that can now be tackled is that of isoform quantification: here one tries to reconstruct the abundances of isoforms of a gene. We have developed a statistical solution for this problem, based on analyzing a set of RNA-Seq reads, and a practical implementation, available from archive.gersteinlab.org/proj/rnaseq/IQSeq, in a tool we call IQSeq (Isoform Quantification in next-generation Sequencing). Here, we present theoretical results which IQSeq is based on, and then use both simulated and real datasets to illustrate various applications of the tool. In order to measure the accuracy of an isoform-quantification result, one would try to estimate the average variance of the estimated isoform abundances for each gene (based on resampling the RNA-seq reads), and IQSeq has a particularly fast algorithm (based on the Fisher Information Matrix) for calculating this, achieving a speedup of  times compared to brute-force resampling. IQSeq also calculates an information theoretic measure of overall transcriptome complexity to describe isoform abundance for a whole experiment. IQSeq has many features that are particularly useful in RNA-Seq experimental design, allowing one to optimally model the integration of different sequencing technologies in a cost-effective way. In particular, the IQSeq formalism integrates the analysis of different sample (i.e. read) sets generated from different technologies within the same statistical framework. It also supports a generalized statistical partial-sample-generation function to model the sequencing process. This allows one to have a modular, \u201cplugin-able\u201d read-generation function to support the particularities of the many evolving sequencing technologies.\n      "], "author_display": ["Jiang Du", "Jing Leng", "Lukas Habegger", "Andrea Sboner", "Drew McDermott", "Mark Gerstein"], "article_type": "Research Article", "score": 0.43879855, "title_display": "IQSeq: Integrated Isoform Quantification Analysis Based on Next-Generation Sequencing", "publication_date": "2012-01-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0029175"}, {"journal": "PLoS ONE", "abstract": ["Background: Comparative genomics, or the study of the relationships of genome structure and function across different species, offers a powerful tool for studying evolution, annotating genomes, and understanding the causes of various genetic disorders. However, aligning multiple sequences of DNA, an essential intermediate step for most types of analyses, is a difficult computational task. In parallel, citizen science, an approach that takes advantage of the fact that the human brain is exquisitely tuned to solving specific types of problems, is becoming increasingly popular. There, instances of hard computational problems are dispatched to a crowd of non-expert human game players and solutions are sent back to a central server. Methodology/Principal Findings: We introduce Phylo, a human-based computing framework applying \u201ccrowd sourcing\u201d techniques to solve the Multiple Sequence Alignment (MSA) problem. The key idea of Phylo is to convert the MSA problem into a casual game that can be played by ordinary web users with a minimal prior knowledge of the biological context. We applied this strategy to improve the alignment of the promoters of disease-related genes from up to 44 vertebrate species. Since the launch in November 2010, we received more than 350,000 solutions submitted from more than 12,000 registered users. Our results show that solutions submitted contributed to improving the accuracy of up to 70% of the alignment blocks considered. Conclusions/Significance: We demonstrate that, combined with classical algorithms, crowd computing techniques can be successfully used to help improving the accuracy of MSA. More importantly, we show that an NP-hard computational problem can be embedded in casual game that can be easily played by people without significant scientific training. This suggests that citizen science approaches can be used to exploit the billions of \u201chuman-brain peta-flops\u201d of computation that are spent every day playing games. Phylo is available at: http://phylo.cs.mcgill.ca. "], "author_display": ["Alexander Kawrykow", "Gary Roumanis", "Alfred Kam", "Daniel Kwak", "Clarence Leung", "Chu Wu", "Eleyine Zarour", "Phylo players ", "Luis Sarmenta", "Mathieu Blanchette", "J\u00e9r\u00f4me Waldisp\u00fchl"], "article_type": "Research Article", "score": 0.43806732, "title_display": "Phylo: A Citizen Science Approach for Improving Multiple Sequence Alignment", "publication_date": "2012-03-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0031362"}, {"journal": "PLoS ONE", "abstract": ["\nGiven a perfect superposition of  states on a quantum system of  qubits. We propose a fast quantum algorithm for collapsing the perfect superposition to a chosen quantum state  without applying any measurements. The basic idea is to use a phase destruction mechanism. Two operators are used, the first operator applies a phase shift and a temporary entanglement to mark  in the superposition, and the second operator applies selective phase shifts on the states in the superposition according to their Hamming distance with . The generated state can be used as an excellent input state for testing quantum memories and linear optics quantum computers. We make no assumptions about the used operators and applied quantum gates, but our result implies that for this purpose the number of qubits in the quantum register offers no advantage, in principle, over the obvious measurement-based feedback protocol.\n"], "author_display": ["Ahmed Younes", "Mahmoud Abdel-Aty"], "article_type": "Research Article", "score": 0.4378007, "title_display": "Collapsing a Perfect Superposition to a Chosen Quantum State without Measurement", "publication_date": "2014-08-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0103612"}, {"journal": "PLoS Computational Biology", "abstract": ["\nActivity in neural circuits is spatiotemporally organized. Its spatial organization consists of multiple, localized coherent patterns, or patchy clusters. These patterns propagate across the circuits over time. This type of collective behavior has ubiquitously been observed, both in spontaneous activity and evoked responses; its function, however, has remained unclear. We construct a spatially extended, spiking neural circuit that generates emergent spatiotemporal activity patterns, thereby capturing some of the complexities of the patterns observed empirically. We elucidate what kind of fundamental function these patterns can serve by showing how they process information. As self-sustained objects, localized coherent patterns can signal information by propagating across the neural circuit. Computational operations occur when these emergent patterns interact, or collide with each other. The ongoing behaviors of these patterns naturally embody both distributed, parallel computation and cascaded logical operations. Such distributed computations enable the system to work in an inherently flexible and efficient way. Our work leads us to propose that propagating coherent activity patterns are the underlying primitives with which neural circuits carry out distributed dynamical computation.\nAuthor Summary: The brain processes information with extraordinary efficiency, and can perform feats such as effortlessly recognizing objects from among thousands of possibilities within a fraction of a second. This is accomplished because the brain represents and processes information in a distributed fashion and in a dynamical way. This processing is manifested in spatiotemporal neural activity patterns of great complexities within the brain. Here, we construct a spiking neural circuit that can reproduce some of the complexities, which are evident in terms of multiple wave patterns with interactions between each other. We show that their dynamics can support propagating pattern-based computation; spiking wave patterns signal information by propagating across neural circuits, and computational operations occur when they collide with each other. Such dynamical computation contrasts sharply with that done by static and physically fixed logic gates operating in other computing machines such as computers. Moreover, we elucidate that the collective dynamics of multiple, interacting wave patterns enable computation processing implemented in a fundamentally distributed and parallel manner in the neural circuit. "], "author_display": ["Pulin Gong", "Cees van Leeuwen"], "article_type": "Research Article", "score": 0.43753213, "title_display": "Distributed Dynamical Computation in Neural Circuits with Propagating Coherent Activity Patterns", "publication_date": "2009-12-18T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1000611"}, {"journal": "PLOS ONE", "abstract": ["\nIn this paper, we study the discrimination power of graph measures that are based on graph-theoretical matrices. The paper generalizes the work of [M. Dehmer, M. Moosbrugger. Y. Shi, Encoding structural information uniquely with polynomial-based descriptors by employing the Randi\u0107 matrix, Applied Mathematics and Computation, 268(2015), 164\u2013168]. We demonstrate that by using the new functional matrix approach, exhaustively generated graphs can be discriminated more uniquely than shown in the mentioned previous work.\n"], "author_display": ["Matthias Dehmer", "Frank Emmert-Streib", "Yongtang Shi", "Monica Stefu", "Shailesh Tripathi"], "article_type": "Research Article", "score": 0.4374212, "title_display": "Discrimination Power of Polynomial-Based Descriptors for Graphs by Using Functional Matrices", "publication_date": "2015-10-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0139265"}, {"journal": "PLOS ONE", "abstract": ["\nSecure multiparty computation allows for a set of users to evaluate a particular function over their inputs without revealing the information they possess to each other. Theoretically, this can be achieved using fully homomorphic encryption systems, but so far they remain in the realm of computational impracticability. An alternative is to consider secure function evaluation using homomorphic public-key cryptosystems or Garbled Circuits, the latter being a popular trend in recent times due to important breakthroughs. We propose a technique for computing the logsum operation using Garbled Circuits. This technique relies on replacing the logsum operation with an equivalent piecewise linear approximation, taking advantage of recent advances in efficient methods for both designing and implementing Garbled Circuits. We elaborate on how all the required blocks should be assembled in order to obtain small errors regarding the original logsum operation and very fast execution times.\n"], "author_display": ["Jos\u00e9 Port\u00ealo", "Bhiksha Raj", "Isabel Trancoso"], "article_type": "Research Article", "score": 0.43727407, "title_display": "Logsum Using Garbled Circuits", "publication_date": "2015-03-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0122236"}, {"journal": "PLoS ONE", "abstract": ["\n        Searching metabolites against databases according to their masses is often the first step in metabolite identification for a mass spectrometry-based untargeted metabolomics study. Major metabolite databases include Human Metabolome DataBase (HMDB), Madison Metabolomics Consortium Database (MMCD), Metlin, and LIPID MAPS. Since each one of these databases covers only a fraction of the metabolome, integration of the search results from these databases is expected to yield a more comprehensive coverage. However, the manual combination of multiple search results is generally difficult when identification of hundreds of metabolites is desired. We have implemented a web-based software tool that enables simultaneous mass-based search against the four major databases, and the integration of the results. In addition, more complete chemical identifier information for the metabolites is retrieved by cross-referencing multiple databases. The search results are merged based on IUPAC International Chemical Identifier (InChI) keys. Besides a simple list of m/z values, the software can accept the ion annotation information as input for enhanced metabolite identification. The performance of the software is demonstrated on mass spectrometry data acquired in both positive and negative ionization modes. Compared with search results from individual databases, MetaboSearch provides better coverage of the metabolome and more complete chemical identifier information. Availability: The software tool is available at http://omics.georgetown.edu/MetaboSearch.html.\n      "], "author_display": ["Bin Zhou", "Jinlian Wang", "Habtom W. Ressom"], "article_type": "Research Article", "score": 0.4368719, "title_display": "MetaboSearch: Tool for Mass-Based Metabolite Identification Using Multiple Databases", "publication_date": "2012-06-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0040096"}, {"journal": "PLoS ONE", "abstract": ["\nForensic DNA evidence often contains mixtures of multiple contributors, or is present in low template amounts. The resulting data signals may appear to be relatively uninformative when interpreted using qualitative inclusion-based methods. However, these same data can yield greater identification information when interpreted by computer using quantitative data-modeling methods. This study applies both qualitative and quantitative interpretation methods to a well-characterized DNA mixture and dilution data set, and compares the inferred match information. The results show that qualitative interpretation loses identification power at low culprit DNA quantities (below 100 pg), but that quantitative methods produce useful information down into the 10 pg range. Thus there is a ten-fold information gap that separates the qualitative and quantitative DNA mixture interpretation approaches. With low quantities of culprit DNA (10 pg to 100 pg), computer-based quantitative interpretation provides greater match sensitivity.\n"], "author_display": ["Mark W. Perlin", "Alexander Sinelnikov"], "article_type": "Research Article", "score": 0.43658057, "title_display": "An Information Gap in DNA Evidence Interpretation", "publication_date": "2009-12-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0008327"}, {"journal": "PLoS ONE", "abstract": ["Background: Previous work has noted that science stands as an ideological force insofar as the answers it offers to a variety of fundamental questions and concerns; as such, those who pursue scientific inquiry have been shown to be concerned with the moral and social ramifications of their scientific endeavors. No studies to date have directly investigated the links between exposure to science and moral or prosocial behaviors. Methodology/Principal Findings: Across four studies, both naturalistic measures of science exposure and experimental primes of science led to increased adherence to moral norms and more morally normative behaviors across domains. Study 1 (n\u200a=\u200a36) tested the natural correlation between exposure to science and likelihood of enforcing moral norms. Studies 2 (n\u200a=\u200a49), 3 (n\u200a=\u200a52), and 4 (n\u200a=\u200a43) manipulated thoughts about science and examined the causal impact of such thoughts on imagined and actual moral behavior. Across studies, thinking about science had a moralizing effect on a broad array of domains, including interpersonal violations (Studies 1, 2), prosocial intentions (Study 3), and economic exploitation (Study 4). Conclusions/Significance: These studies demonstrated the morally normative effects of lay notions of science. Thinking about science leads individuals to endorse more stringent moral norms and exhibit more morally normative behavior. These studies are the first of their kind to systematically and empirically test the relationship between science and morality. The present findings speak to this question and elucidate the value-laden outcomes of the notion of science. "], "author_display": ["Christine Ma-Kellams", "Jim Blascovich"], "article_type": "Research Article", "score": 0.4359338, "title_display": "Does \u201cScience\u201d Make You Moral? The Effects of Priming Science on Moral Judgments and Behavior", "publication_date": "2013-03-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0057989"}, {"journal": "PLoS ONE", "abstract": ["Background: To summarize systematic reviews that 1) assessed the evidence for causal relationships between computer work and the occurrence of carpal tunnel syndrome (CTS) or upper extremity musculoskeletal disorders (UEMSDs), or 2) reported on intervention studies among computer users/or office workers. Methodology/Principal Findings: PubMed, Embase, CINAHL and Web of Science were searched for reviews published between 1999 and 2010. Additional publications were provided by content area experts. The primary author extracted all data using a purpose-built form, while two of the authors evaluated the quality of the reviews using recommended standard criteria from AMSTAR; disagreements were resolved by discussion. The quality of evidence syntheses in the included reviews was assessed qualitatively for each outcome and for the interventions. Conclusions/Significance: Computer use is associated with pain complaints, but it is still not very clear if this association is causal. The evidence for specific disorders or diseases is limited. No effective interventions have yet been documented. "], "author_display": ["Johan H. Andersen", "Nils Fallentin", "Jane F. Thomsen", "Sigurd Mikkelsen"], "article_type": "Research Article", "score": 0.43567088, "title_display": "Risk Factors for Neck and Upper Extremity Disorders among Computers Users and the Effect of Interventions: An Overview of Systematic Reviews", "publication_date": "2011-05-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0019691"}, {"journal": "PLoS ONE", "abstract": ["\n        This study set out to explore the views and motivations of those involved in a number of recent and current advocacy efforts (such as open science, computational provenance, and reproducible research) aimed at making science and scientific artifacts accessible to a wider audience. Using a exploratory approach, the study tested whether a consensus exists among advocates of these initiatives about the key concepts, exploring the meanings that scientists attach to the various mechanisms for sharing their work, and the social context in which this takes place. The study used a purposive sampling strategy to target scientists who have been active participants in these advocacy efforts, and an open-ended questionnaire to collect detailed opinions on the topics of reproducibility, credibility, scooping, data sharing, results sharing, and the effectiveness of the peer review process. We found evidence of a lack of agreement on the meaning of key terminology, and a lack of consensus on some of the broader goals of these advocacy efforts. These results can be explained through a closer examination of the divergent goals and approaches adopted by different advocacy efforts. We suggest that the scientific community could benefit from a broader discussion of what it means to make scientific research more accessible and how this might best be achieved.\n      "], "author_display": ["Alicia M. Grubb", "Steve M. Easterbrook"], "article_type": "Research Article", "score": 0.43556416, "title_display": "On the Lack of Consensus over the Meaning of Openness: An Empirical Study", "publication_date": "2011-08-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0023420"}, {"journal": "PLoS ONE", "abstract": ["\n        When one visual object moves behind another, the object farther from the viewer is progressively occluded and/or disoccluded by the nearer object. For nearly half a century, this dynamic occlusion cue has beenthought to be sufficient by itself for determining the relative depth of the two objects. This view is consistent with the self-evident geometric fact that the surface undergoing dynamic occlusion is always farther from the viewer than the occluding surface. Here we use a contextual manipulation ofa previously known motion illusion, which we refer to as the\u2018Moonwalk\u2019 illusion, to demonstrate that the visual system cannot determine relative depth from dynamic occlusion alone. Indeed, in the Moonwalk illusion, human observers perceive a relative depth contrary to the dynamic occlusion cue. However, the perception of the expected relative depth is restored by contextual manipulations unrelated to dynamic occlusion. On the other hand, we show that an Ideal Observer can determine using dynamic occlusion alone in the same Moonwalk stimuli, indicating that the dynamic occlusion cue is, in principle, sufficient for determining relative depth. Our results indicate that in order to correctly perceive relative depth from dynamic occlusion, the human brain, unlike the Ideal Observer, needs additionalsegmentation information that delineate the occluder from the occluded object. Thus, neural mechanisms of object segmentation must, in addition to motion mechanisms that extract information about relative depth, play a crucial role in the perception of relative depth from motion.\n      "], "author_display": ["Sarah Kromrey", "Evgeniy Bart", "Jay Hegd\u00e9"], "article_type": "Research Article", "score": 0.43553644, "title_display": "What the \u2018Moonwalk\u2019 Illusion Reveals about the Perception of Relative Depth from Motion", "publication_date": "2011-06-22T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0020951"}, {"journal": "PLoS ONE", "abstract": ["\nThe change in exocrine mass is an important parameter to follow in experimental models of pancreatic injury and regeneration. However, at present, the quantitative assessment of exocrine content by histology is tedious and operator-dependent, requiring manual assessment of acinar area on serial pancreatic sections. In this study, we utilized a novel computer-generated learning algorithm to construct an accurate and rapid method of quantifying acinar content. The algorithm works by learning differences in pixel characteristics from input examples provided by human experts. HE-stained pancreatic sections were obtained in mice recovering from a 2-day, hourly caerulein hyperstimulation model of experimental pancreatitis. For training data, a pathologist carefully outlined discrete regions of acinar and non-acinar tissue in 21 sections at various stages of pancreatic injury and recovery (termed the \u201cground truth\u201d). After the expert defined the ground truth, the computer was able to develop a prediction rule that was then applied to a unique set of high-resolution images in order to validate the process. For baseline, non-injured pancreatic sections, the software demonstrated close agreement with the ground truth in identifying baseline acinar tissue area with only a difference of 1%\u00b10.05% (p\u200a=\u200a0.21). Within regions of injured tissue, the software reported a difference of 2.5%\u00b10.04% in acinar area compared with the pathologist (p\u200a=\u200a0.47). Surprisingly, on detailed morphological examination, the discrepancy was primarily because the software outlined acini and excluded inter-acinar and luminal white space with greater precision. The findings suggest that the software will be of great potential benefit to both clinicians and researchers in quantifying pancreatic acinar cell flux in the injured and recovering pancreas.\n"], "author_display": ["John F. Eisses", "Amy W. Davis", "Akif Burak Tosun", "Zachary R. Dionise", "Cheng Chen", "John A. Ozolek", "Gustavo K. Rohde", "Sohail Z. Husain"], "article_type": "Research Article", "score": 0.43505204, "title_display": "A Computer-Based Automated Algorithm for Assessing Acinar Cell Loss after Experimental Pancreatitis", "publication_date": "2014-10-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0110220"}, {"journal": "PLoS ONE", "abstract": ["\nIn an effort to deal with more complicated evaluation situations, scientists have focused their efforts on dynamic comprehensive evaluation research. How to make full use of the subjective and objective information has become one of the noteworthy content. In this paper, a dynamic comprehensive evaluation method with subjective and objective information is proposed. We use the combination weighting method to determine the index weight. Analysis hierarchy process method is applied to dispose the subjective information, and criteria importance through intercriteria correlation method is used to handle the objective information. And for the time weight determination, we consider both time distance and information size to embody the principle of esteeming the present over the past. And then the linear weighted average model is constructed to make the evaluation process more practicable. Finally, an example is presented to illustrate the effectiveness of this method. Overall, the results suggest that the proposed method is reasonable and effective.\n"], "author_display": ["Dinglin Liu", "Xianglian Zhao"], "article_type": "Research Article", "score": 0.43494612, "title_display": "Method and Application for Dynamic Comprehensive Evaluation with Subjective and Objective Information", "publication_date": "2013-12-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0083323"}, {"journal": "PLOS ONE", "abstract": ["\nProjection and back-projection are the most computationally intensive parts in Computed Tomography (CT) reconstruction, and are essential to acceleration of CT reconstruction algorithms. Compared to back-projection, parallelization efficiency in projection is highly limited by racing condition and thread unsynchronization. In this paper, a strategy of Fixed Sampling Number Projection (FSNP) is proposed to ensure the operation synchronization in the ray-driven projection with Graphical Processing Unit (GPU). Texture fetching is also used utilized to further accelerate the interpolations in both projection and back-projection. We validate the performance of this FSNP approach using both simulated and real cone-beam CT data. Experimental results show that compare to the conventional approach, the proposed FSNP method together with texture fetching is 10~16 times faster than the conventional approach based on global memory, and thus leads to more efficient iterative algorithm in CT reconstruction.\n"], "author_display": ["Lizhe Xie", "Yining Hu", "Bin Yan", "Lin Wang", "Benqiang Yang", "Wenyuan Liu", "Libo Zhang", "Limin Luo", "Huazhong Shu", "Yang Chen"], "article_type": "Research Article", "score": 0.43473727, "title_display": "An Effective CUDA Parallelization of Projection in Iterative Tomography Reconstruction", "publication_date": "2015-11-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0142184"}, {"journal": "PLoS ONE", "abstract": ["\nWe discuss the applicability of the Microsoft cloud computing platform, Azure, for bioinformatics. We focus on the usability of the resource rather than its performance. We provide an example of how R can be used on Azure to analyse a large amount of microarray expression data deposited at the public database ArrayExpress. We provide a walk through to demonstrate explicitly how Azure can be used to perform these analyses in Appendix S1 and we offer a comparison with a local computation. We note that the use of the Platform as a Service (PaaS) offering of Azure can represent a steep learning curve for bioinformatics developers who will usually have a Linux and scripting language background. On the other hand, the presence of an additional set of libraries makes it easier to deploy software in a parallel (scalable) fashion and explicitly manage such a production run with only a few hundred lines of code, most of which can be incorporated from a template. We propose that this environment is best suited for running stable bioinformatics software by users not involved with its development.\n"], "author_display": ["Hugh P. Shanahan", "Anne M. Owen", "Andrew P. Harrison"], "article_type": "Research Article", "score": 0.43448836, "title_display": "Bioinformatics on the Cloud Computing Platform Azure", "publication_date": "2014-07-22T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0102642"}, {"journal": "PLOS ONE", "abstract": ["\nObject localization plays a key role in many popular applications of Wireless Multimedia Sensor Networks (WMSN) and as a result, it has acquired a significant status for the research community. A significant body of research performs this task without considering node orientation, object geometry and environmental variations. As a result, the localized object does not reflect the real world scenarios. In this paper, a novel object localization scheme for WMSN has been proposed that utilizes range free localization, computer vision, and principle component analysis based algorithms. The proposed approach provides the best possible approximation of distance between a wmsn sink and an object, and the orientation of the object using image based information. Simulation results report 99% efficiency and an error ratio of 0.01 (around 1 ft) when compared to other popular techniques.\n"], "author_display": ["Yasar Abbas Ur Rehman", "Muhammad Tariq", "Omar Usman Khan"], "article_type": "Research Article", "score": 0.43346587, "title_display": "Improved Object Localization Using Accurate Distance Estimation in Wireless Multimedia Sensor Networks", "publication_date": "2015-11-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0141558"}, {"journal": "PLoS ONE", "abstract": ["\nIn this paper, we introduce a novel graph polynomial called the \u2018information polynomial\u2019 of a graph. This graph polynomial can be derived by using a probability distribution of the vertex set. By using the zeros of the obtained polynomial, we additionally define some novel spectral descriptors. Compared with those based on computing the ordinary characteristic polynomial of a graph, we perform a numerical study using real chemical databases. We obtain that the novel descriptors do have a high discrimination power.\n"], "author_display": ["Matthias Dehmer", "Laurin A. J. Mueller", "Armin Graber"], "article_type": "Research Article", "score": 0.4332615, "title_display": "New Polynomial-Based Molecular Descriptors with Low Degeneracy", "publication_date": "2010-07-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0011393"}, {"journal": "PLoS Computational Biology", "abstract": ["Many cellular systems rely on the ability to interpret spatial heterogeneities in chemoattractant concentration to direct cell migration. The accuracy of this process is limited by stochastic fluctuations in the concentration of the external signal and in the internal signaling components. Here we use information theory to determine the optimal scheme to detect the location of an external chemoattractant source in the presence of noise. We compute the minimum amount of mutual information needed between the chemoattractant gradient and the internal signal to achieve a prespecified chemotactic accuracy. We show that more accurate chemotaxis requires greater mutual information. We also demonstrate that a priori information can improve chemotaxis efficiency. We compare the optimal signaling schemes with existing experimental measurements and models of eukaryotic gradient sensing. Remarkably, there is good quantitative agreement between the optimal response when no a priori assumption is made about the location of the existing source, and the observed experimental response of unpolarized Dictyostelium discoideum cells. In contrast, the measured response of polarized D. discoideum cells matches closely the optimal scheme, assuming prior knowledge of the external gradient\u2014for example, through prolonged chemotaxis in a given direction. Our results demonstrate that different observed classes of responses in cells (polarized and unpolarized) are optimal under varying information assumptions.: For many cell types, the direction of migration is determined in response to spatial differences in the concentration of chemoattractant, a process known as chemotaxis. Precise chemotaxis\u2014that is, motility with low directional distortion\u2014requires that cells make accurate decisions based on the stochastic fluctuations inherent in cell-surface receptor occupancy. Here, we use rate distortion theory, a branch of information theory, to determine chemotaxis strategies for cells based on this imperfect information about their environment. In engineering, rate distortion theory provides the information processing capabilities required to achieve a desired accuracy. We demonstrate that more accurate chemotaxis requires greater information. We also show that a priori information can improve chemotaxis efficiency. We compare the optimal signaling schemes to existing experimental measurements and models of eukaryotic gradient sensing and demonstrate that different observed types of cellular responses (polarized and unpolarized) are optimal under varying information assumptions. Our results also highlight the constraints that noise places on the performance of cellular systems. "], "author_display": ["Burton W Andrews", "Pablo A Iglesias"], "article_type": "Research Article", "score": 0.43320444, "title_display": "An Information-Theoretic Characterization of the Optimal Gradient Sensing Response of Cells", "publication_date": "2007-08-03T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.0030153"}, {"journal": "PLoS ONE", "abstract": ["\nHow easy is it to reproduce the results found in a typical computational biology paper? Either through experience or intuition the reader will already know that the answer is with difficulty or not at all. In this paper we attempt to quantify this difficulty by reproducing a previously published paper for different classes of users (ranging from users with little expertise to domain experts) and suggest ways in which the situation might be improved. Quantification is achieved by estimating the time required to reproduce each of the steps in the method described in the original paper and make them part of an explicit workflow that reproduces the original results. Reproducing the method took several months of effort, and required using new versions and new software that posed challenges to reconstructing and validating the results. The quantification leads to \u201creproducibility maps\u201d that reveal that novice researchers would only be able to reproduce a few of the steps in the method, and that only expert researchers with advance knowledge of the domain would be able to reproduce the method in its entirety. The workflow itself is published as an online resource together with supporting software and data. The paper concludes with a brief discussion of the complexities of requiring reproducibility in terms of cost versus benefit, and a desiderata with our observations and guidelines for improving reproducibility. This has implications not only in reproducing the work of others from published papers, but reproducing work from one\u2019s own laboratory.\n"], "author_display": ["Daniel Garijo", "Sarah Kinnings", "Li Xie", "Lei Xie", "Yinliang Zhang", "Philip E. Bourne", "Yolanda Gil"], "article_type": "Research Article", "score": 0.43273044, "title_display": "Quantifying Reproducibility in Computational Biology: The Case of the Tuberculosis Drugome", "publication_date": "2013-11-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0080278"}, {"journal": "PLoS ONE", "abstract": ["\nOnline citizen science offers a low-cost way to strengthen the infrastructure for scientific research and engage members of the public in science. As the sustainability of online citizen science projects depends on volunteers who contribute their skills, time, and energy, the objective of this study is to investigate effects of motivational factors on the quantity and quality of citizen scientists' contribution. Building on the social movement participation model, findings from a longitudinal empirical study in three different citizen science projects reveal that quantity of contribution is determined by collective motives, norm-oriented motives, reputation, and intrinsic motives. Contribution quality, on the other hand, is positively affected only by collective motives and reputation. We discuss implications for research on the motivation for participation in technology-mediated social participation and for the practice of citizen science.\n"], "author_display": ["Oded Nov", "Ofer Arazy", "David Anderson"], "article_type": "Research Article", "score": 0.4325273, "title_display": "Scientists@Home: What Drives the Quantity and Quality of Online Citizen Science Participation?", "publication_date": "2014-04-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0090375"}, {"journal": "PLOS ONE", "abstract": ["Background: High quality of informed consent form is essential for adequate information transfer between physicians and patients. Current status of medical procedure consent forms in clinical practice in Croatia specifically in terms of the readability and the content is unknown. The aim of this study was to assess the readability and the content of informed consent forms for diagnostic and therapeutic procedures used with patients in Croatia. Methods: 52 informed consent forms from six Croatian hospitals on the secondary and tertiary health-care level were tested for reading difficulty using Simple Measure of Gobbledygook (SMOG) formula adjusted for Croatian language and for qualitative analysis of the content. Results: The averaged SMOG grade of analyzed informed consent forms was 13.25 (SD 1.59, range 10\u201319). Content analysis revealed that informed consent forms included description of risks in 96% of the cases, benefits in 81%, description of procedures in 78%, alternatives in 52%, risks and benefits of alternatives in 17% and risks and benefits of not receiving treatment or undergoing procedures in 13%. Conclusions: Readability of evaluated informed consent forms is not appropriate for the general population in Croatia. The content of the forms failed to include in high proportion of the cases description of alternatives, risks and benefits of alternatives, as well as risks and benefits of not receiving treatments or undergoing procedures. Data obtained from this research could help in development and improvement of informed consent forms in Croatia especially now when Croatian hospitals are undergoing the process of accreditation. "], "author_display": ["Luka Vu\u010demilo", "Ana Borove\u010dki"], "article_type": "Research Article", "score": 0.43219215, "title_display": "Readability and Content Assessment of Informed Consent Forms for Medical Procedures in Croatia", "publication_date": "2015-09-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0138017"}, {"journal": "PLoS ONE", "abstract": ["\nMobile Visual Location Recognition (MVLR) has attracted a lot of researchers' attention in the past few years. Existing MVLR applications commonly use Query-by-Example (QBE) based image retrieval principle to fulfill the location recognition task. However, the QBE framework is not reliable enough due to the variations in the capture conditions and viewpoint changes between the query image and the database images. To solve the above problem, we make following contributions to the design of a panorama based on-device MVLR system. Firstly, we design a heading (from digital compass) aware BOF (Bag-of-features) model to generate the descriptors of panoramic images. Our approach fully considers the characteristics of the panoramic images and can facilitate the panorama based on-device MVLR to a large degree. Secondly, to search high dimensional visual descriptors directly on mobile devices, we propose an effective bilinear compressed sensing based encoding method. While being fast and accurate enough for on-device implementation, our algorithm can also reduce the memory usage of projection matrix significantly. Thirdly, we also release a panoramas database as well as a set of test panoramic quires which can be used as a new benchmark to facilitate further research in the area. Experimental results prove the effectiveness of the proposed methods for on-device MVLR applications.\n"], "author_display": ["Tao Guan", "Yin Fan", "Liya Duan", "Junqing Yu"], "article_type": "Research Article", "score": 0.43188354, "title_display": "On-Device Mobile Visual Location Recognition by Using Panoramic Images and Compressed Sensing Based Visual Descriptors", "publication_date": "2014-06-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0098806"}, {"journal": "PLOS ONE", "abstract": ["\nScience education is progressively more focused on employing inquiry-based learning methods in the classroom and increasing scientific literacy among students. However, due to time and resource constraints, many classroom science activities and laboratory experiments focus on simple inquiry, with a step-by-step approach to reach predetermined outcomes. The science classroom inquiry (SCI) simulations were designed to give students real life, authentic science experiences within the confines of a typical classroom. The SCI simulations allow students to engage with a science problem in a meaningful, inquiry-based manner. Three discrete SCI simulations were created as website applications for use with middle school and high school students. For each simulation, students were tasked with solving a scientific problem through investigation and hypothesis testing. After completion of the simulation, 67% of students reported a change in how they perceived authentic science practices, specifically related to the complex and dynamic nature of scientific research and how scientists approach problems. Moreover, 80% of the students who did not report a change in how they viewed the practice of science indicated that the simulation confirmed or strengthened their prior understanding. Additionally, we found a statistically significant positive correlation between students\u2019 self-reported changes in understanding of authentic science practices and the degree to which each simulation benefitted learning. Since SCI simulations were effective in promoting both student learning and student understanding of authentic science practices with both middle and high school students, we propose that SCI simulations are a valuable and versatile technology that can be used to educate and inspire a wide range of science students on the real-world complexities inherent in scientific study.\n"], "author_display": ["Melanie E. Peffer", "Matthew L. Beckler", "Christian Schunn", "Maggie Renken", "Amanda Revak"], "article_type": "Research Article", "score": 0.43181413, "title_display": "Science Classroom Inquiry (SCI) Simulations: A Novel Method to Scaffold Science Learning", "publication_date": "2015-03-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0120638"}, {"journal": "PLoS ONE", "abstract": ["\nIn this paper, we examine the uniqueness (discrimination power) of a newly proposed graph invariant based on the matrix  defined by Randi\u0107 et al. In order to do so, we use exhaustively generated graphs instead of special graph classes such as trees only. Using these graph classes allow us to generalize the findings towards complex networks as they usually do not possess any structural constraints. We obtain that the uniqueness of this newly proposed graph invariant is approximately as low as the uniqueness of the Balaban  index on exhaustively generated (general) graphs.\n"], "author_display": ["Matthias Dehmer", "Yongtang Shi"], "article_type": "Research Article", "score": 0.43161225, "title_display": "The Uniqueness of -Matrix Graph Invariants", "publication_date": "2014-01-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0083868"}, {"journal": "PLoS ONE", "abstract": ["\nMost of the existing multi-recipient signcryption schemes do not take the anonymity of recipients into consideration because the list of the identities of all recipients must be included in the ciphertext as a necessary element for decryption. Although the signer\u2019s anonymity has been taken into account in several alternative schemes, these schemes often suffer from the cross-comparison attack and joint conspiracy attack. That is to say, there are few schemes that can achieve complete anonymity for both the signer and the recipient. However, in many practical applications, such as network conference, both the signer\u2019s and the recipient\u2019s anonymity should be considered carefully. Motivated by these concerns, we propose a novel multi-recipient signcryption scheme with complete anonymity. The new scheme can achieve both the signer\u2019s and the recipient\u2019s anonymity at the same time. Each recipient can easily judge whether the received ciphertext is from an authorized source, but cannot determine the real identity of the sender, and at the same time, each participant can easily check decryption permission, but cannot determine the identity of any other recipient. The scheme also provides a public verification method which enables anyone to publicly verify the validity of the ciphertext. Analyses show that the proposed scheme is more efficient in terms of computation complexity and ciphertext length and possesses more advantages than existing schemes, which makes it suitable for practical applications. The proposed scheme could be used for network conferences, paid-TV or DVD broadcasting applications to solve the secure communication problem without violating the privacy of each participant.\nKey words: Multi-recipient signcryption; Signcryption; Complete Anonymity; Public verification.\n"], "author_display": ["Liaojun Pang", "Huixian Li", "Lu Gao", "Yumin Wang"], "article_type": "Research Article", "score": 0.43159074, "title_display": "Completely Anonymous Multi-Recipient Signcryption Scheme with Public Verification", "publication_date": "2013-05-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0063562"}, {"journal": "PLoS ONE", "abstract": ["\n        Among proteins, orthologs are defined as those that are derived by vertical descent from a single progenitor in the last common ancestor of their host organisms. Our goal is to compute a complete set of protein orthologs derived from all currently available complete bacterial and archaeal genomes. Traditional approaches typically rely on all-against-all BLAST searching which is prohibitively expensive in terms of hardware requirements or computational time (requiring an estimated 18 months or more on a typical server). Here, we present xBASE-Orth, a system for ongoing ortholog annotation, which applies a \u201cdivide and conquer\u201d approach and adopts a pragmatic scheme that trades accuracy for speed. Starting at species level, xBASE-Orth carefully constructs and uses pan-genomes as proxies for the full collections of coding sequences at each level as it progressively climbs the taxonomic tree using the previously computed data. This leads to a significant decrease in the number of alignments that need to be performed, which translates into faster computation, making ortholog computation possible on a global scale. Using xBASE-Orth, we analyzed an NCBI collection of 1,288 bacterial and 94 archaeal complete genomes with more than 4 million coding sequences in 5 weeks and predicted more than 700 million ortholog pairs, clustered in 175,531 orthologous groups. We have also identified sets of highly conserved bacterial and archaeal orthologs and in so doing have highlighted anomalies in genome annotation and in the proposed composition of the minimal bacterial genome. In summary, our approach allows for scalable and efficient computation of the bacterial and archaeal ortholog annotations. In addition, due to its hierarchical nature, it is suitable for incorporating novel complete genomes and alternative genome annotations. The computed ortholog data and a continuously evolving set of applications based on it are integrated in the xBASE database, available at http://www.xbase.ac.uk/.\n      "], "author_display": ["Mihail R. Halachev", "Nicholas J. Loman", "Mark J. Pallen"], "article_type": "Research Article", "score": 0.43002886, "title_display": "Calculating Orthologs in Bacteria and Archaea: A Divide and Conquer Approach", "publication_date": "2011-12-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0028388"}, {"journal": "PLoS ONE", "abstract": ["\n        This paper presents a collection of computational modules implemented with chemical reactions: an inverter, an incrementer, a decrementer, a copier, a comparator, a multiplier, an exponentiator, a raise-to-a-power operation, and a logarithm in base two. Unlike previous schemes for chemical computation, this method produces designs that are dependent only on coarse rate categories for the reactions (\u201cfast\u201d vs. \u201cslow\u201d). Given such categories, the computation is exact and independent of the specific reaction rates. The designs are validated through stochastic simulations of the chemical kinetics.\n      "], "author_display": ["Phillip Senum", "Marc Riedel"], "article_type": "Research Article", "score": 0.43001854, "title_display": "Rate-Independent Constructs for Chemical Computation", "publication_date": "2011-06-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0021414"}, {"journal": "PLOS ONE", "abstract": ["Background: The isolation with migration (IM) model is important for studies in population genetics and phylogeography. IM program applies the IM model to genetic data drawn from a pair of closely related populations or species based on Markov chain Monte Carlo (MCMC) simulations of gene genealogies. But computational burden of IM program has placed limits on its application. Methodology: With strong computational power, Graphics Processing Unit (GPU) has been widely used in many fields. In this article, we present an effective implementation of IM program on one GPU based on Compute Unified Device Architecture (CUDA), which we call gPGA. Conclusions: Compared with IM program, gPGA can achieve up to 52.30X speedup on one GPU. The evaluation results demonstrate that it allows datasets to be analyzed effectively and rapidly for research on divergence population genetics. The software is freely available with source code at https://github.com/chunbaozhou/gPGA. "], "author_display": ["Chunbao Zhou", "Xianyu Lang", "Yangang Wang", "Chaodong Zhu"], "article_type": "Research Article", "score": 0.42984304, "title_display": "gPGA: GPU Accelerated Population Genetics Analyses", "publication_date": "2015-08-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0135028"}, {"abstract": ["\n        The relationship between focus and new information has been unclear despite being the subject of several information structure studies. Here, we report an eye-tracking experiment that explored the relationship between them in on-line discourse processing in Chinese reading. Focus was marked by the Chinese focus-particle \u201cshi\", which is equivalent to the cleft structure \u201cit was\u2026 who\u2026\" in English. New information was defined as the target word that was not present in previous contexts. Our results show that, in the target region, focused information was processed more quickly than non-focused information, while new information was processed more slowly than given information. These results reveal differences in processing patterns between focus and newness, and suggest that they are different concepts that relate to different aspects of cognitive processing. In addition, the effect of new/given information occurred in the post-target region for the focus condition, but not for the non-focus condition, suggesting a complex relationship between focus and newness in the discourse integration stage.\n      "], "author_display": ["Lijing Chen", "Xingshan Li", "Yufang Yang"], "article_type": "Research Article", "score": 0.4297457, "title_display": "Focus, Newness and Their Combination: Processing of Information Structure in Discourse", "publication_date": "2012-08-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0042533"}, {"journal": "PLOS ONE", "abstract": ["\nThe rapid growth of Internet applications has made communication anonymity an increasingly important or even indispensable security requirement. Onion routing has been employed as an infrastructure for anonymous communication over a public network, which provides anonymous connections that are strongly resistant to both eavesdropping and traffic analysis. However, existing onion routing protocols usually exhibit poor performance due to repeated encryption operations. In this paper, we first present an improved anonymous multi-receiver identity-based encryption (AMRIBE) scheme, and an improved identity-based one-way anonymous key agreement (IBOWAKE) protocol. We then propose an efficient onion routing protocol named AIB-OR that provides provable security and strong anonymity. Our main approach is to use our improved AMRIBE scheme and improved IBOWAKE protocol in onion routing circuit construction. Compared with other onion routing protocols, AIB-OR provides high efficiency, scalability, strong anonymity and fault tolerance. Performance measurements from a prototype implementation show that our proposed AIB-OR can achieve high bandwidths and low latencies when deployed over the Internet.\n"], "author_display": ["Changji Wang", "Dongyuan Shi", "Xilei Xu"], "article_type": "Research Article", "score": 0.42922097, "title_display": "AIB-OR: Improving Onion Routing Circuit Construction Using Anonymous Identity-Based Cryptosystems", "publication_date": "2015-03-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0121226"}, {"journal": "PLOS ONE", "abstract": ["\nPopulation scale sequencing of whole human genomes is becoming economically feasible; however, data management and analysis remains a formidable challenge for many research groups. Large sequencing studies, like the 1000 Genomes Project, have improved our understanding of human demography and the effect of rare genetic variation in disease. Variant calling on datasets of hundreds or thousands of genomes is time-consuming, expensive, and not easily reproducible given the myriad components of a variant calling pipeline. Here, we describe a cloud-based pipeline for joint variant calling in large samples using the Real Time Genomics population caller. We deployed the population caller on the Amazon cloud with the DNAnexus platform in order to achieve low-cost variant calling. Using our pipeline, we were able to identify 68.3 million variants in 2,535 samples from Phase 3 of the 1000 Genomes Project. By performing the variant calling in a parallel manner, the data was processed within 5 days at a compute cost of $7.33 per sample (a total cost of $18,590 for completed jobs and $21,805 for all jobs). Analysis of cost dependence and running time on the data size suggests that, given near linear scalability, cloud computing can be a cheap and efficient platform for analyzing even larger sequencing studies in the future.\n"], "author_display": ["Suyash S. Shringarpure", "Andrew Carroll", "Francisco M. De La Vega", "Carlos D. Bustamante"], "article_type": "Research Article", "score": 0.4292091, "title_display": "Inexpensive and Highly Reproducible Cloud-Based Variant Calling of 2,535 Human Genomes", "publication_date": "2015-06-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0129277"}, {"journal": "PLoS ONE", "abstract": ["\nWe describe the first direct brain-to-brain interface in humans and present results from experiments involving six different subjects. Our non-invasive interface, demonstrated originally in August 2013, combines electroencephalography (EEG) for recording brain signals with transcranial magnetic stimulation (TMS) for delivering information to the brain. We illustrate our method using a visuomotor task in which two humans must cooperate through direct brain-to-brain communication to achieve a desired goal in a computer game. The brain-to-brain interface detects motor imagery in EEG signals recorded from one subject (the \u201csender\u201d) and transmits this information over the internet to the motor cortex region of a second subject (the \u201creceiver\u201d). This allows the sender to cause a desired motor response in the receiver (a press on a touchpad) via TMS. We quantify the performance of the brain-to-brain interface in terms of the amount of information transmitted as well as the accuracies attained in (1) decoding the sender\u2019s signals, (2) generating a motor response from the receiver upon stimulation, and (3) achieving the overall goal in the cooperative visuomotor task. Our results provide evidence for a rudimentary form of direct information transmission from one human brain to another using non-invasive means.\n"], "author_display": ["Rajesh P. N. Rao", "Andrea Stocco", "Matthew Bryan", "Devapratim Sarma", "Tiffany M. Youngquist", "Joseph Wu", "Chantel S. Prat"], "article_type": "Research Article", "score": 0.42896357, "title_display": "A Direct Brain-to-Brain Interface in Humans", "publication_date": "2014-11-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0111332"}, {"journal": "PLoS ONE", "abstract": ["\n        Three studies evaluated the role of 4-year-old children's agency- and animacy-attributions when learning from a computerized ghost control (GC). In GCs, participants observe events occurring without an apparent agent, as if executed by a \u201cghost\u201d or unobserved causal forces. Using a touch-screen, children in Experiment 1 responded to three pictures in a specific order under three learning conditions: (i) trial-and-error (Baseline), (ii) imitation and (iii) Ghost Control. Before testing in the GC, children were read one of three scripts that determined agency attributions. Post-test assessments confirmed that all children attributed agency to the computer and learned in all GCs. In Experiment 2, children were not trained on the computer prior to testing, and no scripts were used. Three different GCs, varying in number of agency cues, were used. Children failed to learn in these GCs, yet attributed agency and animacy to the computer. Experiment 3 evaluated whether children could learn from a human model in the absence of training under conditions where the information presented by the model and the computer was either consistent or inconsistent. Children evidenced learning in both of these conditions. Overall, learning in social conditions (Exp. 3) was significantly better than learning in GCs (Exp. 2). These results, together with other published research, suggest that children privilege social over non-social sources of information and are generally more adept at learning novel tasks from a human than from a computer or GC.\n      "], "author_display": ["Francys Subiaul", "Jennifer Vonk", "M. D. Rutherford"], "article_type": "Research Article", "score": 0.42870134, "title_display": "The Ghosts in the Computer: The Role of Agency and Animacy Attributions in \u201cGhost Controls\u201d", "publication_date": "2011-11-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0026429"}, {"journal": "PLoS ONE", "abstract": ["\nResearch productivity assessment is increasingly relevant for allocation of research funds. On one hand, this assessment is challenging because it involves both qualitative and quantitative analysis of several characteristics, most of them subjective in nature. On the other hand, current tools and academic social networks make bibliometric data web-available to everyone for free. Those tools, especially when combined with other data, are able to create a rich environment from which information on research productivity can be extracted. In this context, our work aims at characterizing the Brazilian Computer Science graduate programs and the relationship among themselves. We (i) present views of the programs from different perspectives, (ii) rank the programs according to each perspective and a combination of them, (iii) show correlation between assessment metrics, (iv) discuss how programs relate to another, and (v) infer aspects that boost programs' research productivity. The results indicate that programs with a higher insertion in the coauthorship network topology also possess a higher research productivity between 2004 and 2009.\n"], "author_display": ["Luciano A. Digiampietri", "Jes\u00fas P. Mena-Chalco", "Pedro O. S. Vaz de Melo", "Ana P. R. Malheiro", "D\u00e2nia N. O. Meira", "Laryssa F. Franco", "Leonardo B. Oliveira"], "article_type": "Research Article", "score": 0.42784464, "title_display": "BraX-Ray: An X-Ray of the Brazilian Computer Science Graduate Programs", "publication_date": "2014-04-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0094541"}, {"journal": "PLoS ONE", "abstract": ["\nMixtures are a commonly encountered form of biological evidence that contain DNA from two or more contributors. Laboratory analysis of mixtures produces data signals that usually cannot be separated into distinct contributor genotypes. Computer modeling can resolve the genotypes up to probability, reflecting the uncertainty inherent in the data. Human analysts address the problem by simplifying the quantitative data in a threshold process that discards considerable identification information. Elevated stochastic threshold levels potentially discard more information. This study examines three different mixture interpretation methods. In 72 criminal cases, 111 genotype comparisons were made between 92 mixture items and relevant reference samples. TrueAllele computer modeling was done on all the evidence samples, and documented in DNA match reports that were provided as evidence for each case. Threshold-based Combined Probability of Inclusion (CPI) and stochastically modified CPI (mCPI) analyses were performed as well. TrueAllele\u2019s identification information in 101 positive matches was used to assess the reliability of its modeling approach. Comparison was made with 81 CPI and 53 mCPI DNA match statistics that were manually derived from the same data. There were statistically significant differences between the DNA interpretation methods. TrueAllele gave an average match statistic of 113 billion, CPI averaged 6.68 million, and mCPI averaged 140. The computer was highly specific, with a false positive rate under 0.005%. The modeling approach was precise, having a factor of two within-group standard deviation. TrueAllele accuracy was indicated by having uniformly distributed match statistics over the data set. The computer could make genotype comparisons that were impossible or impractical using manual methods. TrueAllele computer interpretation of DNA mixture evidence is sensitive, specific, precise, accurate and more informative than manual interpretation alternatives. It can determine DNA match statistics when threshold-based methods cannot. Improved forensic science computation can affect criminal cases by providing reliable scientific evidence.\n"], "author_display": ["Mark W. Perlin", "Kiersten Dormer", "Jennifer Hornyak", "Lisa Schiermeier-Wood", "Susan Greenspoon"], "article_type": "Research Article", "score": 0.42763922, "title_display": "TrueAllele Casework on Virginia DNA Mixture Evidence: Computer and Manual Interpretation in 72 Reported Criminal Cases", "publication_date": "2014-03-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0092837"}, {"journal": "PLOS ONE", "abstract": ["\nReplication is an essential requirement for scientific discovery. The current study aims to generalize and replicate 10 propositions made in previous Twitter studies using a representative dataset. Our findings suggest 6 out of 10 propositions could not be replicated due to the variations of data collection, analytic strategies employed, and inconsistent measurements. The study\u2019s contributions are twofold: First, it systematically summarized and assessed some important claims in the field, which can inform future studies. Second, it proposed a feasible approach to generating a random sample of Twitter users and its associated ego networks, which might serve as a solution for answering social-scientific questions at the individual level without accessing the complete data archive.\n"], "author_display": ["Hai Liang", "King-wa Fu"], "article_type": "Research Article", "score": 0.42697746, "title_display": "Testing Propositions Derived from Twitter Studies: Generalization and Replication in Computational Social Science", "publication_date": "2015-08-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0134270"}, {"journal": "PLoS Computational Biology", "abstract": ["\nComputational approaches have promised to organize collections of functional genomics data into testable predictions of gene and protein involvement in biological processes and pathways. However, few such predictions have been experimentally validated on a large scale, leaving many bioinformatic methods unproven and underutilized in the biology community. Further, it remains unclear what biological concerns should be taken into account when using computational methods to drive real-world experimental efforts. To investigate these concerns and to establish the utility of computational predictions of gene function, we experimentally tested hundreds of predictions generated from an ensemble of three complementary methods for the process of mitochondrial organization and biogenesis in Saccharomyces cerevisiae. The biological data with respect to the mitochondria are presented in a companion manuscript published in PLoS Genetics (doi:10.1371/journal.pgen.1000407). Here we analyze and explore the results of this study that are broadly applicable for computationalists applying gene function prediction techniques, including a new experimental comparison with 48 genes representing the genomic background. Our study leads to several conclusions that are important to consider when driving laboratory investigations using computational prediction approaches. While most genes in yeast are already known to participate in at least one biological process, we confirm that genes with known functions can still be strong candidates for annotation of additional gene functions. We find that different analysis techniques and different underlying data can both greatly affect the types of functional predictions produced by computational methods. This diversity allows an ensemble of techniques to substantially broaden the biological scope and breadth of predictions. We also find that performing prediction and validation steps iteratively allows us to more completely characterize a biological area of interest. While this study focused on a specific functional area in yeast, many of these observations may be useful in the contexts of other processes and organisms.\nAuthor Summary: Genome sequencing has provided us with \u201cparts lists\u201d of genes for many organisms, but many of the biological roles these genes are still unknown. While a great deal of functional genomic data exists, providing information about these genes and their roles, the rate at which these data are leveraged into concrete biological knowledge lags far behind the rate of data generation. Many computational approaches have been developed to generate accurate predictions of gene functions, with the goal of bridging this divide. However, as no large-scale experimental efforts have been based on such approaches, their validity and utility remains unproven. We have performed a study that experimentally evaluates predictions from a combination of three computational function prediction approaches, focusing on mitochondrion-related processes in brewer's yeast as a model system. By using computational predictions to guide our laboratory investigation, we have greatly accelerated the rate at which proteins can be assigned to biological processes. Further, our results demonstrate that in order to achieve the best results, it is important for computational biologists to consider both the underlying data and the algorithmic foundations of the methods used to predict function. Lastly, we demonstrate that iterating through phases of prediction and validation has quickly and extensively expanded our knowledge of mitochondrial biology. "], "author_display": ["Matthew A. Hibbs", "Chad L. Myers", "Curtis Huttenhower", "David C. Hess", "Kai Li", "Amy A. Caudy", "Olga G. Troyanskaya"], "article_type": "Research Article", "score": 0.4267493, "title_display": "Directing Experimental Biology: A Case Study in Mitochondrial Biogenesis", "publication_date": "2009-03-20T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1000322"}, {"abstract": ["Background: The analysis of biological networks has become a major challenge due to the recent development of high-throughput techniques that are rapidly producing very large data sets. The exploding volumes of biological data are craving for extreme computational power and special computing facilities (i.e. super-computers). An inexpensive solution, such as General Purpose computation based on Graphics Processing Units (GPGPU), can be adapted to tackle this challenge, but the limitation of the device internal memory can pose a new problem of scalability. An efficient data and computational parallelism with partitioning is required to provide a fast and scalable solution to this problem. Results: We propose an efficient parallel formulation of the k-Nearest Neighbour (kNN) search problem, which is a popular method for classifying objects in several fields of research, such as pattern recognition, machine learning and bioinformatics. Being very simple and straightforward, the performance of the kNN search degrades dramatically for large data sets, since the task is computationally intensive. The proposed approach is not only fast but also scalable to large-scale instances. Based on our approach, we implemented a software tool GPU-FS-kNN (GPU-based Fast and Scalable k-Nearest Neighbour) for CUDA enabled GPUs. The basic approach is simple and adaptable to other available GPU architectures. We observed speed-ups of 50\u201360 times compared with CPU implementation on a well-known breast microarray study and its associated data sets. Conclusion: Our GPU-based Fast and Scalable k-Nearest Neighbour search technique (GPU-FS-kNN) provides a significant performance improvement for nearest neighbour computation in large-scale networks. Source code and the software tool is available under GNU Public License (GPL) at https://sourceforge.net/p/gpufsknn/. "], "author_display": ["Ahmed Shamsul Arefin", "Carlos Riveros", "Regina Berretta", "Pablo Moscato"], "article_type": "Research Article", "score": 0.42674294, "title_display": "GPU-FS-<i>k</i>NN: A Software Tool for Fast and Scalable <i>k</i>NN Computation Using GPUs", "publication_date": "2012-08-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0044000"}, {"journal": "PLoS ONE", "abstract": ["\nWe examined the presence of maximum information preservation, which may be a fundamental principle of information transmission in all sensory modalities, in the Drosophila antennal lobe using an experimentally grounded network model and physiological data. Recent studies have shown a nonlinear firing rate transformation between olfactory receptor neurons (ORNs) and second-order projection neurons (PNs). As a result, PNs can use their dynamic range more uniformly than ORNs in response to a diverse set of odors. Although this firing rate transformation is thought to assist the decoder in discriminating between odors, there are no comprehensive, quantitatively supported studies examining this notion. Therefore, we quantitatively investigated the efficiency of this firing rate transformation from the viewpoint of information preservation by computing the mutual information between odor stimuli and PN responses in our network model. In the Drosophila olfactory system, all ORNs and PNs are divided into unique functional processing units called glomeruli. The nonlinear transformation between ORNs and PNs is formed by intraglomerular transformation and interglomerular interaction through local neurons (LNs). By exploring possible nonlinear transformations produced by these two factors in our network model, we found that mutual information is maximized when a weak ORN input is preferentially amplified within a glomerulus and the net LN input to each glomerulus is inhibitory. It is noteworthy that this is the very combination observed experimentally. Furthermore, the shape of the resultant nonlinear transformation is similar to that observed experimentally. These results imply that information related to odor stimuli is almost maximally preserved in the Drosophila olfactory circuit. We also discuss how intraglomerular transformation and interglomerular inhibition combine to maximize mutual information.\n"], "author_display": ["Ryota Satoh", "Masafumi Oizumi", "Hokto Kazama", "Masato Okada"], "article_type": "Research Article", "score": 0.42662066, "title_display": "Mechanisms of Maximum Information Preservation in the <i>Drosophila</i> Antennal Lobe", "publication_date": "2010-05-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0010644"}, {"journal": "PLoS Computational Biology", "abstract": ["Biological organisms continuously select and sample information used by their neural structures for perception and action, and for creating coherent cognitive states guiding their autonomous behavior. Information processing, however, is not solely an internal function of the nervous system. Here we show, instead, how sensorimotor interaction and body morphology can induce statistical regularities and information structure in sensory inputs and within the neural control architecture, and how the flow of information between sensors, neural units, and effectors is actively shaped by the interaction with the environment. We analyze sensory and motor data collected from real and simulated robots and reveal the presence of information structure and directed information flow induced by dynamically coupled sensorimotor activity, including effects of motor outputs on sensory inputs. We find that information structure and information flow in sensorimotor networks (a) is spatially and temporally specific; (b) can be affected by learning, and (c) can be affected by changes in body morphology. Our results suggest a fundamental link between physical embeddedness and information, highlighting the effects of embodied interactions on internal (neural) information processing, and illuminating the role of various system components on the generation of behavior.Synopsis: How neurons encode and process information is a key problem in computational biology and neuroscience. In this paper, Lungarella and Sporns present a novel application of computational methods to the integration of neural and sensorimotor processes at the systems-level scale. The central result of their study is that sensorimotor interaction and body morphology can induce statistical regularities and information structure in sensory inputs and within the neural control architecture. The informational content of inputs is thus not independent of output, and the authors suggest that neural coding needs to be considered in the context of the \u201cembeddedness\u201d of the organism within its eco-niche. Using robots and nonlinear time-series analysis techniques, they investigate how the flow of information between sensors, neural units, and effectors is actively shaped by interaction with the environment. This study represents a first step towards the development of an explicit quantitative framework that unifies neural and behavioral processes. Such a framework could also shed significant new light on key constraints shaping the evolution and development of nervous systems and their behavioral and cognitive capacities. In addition, it could provide an important design principle to guide the construction of more efficient artificial cognitive systems. "], "author_display": ["Max Lungarella", "Olaf Sporns"], "article_type": "Research Article", "score": 0.42640656, "title_display": "Mapping Information Flow in Sensorimotor Networks", "publication_date": "2006-10-27T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.0020144"}, {"journal": "PLoS ONE", "abstract": ["\n        We introduce an automated method for the bottom-up reconstruction of the cognitive evolution of science, based on big-data issued from digital libraries, and modeled as lineage relationships between scientific fields. We refer to these dynamic structures as phylomemetic networks or phylomemies, by analogy with biological evolution; and we show that they exhibit strong regularities, with clearly identifiable phylomemetic patterns. Some structural properties of the scientific fields - in particular their density -, which are defined independently of the phylomemy reconstruction, are clearly correlated with their status and their fate in the phylomemy (like their age or their short term survival). Within the framework of a quantitative epistemology, this approach raises the question of predictibility for science evolution, and sketches a prototypical life cycle of the scientific fields: an increase of their cohesion after their emergence, the renewal of their conceptual background through branching or merging events, before decaying when their density is getting too low.\n      "], "author_display": ["David Chavalarias", "Jean-Philippe Cointet"], "article_type": "Research Article", "score": 0.4260864, "title_display": "Phylomemetic Patterns in Science Evolution\u2014The Rise and Fall of Scientific Fields", "publication_date": "2013-02-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0054847"}, {"journal": "PLoS ONE", "abstract": ["\nTopological entropy is one of the most difficult entropies to be used to analyze the DNA sequences, due to the finite sample and high-dimensionality problems. In order to overcome these problems, a generalized topological entropy is introduced. The relationship between the topological entropy and the generalized topological entropy is compared, which shows the topological entropy is a special case of the generalized entropy. As an application the generalized topological entropy in introns, exons and promoter regions was computed, respectively. The results indicate that the entropy of introns is higher than that of exons, and the entropy of the exons is higher than that of the promoter regions for each chromosome, which suggest that DNA sequence of the promoter regions is more regular than the exons and introns.\n"], "author_display": ["Shuilin Jin", "Renjie Tan", "Qinghua Jiang", "Li Xu", "Jiajie Peng", "Yong Wang", "Yadong Wang"], "article_type": "Research Article", "score": 0.425956, "title_display": "A Generalized Topological Entropy for Analyzing the Complexity of DNA Sequences", "publication_date": "2014-02-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0088519"}, {"journal": "PLoS ONE", "abstract": ["\n        The development of biological informatics infrastructure capable of supporting growing data management and analysis environments is an increasing need within the systematics biology community. Although significant progress has been made in recent years on developing new algorithms and tools for analyzing and visualizing large phylogenetic data and trees, implementation of these resources is often carried out by bioinformatics experts, using one-off scripts. Therefore, a gap exists in providing data management support for a large set of non-technical users. The TOLKIN project (Tree of Life Knowledge and Information Network) addresses this need by supporting capabilities to manage, integrate, and provide public access to molecular, morphological, and biocollections data and research outcomes through a collaborative, web application. This data management framework allows aggregation and import of sequences, underlying documentation about their source, including vouchers, tissues, and DNA extraction. It combines features of LIMS and workflow environments by supporting management at the level of individual observations, sequences, and specimens, as well as assembly and versioning of data sets used in phylogenetic inference. As a web application, the system provides multi-user support that obviates current practices of sharing data sets as files or spreadsheets via email.\n      "], "author_display": ["Reed S. Beaman", "Greg H. Traub", "Christopher A. Dell", "Nestor Santiago", "Jin Koh", "Nico Cellinese"], "article_type": "Research Article", "score": 0.42539698, "title_display": "TOLKIN \u2013 Tree of Life Knowledge and Information Network: Filling a Gap for Collaborative Research in Biological Systematics", "publication_date": "2012-06-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0039352"}, {"journal": "PLoS ONE", "abstract": ["\nThe notion that cooperation can aid a group of agents to solve problems more efficiently than if those agents worked in isolation is prevalent in computer science and business circles. Here we consider a primordial form of cooperation \u2013 imitative learning \u2013 that allows an effective exchange of information between agents, which are viewed as the processing units of a social intelligence system or collective brain. In particular, we use agent-based simulations to study the performance of a group of agents in solving a cryptarithmetic problem. An agent can either perform local random moves to explore the solution space of the problem or imitate a model agent \u2013 the best performing agent in its influence network. There is a trade-off between the number of agents  and the imitation probability , and for the optimal balance between these parameters we observe a thirtyfold diminution in the computational cost to find the solution of the cryptarithmetic problem as compared with the independent search. If those parameters are chosen far from the optimal setting, however, then imitative learning can impair greatly the performance of the group.\n"], "author_display": ["Jos\u00e9 F. Fontanari"], "article_type": "Research Article", "score": 0.42468286, "title_display": "Imitative Learning as a Connector of Collective Brains", "publication_date": "2014-10-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0110517"}, {"journal": "PLOS ONE", "abstract": ["\nSurveillance video service (SVS) is one of the most important services provided in a smart city. It is very important for the utilization of SVS to provide design efficient surveillance video analysis techniques. Key frame extraction is a simple yet effective technique to achieve this goal. In surveillance video applications, key frames are typically used to summarize important video content. It is very important and essential to extract key frames accurately and efficiently. A novel approach is proposed to extract key frames from traffic surveillance videos based on GPU (graphics processing units) to ensure high efficiency and accuracy. For the determination of key frames, motion is a more salient feature in presenting actions or events, especially in surveillance videos. The motion feature is extracted in GPU to reduce running time. It is also smoothed to reduce noise, and the frames with local maxima of motion information are selected as the final key frames. The experimental results show that this approach can extract key frames more accurately and efficiently compared with several other methods.\n"], "author_display": ["Ran Zheng", "Chuanwei Yao", "Hai Jin", "Lei Zhu", "Qin Zhang", "Wei Deng"], "article_type": "Research Article", "score": 0.42439845, "title_display": "Parallel Key Frame Extraction for Surveillance Video Service in a Smart City", "publication_date": "2015-08-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0135694"}, {"journal": "PLOS ONE", "abstract": ["Background: Peer evaluation is the cornerstone of science evaluation. In this paper, we analyze whether or not a form of peer evaluation, the pre-publication selection of the best papers in Computer Science (CS) conferences, is better than random, when considering future citations received by the papers. Methods: Considering 12 conferences (for several years), we collected the citation counts from Scopus for both the best papers and the non-best papers. For a different set of 17 conferences, we collected the data from Google Scholar. For each data set, we computed the proportion of cases whereby the best paper has more citations. We also compare this proportion for years before 2010 and after to evaluate if there is a propaganda effect. Finally, we count the proportion of best papers that are in the top 10% and 20% most cited for each conference instance. Results: The probability that a best paper will receive more citations than a non best paper is 0.72 (95% CI = 0.66, 0.77) for the Scopus data, and 0.78 (95% CI = 0.74, 0.81) for the Scholar data. There are no significant changes in the probabilities for different years. Also, 51% of the best papers are among the top 10% most cited papers in each conference/year, and 64% of them are among the top 20% most cited. Discussion: There is strong evidence that the selection of best papers in Computer Science conferences is better than a random selection, and that a significant number of the best papers are among the top cited papers in the conference. "], "author_display": ["Jacques Wainer", "Michael Eckmann", "Anderson Rocha"], "article_type": "Research Article", "score": 0.42426294, "title_display": "Peer-Selected \u201cBest Papers\u201d\u2014Are They Really That \u201cGood\u201d?", "publication_date": "2015-03-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0118446"}, {"journal": "PLoS ONE", "abstract": ["\nWe investigate the computational structure of a paradigmatic example of distributed social interaction: that of the open-source Wikipedia community. We examine the statistical properties of its cooperative behavior, and perform model selection to determine whether this aspect of the system can be described by a finite-state process, or whether reference to an effectively unbounded resource allows for a more parsimonious description. We find strong evidence, in a majority of the most-edited pages, in favor of a collective-state model, where the probability of a \u201crevert\u201d action declines as the square root of the number of non-revert actions seen since the last revert. We provide evidence that the emergence of this social counter is driven by collective interaction effects, rather than properties of individual users.\n"], "author_display": ["Simon DeDeo"], "article_type": "Research Article", "score": 0.42389494, "title_display": "Collective Phenomena and Non-Finite State Computation in a Human Social System", "publication_date": "2013-10-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0075818"}, {"journal": "PLoS Computational Biology", "abstract": ["\n        Chaste \u2014 Cancer, Heart And Soft Tissue Environment \u2014 is an open source C++ library for the computational simulation of mathematical models developed for physiology and biology. Code development has been driven by two initial applications: cardiac electrophysiology and cancer development. A large number of cardiac electrophysiology studies have been enabled and performed, including high-performance computational investigations of defibrillation on realistic human cardiac geometries. New models for the initiation and growth of tumours have been developed. In particular, cell-based simulations have provided novel insight into the role of stem cells in the colorectal crypt. Chaste is constantly evolving and is now being applied to a far wider range of problems. The code provides modules for handling common scientific computing components, such as meshes and solvers for ordinary and partial differential equations (ODEs/PDEs). Re-use of these components avoids the need for researchers to \u2018re-invent the wheel\u2019 with each new project, accelerating the rate of progress in new applications. Chaste is developed using industrially-derived techniques, in particular test-driven development, to ensure code quality, re-use and reliability. In this article we provide examples that illustrate the types of problems Chaste can be used to solve, which can be run on a desktop computer. We highlight some scientific studies that have used or are using Chaste, and the insights they have provided. The source code, both for specific releases and the development version, is available to download under an open source Berkeley Software Distribution (BSD) licence at http://www.cs.ox.ac.uk/chaste, together with details of a mailing list and links to documentation and tutorials.\n      "], "author_display": ["Gary R. Mirams", "Christopher J. Arthurs", "Miguel O. Bernabeu", "Rafel Bordas", "Jonathan Cooper", "Alberto Corrias", "Yohan Davit", "Sara-Jane Dunn", "Alexander G. Fletcher", "Daniel G. Harvey", "Megan E. Marsh", "James M. Osborne", "Pras Pathmanathan", "Joe Pitt-Francis", "James Southern", "Nejib Zemzemi", "David J. Gavaghan"], "article_type": "Research Article", "score": 0.42379367, "title_display": "Chaste: An Open Source C++ Library for Computational Physiology and Biology", "publication_date": "2013-03-14T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1002970"}, {"journal": "PLoS Computational Biology", "abstract": ["\nExplaining the maintenance of communicative behavior in the face of incentives to deceive, conceal information, or exaggerate is an important problem in behavioral biology. When the interests of agents diverge, some form of signal cost is often seen as essential to maintaining honesty. Here, novel computational methods are used to investigate the role of common interest between the sender and receiver of messages in maintaining cost-free informative signaling in a signaling game. Two measures of common interest are defined. These quantify the divergence between sender and receiver in their preference orderings over acts the receiver might perform in each state of the world. Sampling from a large space of signaling games finds that informative signaling is possible at equilibrium with zero common interest in both senses. Games of this kind are rare, however, and the proportion of games that include at least one equilibrium in which informative signals are used increases monotonically with common interest. Common interest as a predictor of informative signaling also interacts with the extent to which agents' preferences vary with the state of the world. Our findings provide a quantitative description of the relation between common interest and informative signaling, employing exact measures of common interest, information use, and contingency of payoff under environmental variation that may be applied to a wide range of models and empirical systems.\nAuthor Summary: How can honest communication evolve, given the many incentives to deceive, conceal information, or exaggerate? In recent work, it has often been supposed that either common interest between the sender and receiver of messages must be present, or special factors (such as a special cost for dishonest production of signals) must be in place. When talk is cheap, what is the minimum degree of common interest that will suffice to maintain communication? We give new quantitative measures of common interest between communicating agents, and then use a computer search of signaling games to work out the relationship between the degree of common interest and the maintenance of signaling that conveys real information. Surprisingly, we find that informative signaling can in some cases be maintained with zero common interest. These cases are rare, and we also find that the degree of common interest is a good predictor of whether informative signaling is a likely outcome of an interaction. The upshot is that two agents with highly incompatible preferences may still find ways to communicate, but the more they see eye-to-eye, the more likely it is that communication will be viable. "], "author_display": ["Peter Godfrey-Smith", "Manolo Mart\u00ednez"], "article_type": "Research Article", "score": 0.4234294, "title_display": "Communication and Common Interest", "publication_date": "2013-11-07T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1003282"}, {"journal": "PLoS ONE", "abstract": ["\nMinimization is a valuable method for allocating participants between the control and experimental arms of clinical studies. The use of minimization reduces differences that might arise by chance between the study arms in the distribution of patient characteristics such as gender, ethnicity and age. However, unlike randomization, minimization requires real time assessment of each new participant with respect to the preceding distribution of relevant participant characteristics within the different arms of the study. For multi-site studies, this necessitates centralized computational analysis that is shared between all study locations. Unfortunately, there is no suitable freely available open source or free software that can be used for this purpose. OxMaR was developed to enable researchers in any location to use minimization for patient allocation and to access the minimization algorithm using any device that can connect to the internet such as a desktop computer, tablet or mobile phone. The software is complete in itself and requires no special packages or libraries to be installed. It is simple to set up and run over the internet using online facilities which are very low cost or even free to the user. Importantly, it provides real time information on allocation to the study lead or administrator and generates real time distributed backups with each allocation. OxMaR can readily be modified and customised and can also be used for standard randomization. It has been extensively tested and has been used successfully in a low budget multi-centre study. Hitherto, the logistical difficulties involved in minimization have precluded its use in many small studies and this software should allow more widespread use of minimization which should lead to studies with better matched control and experimental arms. OxMaR should be particularly valuable in low resource settings.\n"], "author_display": ["Christopher A. O\u2019Callaghan"], "article_type": "Research Article", "score": 0.42280033, "title_display": "OxMaR: Open Source Free Software for Online Minimization and Randomization for Clinical Trials", "publication_date": "2014-10-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0110761"}, {"journal": "PLoS ONE", "abstract": ["\nTelecare Medical Information Systems (TMIS) provide an effective way to enhance the medical process between doctors, nurses and patients. For enhancing the security and privacy of TMIS, it is important while challenging to enhance the TMIS so that a patient and a doctor can perform mutual authentication and session key establishment using a third-party medical server while the privacy of the patient can be ensured. In this paper, we propose an anonymous three-party password-authenticated key exchange (3PAKE) protocol for TMIS. The protocol is based on the efficient elliptic curve cryptosystem. For security, we apply the pi calculus based formal verification tool ProVerif to show that our 3PAKE protocol for TMIS can provide anonymity for patient and doctor while at the same time achieves mutual authentication and session key security. The proposed scheme is secure and efficient, and can be used in TMIS.\n"], "author_display": ["Qi Xie", "Bin Hu", "Na Dong", "Duncan S. Wong"], "article_type": "Research Article", "score": 0.42247415, "title_display": "Anonymous Three-Party Password-Authenticated Key Exchange Scheme for Telecare Medical Information Systems", "publication_date": "2014-07-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0102747"}, {"journal": "PLoS ONE", "abstract": ["\n        Without having direct access to the information that is being exchanged, traces of information flow can be obtained by looking at temporal sequences of user interactions. These sequences can be represented as causality trees whose statistics result from a complex interplay between the topology of the underlying (social) network and the time correlations among the communications. Here, we study causality trees in mobile-phone data, which can be represented as a dynamical directed network. This representation of the data reveals the existence of super-spreaders and super-receivers. We show that the tree statistics, respectively the information spreading process, are extremely sensitive to the in-out degree correlation exhibited by the users. We also learn that a given information, e.g., a rumor, would require users to retransmit it for more than 30 hours in order to cover a macroscopic fraction of the system. Our analysis indicates that topological node-node correlations of the underlying social network, while allowing the existence of information loops, they also promote information spreading. Temporal correlations, and therefore causality effects, are only visible as local phenomena and during short time scales. Consequently, the very idea that there is (intentional) information spreading beyond a small vecinity is called into question. These results are obtained through a combination of theory and data analysis techniques.\n      "], "author_display": ["Fernando Peruani", "Lionel Tabourier"], "article_type": "Research Article", "score": 0.42195916, "title_display": "Directedness of Information Flow in Mobile Phone Communication Networks", "publication_date": "2011-12-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0028860"}, {"journal": "PLoS ONE", "abstract": ["\nFinding the true source of a social network is a crucial component of social network information tracing. Using the new media microblog as an example, this paper provides a source tracing algorithm ITEPE (Initiators and Early Participants Extraction) to solve this problem. First, the cascade (session tree) is built according to the retweeting of a microblog, after which the cascade set (session forest) is clustered by topical relevance. Second, real initiators are identified through the user relationship network and information cascade network. The influence index and conformity index of every node is then iteratively calculated according to text sentiment analysis and information cascades and the early important participants are extracted. Finally, the real initiators and early participants are evaluated through an experiment.\n"], "author_display": ["Xueyan Zhou", "Jing Yang", "Zehong Lin", "Jianpei Zhang"], "article_type": "Research Article", "score": 0.42155704, "title_display": "ITEPE: A Source Tracing Algorithm for the Microblog", "publication_date": "2014-10-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0111380"}, {"journal": "PLoS ONE", "abstract": ["Background: Advanced technical systems and analytic methods promise to provide policy\n                        makers with information to help them recognize the consequences of\n                        alternative courses of action during pandemics. Evaluations still show that\n                        response programs are insufficiently supported by information systems. This\n                        paper sets out to derive a protocol for implementation of integrated\n                        information infrastructures supporting regional and local pandemic response\n                        programs at the stage(s) when the outbreak no longer can be contained at its\n                        source. Methods: Nominal group methods for reaching consensus on complex problems were used to\n                        transform requirements data obtained from international experts into an\n                        implementation protocol. The analysis was performed in a cyclical process in\n                        which the experts first individually provided input to working documents and\n                        then discussed them in conferences calls. Argument-based representation in\n                        design patterns was used to define the protocol at technical, system, and\n                        pandemic evidence levels. Results: The Protocol for a Standardized information infrastructure for Pandemic and\n                        Emerging infectious disease Response (PROSPER) outlines the implementation\n                        of information infrastructure aligned with pandemic response programs. The\n                        protocol covers analyses of the community at risk, the response processes,\n                        and response impacts. For each of these, the protocol outlines the\n                        implementation of a supporting information infrastructure in hierarchical\n                        patterns ranging from technical components and system functions to pandemic\n                        evidence production. Conclusions: The PROSPER protocol provides guidelines for implementation of an information\n                        infrastructure for pandemic response programs both in settings where\n                        sophisticated health information systems already are used and in developing\n                        communities where there is limited access to financial and technical\n                        resources. The protocol is based on a generic health service model and its\n                        functions are adjusted for community-level analyses of outbreak detection\n                        and progress, and response program effectiveness. Scientifically grounded\n                        reporting principles need to be established for interpretation of\n                        information derived from outbreak detection algorithms and predictive\n                        modeling. "], "author_display": ["Toomas Timpka", "Henrik Eriksson", "Elin A. Gursky", "Magnus Str\u00f6mgren", "Einar Holm", "Joakim Ekberg", "Olle Eriksson", "Anders Grimvall", "Lars Valter", "James M. Nyce"], "article_type": "Research Article", "score": 0.4213538, "title_display": "Requirements and Design of the PROSPER Protocol for Implementation of\n                    Information Infrastructures Supporting Pandemic Response: A Nominal Group\n                    Study", "publication_date": "2011-03-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0017941"}, {"journal": "PLoS ONE", "abstract": ["\nSocial animals can use both social and private information to guide decision making. While social information can be relatively economical to acquire, it can lead to maladaptive information cascades if attention to environmental cues is supplanted by unconditional copying. Ants frequently employ pheromone trails, a form of social information, to guide collective processes, and this can include consensus decisions made when choosing a place to live. In this study, I examine how house-hunting ants balance social and private information when these information sources conflict to different degrees. Social information, in the form of pre-established pheromone trails, strongly influenced the decision process in choices between equivalent nests, and lead to a reduced relocation time. When trails lead to non-preferred types of nest, however, social information had less influence when this preference was weak and no influence when the preference was strong. These results suggest that social information is vetted against private information during the house-hunting process in this species. Private information is favoured in cases of conflict and this may help insure colonies against costly wrong decisions.\n"], "author_display": ["Adam L. Cronin"], "article_type": "Research Article", "score": 0.420572, "title_display": "Conditional Use of Social and Private Information Guides House-Hunting Ants", "publication_date": "2013-05-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0064668"}, {"journal": "PLOS ONE", "abstract": ["\nMigrating to cloud computing is one of the current enterprise challenges. This technology provides a new paradigm based on \u201con-demand payment\u201d for information and communication technologies. In this sense, the small and medium enterprise is supposed to be the most interested, since initial investments are avoided and the technology allows gradual implementation. However, even if the characteristics and capacities have been widely discussed, entry into the cloud is still lacking in terms of practical, real frameworks. This paper aims at filling this gap, presenting a real tool already implemented and tested, which can be used as a cloud computing adoption decision tool. This tool uses diagnosis based on specific questions to gather the required information and subsequently provide the user with valuable information to deploy the business within the cloud, specifically in the form of Software as a Service (SaaS) solutions. This information allows the decision makers to generate their particular Cloud Road. A pilot study has been carried out with enterprises at a local level with a two-fold objective: to ascertain the degree of knowledge on cloud computing and to identify the most interesting business areas and their related tools for this technology. As expected, the results show high interest and low knowledge on this subject and the tool presented aims to readdress this mismatch, insofar as possible.\n"], "author_display": ["I\u00f1aki Bildosola", "Rosa R\u00edo-Belver", "Ernesto Cilleruelo", "Gaizka Garechana"], "article_type": "Research Article", "score": 0.42044157, "title_display": "Design and Implementation of a Cloud Computing Adoption Decision Tool: Generating a Cloud Road", "publication_date": "2015-07-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0134563"}, {"journal": "PLoS ONE", "abstract": ["\n        Analysing the properties of a biological system through in silico experimentation requires a satisfactory mathematical representation of the system including accurate values of the model parameters. Fortunately, modern experimental techniques allow obtaining time-series data of appropriate quality which may then be used to estimate unknown parameters. However, in many cases, a subset of those parameters may not be uniquely estimated, independently of the experimental data available or the numerical techniques used for estimation. This lack of identifiability is related to the structure of the model, i.e. the system dynamics plus the observation function. Despite the interest in knowing a priori whether there is any chance of uniquely estimating all model unknown parameters, the structural identifiability analysis for general non-linear dynamic models is still an open question. There is no method amenable to every model, thus at some point we have to face the selection of one of the possibilities. This work presents a critical comparison of the currently available techniques. To this end, we perform the structural identifiability analysis of a collection of biological models. The results reveal that the generating series approach, in combination with identifiability tableaus, offers the most advantageous compromise among range of applicability, computational complexity and information provided.\n      "], "author_display": ["Oana-Teodora Chis", "Julio R. Banga", "Eva Balsa-Canto"], "article_type": "Research Article", "score": 0.4204309, "title_display": "Structural Identifiability of Systems Biology Models: A Critical Comparison of Methods", "publication_date": "2011-11-22T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0027755"}, {"journal": "PLOS ONE", "abstract": ["\nBiometrics authenticated schemes using smart cards have attracted much attention in multi-server environments. Several schemes of this type where proposed in the past. However, many of them were found to have some design flaws. This paper concentrates on the security weaknesses of the three-factor authentication scheme by Mishra et al. After careful analysis, we find their scheme does not really resist replay attack while failing to provide an efficient password change phase. We further propose an improvement of Mishra et al.\u2019s scheme with the purpose of preventing the security threats of their scheme. We demonstrate the proposed scheme is given to strong authentication against several attacks including attacks shown in the original scheme. In addition, we compare the performance and functionality with other multi-server authenticated key schemes.\n"], "author_display": ["Yanrong Lu", "Lixiang Li", "Xing Yang", "Yixian Yang"], "article_type": "Research Article", "score": 0.42002106, "title_display": "Robust Biometrics Based Authentication and Key Agreement Scheme for Multi-Server Environments Using Smart Cards", "publication_date": "2015-05-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0126323"}, {"journal": "PLOS ONE", "abstract": ["\nNeuroimage registration is crucial for brain morphometric analysis and treatment efficacy evaluation. However, existing advanced registration algorithms such as FLIRT and ANTs are not efficient enough for clinical use. In this paper, a GPU implementation of FLIRT with the correlation ratio (CR) as the similarity metric and a GPU accelerated correlation coefficient (CC) calculation for the symmetric diffeomorphic registration of ANTs have been developed. The comparison with their corresponding original tools shows that our accelerated algorithms can greatly outperform the original algorithm in terms of computational efficiency. This paper demonstrates the great potential of applying these registration tools in clinical applications.\n"], "author_display": ["Yun-gang Luo", "Ping Liu", "Lin Shi", "Yishan Luo", "Lei Yi", "Ang Li", "Jing Qin", "Pheng-Ann Heng", "Defeng Wang"], "article_type": "Research Article", "score": 0.41925743, "title_display": "Accelerating Neuroimage Registration through Parallel Computation of Similarity Metric", "publication_date": "2015-09-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0136718"}, {"journal": "PLoS ONE", "abstract": ["\n        In the 1940s, the first generation of modern computers used vacuum tube oscillators as their principle components, however, with the development of the transistor, such oscillator based computers quickly became obsolete. As the demand for faster and lower power computers continues, transistors are themselves approaching their theoretical limit and emerging technologies must eventually supersede them. With the development of optical oscillators and Josephson junction technology, we are again presented with the possibility of using oscillators as the basic components of computers, and it is possible that the next generation of computers will be composed almost entirely of oscillatory devices. Here, we demonstrate how coupled threshold oscillators may be used to perform binary logic in a manner entirely consistent with modern computer architectures. We describe a variety of computational circuitry and demonstrate working oscillator models of both computation and memory.\n      "], "author_display": ["Jon Borresen", "Stephen Lynch"], "article_type": "Research Article", "score": 0.41907743, "title_display": "Oscillatory Threshold Logic", "publication_date": "2012-11-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0048498"}, {"journal": "PLoS ONE", "abstract": ["Background: How do neural networks encode sensory information? Following sensory stimulation, neural coding is commonly assumed to be based on neurons changing their firing rate. In contrast, both theoretical works and experiments in several sensory systems showed that neurons could encode information as coordinated cell assemblies by adjusting their spike timing and without changing their firing rate. Nevertheless, in the olfactory system, there is little experimental evidence supporting such model. Methodology/Principal Findings: To study these issues, we implanted tetrodes in the olfactory bulb of awake mice to record the odorant-evoked activity of mitral/tufted (M/T) cells. We showed that following odorant presentation, most M/T neurons do not significantly change their firing rate over a breathing cycle but rather respond to odorant stimulation by redistributing their firing activity within respiratory cycles. In addition, we showed that sensory information can be encoded by cell assemblies composed of such neurons, thus supporting the idea that coordinated populations of globally rate-invariant neurons could be efficiently used to convey information about the odorant identity. We showed that different coding schemes can convey high amount of odorant information for specific read-out time window. Finally we showed that the optimal readout time window corresponds to the duration of gamma oscillations cycles. Conclusion: We propose that odorant can be encoded by population of cells that exhibit fine temporal tuning of spiking activity while displaying weak or no firing rate change. These cell assemblies may transfer sensory information in spiking packets sequence using the gamma oscillations as a clock. This would allow the system to reach a tradeoff between rapid and accurate odorant discrimination. "], "author_display": ["Olivier Gschwend", "Jonathan Beroud", "Alan Carleton"], "article_type": "Research Article", "score": 0.41865045, "title_display": "Encoding Odorant Identity by Spiking Packets of Rate-Invariant Neurons in Awake Mice", "publication_date": "2012-01-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0030155"}, {"abstract": ["\n        The ability to decode neural activity into meaningful control signals for prosthetic devices is critical to the development of clinically useful brain\u2013 machine interfaces (BMIs). Such systems require input from tens to hundreds of brain-implanted recording electrodes in order to deliver robust and accurate performance; in serving that primary function they should also minimize power dissipation in order to avoid damaging neural tissue; and they should transmit data wirelessly in order to minimize the risk of infection associated with chronic, transcutaneous implants. Electronic architectures for brain\u2013 machine interfaces must therefore minimize size and power consumption, while maximizing the ability to compress data to be transmitted over limited-bandwidth wireless channels. Here we present a system of extremely low computational complexity, designed for real-time decoding of neural signals, and suited for highly scalable implantable systems. Our programmable architecture is an explicit implementation of a universal computing machine emulating the dynamics of a network of integrate-and-fire neurons; it requires no arithmetic operations except for counting, and decodes neural signals using only computationally inexpensive logic operations. The simplicity of this architecture does not compromise its ability to compress raw neural data by factors greater than . We describe a set of decoding algorithms based on this computational architecture, one designed to operate within an implanted system, minimizing its power consumption and data transmission bandwidth; and a complementary set of algorithms for learning, programming the decoder, and postprocessing the decoded output, designed to operate in an external, nonimplanted unit. The implementation of the implantable portion is estimated to require fewer than 5000 operations per second. A proof-of-concept, 32-channel field-programmable gate array (FPGA) implementation of this portion is consequently energy efficient. We validate the performance of our overall system by decoding electrophysiologic data from a behaving rodent.\n      "], "author_display": ["Benjamin I. Rapoport", "Lorenzo Turicchia", "Woradorn Wattanapanitch", "Thomas J. Davidson", "Rahul Sarpeshkar"], "article_type": "Research Article", "score": 0.41823503, "title_display": "Efficient Universal Computing Architectures for Decoding Neural Activity", "publication_date": "2012-09-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0042492"}, {"journal": "PLoS ONE", "abstract": ["\nCitizen science is a research practice that relies on public contributions of data. The strong recognition of its educational value combined with the need for novel methods to handle subsequent large and complex data sets raises the question: Is citizen science effective at science? A quantitative assessment of the contributions of citizen science for its core purpose \u2013 scientific research \u2013 is lacking. We examined the contribution of citizen science to a review paper by ornithologists in which they formulated ten central claims about the impact of climate change on avian migration. Citizen science was never explicitly mentioned in the review article. For each of the claims, these ornithologists scored their opinions about the amount of research effort invested in each claim and how strongly the claim was supported by evidence. This allowed us to also determine whether their trust in claims was, unwittingly or not, related to the degree to which the claims relied primarily on data generated by citizen scientists. We found that papers based on citizen science constituted between 24 and 77% of the references backing each claim, with no evidence of a mistrust of claims that relied heavily on citizen-science data. We reveal that many of these papers may not easily be recognized as drawing upon volunteer contributions, as the search terms \u201ccitizen science\u201d and \u201cvolunteer\u201d would have overlooked the majority of the studies that back the ten claims about birds and climate change. Our results suggest that the significance of citizen science to global research, an endeavor that is reliant on long-term information at large spatial scales, might be far greater than is readily perceived. To better understand and track the contributions of citizen science in the future, we urge researchers to use the keyword \u201ccitizen science\u201d in papers that draw on efforts of non-professionals.\n"], "author_display": ["Caren B. Cooper", "Jennifer Shirk", "Benjamin Zuckerberg"], "article_type": "Research Article", "score": 0.4182073, "title_display": "The Invisible Prevalence of Citizen Science in Global Research: Migratory Birds and Climate Change", "publication_date": "2014-09-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0106508"}, {"journal": "PLoS ONE", "abstract": ["\n        Some of the approaches have been developed to retrieve data automatically from one or multiple remote biological data sources. However, most of them require researchers to remain online and wait for returned results. The latter not only requires highly available network connection, but also may cause the network overload. Moreover, so far none of the existing approaches has been designed to address the following problems when retrieving the remote data in a mobile network environment: (1) the resources of mobile devices are limited; (2) network connection is relatively of low quality; and (3) mobile users are not always online. To address the aforementioned problems, we integrate an agent migration approach with a multi-agent system to overcome the high latency or limited bandwidth problem by moving their computations to the required resources or services. More importantly, the approach is fit for the mobile computing environments. Presented in this paper are also the system architecture, the migration strategy, as well as the security authentication of agent migration. As a demonstration, the remote data retrieval from GenBank was used to illustrate the feasibility of the proposed approach.\n      "], "author_display": ["Lei Gao", "Hua Dai", "Tong-Liang Zhang", "Kuo-Chen Chou"], "article_type": "Research Article", "score": 0.41742924, "title_display": "Remote Data Retrieval for Bioinformatics Applications: An Agent Migration Approach", "publication_date": "2011-06-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0020949"}, {"journal": "PLoS ONE", "abstract": ["\nInformation overload is a serious problem in modern society and many solutions such as recommender system have been proposed to filter out irrelevant information. In the literature, researchers have been mainly dedicated to improving the recommendation performance (accuracy and diversity) of the algorithms while they have overlooked the influence of topology of the online user-object bipartite networks. In this paper, we find that some information provided by the bipartite networks is not only redundant but also misleading. With such \u201cless can be more\u201d feature, we design some algorithms to improve the recommendation performance by eliminating some links from the original networks. Moreover, we propose a hybrid method combining the time-aware and topology-aware link removal algorithms to extract the backbone which contains the essential information for the recommender systems. From the practical point of view, our method can improve the performance and reduce the computational time of the recommendation system, thus improving both of their effectiveness and efficiency.\n"], "author_display": ["Qian-Ming Zhang", "An Zeng", "Ming-Sheng Shang"], "article_type": "Research Article", "score": 0.4172099, "title_display": "Extracting the Information Backbone in Online System", "publication_date": "2013-05-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0062624"}, {"journal": "PLoS ONE", "abstract": ["\n        The characterization and the definition of the complexity of objects is an important but very difficult problem that attracted much interest in many different fields. In this paper we introduce a new measure, called network diversity score (NDS), which allows us to quantify structural properties of networks. We demonstrate numerically that our diversity score is capable of distinguishing ordered, random and complex networks from each other and, hence, allowing us to categorize networks with respect to their structural complexity. We study 16 additional network complexity measures and find that none of these measures has similar good categorization capabilities. In contrast to many other measures suggested so far aiming for a characterization of the structural complexity of networks, our score is different for a variety of reasons. First, our score is multiplicatively composed of four individual scores, each assessing different structural properties of a network. That means our composite score reflects the structural diversity of a network. Second, our score is defined for a population of networks instead of individual networks. We will show that this removes an unwanted ambiguity, inherently present in measures that are based on single networks. In order to apply our measure practically, we provide a statistical estimator for the diversity score, which is based on a finite number of samples.\n      "], "author_display": ["Frank Emmert-Streib", "Matthias Dehmer"], "article_type": "Research Article", "score": 0.41691923, "title_display": "Exploring Statistical and Population Aspects of Network Complexity", "publication_date": "2012-05-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0034523"}, {"journal": "PLoS ONE", "abstract": ["\nReal-time human activity recognition is essential for human-robot interactions for assisted healthy independent living. Most previous work in this area is performed on traditional two-dimensional (2D) videos and both global and local methods have been used. Since 2D videos are sensitive to changes of lighting condition, view angle, and scale, researchers begun to explore applications of 3D information in human activity understanding in recently years. Unfortunately, features that work well on 2D videos usually don't perform well on 3D videos and there is no consensus on what 3D features should be used. Here we propose a model of human activity recognition based on 3D movements of body joints. Our method has three steps, learning dictionaries of sparse codes of 3D movements of joints, sparse coding, and classification. In the first step, space-time volumes of 3D movements of body joints are obtained via dense sampling and independent component analysis is then performed to construct a dictionary of sparse codes for each activity. In the second step, the space-time volumes are projected to the dictionaries and a set of sparse histograms of the projection coefficients are constructed as feature representations of the activities. Finally, the sparse histograms are used as inputs to a support vector machine to recognize human activities. We tested this model on three databases of human activities and found that it outperforms the state-of-the-art algorithms. Thus, this model can be used for real-time human activity recognition in many applications.\n"], "author_display": ["Jin Qi", "Zhiyong Yang"], "article_type": "Research Article", "score": 0.41660616, "title_display": "Learning Dictionaries of Sparse Codes of 3D Movements of Body Joints for Real-Time Human Activity Understanding", "publication_date": "2014-12-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0114147"}, {"journal": "PLOS ONE", "abstract": ["\nmicroRNAs (miRNAs) are important gene regulators at post-transcriptional level, and inferring miRNA-mRNA regulatory relationships is a crucial problem. Consequently, several computational methods of predicting miRNA targets have been proposed using expression data with or without sequence based miRNA target information. A typical procedure for applying and evaluating such a method is i) collecting matched miRNA and mRNA expression profiles in a specific condition, e.g. a cancer dataset from The Cancer Genome Atlas (TCGA), ii) applying the new computational method to the selected dataset, iii) validating the predictions against knowledge from literature and third-party databases, and comparing the performance of the method with some existing methods. This procedure is time consuming given the time elapsed when collecting and processing data, repeating the work from existing methods, searching for knowledge from literature and third-party databases to validate the results, and comparing the results from different methods. The time consuming procedure prevents researchers from quickly testing new computational models, analysing new datasets, and selecting suitable methods for assisting with the experiment design. Here, we present an R package, miRLAB, for automating the procedure of inferring and validating miRNA-mRNA regulatory relationships. The package provides a complete set of pipelines for testing new methods and analysing new datasets. miRLAB includes a pipeline to obtain matched miRNA and mRNA expression datasets directly from TCGA, 12 benchmark computational methods for inferring miRNA-mRNA regulatory relationships, the functions for validating the predictions using experimentally validated miRNA target data and miRNA perturbation data, and the tools for comparing the results from different computational methods.\n"], "author_display": ["Thuc Duy Le", "Junpeng Zhang", "Lin Liu", "Huawen Liu", "Jiuyong Li"], "article_type": "Research Article", "score": 0.41654474, "title_display": "miRLAB: An R Based Dry Lab for Exploring miRNA-mRNA Regulatory Relationships", "publication_date": "2015-12-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0145386"}, {"journal": "PLoS ONE", "abstract": ["\nLewis Carroll's English word game Doublets is represented as a system of networks with each node being an English word and each connectivity edge confirming that its two ending words are equal in letter length, but different by exactly one letter. We show that this system, which we call the Doublets net, constitutes a complex body of linguistic knowledge concerning English word structure that has computable multiscale features. Distributed morphological, phonological and orthographic constraints and the language's local redundancy are seen at the node level. Phonological communities are seen at the network level. And a balancing act between the language's global efficiency and redundancy is seen at the system level. We develop a new measure of intrinsic node-to-node distance and a computational algorithm, called community geometry, which reveal the implicit multiscale structure within binary networks. Because the Doublets net is a modular complex cognitive system, the community geometry and computable multi-scale structural information may provide a foundation for understanding computational learning in many systems whose network structure has yet to be fully analyzed.\n"], "author_display": ["Hsieh Fushing", "Chen Chen", "Yin-Chen Hsieh", "Patrick Farrell"], "article_type": "Research Article", "score": 0.4163629, "title_display": "Lewis Carroll's Doublets Net of English Words: Network Heterogeneity in a Complex System", "publication_date": "2014-12-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0114177"}, {"journal": "PLoS ONE", "abstract": ["Background: Among the primary goals of microarray analysis is the identification of genes that could distinguish between different phenotypes (feature selection). Previous studies indicate that incorporating prior information of the genes' function could help identify physiologically relevant features. However, current methods that incorporate prior functional information do not provide a relative estimate of the effect of different genes on the biological processes of interest. Results: Here, we present a method that integrates gene ontology (GO) information and expression data using Bayesian regression mixture models to perform unsupervised clustering of the samples and identify physiologically relevant discriminating features. As a model application, the method was applied to identify the genes that play a role in the cytotoxic responses of human hepatoblastoma cell line (HepG2) to saturated fatty acid (SFA) and tumor necrosis factor (TNF)-\u03b1, as compared to the non-toxic response to the unsaturated FFAs (UFA) and TNF-\u03b1. Incorporation of prior knowledge led to a better discrimination of the toxic phenotypes from the others. The model identified roles of lysosomal ATPases and adenylate cyclase (AC9) in the toxicity of palmitate. To validate the role of AC in palmitate-treated cells, we measured the intracellular levels of cyclic AMP (cAMP). The cAMP levels were found to be significantly reduced by palmitate treatment and not by the other FFAs, in accordance with the model selection of AC9. Conclusions: A framework is presented that incorporates prior ontology information, which helped to (a) perform unsupervised clustering of the phenotypes, and (b) identify the genes relevant to each cluster of phenotypes. We demonstrate the proposed framework by applying it to identify physiologically-relevant feature genes that conferred differential toxicity to saturated vs. unsaturated FFAs. The framework can be applied to other problems to efficiently integrate ontology information and expression data in order to identify feature genes. "], "author_display": ["Shireesh Srivastava", "Linxia Zhang", "Rong Jin", "Christina Chan"], "article_type": "Research Article", "score": 0.416082, "title_display": "A Novel Method Incorporating Gene Ontology Information for Unsupervised Clustering and Feature Selection", "publication_date": "2008-12-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0003860"}, {"journal": "PLoS ONE", "abstract": ["\n        We have created the Knowledgebase of Standard Biological Parts (SBPkb) as a publically accessible Semantic Web resource for synthetic biology (sbolstandard.org). The SBPkb allows researchers to query and retrieve standard biological parts for research and use in synthetic biology. Its initial version includes all of the information about parts stored in the Registry of Standard Biological Parts (partsregistry.org). SBPkb transforms this information so that it is computable, using our semantic framework for synthetic biology parts. This framework, known as SBOL-semantic, was built as part of the Synthetic Biology Open Language (SBOL), a project of the Synthetic Biology Data Exchange Group. SBOL-semantic represents commonly used synthetic biology entities, and its purpose is to improve the distribution and exchange of descriptions of biological parts. In this paper, we describe the data, our methods for transformation to SBPkb, and finally, we demonstrate the value of our knowledgebase with a set of sample queries. We use RDF technology and SPARQL queries to retrieve candidate \u201cpromoter\u201d parts that are known to be both negatively and positively regulated. This method provides new web based data access to perform searches for parts that are not currently possible.\n      "], "author_display": ["Michal Galdzicki", "Cesar Rodriguez", "Deepak Chandran", "Herbert M. Sauro", "John H. Gennari"], "article_type": "Research Article", "score": 0.41608182, "title_display": "Standard Biological Parts Knowledgebase", "publication_date": "2011-02-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0017005"}, {"journal": "PLoS ONE", "abstract": ["\nThis study aims to improve the performance of Dynamic Causal Modelling for Event Related Potentials (DCM for ERP) in MATLAB by using external function calls to a graphics processing unit (GPU). DCM for ERP is an advanced method for studying neuronal effective connectivity. DCM utilizes an iterative procedure, the expectation maximization (EM) algorithm, to find the optimal parameters given a set of observations and the underlying probability model. As the EM algorithm is computationally demanding and the analysis faces possible combinatorial explosion of models to be tested, we propose a parallel computing scheme using the GPU to achieve a fast estimation of DCM for ERP. The computation of DCM for ERP is dynamically partitioned and distributed to threads for parallel processing, according to the DCM model complexity and the hardware constraints. The performance efficiency of this hardware-dependent thread arrangement strategy was evaluated using the synthetic data. The experimental data were used to validate the accuracy of the proposed computing scheme and quantify the time saving in practice. The simulation results show that the proposed scheme can accelerate the computation by a factor of 155 for the parallel part. For experimental data, the speedup factor is about 7 per model on average, depending on the model complexity and the data. This GPU-based implementation of DCM for ERP gives qualitatively the same results as the original MATLAB implementation does at the group level analysis. In conclusion, we believe that the proposed GPU-based implementation is very useful for users as a fast screen tool to select the most likely model and may provide implementation guidance for possible future clinical applications such as online diagnosis.\n"], "author_display": ["Wei-Jen Wang", "I-Fan Hsieh", "Chun-Chuan Chen"], "article_type": "Research Article", "score": 0.41595897, "title_display": "Accelerating Computation of DCM for ERP in MATLAB by External Function Calls to the GPU", "publication_date": "2013-06-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0066599"}, {"journal": "PLoS Computational Biology", "abstract": ["\nThe ongoing functional annotation of proteins relies upon the work of curators to capture experimental findings from scientific literature and apply them to protein sequence and structure data. However, with the increasing use of high-throughput experimental assays, a small number of experimental studies dominate the functional protein annotations collected in databases. Here, we investigate just how prevalent is the \u201cfew articles - many proteins\u201d phenomenon. We examine the experimentally validated annotation of proteins provided by several groups in the GO Consortium, and show that the distribution of proteins per published study is exponential, with 0.14% of articles providing the source of annotations for 25% of the proteins in the UniProt-GOA compilation. Since each of the dominant articles describes the use of an assay that can find only one function or a small group of functions, this leads to substantial biases in what we know about the function of many proteins. Mass-spectrometry, microscopy and RNAi experiments dominate high throughput experiments. Consequently, the functional information derived from these experiments is mostly of the subcellular location of proteins, and of the participation of proteins in embryonic developmental pathways. For some organisms, the information provided by different studies overlap by a large amount. We also show that the information provided by high throughput experiments is less specific than those provided by low throughput experiments. Given the experimental techniques available, certain biases in protein function annotation due to high-throughput experiments are unavoidable. Knowing that these biases exist and understanding their characteristics and extent is important for database curators, developers of function annotation programs, and anyone who uses protein function annotation data to plan experiments.\nAuthor Summary: Experiments and observations are the vehicles used by science to understand the world around us. In the field of molecular biology, we are increasingly relying on high-throughput, genome-wide experiments to provide answers about the function of biological macromolecules. However, any experimental assay is essentially limited in the type of information it can discover. Here, we show that our increasing reliance on high-throughput experiments biases our understanding of protein function. While the primary source of information is experiments, the functions of many proteins are computationally annotated by sequence-based similarity, either directly or indirectly, to proteins whose function is experimentally determined. Therefore, any biases in experimental annotations can get amplified and entrenched in the majority of protein databases. We show here that high-throughput studies are biased towards certain aspects of protein function, and that they provide less information than low-throughput studies. While there is no clear solution to the phenomenon of bias from high-throughput experiments, recognizing its existence and its impact can help take steps to mitigate its effect. "], "author_display": ["Alexandra M. Schnoes", "David C. Ream", "Alexander W. Thorman", "Patricia C. Babbitt", "Iddo Friedberg"], "article_type": "Research Article", "score": 0.41572398, "title_display": "Biases in the Experimental Annotations of Protein Function and Their Effect on Our Understanding of Protein Function Space", "publication_date": "2013-05-30T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1003063"}, {"journal": "PLOS Computational Biology", "abstract": ["\nThe future is uncertain because some forthcoming events are unpredictable and also because our ability to foresee the myriad consequences of our own actions is limited. Here we studied how humans select actions under such extrinsic and intrinsic uncertainty, in view of an exponentially expanding number of prospects on a branching multivalued visual stimulus. A triangular grid of disks of different sizes scrolled down a touchscreen at a variable speed. The larger disks represented larger rewards. The task was to maximize the cumulative reward by touching one disk at a time in a rapid sequence, forming an upward path across the grid, while every step along the path constrained the part of the grid accessible in the future. This task captured some of the complexity of natural behavior in the risky and dynamic world, where ongoing decisions alter the landscape of future rewards. By comparing human behavior with behavior of ideal actors, we identified the strategies used by humans in terms of how far into the future they looked (their \u201cdepth of computation\u201d) and how often they attempted to incorporate new information about the future rewards (their \u201crecalculation period\u201d). We found that, for a given task difficulty, humans traded off their depth of computation for the recalculation period. The form of this tradeoff was consistent with a complete, brute-force exploration of all possible paths up to a resource-limited finite depth. A step-by-step analysis of the human behavior revealed that participants took into account very fine distinctions between the future rewards and that they abstained from some simple heuristics in assessment of the alternative paths, such as seeking only the largest disks or avoiding the smaller disks. The participants preferred to reduce their depth of computation or increase the recalculation period rather than sacrifice the precision of computation.\nAuthor Summary: We investigated the human ability to organize behavior prospectively, for multiple future steps in risky, dynamic environments. In a setting that resembled a video game, participants selected the most rewarding paths traversing a triangular lattice of disks of different sizes, while the lattice scrolled down a touchscreen at a constant speed. Disk sizes represented the rewards; missing a disk incurred a penalty. Every choice excluded a number of the disks accessible in the future, encouraging subjects to examine prospective paths as far into the future as they could. In contrast to previous evidence that humans tend to reduce the computational difficulty of decision making by means of simplifying heuristics, our participants appeared to perform an exhaustive computation of all possible future scenarios within a horizon limited by a fixed number of computations. Under increasing time pressure, participants either reduced the computational horizon or recalculated the expected rewards less frequently, revealing a resource-limited ability for rapid detailed computation of prospective actions. To perform such intensive computations, participants could take advantage of the massively parallel neural architecture of the visual system allowing one to concurrently process information from multiple retinal locations. "], "author_display": ["Joseph Snider", "Dongpyo Lee", "Howard Poizner", "Sergei Gepshtein"], "article_type": "Research Article", "score": 0.41542655, "title_display": "Prospective Optimization with Limited Resources", "publication_date": "2015-09-14T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1004501"}, {"journal": "PLoS ONE", "abstract": ["\nThe rise of smartphones and web services made possible the large-scale collection of personal metadata. Information about individuals' location, phone call logs, or web-searches, is collected and used intensively by organizations and big data researchers. Metadata has however yet to realize its full potential. Privacy and legal concerns, as well as the lack of technical solutions for personal metadata management is preventing metadata from being shared and reconciled under the control of the individual. This lack of access and control is furthermore fueling growing concerns, as it prevents individuals from understanding and managing the risks associated with the collection and use of their data. Our contribution is two-fold: (1) we describe openPDS, a personal metadata management framework that allows individuals to collect, store, and give fine-grained access to their metadata to third parties. It has been implemented in two field studies; (2) we introduce and analyze SafeAnswers, a new and practical way of protecting the privacy of metadata at an individual level. SafeAnswers turns a hard anonymization problem into a more tractable security one. It allows services to ask questions whose answers are calculated against the metadata instead of trying to anonymize individuals' metadata. The dimensionality of the data shared with the services is reduced from high-dimensional metadata to low-dimensional answers that are less likely to be re-identifiable and to contain sensitive information. These answers can then be directly shared individually or in aggregate. openPDS and SafeAnswers provide a new way of dynamically protecting personal metadata, thereby supporting the creation of smart data-driven services and data science research.\n"], "author_display": ["Yves-Alexandre de Montjoye", "Erez Shmueli", "Samuel S. Wang", "Alex Sandy Pentland"], "article_type": "Research Article", "score": 0.41521397, "title_display": "openPDS: Protecting the Privacy of Metadata through SafeAnswers", "publication_date": "2014-07-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0098790"}, {"journal": "PLoS ONE", "abstract": ["\nInformation theory allows us to investigate information processing in neural systems in terms of information transfer, storage and modification. Especially the measure of information transfer, transfer entropy, has seen a dramatic surge of interest in neuroscience. Estimating transfer entropy from two processes requires the observation of multiple realizations of these processes to estimate associated probability density functions. To obtain these necessary observations, available estimators typically assume stationarity of processes to allow pooling of observations over time. This assumption however, is a major obstacle to the application of these estimators in neuroscience as observed processes are often non-stationary. As a solution, Gomez-Herrero and colleagues theoretically showed that the stationarity assumption may be avoided by estimating transfer entropy from an ensemble of realizations. Such an ensemble of realizations is often readily available in neuroscience experiments in the form of experimental trials. Thus, in this work we combine the ensemble method with a recently proposed transfer entropy estimator to make transfer entropy estimation applicable to non-stationary time series. We present an efficient implementation of the approach that is suitable for the increased computational demand of the ensemble method's practical application. In particular, we use a massively parallel implementation for a graphics processing unit to handle the computationally most heavy aspects of the ensemble method for transfer entropy estimation. We test the performance and robustness of our implementation on data from numerical simulations of stochastic processes. We also demonstrate the applicability of the ensemble method to magnetoencephalographic data. While we mainly evaluate the proposed method for neuroscience data, we expect it to be applicable in a variety of fields that are concerned with the analysis of information transfer in complex biological, social, and artificial systems.\n"], "author_display": ["Patricia Wollstadt", "Mario Mart\u00ednez-Zarzuela", "Raul Vicente", "Francisco J. D\u00edaz-Pernas", "Michael Wibral"], "article_type": "Research Article", "score": 0.41498154, "title_display": "Efficient Transfer Entropy Analysis of Non-Stationary Neural Time Series", "publication_date": "2014-07-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0102833"}, {"journal": "PLoS ONE", "abstract": ["\nGenomic signal processing (GSP) refers to the use of digital signal processing (DSP) tools for analyzing genomic data such as DNA sequences. A possible application of GSP that has not been fully explored is the computation of the distance between a pair of sequences. In this work we present GAFD, a novel GSP alignment-free distance computation method. We introduce a DNA sequence-to-signal mapping function based on the employment of doublet values, which increases the number of possible amplitude values for the generated signal. Additionally, we explore the use of three DSP distance metrics as descriptors for categorizing DNA signal fragments. Our results indicate the feasibility of employing GAFD for computing sequence distances and the use of descriptors for characterizing DNA fragments.\n"], "author_display": ["Ernesto Borrayo", "E. Gerardo Mendizabal-Ruiz", "Hugo V\u00e9lez-P\u00e9rez", "Rebeca Romo-V\u00e1zquez", "Adriana P. Mendizabal", "J. Alejandro Morales"], "article_type": "Research Article", "score": 0.41496938, "title_display": "Genomic Signal Processing Methods for Computation of Alignment-Free Distances from DNA Sequences", "publication_date": "2014-11-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0110954"}, {"journal": "PLoS ONE", "abstract": ["Background: Monitoring change in species diversity, community composition and phenology is vital to assess the impacts of anthropogenic activity and natural change. However, monitoring by trained scientists is time consuming and expensive. Methodology/Principal Findings: Using social networks, we assess whether it is possible to obtain accurate data on bee distribution across the UK from photographic records submitted by untrained members of the public, and if these data are in sufficient quantity for ecological studies. We used Flickr and Facebook as social networks and Flickr for the storage of photographs and associated data on date, time and location linked to them. Within six weeks, the number of pictures uploaded to the Flickr BeeID group exceeded 200. Geographic coverage was excellent; the distribution of photographs covered most of the British Isles, from the south coast of England to the Highlands of Scotland. However, only 59% of photographs were properly uploaded according to instructions, with vital information such as \u2018tags\u2019 or location information missing from the remainder. Nevertheless, this incorporation of information on location of photographs was much higher than general usage on Flickr (\u223c13%), indicating the need for dedicated projects to collect spatial ecological data. Furthermore, we found identification of bees is not possible from all photographs, especially those excluding lower abdomen detail. This suggests that giving details regarding specific anatomical features to include on photographs would be useful to maximise success. Conclusions/Significance: The study demonstrates the power of social network sites to generate public interest in a project and details the advantages of using a group within an existing popular social network site over a traditional (specifically-designed) web-based or paper-based submission process. Some advantages include the ability to network with other individuals or groups with similar interests, and thus increasing the size of the dataset and participation in the project. "], "author_display": ["Richard Stafford", "Adam G. Hart", "Laura Collins", "Claire L. Kirkhope", "Rachel L. Williams", "Samuel G. Rees", "Jane R. Lloyd", "Anne E. Goodenough"], "article_type": "Research Article", "score": 0.41457403, "title_display": "Eu-Social Science: The Role of Internet Social Networks in the Collection of Bee Biodiversity Data", "publication_date": "2010-12-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0014381"}, {"journal": "PLoS ONE", "abstract": ["\n        Production of official statistics frequently requires expert judgement to evaluate and reconcile data of unknown and varying quality from multiple and potentially conflicting sources. Moreover, exceptional events may be difficult to incorporate in modelled estimates. Computational logic provides a methodology and tools for incorporating analyst's judgement, integrating multiple data sources and modelling methods, ensuring transparency and replicability, and making documentation computationally accessible. Representations using computational logic can be implemented in a variety of computer-based languages for automated production. Computational logic complements standard mathematical and statistical techniques and extends the flexibility of mathematical and statistical modelling. A basic overview of computational logic is presented and its application to official statistics is illustrated with the WHO & UNICEF estimates of national immunization coverage.\n      "], "author_display": ["Anthony Burton", "Robert Kowalski", "Marta Gacic-Dobo", "Rouslan Karimov", "David Brown"], "article_type": "Research Article", "score": 0.41452587, "title_display": "A Formal Representation of the WHO and UNICEF Estimates of National Immunization Coverage: A Computational Logic Approach", "publication_date": "2012-10-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0047806"}, {"journal": "PLoS ONE", "abstract": ["\nPrediction markets are powerful forecasting tools. They have the potential to aggregate private information, to generate and disseminate a consensus among the market participants, and to provide incentives for information acquisition. These market functionalities can be very valuable for scientific research. Here, we report an experiment that examines the compatibility of prediction markets with the current practice of scientific publication. We investigated three settings. In the first setting, different pieces of information were disclosed to the public during the experiment. In the second setting, participants received private information. In the third setting, each piece of information was private at first, but was subsequently disclosed to the public. An automated, subsidizing market maker provided additional incentives for trading and mitigated liquidity problems. We find that the third setting combines the advantages of the first and second settings. Market performance was as good as in the setting with public information, and better than in the setting with private information. In contrast to the first setting, participants could benefit from information advantages. Thus the publication of information does not detract from the functionality of prediction markets. We conclude that for integrating prediction markets into the practice of scientific research it is of advantage to use subsidizing market makers, and to keep markets aligned with current publication practice.\n"], "author_display": ["Johan Almenberg", "Ken Kittlitz", "Thomas Pfeiffer"], "article_type": "Research Article", "score": 0.4144955, "title_display": "An Experiment on Prediction Markets in Science", "publication_date": "2009-12-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0008500"}, {"journal": "PLoS ONE", "abstract": ["\n        The presence of web-based communities is a distinctive signature of Web 2.0. The web-based feature means that information propagation within each community is highly facilitated, promoting complex collective dynamics in view of information exchange. In this work, we focus on a community of scientists and study, in particular, how the awareness of a scientific paper is spread. Our work is based on the web usage statistics obtained from the PLoS Article Level Metrics dataset compiled by PLoS. The cumulative number of HTML views was found to follow a long tail distribution which is reasonably well-fitted by a lognormal one. We modeled the diffusion of information by a random multiplicative process, and thus extracted the rates of information spread at different stages after the publication of a paper. We found that the spread of information displays two distinct decay regimes: a rapid downfall in the first month after publication, and a gradual power law decay afterwards. We identified these two regimes with two distinct driving processes: a short-term behavior driven by the fame of a paper, and a long-term behavior consistent with citation statistics. The patterns of information spread were found to be remarkably similar in data from different journals, but there are intrinsic differences for different types of web usage (HTML views and PDF downloads versus XML). These similarities and differences shed light on the theoretical understanding of different complex systems, as well as a better design of the corresponding web applications that is of high potential marketing impact.\n      "], "author_display": ["Koon-Kiu Yan", "Mark Gerstein"], "article_type": "Research Article", "score": 0.41440415, "title_display": "The Spread of Scientific Information: Insights from the Web Usage Statistics in PLoS Article-Level Metrics", "publication_date": "2011-05-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0019917"}, {"journal": "PLoS ONE", "abstract": ["\n        Betweenness centrality is an essential index for analysis of complex networks. However, the calculation of betweenness centrality is quite time-consuming and the fastest known algorithm uses  time and  space for weighted networks, where  and  are the number of nodes and edges in the network, respectively. By inserting virtual nodes into the weighted edges and transforming the shortest path problem into a breadth-first search (BFS) problem, we propose an algorithm that can compute the betweenness centrality in  time for integer-weighted networks, where  is the average weight of edges and  is the average degree in the network. Considerable time can be saved with the proposed algorithm when , indicating that it is suitable for lightly weighted large sparse networks. A similar concept of virtual node transformation can be used to calculate other shortest path based indices such as closeness centrality, graph centrality, stress centrality, and so on. Numerical simulations on various randomly generated networks reveal that it is feasible to use the proposed algorithm in large network analysis.\n      "], "author_display": ["Jing Yang", "Yingwu Chen"], "article_type": "Research Article", "score": 0.41403592, "title_display": "Fast Computing Betweenness Centrality with Virtual Nodes on Large Sparse Networks", "publication_date": "2011-07-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0022557"}, {"journal": "PLoS ONE", "abstract": ["\nCausal ordering is a useful tool for mobile distributed systems (MDS) to reduce the non-determinism induced by three main aspects: host mobility, asynchronous execution, and unpredictable communication delays. Several causal protocols for MDS exist. Most of them, in order to reduce the overhead and the computational cost over wireless channels and mobile hosts (MH), ensure causal ordering at and according to the causal view of the Base Stations. Nevertheless, these protocols introduce certain disadvantage, such as unnecessary inhibition at the delivery of messages. In this paper, we present an efficient causal protocol for groupware that satisfies the MDS's constraints, avoiding unnecessary inhibitions and ensuring the causal delivery based on the view of the MHs. One interesting aspect of our protocol is that it dynamically adapts the causal information attached to each message based on the number of messages with immediate dependency relation, and this is not directly proportional to the number of MHs.\n"], "author_display": ["Eduardo Lopez Dominguez", "Saul E. Pomares Hernandez", "Gustavo Rodriguez Gomez", "Maria Auxilio Medina"], "article_type": "Research Article", "score": 0.4139755, "title_display": "An Efficient Two-Tier Causal Protocol for Mobile Distributed Systems", "publication_date": "2013-04-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0059904"}, {"journal": "PLoS ONE", "abstract": ["Background: Human knowledge and innovation are recorded in two media: scholarly publication and patents. These records not only document a new scientific insight or new method developed, but they also carefully cite prior work upon which the innovation is built. Methodology: We quantify the impact of information flow across fields using two large citation dataset: one spanning over a century of scholarly work in the natural sciences, social sciences and humanities, and second spanning a quarter century of United States patents. Conclusions: We find that a publication's citing across disciplines is tied to its subsequent impact. In the case of patents and natural science publications, those that are cited at least once are cited slightly more when they draw on research outside of their area. In contrast, in the social sciences, citing within one's own field tends to be positively correlated with impact. "], "author_display": ["Xiaolin Shi", "Lada A. Adamic", "Belle L. Tseng", "Gavin S. Clarkson"], "article_type": "Research Article", "score": 0.4138374, "title_display": "The Impact of Boundary Spanning Scholarly Publications and Patents", "publication_date": "2009-08-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0006547"}, {"journal": "PLoS ONE", "abstract": ["\nRapid developments in the biomedical sciences have increased the demand for automatic clustering of biomedical publications. In contrast to current approaches to text clustering, which focus exclusively on the contents of abstracts, a novel method is proposed for clustering and analysis of complete biomedical article texts. To reduce dimensionality, Cosine Coefficient is used on a sub-space of only two vectors, instead of computing the Euclidean distance within the space of all vectors. Then a strategy and algorithm is introduced for Semi-supervised Affinity Propagation (SSAP) to improve analysis efficiency, using biomedical journal names as an evaluation background. Experimental results show that by avoiding high-dimensional sparse matrix computations, SSAP outperforms conventional k-means methods and improves upon the standard Affinity Propagation algorithm. In constructing a directed relationship network and distribution matrix for the clustering results, it can be noted that overlaps in scope and interests among BioMed publications can be easily identified, providing a valuable analytical tool for editors, authors and readers.\n"], "author_display": ["Renchu Guan", "Chen Yang", "Maurizio Marchese", "Yanchun Liang", "Xiaohu Shi"], "article_type": "Research Article", "score": 0.41372252, "title_display": "Full Text Clustering and Relationship Network Analysis of Biomedical Publications", "publication_date": "2014-09-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0108847"}, {"journal": "PLoS ONE", "abstract": ["\nThis paper presents an implementation of the brute-force exact k-Nearest Neighbor Graph (k-NNG) construction for ultra-large high-dimensional data cloud. The proposed method uses Graphics Processing Units (GPUs) and is scalable with multi-levels of parallelism (between nodes of a cluster, between different GPUs on a single node, and within a GPU). The method is applicable to homogeneous computing clusters with a varying number of nodes and GPUs per node. We achieve a 6-fold speedup in data processing as compared with an optimized method running on a cluster of CPUs and bring a hitherto impossible -NNG generation for a dataset of twenty million images with 15 k dimensionality into the realm of practical possibility.\n"], "author_display": ["Ali Dashti", "Ivan Komarov", "Roshan M. D\u2019Souza"], "article_type": "Research Article", "score": 0.4125298, "title_display": "Efficient Computation of k-Nearest Neighbour Graphs for Large High-Dimensional Data Sets on GPU Clusters", "publication_date": "2013-09-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0074113"}, {"journal": "PLoS ONE", "abstract": ["\nSpoken words carry linguistic and indexical information to listeners. Abstractionist models of spoken word recognition suggest that indexical information is stripped away in a process called normalization to allow processing of the linguistic message to proceed. In contrast, exemplar models of the lexicon suggest that indexical information is retained in memory, and influences the process of spoken word recognition. In the present study native Spanish listeners heard Spanish words that varied in grammatical gender (masculine, ending in -o, or feminine, ending in -a) produced by either a male or a female speaker. When asked to indicate the grammatical gender of the words, listeners were faster and more accurate when the sex of the speaker \u201cmatched\u201d the grammatical gender than when the sex of the speaker and the grammatical gender \u201cmismatched.\u201d No such interference was observed when listeners heard the same stimuli, but identified whether the speaker was male or female. This finding suggests that indexical information, in this case the sex of the speaker, influences not just processes associated with word recognition, but also higher-level processes associated with grammatical processing. This result also raises questions regarding the widespread assumption about the cognitive independence and automatic nature of grammatical processes.\n"], "author_display": ["Michael S. Vitevitch", "Joan Sereno", "Allard Jongman", "Rutherford Goldstein"], "article_type": "Research Article", "score": 0.41238028, "title_display": "Speaker Sex Influences Processing of Grammatical Gender", "publication_date": "2013-11-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0079701"}, {"journal": "PLoS ONE", "abstract": ["\nPrecision-recall curves are highly informative about the performance of binary classifiers, and the area under these curves is a popular scalar performance measure for comparing different classifiers. However, for many applications class labels are not provided with absolute certainty, but with some degree of confidence, often reflected by weights or soft labels assigned to data points. Computing the area under the precision-recall curve requires interpolating between adjacent supporting points, but previous interpolation schemes are not directly applicable to weighted data. Hence, even in cases where weights were available, they had to be neglected for assessing classifiers using precision-recall curves. Here, we propose an interpolation for precision-recall curves that can also be used for weighted data, and we derive conditions for classification scores yielding the maximum and minimum area under the precision-recall curve. We investigate accordances and differences of the proposed interpolation and previous ones, and we demonstrate that taking into account existing weights of test data is important for the comparison of classifiers.\n"], "author_display": ["Jens Keilwagen", "Ivo Grosse", "Jan Grau"], "article_type": "Research Article", "score": 0.41232407, "title_display": "Area under Precision-Recall Curves for Weighted and Unweighted Data", "publication_date": "2014-03-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0092209"}, {"journal": "PLOS ONE", "abstract": ["\nScience is a social process with far-reaching impact on our modern society. In recent years, for the first time we are able to scientifically study the science itself. This is enabled by massive amounts of data on scientific publications that is increasingly becoming available. The data is contained in several databases such as Web of Science or PubMed, maintained by various public and private entities. Unfortunately, these databases are not always consistent, which considerably hinders this study. Relying on the powerful framework of complex networks, we conduct a systematic analysis of the consistency among six major scientific databases. We found that identifying a single \"best\" database is far from easy. Nevertheless, our results indicate appreciable differences in mutual consistency of different databases, which we interpret as recipes for future bibliometric studies.\n"], "author_display": ["Lovro \u0160ubelj", "Marko Bajec", "Biljana Mileva Boshkoska", "Andrej Kastrin", "Zoran Levnaji\u0107"], "article_type": "Research Article", "score": 0.4122634, "title_display": "Quantifying the Consistency of Scientific Databases", "publication_date": "2015-05-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0127390"}, {"journal": "PLoS Computational Biology", "abstract": ["\n        Local supra-linear summation of excitatory inputs occurring in pyramidal cell dendrites, the so-called dendritic spikes, results in independent spiking dendritic sub-units, which turn pyramidal neurons into two-layer neural networks capable of computing linearly non-separable functions, such as the exclusive OR. Other neuron classes, such as interneurons, may possess only a few independent dendritic sub-units, or only passive dendrites where input summation is purely sub-linear, and where dendritic sub-units are only saturating. To determine if such neurons can also compute linearly non-separable functions, we enumerate, for a given parameter range, the Boolean functions implementable by a binary neuron model with a linear sub-unit and either a single spiking or a saturating dendritic sub-unit. We then analytically generalize these numerical results to an arbitrary number of non-linear sub-units. First, we show that a single non-linear dendritic sub-unit, in addition to the somatic non-linearity, is sufficient to compute linearly non-separable functions. Second, we analytically prove that, with a sufficient number of saturating dendritic sub-units, a neuron can compute all functions computable with purely excitatory inputs. Third, we show that these linearly non-separable functions can be implemented with at least two strategies: one where a dendritic sub-unit is sufficient to trigger a somatic spike; another where somatic spiking requires the cooperation of multiple dendritic sub-units. We formally prove that implementing the latter architecture is possible with both types of dendritic sub-units whereas the former is only possible with spiking dendrites. Finally, we show how linearly non-separable functions can be computed by a generic two-compartment biophysical model and a realistic neuron model of the cerebellar stellate cell interneuron. Taken together our results demonstrate that passive dendrites are sufficient to enable neurons to compute linearly non-separable functions.\n      Author Summary: Classical views on single neuron computation treat dendrites as mere collectors of inputs, that is forwarded to the soma for linear summation and causes a spike output if it is sufficiently large. Such a single neuron model can only compute linearly separable input-output functions, representing a small fraction of all possible functions. Recent experimental findings show that in certain pyramidal cells excitatory inputs can be supra-linearly integrated within a dendritic branch, turning this branch into a spiking dendritic sub-unit. Neurons containing many of these dendritic sub-units can compute both linearly separable and linearly non-separable functions. Nevertheless, other neuron types have dendrites which do not spike because the required voltage gated channels are absent. However, these dendrites sub-linearly sum excitatory inputs turning branches into saturating sub-units. We wanted to test if this last type of non-linear summation is sufficient for a single neuron to compute linearly non-separable functions. Using a combination of Boolean algebra and biophysical modeling, we show that a neuron with a single non-linear dendritic sub-unit whether spiking or saturating is able to compute linearly non-separable functions. Thus, in principle, any neuron with a dendritic tree, even passive, can compute linearly non-separable functions. "], "author_display": ["Romain Daniel Caz\u00e9", "Mark Humphries", "Boris Gutkin"], "article_type": "Research Article", "score": 0.41196325, "title_display": "Passive Dendrites Enable Single Neurons to Compute Linearly Non-separable Functions", "publication_date": "2013-02-28T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1002867"}, {"journal": "PLOS ONE", "abstract": ["\nIn recent years, traceability systems have been developed as effective tools for improving the transparency of supply chains, thereby guaranteeing the quality and safety of food products. In this study, we proposed a cattle/beef supply chain traceability model and a traceability system based on radio frequency identification (RFID) technology and the EPCglobal network. First of all, the transformations of traceability units were defined and analyzed throughout the cattle/beef chain. Secondly, we described the internal and external traceability information acquisition, transformation, and transmission processes throughout the beef supply chain in detail, and explained a methodology for modeling traceability information using the electronic product code information service (EPCIS) framework. Then, the traceability system was implemented based on Fosstrak and FreePastry software packages, and animal ear tag code and electronic product code (EPC) were employed to identify traceability units. Finally, a cattle/beef supply chain included breeding business, slaughter and processing business, distribution business and sales outlet was used as a case study to evaluate the beef supply chain traceability system. The results demonstrated that the major advantages of the traceability system are the effective sharing of information among business and the gapless traceability of the cattle/beef supply chain.\n"], "author_display": ["Wanjie Liang", "Jing Cao", "Yan Fan", "Kefeng Zhu", "Qiwei Dai"], "article_type": "Research Article", "score": 0.41162583, "title_display": "Modeling and Implementation of Cattle/Beef Supply Chain Traceability Using a Distributed RFID-Based Framework in China", "publication_date": "2015-10-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0139558"}, {"journal": "PLoS ONE", "abstract": ["\nIn this paper, we derive interrelations of graph distance measures by means of inequalities. For this investigation we are using graph distance measures based on topological indices that have not been studied in this context. Specifically, we are using the well-known Wiener index, Randi\u0107 index, eigenvalue-based quantities and graph entropies. In addition to this analysis, we present results from numerical studies exploring various properties of the measures and aspects of their quality. Our results could find application in chemoinformatics and computational biology where the structural investigation of chemical components and gene networks is currently of great interest.\n"], "author_display": ["Matthias Dehmer", "Frank Emmert-Streib", "Yongtang Shi"], "article_type": "Research Article", "score": 0.41134462, "title_display": "Interrelations of Graph Distance Measures Based on Topological Indices", "publication_date": "2014-04-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0094985"}, {"journal": "PLOS ONE", "abstract": ["\nTo automatically adapt to various hardware and software environments on different devices, this paper presents a time-critical adaptive approach for visualizing natural scenes. In this method, a simplified expression of a tree model is used for different devices. The best rendering scheme is intelligently selected to generate a particular scene by estimating the rendering time of trees based on their visual importance. Therefore, this approach can ensure the reality of natural scenes while maintaining a constant frame rate for their interactive display. To verify its effectiveness and flexibility, this method is applied in different devices, such as a desktop computer, laptop, iPad and smart phone. Applications show that the method proposed in this paper can not only adapt to devices with different computing abilities and system resources very well but can also achieve rather good visual realism and a constant frame rate for natural scenes.\n"], "author_display": ["Tianyang Dong", "Siyuan Liu", "Jiajia Xia", "Jing Fan", "Ling Zhang"], "article_type": "Research Article", "score": 0.41022527, "title_display": "A Time-Critical Adaptive Approach for Visualizing Natural Scenes on Different Devices", "publication_date": "2015-02-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0117586"}, {"journal": "PLoS ONE", "abstract": ["\nThe number of scholarly documents available on the web is estimated using capture/recapture methods by studying the coverage of two major academic search engines: Google Scholar and Microsoft Academic Search. Our estimates show that at least 114 million English-language scholarly documents are accessible on the web, of which Google Scholar has nearly 100 million. Of these, we estimate that at least 27 million (24%) are freely available since they do not require a subscription or payment of any kind. In addition, at a finer scale, we also estimate the number of scholarly documents on the web for fifteen fields: Agricultural Science, Arts and Humanities, Biology, Chemistry, Computer Science, Economics and Business, Engineering, Environmental Sciences, Geosciences, Material Science, Mathematics, Medicine, Physics, Social Sciences, and Multidisciplinary, as defined by Microsoft Academic Search. In addition, we show that among these fields the percentage of documents defined as freely available varies significantly, i.e., from 12 to 50%.\n"], "author_display": ["Madian Khabsa", "C. Lee Giles"], "article_type": "Research Article", "score": 0.4095676, "title_display": "The Number of Scholarly Documents on the Public Web", "publication_date": "2014-05-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0093949"}, {"journal": "PLoS ONE", "abstract": ["\n        Investigation of mechanisms of information handling in neural assemblies involved in computational and cognitive tasks is a challenging problem. Synergetic cooperation of neurons in time domain, through synchronization of firing of multiple spatially distant neurons, has been widely spread as the main paradigm. Complementary, the brain may also employ information coding and processing in spatial dimension. Then, the result of computation depends also on the spatial distribution of long-scale information. The latter bi-dimensional alternative is notably less explored in the literature. Here, we propose and theoretically illustrate a concept of spatiotemporal representation and processing of long-scale information in laminar neural structures. We argue that relevant information may be hidden in self-sustained traveling waves of neuronal activity and then their nonlinear interaction yields efficient wave-processing of spatiotemporal information. Using as a testbed a chain of FitzHugh-Nagumo neurons, we show that the wave-processing can be achieved by incorporating into the single-neuron dynamics an additional voltage-gated membrane current. This local mechanism provides a chain of such neurons with new emergent network properties. In particular, nonlinear waves as a carrier of long-scale information exhibit a variety of functionally different regimes of interaction: from complete or asymmetric annihilation to transparent crossing. Thus neuronal chains can work as computational units performing different operations over spatiotemporal information. Exploiting complexity resonance these composite units can discard stimuli of too high or too low frequencies, while selectively compress those in the natural frequency range. We also show how neuronal chains can contextually interpret raw wave information. The same stimulus can be processed differently or identically according to the context set by a periodic wave train injected at the opposite end of the chain.\n      "], "author_display": ["Jos\u00e9 Antonio Villacorta-Atienza", "Valeri A. Makarov"], "article_type": "Research Article", "score": 0.40913105, "title_display": "Wave-Processing of Long-Scale Information by Neuronal Chains", "publication_date": "2013-02-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0057440"}, {"journal": "PLOS ONE", "abstract": ["\nDigital data from the political sphere is abundant, omnipresent, and more and more directly accessible through the Internet. Project Vote Smart (PVS) is a prominent example of this big public data and covers various aspects of U.S. politics in astonishing detail. Despite the vast potential of PVS\u2019 data for political science, economics, and sociology, it is hardly used in empirical research. The systematic compilation of semi-structured data can be complicated and time consuming as the data format is not designed for conventional scientific research. This paper presents a new tool that makes the data easily accessible to a broad scientific community. We provide the software called pvsR as an add-on to the R programming environment for statistical computing. This open source interface (OSI) serves as a direct link between a statistical analysis and the large PVS database. The free and open code is expected to substantially reduce the cost of research with PVS\u2019 new big public data in a vast variety of possible applications. We discuss its advantages vis-\u00e0-vis traditional methods of data generation as well as already existing interfaces. The validity of the library is documented based on an illustration involving female representation in local politics. In addition, pvsR facilitates the replication of research with PVS data at low costs, including the pre-processing of data. Similar OSIs are recommended for other big public databases.\n"], "author_display": ["Ulrich Matter", "Alois Stutzer"], "article_type": "Research Article", "score": 0.40902972, "title_display": "pvsR: An Open Source Interface to Big Data on the American Political Sphere", "publication_date": "2015-07-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0130501"}, {"journal": "PLoS ONE", "abstract": ["\nThe most informative probability distribution functions (PDFs) describing the Ramachandran phi-psi dihedral angle pair, a fundamental descriptor of backbone conformation of protein molecules, are derived from high-resolution X-ray crystal structures using an information-theoretic approach. The Information Maximization Device (IMD) is established, based on fundamental information-theoretic concepts, and then applied specifically to derive highly resolved phi-psi maps for all 20 single amino acid and all 8000 triplet sequences at an optimal resolution determined by the volume of current data. The paper shows that utilizing the latent information contained in all viable high-resolution crystal structures found in the Protein Data Bank (PDB), totaling more than 77,000 chains, permits the derivation of a large number of optimized sequence-dependent PDFs. This work demonstrates the effectiveness of the IMD and the superiority of the resulting PDFs by extensive fold recognition experiments and rigorous comparisons with previously published triplet PDFs. Because it automatically optimizes PDFs, IMD results in improved performance of knowledge-based potentials, which rely on such PDFs. Furthermore, it provides an easy computational recipe for empirically deriving other kinds of sequence-dependent structural PDFs with greater detail and precision. The high-resolution phi-psi maps derived in this work are available for download.\n"], "author_display": ["Armando D. Solis"], "article_type": "Research Article", "score": 0.40893582, "title_display": "Deriving High-Resolution Protein Backbone Structure Propensities from All Crystal Data Using the Information Maximization Device", "publication_date": "2014-06-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0094334"}, {"journal": "PLoS ONE", "abstract": ["Background: Early and accurate identification of adverse drug reactions (ADRs) is critically important for drug development and clinical safety. Computer-aided prediction of ADRs has attracted increasing attention in recent years, and many computational models have been proposed. However, because of the lack of systematic analysis and comparison of the different computational models, there remain limitations in designing more effective algorithms and selecting more useful features. There is therefore an urgent need to review and analyze previous computation models to obtain general conclusions that can provide useful guidance to construct more effective computational models to predict ADRs. Principal Findings: In the current study, the main work is to compare and analyze the performance of existing computational methods to predict ADRs, by implementing and evaluating additional algorithms that have been earlier used for predicting drug targets. Our results indicated that topological and intrinsic features were complementary to an extent and the Jaccard coefficient had an important and general effect on the prediction of drug-ADR associations. By comparing the structure of each algorithm, final formulas of these algorithms were all converted to linear model in form, based on this finding we propose a new algorithm called the general weighted profile method and it yielded the best overall performance among the algorithms investigated in this paper. Conclusion: Several meaningful conclusions and useful findings regarding the prediction of ADRs are provided for selecting optimal features and algorithms. "], "author_display": ["Qifan Kuang", "MinQi Wang", "Rong Li", "YongCheng Dong", "Yizhou Li", "Menglong Li"], "article_type": "Research Article", "score": 0.40887922, "title_display": "A Systematic Investigation of Computation Models for Predicting Adverse Drug Reactions (ADRs)", "publication_date": "2014-09-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0105889"}, {"journal": "PLOS ONE", "abstract": ["\nCardiac atlases play an important role in the computer-aided diagnosis of cardiovascular diseases, in particular they need to deal with large and highly variable image datasets. In this paper, we propose a new nonrigid registration algorithm incorporating shape information, to produce comprehensive atlases. For one thing, the multiscale gradient orientation features of images are combined to form the construction of multifeature mutual information. Additionally, the shape information of multiple-objects in images is incorporated into the cost function for registration. We demonstrate the merits of the new registration algorithm on the 3D data sets of 15 patients. The experimental results show that the new registration algorithm can outperform the conventional intensity-based registration method. The obtained atlas can represent the cardiac structures more accurately.\n"], "author_display": ["Yunfei Zha", "Xuesong Lu", "Li Wang", "Rongqian Yang", "Shanxing Ou", "Dong Xing", "Defeng Wang"], "article_type": "Research Article", "score": 0.40856943, "title_display": "Nonrigid Registration Regularized by Shape Information: Application to Atlas Construction of Cardiac CT Images", "publication_date": "2015-06-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0130730"}, {"journal": "PLOS ONE", "abstract": ["Background: Several studies have suggested that high levels of computer use are linked to psychopathology. However, there is ambiguity about what should be considered normal or over-use of computers. Furthermore, the nature of the link between computer usage and psychopathology is controversial. The current study utilized the context of age to address these questions. Our hypothesis was that the context of age will be paramount for differentiating normal from excessive use, and that this context will allow a better understanding of the link to psychopathology. Methods: In a cross-sectional study, 185 parents and children aged 3\u201318 years were recruited in clinical and community settings. They were asked to fill out questionnaires regarding demographics, functional and academic variables, computer use as well as psychiatric screening questionnaires. Using a regression model, we identified 3 groups of normal-use, over-use and under-use and examined known factors as putative differentiators between the over-users and the other groups. Results: After modeling computer screen time according to age, factors linked to over-use were: decreased socialization (OR 3.24, Confidence interval [CI] 1.23\u20138.55, p = 0.018), difficulty to disengage from the computer (OR 1.56, CI 1.07\u20132.28, p = 0.022) and age, though borderline-significant (OR 1.1 each year, CI 0.99\u20131.22, p = 0.058). While psychopathology was not linked to over-use, post-hoc analysis revealed that the link between increased computer screen time and psychopathology was age-dependent and solidified as age progressed (p = 0.007). Unlike computer usage, the use of small-screens and smartphones was not associated with psychopathology. Conclusions: The results suggest that computer screen time follows an age-based course. We conclude that differentiating normal from over-use as well as defining over-use as a possible marker for psychiatric difficulties must be performed within the context of age. If verified by additional studies, future research should integrate those views in order to better understand the intricacies of computer over-use. "], "author_display": ["Aviv Segev", "Aviva Mimouni-Bloch", "Sharon Ross", "Zmira Silman", "Hagai Maoz", "Yuval Bloch"], "article_type": "Research Article", "score": 0.40735865, "title_display": "Evaluating Computer Screen Time and Its Possible Link to Psychopathology in the Context of Age: A Cross-Sectional Study of Parents and Children", "publication_date": "2015-11-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0140542"}, {"journal": "PLoS ONE", "abstract": ["\n        Posttranslational modifications (PTMs) of proteins are responsible for sensing and transducing signals to regulate various cellular functions and signaling events. S-nitrosylation (SNO) is one of the most important and universal PTMs. With the avalanche of protein sequences generated in the post-genomic age, it is highly desired to develop computational methods for timely identifying the exact SNO sites in proteins because this kind of information is very useful for both basic research and drug development. Here, a new predictor, called iSNO-PseAAC, was developed for identifying the SNO sites in proteins by incorporating the position-specific amino acid propensity (PSAAP) into the general form of pseudo amino acid composition (PseAAC). The predictor was implemented using the conditional random field (CRF) algorithm. As a demonstration, a benchmark dataset was constructed that contains 731 SNO sites and 810 non-SNO sites. To reduce the homology bias, none of these sites were derived from the proteins that had  pairwise sequence identity to any other. It was observed that the overall cross-validation success rate achieved by iSNO-PseAAC in identifying nitrosylated proteins on an independent dataset was over 90%, indicating that the new predictor is quite promising. Furthermore, a user-friendly web-server for iSNO-PseAAC was established at http://app.aporc.org/iSNO-PseAAC/, by which users can easily obtain the desired results without the need to follow the mathematical equations involved during the process of developing the prediction method. It is anticipated that iSNO-PseAAC may become a useful high throughput tool for identifying the SNO sites, or at the very least play a complementary role to the existing methods in this area.\n      "], "author_display": ["Yan Xu", "Jun Ding", "Ling-Yun Wu", "Kuo-Chen Chou"], "article_type": "Research Article", "score": 0.4070589, "title_display": "iSNO-PseAAC: Predict Cysteine S-Nitrosylation Sites in Proteins by Incorporating Position Specific Amino Acid Propensity into Pseudo Amino Acid Composition", "publication_date": "2013-02-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0055844"}, {"journal": "PLOS ONE", "abstract": ["\nThe large availability of user provided contents on online social media facilitates people aggregation around shared beliefs, interests, worldviews and narratives. In spite of the enthusiastic rhetoric about the so called collective intelligence unsubstantiated rumors and conspiracy theories\u2014e.g., chemtrails, reptilians or the Illuminati\u2014are pervasive in online social networks (OSN). In this work we study, on a sample of 1.2 million of individuals, how information related to very distinct narratives\u2014i.e. main stream scientific and conspiracy news\u2014are consumed and shape communities on Facebook. Our results show that polarized communities emerge around distinct types of contents and usual consumers of conspiracy news result to be more focused and self-contained on their specific contents. To test potential biases induced by the continued exposure to unsubstantiated rumors on users\u2019 content selection, we conclude our analysis measuring how users respond to 4,709 troll information\u2014i.e. parodistic and sarcastic imitation of conspiracy theories. We find that 77.92% of likes and 80.86% of comments are from users usually interacting with conspiracy stories.\n"], "author_display": ["Alessandro Bessi", "Mauro Coletto", "George Alexandru Davidescu", "Antonio Scala", "Guido Caldarelli", "Walter Quattrociocchi"], "article_type": "Research Article", "score": 0.40693247, "title_display": "Science vs Conspiracy: Collective Narratives in the Age of Misinformation", "publication_date": "2015-02-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0118093"}, {"journal": "PLOS ONE", "abstract": ["\nIn apple cultivation, simulation models may be used to monitor fruit size during the growth and development process to predict production levels and to optimize fruit quality. Here, Fuji apples cultivated in spindle-type systems were used as the model crop. Apple size was measured during the growing period at an interval of about 20 days after full bloom, with three weather stations being used to collect orchard temperature and solar radiation data at different sites. Furthermore, a 2-year dataset (2011 and 2012) of apple fruit size measurements were integrated according to the weather station deployment sites, in addition to the top two most important environment factors, thermal and sunshine hours, into the model. The apple fruit diameter and length were simulated using physiological development time (PDT), an indicator that combines important environment factors, such as temperature and photoperiod, as the driving variable. Compared to the model of calendar-based development time (CDT), an indicator counting the days that elapse after full bloom, we confirmed that the PDT model improved the estimation accuracy to within 0.2 cm for fruit diameter and 0.1 cm for fruit length in independent years using a similar data collection method in 2013. The PDT model was implemented to realize a web-based management information system for a digital orchard, and the digital system had been applied in Shandong Province, China since 2013. This system may be used to compute the dynamic curve of apple fruit size based on data obtained from a nearby weather station. This system may provide an important decision support for farmers using the website and short message service to optimize crop production and, hence, economic benefit.\n"], "author_display": ["Ming Li", "Meixiang Chen", "Yong Zhang", "Chunxia Fu", "Bin Xing", "Wenyong Li", "Jianping Qian", "Sha Li", "Hui Wang", "Xiaodan Fan", "Yujing Yan", "Yan\u2019an Wang", "Xinting Yang"], "article_type": "Research Article", "score": 0.40652147, "title_display": "Apple Fruit Diameter and Length Estimation by Using the Thermal and Sunshine Hours Approach and Its Application to the Digital Orchard Management Information System", "publication_date": "2015-04-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0120124"}, {"journal": "PLoS ONE", "abstract": ["\n        The advent of next-generation sequencing technologies is accompanied with the development of many whole-genome sequence assembly methods and software, especially for de novo fragment assembly. Due to the poor knowledge about the applicability and performance of these software tools, choosing a befitting assembler becomes a tough task. Here, we provide the information of adaptivity for each program, then above all, compare the performance of eight distinct tools against eight groups of simulated datasets from Solexa sequencing platform. Considering the computational time, maximum random access memory (RAM) occupancy, assembly accuracy and integrity, our study indicate that string-based assemblers, overlap-layout-consensus (OLC) assemblers are well-suited for very short reads and longer reads of small genomes respectively. For large datasets of more than hundred millions of short reads, De Bruijn graph-based assemblers would be more appropriate. In terms of software implementation, string-based assemblers are superior to graph-based ones, of which SOAPdenovo is complex for the creation of configuration file. Our comparison study will assist researchers in selecting a well-suited assembler and offer essential information for the improvement of existing assemblers or the developing of novel assemblers.\n      "], "author_display": ["Wenyu Zhang", "Jiajia Chen", "Yang Yang", "Yifei Tang", "Jing Shang", "Bairong Shen"], "article_type": "Research Article", "score": 0.4064695, "title_display": "A Practical Comparison of <i>De Novo</i> Genome Assembly Software Tools for Next-Generation Sequencing Technologies", "publication_date": "2011-03-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0017915"}, {"journal": "PLOS ONE", "abstract": ["\nIn this study, we introduce an original distance definition for graphs, called the Markov-inverse-F measure (MiF). This measure enables the integration of classical graph theory indices with new knowledge pertaining to structural feature extraction from semantic networks. MiF improves the conventional Jaccard and/or Simpson indices, and reconciles both the geodesic information (random walk) and co-occurrence adjustment (degree balance and distribution). We measure the effectiveness of graph-based coefficients through the application of linguistic graph information for a neural activity recorded during conceptual processing in the human brain. Specifically, the MiF distance is computed between each of the nouns used in a previous neural experiment and each of the in-between words in a subgraph derived from the Edinburgh Word Association Thesaurus of English. From the MiF-based information matrix, a machine learning model can accurately obtain a scalar parameter that specifies the degree to which each voxel in (the MRI image of) the brain is activated by each word or each principal component of the intermediate semantic features. Furthermore, correlating the voxel information with the MiF-based principal components, a new computational neurolinguistics model with a network connectivity paradigm is created. This allows two dimensions of context space to be incorporated with both semantic and neural distributional representations.\n"], "author_display": ["Hiroyuki Akama", "Maki Miyake", "Jaeyoung Jung", "Brian Murphy"], "article_type": "Research Article", "score": 0.40642187, "title_display": "Using Graph Components Derived from an Associative Concept Dictionary to Predict fMRI Neural Activation Patterns that Represent the Meaning of Nouns", "publication_date": "2015-04-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0125725"}, {"journal": "PLOS ONE", "abstract": ["\nCollision-based computing (CBC) is a form of unconventional computing in which travelling localisations represent data and conditional routing of signals determines the output state; collisions between localisations represent logical operations. We investigated patterns of Ca2+-containing vesicle distribution within a live organism, slime mould Physarum polycephalum, with confocal microscopy and observed them colliding regularly. Vesicles travel down cytoskeletal \u2018circuitry\u2019 and their collisions may result in reflection, fusion or annihilation. We demonstrate through experimental observations that naturally-occurring vesicle dynamics may be characterised as a computationally-universal set of Boolean logical operations and present a \u2018vesicle modification\u2019 of the archetypal CBC \u2018billiard ball model\u2019 of computation. We proceed to discuss the viability of intracellular vesicles as an unconventional computing substrate in which we delineate practical considerations for reliable vesicle \u2018programming\u2019 in both in vivo and in vitro vesicle computing architectures and present optimised designs for both single logical gates and combinatorial logic circuits based on cytoskeletal network conformations. The results presented here demonstrate the first characterisation of intracelluar phenomena as collision-based computing and hence the viability of biological substrates for computing.\n"], "author_display": ["Richard Mayne", "Andrew Adamatzky"], "article_type": "Research Article", "score": 0.4063483, "title_display": "On the Computing Potential of Intracellular Vesicles", "publication_date": "2015-10-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0139617"}, {"journal": "PLoS ONE", "abstract": ["\n        We introduce a novel hybrid of two fields\u2014Computational Fluid Dynamics (CFD) and Agent-Based Modeling (ABM)\u2014as a powerful new technique for urban evacuation planning. CFD is a predominant technique for modeling airborne transport of contaminants, while ABM is a powerful approach for modeling social dynamics in populations of adaptive individuals. The hybrid CFD-ABM method is capable of simulating how large, spatially-distributed populations might respond to a physically realistic contaminant plume. We demonstrate the overall feasibility of CFD-ABM evacuation design, using the case of a hypothetical aerosol release in Los Angeles to explore potential effectiveness of various policy regimes. We conclude by arguing that this new approach can be powerfully applied to arbitrary population centers, offering an unprecedented preparedness and catastrophic event response tool.\n      "], "author_display": ["Joshua M. Epstein", "Ramesh Pankajakshan", "Ross A. Hammond"], "article_type": "Research Article", "score": 0.40625626, "title_display": "Combining Computational Fluid Dynamics and Agent-Based Modeling: A New Approach to Evacuation Planning", "publication_date": "2011-05-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0020139"}, {"journal": "PLOS Computational Biology", "abstract": ["\nThe manner in which populations of inhibitory (INH) and excitatory (EXC) neocortical neurons collectively encode stimulus-related information is a fundamental, yet still unresolved question. Here we address this question by simultaneously recording with large-scale multi-electrode arrays (of up to 128 channels) the activity of cell ensembles (of up to 74 neurons) distributed along all layers of 3\u20134 neighboring cortical columns in the anesthetized adult rat somatosensory barrel cortex in vivo. Using two different whisker stimulus modalities (location and frequency) we show that individual INH neurons \u2013 classified as such according to their distinct extracellular spike waveforms \u2013 discriminate better between restricted sets of stimuli (\u22646 stimulus classes) than EXC neurons in granular and infra-granular layers. We also demonstrate that ensembles of INH cells jointly provide as much information about such stimuli as comparable ensembles containing the ~20% most informative EXC neurons, however presenting less information redundancy \u2013 a result which was consistent when applying both theoretical information measurements and linear discriminant analysis classifiers. These results suggest that a consortium of INH neurons dominates the information conveyed to the neocortical network, thereby efficiently processing incoming sensory activity. This conclusion extends our view on the role of the inhibitory system to orchestrate cortical activity.\nAuthor Summary: Perception of the environment relies on neuronal computation in the cerebral cortex. However, the exact algorithms by which cortical neuronal networks process relevant information from the inputs of sensory organs are only poorly understood. To address this problem we stimulated distinct whiskers and recorded the neuronal responses from identified cortical whisker representations of the rat using multi-site electrodes. For rodents the whisker system is one main sensory input channel, offering the unique property that for each whisker an identified cortical area (\"barrel-related column\") represents its main cortical input station. In the present study we were able to demonstrate that the action potential firing of single inhibitory neurons provides more information about behaviorally relevant qualities of whisker stimulation (identity of the stimulated whisker and frequency of stimulation) than excitatory neurons. In addition, information about stimulation qualities was encoded with less redundancy in inhibitory neurons. In summary, the results of our study suggest that inhibitory neurons carry substantial information about the sensory environment and can thereby adequately orchestrate neuronal activity in sensory cortices. "], "author_display": ["Vicente Reyes-Puerta", "Suam Kim", "Jyh-Jang Sun", "Barbara Imbrosci", "Werner Kilb", "Heiko J. Luhmann"], "article_type": "Research Article", "score": 0.40592104, "title_display": "High Stimulus-Related Information in Barrel Cortex Inhibitory Interneurons", "publication_date": "2015-06-22T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1004121"}, {"journal": "PLoS ONE", "abstract": ["\nAlthough there has been tremendous progress in understanding the mechanics of the nervous system, there has not been a general theory of its computational function. Here I present a theory that relates the established biophysical properties of single generic neurons to principles of Bayesian probability theory, reinforcement learning and efficient coding. I suggest that this theory addresses the general computational problem facing the nervous system. Each neuron is proposed to mirror the function of the whole system in learning to predict aspects of the world related to future reward. According to the model, a typical neuron receives current information about the state of the world from a subset of its excitatory synaptic inputs, and prior information from its other inputs. Prior information would be contributed by synaptic inputs representing distinct regions of space, and by different types of non-synaptic, voltage-regulated channels representing distinct periods of the past. The neuron's membrane voltage is proposed to signal the difference between current and prior information (\u201cprediction error\u201d or \u201csurprise\u201d). A neuron would apply a Hebbian plasticity rule to select those excitatory inputs that are the most closely correlated with reward but are the least predictable, since unpredictable inputs provide the neuron with the most \u201cnew\u201d information about future reward. To minimize the error in its predictions and to respond only when excitation is \u201cnew and surprising,\u201d the neuron selects amongst its prior information sources through an anti-Hebbian rule. The unique inputs of a mature neuron would therefore result from learning about spatial and temporal patterns in its local environment, and by extension, the external world. Thus the theory describes how the structure of the mature nervous system could reflect the structure of the external world, and how the complexity and intelligence of the system might develop from a population of undifferentiated neurons, each implementing similar learning algorithms.\n"], "author_display": ["Christopher D. Fiorillo"], "article_type": "Research Article", "score": 0.40532044, "title_display": "Towards a General Theory of Neural Computation Based on Prediction by Single Neurons", "publication_date": "2008-10-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0003298"}, {"journal": "PLoS ONE", "abstract": ["\n        Opportunities for associationist learning of word meaning, where a word is heard or read contemperaneously with information being available on its meaning, are considered too infrequent to account for the rate of language acquisition in children. It has been suggested that additional learning could occur in a distributional mode, where information is gleaned from the distributional statistics (word co-occurrence etc.) of natural language. Such statistics are relevant to meaning because of the Distributional Principle that \u2018words of similar meaning tend to occur in similar contexts\u2019. Computational systems, such as Latent Semantic Analysis, have substantiated the viability of distributional learning of word meaning, by showing that semantic similarities between words can be accurately estimated from analysis of the distributional statistics of a natural language corpus. We consider whether appearance similarities can also be learnt in a distributional mode. As grounds for such a mode we advance the Appearance Hypothesis that \u2018words with referents of similar appearance tend to occur in similar contexts\u2019. We assess the viability of such learning by looking at the performance of a computer system that interpolates, on the basis of distributional and appearance similarity, from words that it has been explicitly taught the appearance of, in order to identify and name objects that it has not been taught about. Our experiment tests with a set of 660 simple concrete noun words. Appearance information on words is modelled using sets of images of examples of the word. Distributional similarity is computed from a standard natural language corpus. Our computation results support the viability of distributional learning of appearance.\n      "], "author_display": ["Lewis D. Griffin", "M. Husni Wahab", "Andrew J. Newell"], "article_type": "Research Article", "score": 0.40505803, "title_display": "Distributional Learning of Appearance", "publication_date": "2013-02-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0058074"}, {"journal": "PLoS ONE", "abstract": ["\nThis work describes an approach inspired by the primary visual cortex using the stimulus response of the receptive field profiles of binocular cells for disparity computation. Using the energy model based on the mechanism of log-Gabor filters for disparity encodings, we propose a suitable model to consistently represent the complex cells by computing the wide bandwidths of the cortical cells. This way, the model ensures the general neurophysiological findings in the visual cortex (V1), emphasizing the physical disparities and providing a simple selection method for the complex cell response. The results suggest that our proposed approach can achieve better results than a hybrid model with phase-shift and position-shift using position disparity alone.\n"], "author_display": ["Fernanda da C. e C. Faria", "Jorge Batista", "Helder Ara\u00fajo"], "article_type": "Research Article", "score": 0.4049308, "title_display": "Stereoscopic Depth Perception Using a Model Based on the Primary Visual Cortex", "publication_date": "2013-12-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0080745"}, {"journal": "PLOS ONE", "abstract": ["\nThe present paper introduces a condition number estimation method for preconditioned matrices. The newly developed method provides reasonable results, while the conventional method which is based on the Lanczos connection gives meaningless results. The Lanczos connection based method provides the condition numbers of coefficient matrices of systems of linear equations with information obtained through the preconditioned conjugate gradient method. Estimating the condition number of preconditioned matrices is sometimes important when describing the effectiveness of new preconditionerers or selecting adequate preconditioners. Operating a preconditioner on a coefficient matrix is the simplest method of estimation. However, this is not possible for large-scale computing, especially if computation is performed on distributed memory parallel computers. This is because, the preconditioned matrices become dense, even if the original matrices are sparse. Although the Lanczos connection method can be used to calculate the condition number of preconditioned matrices, it is not considered to be applicable to large-scale problems because of its weakness with respect to numerical errors. Therefore, we have developed a robust and parallelizable method based on Hager\u2019s method. The feasibility studies are curried out for the diagonal scaling preconditioner and the SSOR preconditioner with a diagonal matrix, a tri-daigonal matrix and Pei\u2019s matrix. As a result, the Lanczos connection method contains around 10% error in the results even with a simple problem. On the other hand, the new method contains negligible errors. In addition, the newly developed method returns reasonable solutions when the Lanczos connection method fails with Pei\u2019s matrix, and matrices generated with the finite element method.\n"], "author_display": ["Noriyuki Kushida"], "article_type": "Research Article", "score": 0.4047766, "title_display": "Condition Number Estimation of Preconditioned Matrices", "publication_date": "2015-03-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0122331"}, {"journal": "PLoS ONE", "abstract": ["Background: We investigate the accuracy of different similarity approaches for clustering over two million biomedical documents. Clustering large sets of text documents is important for a variety of information needs and applications such as collection management and navigation, summary and analysis. The few comparisons of clustering results from different similarity approaches have focused on small literature sets and have given conflicting results. Our study was designed to seek a robust answer to the question of which similarity approach would generate the most coherent clusters of a biomedical literature set of over two million documents. Methodology: We used a corpus of 2.15 million recent (2004-2008) records from MEDLINE, and generated nine different document-document similarity matrices from information extracted from their bibliographic records, including titles, abstracts and subject headings. The nine approaches were comprised of five different analytical techniques with two data sources. The five analytical techniques are cosine similarity using term frequency-inverse document frequency vectors (tf-idf cosine), latent semantic analysis (LSA), topic modeling, and two Poisson-based language models \u2013 BM25 and PMRA (PubMed Related Articles). The two data sources were a) MeSH subject headings, and b) words from titles and abstracts. Each similarity matrix was filtered to keep the top-n highest similarities per document and then clustered using a combination of graph layout and average-link clustering. Cluster results from the nine similarity approaches were compared using (1) within-cluster textual coherence based on the Jensen-Shannon divergence, and (2) two concentration measures based on grant-to-article linkages indexed in MEDLINE. Conclusions: PubMed's own related article approach (PMRA) generated the most coherent and most concentrated cluster solution of the nine text-based similarity approaches tested, followed closely by the BM25 approach using titles and abstracts. Approaches using only MeSH subject headings were not competitive with those based on titles and abstracts. "], "author_display": ["Kevin W. Boyack", "David Newman", "Russell J. Duhon", "Richard Klavans", "Michael Patek", "Joseph R. Biberstine", "Bob Schijvenaars", "Andr\u00e9 Skupin", "Nianli Ma", "Katy B\u00f6rner"], "article_type": "Research Article", "score": 0.40474787, "title_display": "Clustering More than Two Million Biomedical Publications: Comparing the Accuracies of Nine Text-Based Similarity Approaches", "publication_date": "2011-03-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0018029"}, {"journal": "PLoS ONE", "abstract": ["\n        The assessment of information transfer in the global economic network helps to understand the current environment and the outlook of an economy. Most approaches on global networks extract information transfer based mainly on a single variable. This paper establishes an entirely new bioinformatics-inspired approach to integrating information transfer derived from multiple variables and develops an international economic network accordingly. In the proposed methodology, we first construct the transfer entropies (TEs) between various intra- and inter-country pairs of economic time series variables, test their significances, and then use a weighted sum approach to aggregate information captured in each TE. Through a simulation study, the new method is shown to deliver better information integration compared to existing integration methods in that it can be applied even when intra-country variables are correlated. Empirical investigation with the real world data reveals that Western countries are more influential in the global economic network and that Japan has become less influential following the Asian currency crisis.\n      "], "author_display": ["Jinkyu Kim", "Gunn Kim", "Sungbae An", "Young-Kyun Kwon", "Sungroh Yoon"], "article_type": "Research Article", "score": 0.40439358, "title_display": "Entropy-Based Analysis and Bioinformatics-Inspired Integration of Global Economic Information Transfer", "publication_date": "2013-01-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0051986"}, {"journal": "PLOS ONE", "abstract": ["\nRecent developments have led to an enormous increase of publicly available large genomic data, including complete genomes. The 1000 Genomes Project was a major contributor, releasing the results of sequencing a large number of individual genomes, and allowing for a myriad of large scale studies on human genetic variation. However, the tools currently available are insufficient when the goal concerns some analyses of data sets encompassing more than hundreds of base pairs and when considering haplotype sequences of single nucleotide polymorphisms (SNPs). Here, we present a new and potent tool to deal with large data sets allowing the computation of a variety of summary statistics of population genetic data, increasing the speed of data analysis.\n"], "author_display": ["In\u00eas Soares", "Ana Moleirinho", "Gon\u00e7alo N. P. Oliveira", "Ant\u00f3nio Amorim"], "article_type": "Research Article", "score": 0.40434363, "title_display": "DivStat: A User-Friendly Tool for Single Nucleotide Polymorphism Analysis of Genomic Diversity", "publication_date": "2015-03-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0119851"}, {"journal": "PLoS ONE", "abstract": ["\nThe metagenomic method directly sequences and analyses genome information from microbial communities. The main computational tasks for metagenomic analyses include taxonomical and functional structure analysis for all genomes in a microbial community (also referred to as a metagenomic sample). With the advancement of Next Generation Sequencing (NGS) techniques, the number of metagenomic samples and the data size for each sample are increasing rapidly. Current metagenomic analysis is both data- and computation- intensive, especially when there are many species in a metagenomic sample, and each has a large number of sequences. As such, metagenomic analyses require extensive computational power. The increasing analytical requirements further augment the challenges for computation analysis. In this work, we have proposed Parallel-META 2.0, a metagenomic analysis software package, to cope with such needs for efficient and fast analyses of taxonomical and functional structures for microbial communities. Parallel-META 2.0 is an extended and improved version of Parallel-META 1.0, which enhances the taxonomical analysis using multiple databases, improves computation efficiency by optimized parallel computing, and supports interactive visualization of results in multiple views. Furthermore, it enables functional analysis for metagenomic samples including short-reads assembly, gene prediction and functional annotation. Therefore, it could provide accurate taxonomical and functional analyses of the metagenomic samples in high-throughput manner and on large scale.\n"], "author_display": ["Xiaoquan Su", "Weihua Pan", "Baoxing Song", "Jian Xu", "Kang Ning"], "article_type": "Research Article", "score": 0.40427098, "title_display": "Parallel-META 2.0: Enhanced Metagenomic Data Analysis with Functional Annotation, High Performance Computing and Advanced Visualization", "publication_date": "2014-03-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0089323"}, {"journal": "PLOS ONE", "abstract": ["\nModern social media are becoming overloaded with information because of the rapidly-expanding number of information feeds. We analyze the user-generated content in Sina Weibo, and find evidence that the spread of popular messages often follow a mechanism that differs from the spread of disease, in contrast to common belief. In this mechanism, an individual with more friends needs more repeated exposures to spread further the information. Moreover, our data suggest that for certain messages the chance of an individual to share the message is proportional to the fraction of its neighbours who shared it with him/her, which is a result of competition for attention. We model this process using a fractional susceptible infected recovered (FSIR) model, where the infection probability of a node is proportional to its fraction of infected neighbors. Our findings have dramatic implications for information contagion. For example, using the FSIR model we find that real-world social networks have a finite epidemic threshold in contrast to the zero threshold in disease epidemic models. This means that when individuals are overloaded with excess information feeds, the information either reaches out the population if it is above the critical epidemic threshold, or it would never be well received.\n"], "author_display": ["Ling Feng", "Yanqing Hu", "Baowen Li", "H. Eugene Stanley", "Shlomo Havlin", "Lidia A. Braunstein"], "article_type": "Research Article", "score": 0.40425986, "title_display": "Competing for Attention in Social Media under Information Overload Conditions", "publication_date": "2015-07-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0126090"}, {"journal": "PLOS ONE", "abstract": ["\nGene co-expression networks comprise one type of valuable biological networks. Many methods and tools have been published to construct gene co-expression networks; however, most of these tools and methods are inconvenient and time consuming for large datasets. We have developed a user-friendly, accelerated and optimized tool for constructing gene co-expression networks that can fully harness the parallel nature of GPU (Graphic Processing Unit) architectures. Genetic entropies were exploited to filter out genes with no or small expression changes in the raw data preprocessing step. Pearson correlation coefficients were then calculated. After that, we normalized these coefficients and employed the False Discovery Rate to control the multiple tests. At last, modules identification was conducted to construct the co-expression networks. All of these calculations were implemented on a GPU. We also compressed the coefficient matrix to save space. We compared the performance of the GPU implementation with those of multi-core CPU implementations with 16 CPU threads, single-thread C/C++ implementation and single-thread R implementation. Our results show that GPU implementation largely outperforms single-thread C/C++ implementation and single-thread R implementation, and GPU implementation outperforms multi-core CPU implementation when the number of genes increases. With the test dataset containing 16,000 genes and 590 individuals, we can achieve greater than 63 times the speed using a GPU implementation compared with a single-thread R implementation when 50 percent of genes were filtered out and about 80 times the speed when no genes were filtered out.\n"], "author_display": ["Meimei Liang", "Futao Zhang", "Gulei Jin", "Jun Zhu"], "article_type": "Research Article", "score": 0.40419185, "title_display": "FastGCN: A GPU Accelerated Tool for Fast Gene Co-Expression Networks", "publication_date": "2015-01-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0116776"}, {"journal": "PLOS ONE", "abstract": ["\nBiometrics has the advantages of efficiency and convenience in identity authentication. As one of the most promising biometric-based methods, ear recognition has received broad attention and research. Previous studies have achieved remarkable performance with multiple samples per person (MSPP) in the gallery. However, most conventional methods are insufficient when there is only one sample per person (OSPP) available in the gallery. To solve the OSPP problem by maximizing the use of a single sample, this paper proposes a hybrid multi-keypoint descriptor sparse representation-based classification (MKD-SRC) ear recognition approach based on 2D and 3D information. Because most 3D sensors capture 3D data accessorizing the corresponding 2D data, it is sensible to use both types of information. First, the ear region is extracted from the profile. Second, keypoints are detected and described for both the 2D texture image and 3D range image. Then, the hybrid MKD-SRC algorithm is used to complete the recognition with only OSPP in the gallery. Experimental results on a benchmark dataset have demonstrated the feasibility and effectiveness of the proposed method in resolving the OSPP problem. A Rank-one recognition rate of 96.4% is achieved for a gallery of 415 subjects, and the time involved in the computation is satisfactory compared to conventional methods.\n"], "author_display": ["Long Chen", "Zhichun Mu", "Baoqing Zhang", "Yi Zhang"], "article_type": "Research Article", "score": 0.40408534, "title_display": "Ear Recognition from One Sample Per Person", "publication_date": "2015-05-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0129505"}, {"journal": "PLoS ONE", "abstract": ["\nDo different fields of knowledge require different research strategies? A numerical model exploring different virtual knowledge landscapes, revealed two diverging optimal search strategies. Trend following is maximized when the popularity of new discoveries determine the number of individuals researching it. This strategy works best when many researchers explore few large areas of knowledge. In contrast, individuals or small groups of researchers are better in discovering small bits of information in dispersed knowledge landscapes. Bibliometric data of scientific publications showed a continuous bipolar distribution of these strategies, ranging from natural sciences, with highly cited publications in journals containing a large number of articles, to the social sciences, with rarely cited publications in many journals containing a small number of articles. The natural sciences seem to adapt their research strategies to landscapes with large concentrated knowledge clusters, whereas social sciences seem to have adapted to search in landscapes with many small isolated knowledge clusters. Similar bipolar distributions were obtained when comparing levels of insularity estimated by indicators of international collaboration and levels of country-self citations: researchers in academic areas with many journals such as social sciences, arts and humanities, were the most isolated, and that was true in different regions of the world. The work shows that quantitative measures estimating differences between academic disciplines improve our understanding of different research strategies, eventually helping interdisciplinary research and may be also help improve science policies worldwide.\n"], "author_display": ["Klaus Jaffe"], "article_type": "Research Article", "score": 0.403854, "title_display": "Social and Natural Sciences Differ in Their Research Strategies, Adapted to Work for Different Knowledge Landscapes", "publication_date": "2014-11-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0113901"}, {"journal": "PLoS Computational Biology", "abstract": ["\n        Experimental NMR relaxation studies have shown that peptide binding induces dynamical changes at the side-chain level throughout the second PDZ domain of PTP1e, identifying as such the collection of residues involved in long-range communication. Even though different computational approaches have identified subsets of residues that were qualitatively comparable, no quantitative analysis of the accuracy of these predictions was thus far determined. Here, we show that our information theoretical method produces quantitatively better results with respect to the experimental data than some of these earlier methods. Moreover, it provides a global network perspective on the effect experienced by the different residues involved in the process. We also show that these predictions are consistent within both the human and mouse variants of this domain. Together, these results improve the understanding of intra-protein communication and allostery in PDZ domains, underlining at the same time the necessity of producing similar data sets for further validation of thses kinds of methods.\n      Author Summary: Intra-protein communication has recently attracted an increasing interest from the scientific community, because of its important functional consequences: allostery and signalling. Unravelling how information is processed and transferred within a protein structure requires the study of the dynamical effects of, for instance, binding events, which may be captured experimentally by NMR relaxation experiments. Given the complexity of this experimental analysis, computational approaches, often based on molecular dynamics simulations, have been proposed for predicting these dynamical effects, using protein structural information as input. We examine here the accuracy of these predictors in the context of a well-studied domain, i.e. the second PSD95/Disc-large/ZO-1 domain (or PDZ domain) of PTP1e, and compare it to our approach that combines Monte-Carlo sampling of the conformational space of the side-chains and an information theoretical analysis. The results we discuss in this manuscript show clearly that the latter method provides very accurate predictions when compared to the experimental results, and has a better predictive quality compared to other computational approaches. The predictions, which are consistent between closely related structures, and the global network perspective provided by this approach, improve our understanding of intra-protein communication and allostery in these domains. "], "author_display": ["Elisa Cilia", "Geerten W. Vuister", "Tom Lenaerts"], "article_type": "Research Article", "score": 0.40354598, "title_display": "Accurate Prediction of the Dynamical Changes within the Second PDZ Domain of PTP1e", "publication_date": "2012-11-29T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1002794"}, {"journal": "PLoS ONE", "abstract": ["\nDNA strand displacement technology performs well in sensing and programming DNA segments. In this work, we construct DNA molecular systems based on DNA strand displacement performing computation of logic gates. Specifically, a class of so-called \u201cDNA neurons\u201d are achieved, in which a \u201csmart\u201d way inspired by biological neurons encoding information is developed to encode and deliver information using DNA molecules. The \u201cDNA neuron\u201d is bistable, that is, it can sense DNA molecules as input signals, and release \u201cnegative\u201d or \u201cpositive\u201d signals DNA molecules. We design intelligent DNA molecular systems that are constructed by cascading some particularly organized \u201cDNA neurons\u201d, which could perform logic computation, including AND, OR, XOR logic gates, automatically. Both simulation results using visual DSD (DNA strand displacement) software and experimental results are obtained, which shows that the proposed systems can detect DNA signals with high sensitivity and accretion; moreover, the systems can process input signals automatically with complex nonlinear logic. The method proposed in this work may provide a new way to construct a sensitive molecular signal detection system with neurons spiking behavior in vitro, and can be used to develop intelligent molecular processing systems in vivo.\n"], "author_display": ["Xiaolong Shi", "Zhiyu Wang", "Chenyan Deng", "Tao Song", "Linqiang Pan", "Zhihua Chen"], "article_type": "Research Article", "score": 0.40341622, "title_display": "A Novel Bio-Sensor Based on DNA Strand Displacement", "publication_date": "2014-10-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0108856"}, {"journal": "PLoS ONE", "abstract": ["\n        In chemistry and computational biology, structural graph descriptors have been proven essential for characterizing the structure of chemical and biological networks. It has also been demonstrated that they are useful to derive empirical models for structure-oriented drug design. However, from a more general (complex network-oriented) point of view, investigating mathematical properties of structural descriptors, such as their uniqueness and structural interpretation, is also important for an in-depth understanding of the underlying methods. In this paper, we emphasize the evaluation of the uniqueness of distance, degree and eigenvalue-based measures. Among these are measures that have been recently investigated extensively. We report numerical results using chemical and exhaustively generated graphs and also investigate correlations between the measures.\n      "], "author_display": ["Matthias Dehmer", "Martin Grabner", "Boris Furtula"], "article_type": "Research Article", "score": 0.40325725, "title_display": "Structural Discrimination of Networks by Using Distance, Degree and Eigenvalue-Based Measures", "publication_date": "2012-07-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0038564"}, {"journal": "PLoS ONE", "abstract": ["\nIn this paper, the performance of an individual aiming at guiding a self-organized group is numerically investigated. A collective behavioural model is adopted, accounting for the mutual repulsion, attraction and orientation experienced by the individuals. Moreover, these represent a set of solid particles which are supposed to be immersed in a fictitious viscous fluid. In particular, the lattice Boltzmann and Immersed boundary methods are used to predict the fluid dynamics, whereas the effect of the hydrodynamic forces on particles is accounted for by solving the equation of the solid motion through the time discontinuous Galerkin scheme. Numerical simulations are carried out by involving the individuals in a dichotomous process. On the one hand, an aspirant leader (AL) additional individual is added to the system. AL is forced to move along a prescribed direction which intersects the group. On the other hand, these tend to depart from an obstacle represented by a rotating lamina which is placed in the fluid domain. A numerical campaign is carried out by varying the fluid viscosity and, as a consequence, the hydrodynamic field. Moreover, scenarios characterized by different values of the size of the group are investigated. In order to estimate the AL's performance, a proper parameter is introduced, depending on the number of individuals following AL. Present findings show that the sole collective behavioural equations are insufficient to predict the AL's performance, since the motion is drastically affected by the presence of the surrounding fluid. With respect to the existing literature, the proposed numerical model is enriched by accounting for the presence of the encompassing fluid, thus computing the hydrodynamic forces arising when the individuals move.\n"], "author_display": ["Alessandro De Rosis"], "article_type": "Research Article", "score": 0.40308213, "title_display": "Fluid Forces Enhance the Performance of an Aspirant Leader in Self-Organized Living Groups", "publication_date": "2014-12-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0114687"}, {"journal": "PLOS ONE", "abstract": ["\nHumans can easily understand other people\u2019s actions through visual systems, while computers cannot. Therefore, a new bio-inspired computational model is proposed in this paper aiming for automatic action recognition. The model focuses on dynamic properties of neurons and neural networks in the primary visual cortex (V1), and simulates the procedure of information processing in V1, which consists of visual perception, visual attention and representation of human action. In our model, a family of the three-dimensional spatial-temporal correlative Gabor filters is used to model the dynamic properties of the classical receptive field of V1 simple cell tuned to different speeds and orientations in time for detection of spatiotemporal information from video sequences. Based on the inhibitory effect of stimuli outside the classical receptive field caused by lateral connections of spiking neuron networks in V1, we propose surround suppressive operator to further process spatiotemporal information. Visual attention model based on perceptual grouping is integrated into our model to filter and group different regions. Moreover, in order to represent the human action, we consider the characteristic of the neural code: mean motion map based on analysis of spike trains generated by spiking neurons. The experimental evaluation on some publicly available action datasets and comparison with the state-of-the-art approaches demonstrate the superior performance of the proposed model.\n"], "author_display": ["Na Shu", "Zhiyong Gao", "Xiangan Chen", "Haihua Liu"], "article_type": "Research Article", "score": 0.4029831, "title_display": "Computational Model of Primary Visual Cortex Combining Visual Attention for Action Recognition", "publication_date": "2015-07-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0130569"}, {"journal": "PLoS ONE", "abstract": ["\nThe substantial use of social network sites by teenagers has raised concerns about privacy and security. Previous research about behavior on social network sites was mostly based on surveys and interviews. Observational research overcomes problems inherent to this research method, for example social desirability. However, existing observational research mostly focuses on public profiles of young adults. Therefore, the current observation-study includes 1050 public and non-public Facebook-profiles of teenagers (13\u201318) to investigate (1) what kind of information teenagers post on their profile, (2) to what extent they protect this information using privacy-settings and (3) how much risky information they have on their profile. It was found that young people mostly post pictures, interests and some basic personal information on their profile. Some of them manage their privacy-settings as such that this information is reserved for friends' eyes only, but a lot of information is accessible on the friends-of-friends' pages. Although general risk scores are rather low, more detailed analyses show that teenagers nevertheless post a significant amount of risky information. Moreover, older teenagers and girls post more (risky) information while there are no differences in applying privacy settings. We found no differences in the Facebook behavior of teenagers enrolled in different education forms. Implications of these results are discussed.\n"], "author_display": ["Ellen Vanderhoven", "Tammy Schellens", "Martin Valcke", "Annelies Raes"], "article_type": "Research Article", "score": 0.40269655, "title_display": "How Safe Do Teenagers Behave on Facebook? An Observational Study", "publication_date": "2014-08-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0104036"}, {"journal": "PLoS ONE", "abstract": ["Background: Difficulties associated with implementing gene therapy are caused by the complexity of the underlying regulatory networks. The forms of interactions between the hundreds of genes, proteins, and metabolites in these networks are not known very accurately. An alternative approach is to limit consideration to genes on the network. Steady state measurements of these influence networks can be obtained from DNA microarray experiments. However, since they contain a large number of nodes, the computation of influence networks requires a prohibitively large set of microarray experiments. Furthermore, error estimates of the network make verifiable predictions impossible. Methodology/Principal Findings: Here, we propose an alternative approach. Rather than attempting to derive an accurate model of the network, we ask what questions can be addressed using lower dimensional, highly simplified models. More importantly, is it possible to use such robust features in applications? We first identify a small group of genes that can be used to affect changes in other nodes of the network. The reduced effective empirical subnetwork (EES) can be computed using steady state measurements on a small number of genetically perturbed systems. We show that the EES can be used to make predictions on expression profiles of other mutants, and to compute how to implement pre-specified changes in the steady state of the underlying biological process. These assertions are verified in a synthetic influence network. We also use previously published experimental data to compute the EES associated with an oxygen deprivation network of E.coli, and use it to predict gene expression levels on a double mutant. The predictions are significantly different from the experimental results for less than  of genes. Conclusions/Significance: The constraints imposed by gene expression levels of mutants can be used to address a selected set of questions about a gene network. "], "author_display": ["Gemunu H. Gunaratne", "Preethi H. Gunaratne", "Lars Seemann", "Andrei T\u00f6r\u00f6k"], "article_type": "Research Article", "score": 0.40261602, "title_display": "Using Effective Subnetworks to Predict Selected Properties of Gene Networks", "publication_date": "2010-10-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0013080"}, {"journal": "PLoS ONE", "abstract": ["\n        The modern metaphor of the brain is that of a dynamic information processing device. In the current study we investigate how a core cognitive network of the human brain, the perceptual decision system, can be characterized regarding its spatiotemporal representation of task-relevant information. We capitalize on a recently developed information theoretic framework for the analysis of simultaneously acquired electroencephalography (EEG) and functional magnetic resonance imaging data (fMRI) (Ostwald et al. (2010), NeuroImage 49: 498\u2013516). We show how this framework naturally extends from previous validations in the sensory to the cognitive domain and how it enables the economic description of neural spatiotemporal information encoding. Specifically, based on simultaneous EEG-fMRI data features from n\u200a=\u200a13 observers performing a visual perceptual decision task, we demonstrate how the information theoretic framework is able to reproduce earlier findings on the neurobiological underpinnings of perceptual decisions from the response signal features' marginal distributions. Furthermore, using the joint EEG-fMRI feature distribution, we provide novel evidence for a highly distributed and dynamic encoding of task-relevant information in the human brain.\n      "], "author_display": ["Dirk Ostwald", "Camillo Porcaro", "Stephen D. Mayhew", "Andrew P. Bagshaw"], "article_type": "Research Article", "score": 0.4019915, "title_display": "EEG-fMRI Based Information Theoretic Characterization of the Human Perceptual Decision System", "publication_date": "2012-04-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0033896"}, {"journal": "PLoS ONE", "abstract": ["\nIn this paper, based on the coupled social networks (CSN), we propose a hybrid algorithm to nonlinearly integrate both social and behavior information of online users. Filtering algorithm, based on the coupled social networks, considers the effects of both social similarity and personalized preference. Experimental results based on two real datasets, Epinions and Friendfeed, show that the hybrid pattern can not only provide more accurate recommendations, but also enlarge the recommendation coverage while adopting global metric. Further empirical analyses demonstrate that the mutual reinforcement and rich-club phenomenon can also be found in coupled social networks where the identical individuals occupy the core position of the online system. This work may shed some light on the in-depth understanding of the structure and function of coupled social networks.\n"], "author_display": ["Da-Cheng Nie", "Zi-Ke Zhang", "Jun-Lin Zhou", "Yan Fu", "Kui Zhang"], "article_type": "Research Article", "score": 0.40180522, "title_display": "Information Filtering on Coupled Social Networks", "publication_date": "2014-07-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0101675"}, {"journal": "PLoS ONE", "abstract": ["\n        Quantum phase estimation is one of the key algorithms in the field of quantum\n                    computing, but up until now, only approximate expressions have been derived for\n                    the probability of error. We revisit these derivations, and find that by\n                    ensuring symmetry in the error definitions, an exact formula can be found. This\n                    new approach may also have value in solving other related problems in quantum\n                    computing, where an expected error is calculated. Expressions for two special\n                    cases of the formula are also developed, in the limit as the number of qubits in\n                    the quantum computer approaches infinity and in the limit as the extra added\n                    qubits to improve reliability goes to infinity. It is found that this formula is\n                    useful in validating computer simulations of the phase estimation procedure and\n                    in avoiding the overestimation of the number of qubits required in order to\n                    achieve a given reliability. This formula thus brings improved precision in the\n                    design of quantum computers.\n      "], "author_display": ["James M. Chappell", "Max A. Lohe", "Lorenz von Smekal", "Azhar Iqbal", "Derek Abbott"], "article_type": "Research Article", "score": 0.40180066, "title_display": "A Precise Error Bound for Quantum Phase Estimation", "publication_date": "2011-05-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0019663"}, {"journal": "PLOS ONE", "abstract": ["\nThe increasing use of mathematical techniques in scientific research leads to the interdisciplinarity of applied mathematics. This viewpoint is validated quantitatively here by statistical and network analysis on the corpus PNAS 1999\u20132013. A network describing the interdisciplinary relationships between disciplines in a panoramic view is built based on the corpus. Specific network indicators show the hub role of applied mathematics in interdisciplinary research. The statistical analysis on the corpus content finds that algorithms, a primary topic of applied mathematics, positively correlates, increasingly co-occurs, and has an equilibrium relationship in the long-run with certain typical research paradigms and methodologies. The finding can be understood as an intrinsic cause of the interdisciplinarity of applied mathematics.\n"], "author_display": ["Zheng Xie", "Xiaojun Duan", "Zhenzheng Ouyang", "Pengyuan Zhang"], "article_type": "Research Article", "score": 0.4017502, "title_display": "Quantitative Analysis of the Interdisciplinarity of Applied Mathematics", "publication_date": "2015-09-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0137424"}, {"journal": "PLoS ONE", "abstract": ["\n        Prior research has shown that representations of retinal surfaces can be learned from the intrinsic structure of visual sensory data in neural simulations, in robots, as well as by animals. Furthermore, representations of cochlear (frequency) surfaces can be learned from auditory data in neural simulations. Advances in hardware technology have allowed the development of artificial skin for robots, realising a new sensory modality which differs in important respects from vision and audition in its sensorimotor characteristics. This provides an opportunity to further investigate ordered sensory map formation using computational tools. We show that it is possible to learn representations of non-trivial tactile surfaces, which require topologically and geometrically involved three-dimensional embeddings. Our method automatically constructs a somatotopic map corresponding to the configuration of tactile sensors on a rigid body, using only intrinsic properties of the tactile data. The additional complexities involved in processing the tactile modality require the development of a novel multi-dimensional scaling algorithm. This algorithm, ANISOMAP, extends previous methods and outperforms them, producing high-quality reconstructions of tactile surfaces in both simulation and hardware tests. In addition, the reconstruction turns out to be robust to unanticipated hardware failure.\n      "], "author_display": ["Simon McGregor", "Daniel Polani", "Kerstin Dautenhahn"], "article_type": "Research Article", "score": 0.40167496, "title_display": "Generation of Tactile Maps for Artificial Skin", "publication_date": "2011-11-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0026561"}, {"journal": "PLoS ONE", "abstract": ["Background: Consent forms have lengthened over time and become harder for participants to understand. We sought to demonstrate the feasibility of creating a simplified consent form for biobanking that comprises the minimum information necessary to meet ethical and regulatory requirements. We then gathered preliminary data concerning its content from hypothetical biobank participants. Methodology/Principal Findings: We followed basic principles of plain-language writing and incorporated into a 2-page form (not including the signature page) those elements of information required by federal regulations and recommended by best practice guidelines for biobanking. We then recruited diabetes patients from community-based practices and randomized half (n\u200a=\u200a56) to read the 2-page form, first on paper and then a second time on a tablet computer. Participants were encouraged to use \u201cMore information\u201d buttons on the electronic version whenever they had questions or desired further information. These buttons led to a series of \u201cFrequently Asked Questions\u201d (FAQs) that contained additional detailed information. Participants were asked to identify specific sentences in the FAQs they thought would be important if they were considering taking part in a biorepository. On average, participants identified 7 FAQ sentences as important (mean 6.6, SD 14.7, range: 0\u201371). No one sentence was highlighted by a majority of participants; further, 34 (60.7%) participants did not highlight any FAQ sentences. Conclusions: Our preliminary findings suggest that our 2-page form contains the information that most prospective participants identify as important. Combining simplified forms with supplemental material for those participants who desire more information could help minimize consent form length and complexity, allowing the most substantively material information to be better highlighted and enabling potential participants to read the form and ask questions more effectively. "], "author_display": ["Laura M. Beskow", "Jo\u00eblle Y. Friedman", "N. Chantelle Hardy", "Li Lin", "Kevin P. Weinfurt"], "article_type": "Research Article", "score": 0.4014626, "title_display": "Developing a Simplified Consent Form for Biobanking", "publication_date": "2010-10-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0013302"}, {"journal": "PLoS ONE", "abstract": ["\n        We present a web-based network-construction system, CINPER (CSBL INteractive Pathway BuildER), to assist a user to build a user-specified gene network for a prokaryotic organism in an intuitive manner. CINPER builds a network model based on different types of information provided by the user and stored in the system. CINPER\u2019s prediction process has four steps: (i) collection of template networks based on (partially) known pathways of related organism(s) from the SEED or BioCyc database and the published literature; (ii) construction of an initial network model based on the template networks using the P-Map program; (iii) expansion of the initial model, based on the association information derived from operons, protein-protein interactions, co-expression modules and phylogenetic profiles; and (iv) computational validation of the predicted models based on gene expression data. To facilitate easy applications, CINPER provides an interactive visualization environment for a user to enter, search and edit relevant data and for the system to display (partial) results and prompt for additional data. Evaluation of CINPER on 17 well-studied pathways in the MetaCyc database shows that the program achieves an average recall rate of 76% and an average precision rate of 90% on the initial models; and a higher average recall rate at 87% and an average precision rate at 28% on the final models. The reduced precision rate in the final models versus the initial models reflects the reality that the final models have large numbers of novel genes that have no experimental evidences and hence are not yet collected in the MetaCyc database. To demonstrate the usefulness of this server, we have predicted an iron homeostasis gene network of Synechocystis sp. PCC6803 using the server. The predicted models along with the server can be accessed at http://csbl.bmb.uga.edu/cinper/.\n      "], "author_display": ["Xizeng Mao", "Xin Chen", "Yu Zhang", "Spencer Pangle", "Ying Xu"], "article_type": "Research Article", "score": 0.4012279, "title_display": "CINPER: An Interactive Web System for Pathway Prediction for Prokaryotes", "publication_date": "2012-12-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0051252"}, {"journal": "PLoS ONE", "abstract": ["\n         While the evolution of cooperation has been widely studied, little attention has been devoted to adversarial settings wherein one actor can directly harm another. Recent theoretical work addresses this issue, introducing an adversarial game in which the emergence of cooperation is heavily reliant on the presence of \u201cInformants,\u201d actors who defect at first-order by harming others, but who cooperate at second-order by punishing other defectors. We experimentally study this adversarial environment in the laboratory with human subjects to test whether Informants are indeed critical for the emergence of cooperation. We find in these experiments that, even more so than predicted by theory, Informants are crucial for the emergence and sustenance of a high cooperation state. A key lesson is that successfully reaching and maintaining a low defection society may require the cultivation of criminals who will also aid in the punishment of others.\n      "], "author_display": ["Maria R. D'Orsogna", "Ryan Kendall", "Michael McBride", "Martin B. Short"], "article_type": "Research Article", "score": 0.40072757, "title_display": "Criminal Defectors Lead to the Emergence of Cooperation in an Experimental, Adversarial Game", "publication_date": "2013-04-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0061458"}, {"journal": "PLoS ONE", "abstract": ["\nDNA sequences are translated into protein coding sequences and then further assigned to protein families in metagenomic analyses, because of the need for sensitivity. However, huge amounts of sequence data create the problem that even general homology search analyses using BLASTX become difficult in terms of computational cost. We designed a new homology search algorithm that finds seed sequences based on the suffix arrays of a query and a database, and have implemented it as GHOSTX. GHOSTX achieved approximately 131\u2013165 times acceleration over a BLASTX search at similar levels of sensitivity. GHOSTX is distributed under the BSD 2-clause license and is available for download at http://www.bi.cs.titech.ac.jp/ghostx/. Currently, sequencing technology continues to improve, and sequencers are increasingly producing larger and larger quantities of data. This explosion of sequence data makes computational analysis with contemporary tools more difficult. We offer this tool as a potential solution to this problem.\n"], "author_display": ["Shuji Suzuki", "Masanori Kakuta", "Takashi Ishida", "Yutaka Akiyama"], "article_type": "Research Article", "score": 0.4004105, "title_display": "GHOSTX: An Improved Sequence Homology Search Algorithm Using a Query Suffix Array and a Database Suffix Array", "publication_date": "2014-08-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0103833"}, {"journal": "PLoS ONE", "abstract": ["\n        The Gillespie \u03c4-Leaping Method is an approximate algorithm that is faster than the exact Direct Method (DM) due to the progression of the simulation with larger time steps. However, the procedure to compute the time leap \u03c4 is quite expensive. In this paper, we explore the acceleration of the \u03c4-Leaping Method using Graphics Processing Unit (GPUs) for ultra-large networks ( reaction channels). We have developed data structures and algorithms that take advantage of the unique hardware architecture and available libraries. Our results show that we obtain a performance gain of over 60x when compared with the best conventional implementations.\n      "], "author_display": ["Ivan Komarov", "Roshan M. D\u2019Souza", "Jose-Juan Tapia"], "article_type": "Research Article", "score": 0.3989595, "title_display": "Accelerating the Gillespie <i>\u03c4</i>-Leaping Method Using Graphics Processing Units", "publication_date": "2012-06-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0037370"}, {"journal": "PLoS ONE", "abstract": ["Background: The variety of ways in which faces are categorized makes face recognition challenging for both synthetic and biological vision systems. Here we focus on two face processing tasks, detection and individuation, and explore whether differences in task demands lead to differences both in the features most effective for automatic recognition and in the featural codes recruited by neural processing. Methodology/Principal Findings: Our study appeals to a computational framework characterizing the features representing object categories as sets of overlapping image fragments. Within this framework, we assess the extent to which task-relevant information differs across image fragments. Based on objective differences we find among task-specific representations, we test the sensitivity of the human visual system to these different face descriptions independently of one another. Both behavior and functional magnetic resonance imaging reveal effects elicited by objective task-specific levels of information. Behaviorally, recognition performance with image fragments improves with increasing task-specific information carried by different face fragments. Neurally, this sensitivity to the two tasks manifests as differential localization of neural responses across the ventral visual pathway. Fragments diagnostic for detection evoke larger neural responses than non-diagnostic ones in the right posterior fusiform gyrus and bilaterally in the inferior occipital gyrus. In contrast, fragments diagnostic for individuation evoke larger responses than non-diagnostic ones in the anterior inferior temporal gyrus. Finally, for individuation only, pattern analysis reveals sensitivity to task-specific information within the right \u201cfusiform face area\u201d. Conclusions/Significance: Our results demonstrate: 1) information diagnostic for face detection and individuation is roughly separable; 2) the human visual system is independently sensitive to both types of information; 3) neural responses differ according to the type of task-relevant information considered. More generally, these findings provide evidence for the computational utility and the neural validity of fragment-based visual representation and recognition. "], "author_display": ["Adrian Nestor", "Jean M. Vettel", "Michael J. Tarr"], "article_type": "Research Article", "score": 0.39885384, "title_display": "Task-Specific Codes for Face Recognition: How they Shape the Neural Representation of Features for Detection and Individuation", "publication_date": "2008-12-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0003978"}, {"journal": "PLoS ONE", "abstract": ["\n        In complex networks such as gene networks, traffic systems or brain circuits it is important to understand how long it takes for the different parts of the network to effectively influence one another. In the brain, for example, axonal delays between brain areas can amount to several tens of milliseconds, adding an intrinsic component to any timing-based processing of information. Inferring neural interaction delays is thus needed to interpret the information transfer revealed by any analysis of directed interactions across brain structures. However, a robust estimation of interaction delays from neural activity faces several challenges if modeling assumptions on interaction mechanisms are wrong or cannot be made. Here, we propose a robust estimator for neuronal interaction delays rooted in an information-theoretic framework, which allows a model-free exploration of interactions. In particular, we extend transfer entropy to account for delayed source-target interactions, while crucially retaining the conditioning on the embedded target state at the immediately previous time step. We prove that this particular extension is indeed guaranteed to identify interaction delays between two coupled systems and is the only relevant option in keeping with Wiener\u2019s principle of causality. We demonstrate the performance of our approach in detecting interaction delays on finite data by numerical simulations of stochastic and deterministic processes, as well as on local field potential recordings. We also show the ability of the extended transfer entropy to detect the presence of multiple delays, as well as feedback loops. While evaluated on neuroscience data, we expect the estimator to be useful in other fields dealing with network dynamics.\n      "], "author_display": ["Michael Wibral", "Nicolae Pampu", "Viola Priesemann", "Felix Siebenh\u00fchner", "Hannes Seiwert", "Michael Lindner", "Joseph T. Lizier", "Raul Vicente"], "article_type": "Research Article", "score": 0.39860073, "title_display": "Measuring Information-Transfer Delays", "publication_date": "2013-02-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0055809"}, {"journal": "PLoS ONE", "abstract": ["\nThe automatic detection of bilateral symmetry is a challenging task in computer vision and pattern recognition. This paper presents an approach for the detection of bilateral symmetry in digital single object images. Our method relies on the extraction of Scale Invariant Feature Transform (SIFT) based feature points, which serves as the basis for the ascertainment of the centroid of the object; the latter being the origin under the Cartesian coordinate system to be converted to the polar coordinate system in order to facilitate the selection symmetric coordinate pairs. This is followed by comparing the gradient magnitude and orientation of the corresponding points to evaluate the amount of symmetry exhibited by each pair of points. The experimental results show that our approach draw the symmetry line accurately, provided that the observed centroid point is true.\n"], "author_display": ["Habib Akbar", "Khizar Hayat", "Nuhman ul Haq", "Usama Ijaz Bajwa"], "article_type": "Research Article", "score": 0.3985651, "title_display": "Bilateral Symmetry Detection on the Basis of Scale Invariant Feature Transform", "publication_date": "2014-08-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0103561"}, {"journal": "PLoS Computational Biology", "abstract": ["\n        Conventional methods used to characterize multidimensional neural feature selectivity, such as spike-triggered covariance (STC) or maximally informative dimensions (MID), are limited to Gaussian stimuli or are only able to identify a small number of features due to the curse of dimensionality. To overcome these issues, we propose two new dimensionality reduction methods that use minimum and maximum information models. These methods are information theoretic extensions of STC that can be used with non-Gaussian stimulus distributions to find relevant linear subspaces of arbitrary dimensionality. We compare these new methods to the conventional methods in two ways: with biologically-inspired simulated neurons responding to natural images and with recordings from macaque retinal and thalamic cells responding to naturalistic time-varying stimuli. With non-Gaussian stimuli, the minimum and maximum information methods significantly outperform STC in all cases, whereas MID performs best in the regime of low dimensional feature spaces.\n      Author Summary: Neurons are capable of simultaneously encoding information about multiple features of sensory stimuli in their spikes. The dimensionality reduction methods that currently exist to extract those relevant features are either biased for non-Gaussian stimuli or fall victim to the curse of dimensionality. In this paper we introduce two information theoretic extensions of the spike-triggered covariance method. These new methods use the concepts of minimum and maximum mutual information to identify the stimulus features encoded in the spikes of a neuron. Using simulated and experimental neural data, these methods are shown to perform well both in situations where conventional approaches are appropriate and where they fail. These new techniques should improve the characterization of neural feature selectivity in areas of the brain where the application of currently available approaches is restricted. "], "author_display": ["Jeffrey D. Fitzgerald", "Ryan J. Rowekamp", "Lawrence C. Sincich", "Tatyana O. Sharpee"], "article_type": "Research Article", "score": 0.39852345, "title_display": "Second Order Dimensionality Reduction Using Minimum and Maximum Mutual Information Models", "publication_date": "2011-10-27T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1002249"}, {"journal": "PLoS ONE", "abstract": ["\n        Human memory is limited in the number of items held in one's mind\u2014a limit known as \u201cMiller's magic number\u201d. We study the emergence of such limits as a result of the statistics of large bitvectors used to represent items in memory, given two postulates: i) the Sparse Distributed Memory; and ii) chunking through averaging. Potential implications for theoretical neuroscience are discussed.\n      "], "author_display": ["Alexandre Linhares", "Daniel M. Chada", "Christian N. Aranha"], "article_type": "Research Article", "score": 0.3981094, "title_display": "The Emergence of Miller's Magic Number on a Sparse Distributed Memory", "publication_date": "2011-01-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0015592"}, {"journal": "PLOS ONE", "abstract": ["\nSelf-entropy (SE) and transfer entropy (TE) are widely utilized in biomedical signal processing to assess the information stored into a system and transferred from a source to a destination respectively. The study proposes a more specific definition of the SE, namely the conditional SE (CSE), and a more flexible definition of the TE based on joint TE (JTE), namely the conditional JTE (CJTE), for the analysis of information dynamics in multivariate time series. In a protocol evoking a gradual sympathetic activation and vagal withdrawal proportional to the magnitude of the orthostatic stimulus, such as the graded head-up tilt, we extracted the beat-to-beat spontaneous variability of heart period (HP), systolic arterial pressure (SAP) and respiratory activity (R) in 19 healthy subjects and we computed SE of HP, CSE of HP given SAP and R, JTE from SAP and R to HP, CJTE from SAP and R to HP given SAP and CJTE from SAP and R to HP given R. CSE of HP given SAP and R was significantly smaller than SE of HP and increased progressively with the amplitude of the stimulus, thus suggesting that dynamics internal to HP and unrelated to SAP and R, possibly linked to sympathetic activation evoked by head-up tilt, might play a role during the orthostatic challenge. While JTE from SAP and R to HP was independent of tilt table angle, CJTE from SAP and R to HP given R and from SAP and R to HP given SAP showed opposite trends with tilt table inclination, thus suggesting that the importance of the cardiac baroreflex increases and the relevance of the cardiopulmonary pathway decreases during head-up tilt. The study demonstrates the high specificity of CSE and the high flexibility of CJTE over real data and proves that they are particularly helpful in disentangling physiological mechanisms and in assessing their different contributions to the overall cardiovascular regulation.\n"], "author_display": ["Alberto Porta", "Luca Faes", "Giandomenico Nollo", "Vlasta Bari", "Andrea Marchi", "Beatrice De Maria", "Anielle C. M. Takahashi", "Aparecida M. Catai"], "article_type": "Research Article", "score": 0.3980986, "title_display": "Conditional Self-Entropy and Conditional Joint Transfer Entropy in Heart Period Variability during Graded Postural Challenge", "publication_date": "2015-07-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0132851"}, {"journal": "PLoS ONE", "abstract": ["\nAlong with physical luminance, the perceived brightness is known to depend on the spatial structure of the stimulus. Often it is assumed that neural computation of the brightness is based on the analysis of luminance borders of the stimulus. However, this has not been tested directly. We introduce a new variant of the psychophysical reverse-correlation or classification image method to estimate and localize the physical features of the stimuli which correlate with the perceived brightness, using a brightness-matching task. We derive classification images for the illusory Craik-O'Brien-Cornsweet stimulus and a \u201creal\u201d uniform step stimulus. For both stimuli, classification images reveal a positive peak at the stimulus border, along with a negative peak at the background, but are flat at the center of the stimulus, suggesting that brightness is determined solely by the border information. Features in the perceptually completed area in the Craik-O'Brien-Cornsweet do not contribute to its brightness, nor could we see low-frequency boosting, which has been offered as an explanation for the illusion. Tuning of the classification image profiles changes remarkably little with stimulus size. This supports the idea that only certain spatial scales are used for computing the brightness of a surface.\n"], "author_display": ["Ilmari Kurki", "Tarja Peromaa", "Aapo Hyv\u00e4rinen", "Jussi Saarinen"], "article_type": "Research Article", "score": 0.3977969, "title_display": "Visual Features Underlying Perceived Brightness as Revealed by Classification Images", "publication_date": "2009-10-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0007432"}, {"journal": "PLOS ONE", "abstract": ["\nThis paper addresses the impact of the structure of the viral propagation network on the viral prevalence. For that purpose, a new epidemic model of computer virus, known as the node-based SLBS model, is proposed. Our analysis shows that the maximum eigenvalue of the underlying network is a key factor determining the viral prevalence. Specifically, the value range of the maximum eigenvalue is partitioned into three subintervals: viruses tend to extinction very quickly or approach extinction or persist depending on into which subinterval the maximum eigenvalue of the propagation network falls. Consequently, computer virus can be contained by adjusting the propagation network so that its maximum eigenvalue falls into the desired subinterval.\n"], "author_display": ["Lu-Xing Yang", "Moez Draief", "Xiaofan Yang"], "article_type": "Research Article", "score": 0.39758658, "title_display": "The Impact of the Network Topology on the Viral Prevalence: A Node-Based Approach", "publication_date": "2015-07-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0134507"}, {"abstract": ["\n        Combining information from various image features has become a standard technique in concept recognition tasks. However, the optimal way of fusing the resulting kernel functions is usually unknown in practical applications. Multiple kernel learning (MKL) techniques allow to determine an optimal linear combination of such similarity matrices. Classical approaches to MKL promote sparse mixtures. Unfortunately, 1-norm regularized MKL variants are often observed to be outperformed by an unweighted sum kernel. The main contributions of this paper are the following: we apply a recently developed non-sparse MKL variant to state-of-the-art concept recognition tasks from the application domain of computer vision. We provide insights on benefits and limits of non-sparse MKL and compare it against its direct competitors, the sum-kernel SVM and sparse MKL. We report empirical results for the PASCAL VOC 2009 Classification and ImageCLEF2010 Photo Annotation challenge data sets. Data sets (kernel matrices) as well as further information are available at http://doc.ml.tu-berlin.de/image_mkl/(Accessed 2012 Jun 25).\n      "], "author_display": ["Alexander Binder", "Shinichi Nakajima", "Marius Kloft", "Christina M\u00fcller", "Wojciech Samek", "Ulf Brefeld", "Klaus-Robert M\u00fcller", "Motoaki Kawanabe"], "article_type": "Research Article", "score": 0.39736962, "title_display": "Insights from Classifying Visual Concepts with Multiple Kernel Learning", "publication_date": "2012-08-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0038897"}, {"abstract": ["\n        The use of quantitative metrics to gauge the impact of scholarly publications, authors, and disciplines is predicated on the availability of reliable usage and annotation data. Citation and download counts are widely available from digital libraries. However, current annotation systems rely on proprietary labels, refer to journals but not articles or authors, and are manually curated. To address these limitations, we propose a social framework based on crowdsourced annotations of scholars, designed to keep up with the rapidly evolving disciplinary and interdisciplinary landscape. We describe a system called Scholarometer, which provides a service to scholars by computing citation-based impact measures. This creates an incentive for users to provide disciplinary annotations of authors, which in turn can be used to compute disciplinary metrics. We first present the system architecture and several heuristics to deal with noisy bibliographic and annotation data. We report on data sharing and interactive visualization services enabled by Scholarometer. Usage statistics, illustrating the data collected and shared through the framework, suggest that the proposed crowdsourcing approach can be successful. Secondly, we illustrate how the disciplinary bibliometric indicators elicited by Scholarometer allow us to implement for the first time a universal impact measure proposed in the literature. Our evaluation suggests that this metric provides an effective means for comparing scholarly impact across disciplinary boundaries.\n      "], "author_display": ["Jasleen Kaur", "Diep Thi Hoang", "Xiaoling Sun", "Lino Possamai", "Mohsen JafariAsbagh", "Snehal Patil", "Filippo Menczer"], "article_type": "Research Article", "score": 0.39707074, "title_display": "Scholarometer: A Social Framework for Analyzing Impact across Disciplines", "publication_date": "2012-09-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0043235"}, {"journal": "PLoS Computational Biology", "abstract": ["\n        In systems and computational biology, much effort is devoted to functional identification of systems and networks at the molecular-or cellular scale. However, similarly important networks exist at anatomical scales such as the tendon network of human fingers: the complex array of collagen fibers that transmits and distributes muscle forces to finger joints. This network is critical to the versatility of the human hand, and its function has been debated since at least the 16th century. Here, we experimentally infer the structure (both topology and parameter values) of this network through sparse interrogation with force inputs. A population of models representing this structure co-evolves in simulation with a population of informative future force inputs via the predator-prey estimation-exploration algorithm. Model fitness depends on their ability to explain experimental data, while the fitness of future force inputs depends on causing maximal functional discrepancy among current models. We validate our approach by inferring two known synthetic Latex networks, and one anatomical tendon network harvested from a cadaver's middle finger. We find that functionally similar but structurally diverse models can exist within a narrow range of the training set and cross-validation errors. For the Latex networks, models with low training set error [<4%] and resembling the known network have the smallest cross-validation errors [\u223c5%]. The low training set [<4%] and cross validation [<7.2%] errors for models for the cadaveric specimen demonstrate what, to our knowledge, is the first experimental inference of the functional structure of complex anatomical networks. This work expands current bioinformatics inference approaches by demonstrating that sparse, yet informative interrogation of biological specimens holds significant computational advantages in accurate and efficient inference over random testing, or assuming model topology and only inferring parameters values. These findings also hold clues to both our evolutionary history and the development of versatile machines.\n      Author Summary: In science and medicine alike, one of the critical steps to understand the working of organisms is to identify how a given individual is similar or different from others. Only then can the specific features of an individual be distinguished from the general properties of that species. However, doing enough input-output experiments on a given organism to obtain a reliable description of its function (i.e., a model) can often harm the organism, or require too much time when testing perishable tissues or human subjects. We have met this challenge by demonstrating that our novel algorithm can accelerate the extraction of accurate functional models in complex tissues by continually tailoring each successive experiment to be more informative. We apply this new method to the problem of describing how the tendons of the fingers interact, which has puzzled scientists and clinicians since the time of Da Vinci. This new computational-experimental method now enables fresh research directions in biological and medical research by allowing the experimental extraction of accurate functional models with minimal damage to the organism. For example, it will allow a better understanding of similarities and differences among related species, and the development of personalized medical treatment. "], "author_display": ["Anupam Saxena", "Hod Lipson", "Francisco J. Valero-Cuevas"], "article_type": "Research Article", "score": 0.3969001, "title_display": "Functional Inference of Complex Anatomical Tendinous Networks at a Macroscopic Scale via Sparse Experimentation", "publication_date": "2012-11-08T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1002751"}, {"journal": "PLOS ONE", "abstract": ["\nProxy Mobile IPv6 is a network-based localized mobility management protocol that supports mobility without mobile nodes\u2019 participation in mobility signaling. The details of user authentication procedure are not specified in this standard, hence, many authentication schemes have been proposed for this standard. In 2013, Chuang et al., proposed an authentication method for PMIPv6, called SPAM. However, Chuang et al.\u2019s Scheme protects the network against some security attacks, but it is still vulnerable to impersonation and password guessing attacks. In addition, we discuss other security drawbacks such as lack of revocation procedure in case of loss or stolen device, and anonymity issues of the Chuang et al.\u2019s scheme. We further propose an enhanced authentication method to mitigate the security issues of SPAM method and evaluate our scheme using BAN logic.\n"], "author_display": ["Mojtaba Alizadeh", "Mazdak Zamani", "Sabariah Baharun", "Azizah Abdul Manaf", "Kouichi Sakurai", "Hiroki Anada", "Hassan Keshavarz", "Shehzad Ashraf Chaudhry", "Muhammad Khurram Khan"], "article_type": "Research Article", "score": 0.39672306, "title_display": "Cryptanalysis and Improvement of \"A Secure Password Authentication Mechanism for Seamless Handover in Proxy Mobile IPv6 Networks\"", "publication_date": "2015-11-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0142716"}, {"journal": "PLOS ONE", "abstract": ["\nCan knowledge help viewers when they appreciate an artwork? Experts\u2019 judgments of the aesthetic value of a painting often differ from the estimates of na\u00efve viewers, and this phenomenon is especially pronounced in the aesthetic judgment of abstract paintings. We compared the changes in aesthetic judgments of na\u00efve viewers while they were progressively exposed to five pieces of background information. The participants were asked to report their aesthetic judgments of a given painting after each piece of information was presented. We found that commentaries by the artist and a critic significantly increased the subjective aesthetic ratings. Does knowledge enable experts to attend to the visual features in a painting and to link it to the evaluative conventions, thus potentially causing different aesthetic judgments? To investigate whether a specific pattern of attention is essential for the knowledge-based appreciation, we tracked the eye movements of subjects while viewing a painting with a commentary by the artist and with a commentary by a critic. We observed that critics\u2019 commentaries directed the viewers\u2019 attention to the visual components that were highly relevant to the presented commentary. However, attention to specific features of a painting was not necessary for increasing the subjective aesthetic judgment when the artists\u2019 commentary was presented. Our results suggest that at least two different cognitive mechanisms may be involved in knowledge- guided aesthetic judgments while viewers reappraise a painting.\n"], "author_display": ["Seongmin A. Park", "Kyongsik Yun", "Jaeseung Jeong"], "article_type": "Research Article", "score": 0.3967194, "title_display": "Reappraising Abstract Paintings after Exposure to Background Information", "publication_date": "2015-05-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0124159"}, {"journal": "PLoS Biology", "abstract": [": Analyzing brain activity in songbirds suggests that the nervous system controls behavior by precisely modulating the timing pattern of electrical events. \nStudies of motor control have almost universally examined firing rates to investigate how the brain shapes behavior. In principle, however, neurons could encode information through the precise temporal patterning of their spike trains as well as (or instead of) through their firing rates. Although the importance of spike timing has been demonstrated in sensory systems, it is largely unknown whether timing differences in motor areas could affect behavior. We tested the hypothesis that significant information about trial-by-trial variations in behavior is represented by spike timing in the songbird vocal motor system. We found that neurons in motor cortex convey information via spike timing far more often than via spike rate and that the amount of information conveyed at the millisecond timescale greatly exceeds the information available from spike counts. These results demonstrate that information can be represented by spike timing in motor circuits and suggest that timing variations evoke differences in behavior.\nAuthor Summary: A central question in neuroscience is how neurons use patterns of electrical events to represent sensory information and control behavior. Neurons might use two different codes to transmit information. First, signals might be conveyed by the total number of electrical events (called \u201caction potentials\u201d) that a neuron produces. Alternately, the timing pattern of action potentials, as distinct from the total number of action potentials produced, might be used to transmit information. Although many studies have shown that timing can convey information about sensory inputs, such as visual scenery or sound waveforms, the role of action potential timing in the control of complex, learned behaviors is largely unknown. Here, by analyzing the pattern of action potentials produced in a songbird's brain as it precisely controls vocal behavior, we demonstrate that far more information about upcoming behavior is present in spike timing than in the total number of spikes fired. This work suggests that timing can be equally (or more) important in motor systems as in sensory systems. "], "author_display": ["Claire Tang", "Diala Chehayeb", "Kyle Srivastava", "Ilya Nemenman", "Samuel J. Sober"], "article_type": "Research Article", "score": 0.39665052, "title_display": "Millisecond-Scale Motor Encoding in a Cortical Vocal Area", "publication_date": "2014-12-09T00:00:00Z", "eissn": "1545-7885", "id": "10.1371/journal.pbio.1002018"}, {"journal": "PLoS Computational Biology", "abstract": ["\n        Determining distances to objects is one of the most ubiquitous perceptual tasks in everyday life. Nevertheless, it is challenging because the information from a single image confounds object size and distance. Though our brains frequently judge distances accurately, the underlying computations employed by the brain are not well understood. Our work illuminates these computions by formulating a family of probabilistic models that encompass a variety of distinct hypotheses about distance and size perception. We compare these models' predictions to a set of human distance judgments in an interception experiment and use Bayesian analysis tools to quantitatively select the best hypothesis on the basis of its explanatory power and robustness over experimental data. The central question is: whether, and how, human distance perception incorporates size cues to improve accuracy. Our conclusions are: 1) humans incorporate haptic object size sensations for distance perception, 2) the incorporation of haptic sensations is suboptimal given their reliability, 3) humans use environmentally accurate size and distance priors, 4) distance judgments are produced by perceptual \u201cposterior sampling\u201d. In addition, we compared our model's estimated sensory and motor noise parameters with previously reported measurements in the perceptual literature and found good correspondence between them. Taken together, these results represent a major step forward in establishing the computational underpinnings of human distance perception and the role of size information.\n      Author Summary: Perceiving the distance to an object can be difficult because a monocular visual image is influenced by the object's distance and size, so the object's image size alone cannot uniquely determine the distance. However, because object distance is so important in everyday life, our brains have developed various strategies to overcome this difficulty and enable accurate perceptual distance estimates. A key strategy the brain employs is to use touched size sensations, as well as background information regarding the object's size, to rule out incorrect size/distance combinations; our work studies the brain's computations that underpin this strategy. We modified a sophisticated model that prescribes how humans should estimate object distance to encompass a broad set of hypotheses about how humans do estimate distance in actuality. We then used data from a distance perception experiment to select which modified model best accounts for human performance. Our analysis reveals how people use touch sensations and how they bias their distance judgments to conform with true object statistics in the enviroment. Our results provide a comprehensive account of human distance perception and the role of size information, which significantly improves cognitive scientists' understanding of this fundamental, important, and ubiquitous behavior. "], "author_display": ["Peter W. Battaglia", "Daniel Kersten", "Paul R. Schrater"], "article_type": "Research Article", "score": 0.3966072, "title_display": "How Haptic Size Sensations Improve Distance Perception", "publication_date": "2011-06-30T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1002080"}, {"journal": "PLoS Computational Biology", "abstract": ["\nWhat is the relationship between the complexity and the fitness of evolved organisms, whether natural or artificial? It has been asserted, primarily based on empirical data, that the complexity of plants and animals increases as their fitness within a particular environment increases via evolution by natural selection. We simulate the evolution of the brains of simple organisms living in a planar maze that they have to traverse as rapidly as possible. Their connectome evolves over 10,000s of generations. We evaluate their circuit complexity, using four information-theoretical measures, including one that emphasizes the extent to which any network is an irreducible entity. We find that their minimal complexity increases with their fitness.\nAuthor Summary: It has often been asserted that as organisms adapt to natural environments with many independent forces and actors acting over a variety of different time scales, they become more complex. We investigate this question from the point of view of information theory as applied to the nervous systems of simple creatures evolving in a stereotyped environment. We performed a controlled in silico evolution experiment to study the relationship between complexity, as measured using different information-theoretic measures, and fitness, by evolving animats with brains of twelve binary variables over 60,000 generations. We compute the complexity of these evolved networks using three measures based on mutual information and one measure based on the extent to which their brain contain states that are both differentiated and integrated. All measures show the same trend - the minimal complexity at any one fitness level increases as the organisms become more adapted to their environment, that is, as they become fitter. Above this minimum, there exists a large degree of degeneracy in evidence. "], "author_display": ["Nikhil J. Joshi", "Giulio Tononi", "Christof Koch"], "article_type": "Research Article", "score": 0.396088, "title_display": "The Minimal Complexity of Adapting Agents Increases with Fitness", "publication_date": "2013-07-11T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1003111"}, {"journal": "PLOS ONE", "abstract": ["\nSocial media are used as main discussion channels by millions of individuals every day. The content individuals produce in daily social-media-based micro-communications, and the emotions therein expressed, may impact the emotional states of others. A recent experiment performed on Facebook hypothesized that emotions spread online, even in absence of non-verbal cues typical of in-person interactions, and that individuals are more likely to adopt positive or negative emotions if these are over-expressed in their social network. Experiments of this type, however, raise ethical concerns, as they require massive-scale content manipulation with unknown consequences for the individuals therein involved. Here, we study the dynamics of emotional contagion using a random sample of Twitter users, whose activity (and the stimuli they were exposed to) was observed during a week of September 2014. Rather than manipulating content, we devise a null model that discounts some confounding factors (including the effect of emotional contagion). We measure the emotional valence of content the users are exposed to before posting their own tweets. We determine that on average a negative post follows an over-exposure to 4.34% more negative content than baseline, while positive posts occur after an average over-exposure to 4.50% more positive contents. We highlight the presence of a linear relationship between the average emotional valence of the stimuli users are exposed to, and that of the responses they produce. We also identify two different classes of individuals: highly and scarcely susceptible to emotional contagion. Highly susceptible users are significantly less inclined to adopt negative emotions than the scarcely susceptible ones, but equally likely to adopt positive emotions. In general, the likelihood of adopting positive emotions is much greater than that of negative emotions.\n"], "author_display": ["Emilio Ferrara", "Zeyao Yang"], "article_type": "Research Article", "score": 0.39561775, "title_display": "Measuring Emotional Contagion in Social Media", "publication_date": "2015-11-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0142390"}, {"journal": "PLoS ONE", "abstract": ["\nIn this work, we propose an innovative adaptive recommendation mechanism for smart parking. The cognitive RF module will transmit the vehicle location information and the parking space requirements to the parking congestion computing center (PCCC) when the driver must find a parking space. Moreover, for the parking spaces, we use a cellular automata (CA) model mechanism that can adjust to full and not full parking lot situations. Here, the PCCC can compute the nearest parking lot, the parking lot status and the current or opposite driving direction with the vehicle location information. By considering the driving direction, we can determine when the vehicles must turn around and thus reduce road congestion and speed up finding a parking space. The recommendation will be sent to the drivers through a wireless communication cognitive radio (CR) model after the computation and analysis by the PCCC. The current study evaluates the performance of this approach by conducting computer simulations. The simulation results show the strengths of the proposed smart parking mechanism in terms of avoiding increased congestion and decreasing the time to find a parking space.\n"], "author_display": ["Gwo-Jiun Horng"], "article_type": "Research Article", "score": 0.39502636, "title_display": "Using Cellular Automata for Parking Recommendations in Smart Environments", "publication_date": "2014-08-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0105973"}, {"journal": "PLoS ONE", "abstract": ["Introduction: The quantification of social media impacts on societal and political events is a difficult undertaking. The Japanese Society of Oriental Medicine started a signature-collecting campaign to oppose a medical policy of the Government Revitalization Unit to exclude a traditional Japanese medicine, \u201cKampo,\u201d from the public insurance system. The signature count showed a series of aberrant bursts from November 26 to 29, 2009. In the same interval, the number of messages on Twitter including the keywords \u201cSignature\u201d and \u201cKampo,\u201d increased abruptly. Moreover, the number of messages on an Internet forum that discussed the policy and called for signatures showed a train of spikes. Methods and Findings: In order to estimate the contributions of social media, we developed a statistical model with state-space modeling framework that distinguishes the contributions of multiple social media in time-series of collected public opinions. We applied the model to the time-series of signature counts of the campaign and quantified contributions of two social media, i.e., Twitter and an Internet forum, by the estimation. We found that a considerable portion (78%) of the signatures was affected from either of the social media throughout the campaign and the Twitter effect (26%) was smaller than the Forum effect (52%) in total, although Twitter probably triggered the initial two bursts of signatures. Comparisons of the estimated profiles of the both effects suggested distinctions between the social media in terms of sustainable impact of messages or tweets. Twitter shows messages on various topics on a time-line; newer messages push out older ones. Twitter may diminish the impact of messages that are tweeted intermittently. Conclusions: The quantification of social media impacts is beneficial to better understand people\u2019s tendency and may promote developing strategies to engage public opinions effectively. Our proposed method is a promising tool to explore information hidden in social phenomena. "], "author_display": ["Rui Yamaguchi", "Seiya Imoto", "Masahiro Kami", "Kenji Watanabe", "Satoru Miyano", "Koichiro Yuji"], "article_type": "Research Article", "score": 0.3949414, "title_display": "Does Twitter Trigger Bursts in Signature Collections?", "publication_date": "2013-03-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0058252"}, {"journal": "PLoS ONE", "abstract": ["\nInspired by theories of higher local order autocorrelation (HLAC), this paper presents a simple, novel, yet very powerful approach for wood recognition. The method is suitable for wood database applications, which are of great importance in wood related industries and administrations. At the feature extraction stage, a set of features is extracted from Mask Matching Image (MMI). The MMI features preserve the mask matching information gathered from the HLAC methods. The texture information in the image can then be accurately extracted from the statistical and geometrical features. In particular, richer information and enhanced discriminative power is achieved through the length histogram, a new histogram that embodies the width and height histograms. The performance of the proposed approach is compared to the state-of-the-art HLAC approaches using the wood stereogram dataset ZAFU WS 24. By conducting extensive experiments on ZAFU WS 24, we show that our approach significantly improves the classification accuracy.\n"], "author_display": ["Hang-jun Wang", "Guang-qun Zhang", "Heng-nian Qi"], "article_type": "Research Article", "score": 0.39431024, "title_display": "Wood Recognition Using Image Texture Features", "publication_date": "2013-10-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0076101"}, {"journal": "PLoS ONE", "abstract": ["\nThe reverse engineering of gene regulatory networks using gene expression profile data has become crucial to gain novel biological knowledge. Large amounts of data that need to be analyzed are currently being produced due to advances in microarray technologies. Using current reverse engineering algorithms to analyze large data sets can be very computational-intensive. These emerging computational requirements can be met using parallel computing techniques. It has been shown that the Network Identification by multiple Regression (NIR) algorithm performs better than the other ready-to-use reverse engineering software. However it cannot be used with large networks with thousands of nodes - as is the case in biological networks - due to the high time and space complexity. In this work we overcome this limitation by designing and developing a parallel version of the NIR algorithm. The new implementation of the algorithm reaches a very good accuracy even for large gene networks, improving our understanding of the gene regulatory networks that is crucial for a wide range of biomedical applications.\n"], "author_display": ["Francesco Gregoretti", "Vincenzo Belcastro", "Diego di Bernardo", "Gennaro Oliva"], "article_type": "Research Article", "score": 0.39388666, "title_display": "A Parallel Implementation of the Network Identification by Multiple Regression (NIR) Algorithm to Reverse-Engineer Regulatory Gene Networks", "publication_date": "2010-04-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0010179"}, {"journal": "PLoS ONE", "abstract": ["Objective: Offering calories on restaurant websites might be particularly important for consumer meal planning, but the availability of and ease of accessing this information are unknown. Methods: We assessed websites for the top 100 U.S. chain restaurants to determine the availability of and ease of access to calorie information as well as website design characteristics. We also examined potential predictors of calorie availability and ease of access. Results: Eighty-two percent of restaurants provided calorie information on their websites; 25% presented calories on a mobile-formatted website. On average, calories could be accessed in 2.35\u00b10.99 clicks. About half of sites (51.2%) linked to calorie information via the homepage. Fewer than half had a separate section identifying healthful options (46.3%), or utilized interactive meal planning tools (35.4%). Quick service/fast casual, larger restaurants, and those with less expensive entr\u00e9es and lower revenue were more likely to make calorie information available. There were no predictors of ease of access. Conclusion: Calorie information is both available and largely accessible on the websites of America\u2019s leading restaurants. It is unclear whether consumer behavior is affected by the variability in the presentation of calorie information. "], "author_display": ["Gary G. Bennett", "Dori M. Steinberg", "Michele G. Lanpher", "Sandy Askew", "Ilana B. Lane", "Erica L. Levine", "Melody S. Goodman", "Perry B. Foley"], "article_type": "Research Article", "score": 0.39361012, "title_display": "Availability of and Ease of Access to Calorie Information on Restaurant Websites", "publication_date": "2013-08-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0072009"}, {"journal": "PLoS ONE", "abstract": ["\n        The processes by which disease spreads in a population of individuals are inherently stochastic. The master equation has proven to be a useful tool for modeling such processes. Unfortunately, solving the master equation analytically is possible only in limited cases (e.g., when the model is linear), and thus numerical procedures or approximation methods must be employed. Available approximation methods, such as the system size expansion method of van Kampen, may fail to provide reliable solutions, whereas current numerical approaches can induce appreciable computational cost. In this paper, we propose a new numerical technique for solving the master equation. Our method is based on a more informative stochastic process than the population process commonly used in the literature. By exploiting the structure of the master equation governing this process, we develop a novel technique for calculating the exact solution of the master equation \u2013 up to a desired precision \u2013 in certain models of stochastic epidemiology. We demonstrate the potential of our method by solving the master equation associated with the stochastic SIR epidemic model. MATLAB software that implements the methods discussed in this paper is freely available as Supporting Information S1.\n      "], "author_display": ["Garrett Jenkinson", "John Goutsias"], "article_type": "Research Article", "score": 0.3933344, "title_display": "Numerical Integration of the Master Equation in Some Models of Stochastic Epidemiology", "publication_date": "2012-05-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0036160"}, {"journal": "PLOS ONE", "abstract": ["\nAlthough science and technology impact every aspect of modern societies, there is still an extensive gap between science and society, which impairs the full exercise of citizenship. In the particular case of biomedical research increased investment should be accompanied by parallel efforts in terms of public information and engagement. We have carried out a project involving the production and evaluation of educational contents focused on stem cells - illustrated newspaper chronicles, radio interviews, a comic book, and animated videos - and monitored their impact on the Portuguese population. The study of the outreach materials in a heterogeneous sample of the population suggests that they are valuable tools to disseminate scientific messages, and that this is especially true for the comic-book format. Furthermore, the data showed that clear and stimulating outreach materials, that are able to teach new concepts and to promote critical thinking, increase engagement in science at different levels, depending on the depth of the concepts involved. Additionally, these materials can influence political, social and personal attitudes toward science. These results, together with the importance attributed to scientific research in stem cells by the population sampled, validates the diffusion of such materials as a significant contribution towards an overall public understanding and engagement in contemporary science, and this strategy should thus be considered in future projects. Regardless, stringent quality control must be implemented in order to efficiently communicate accurate scientific developments, and the public stimulated in terms of finding additional sources of reliable information.\n"], "author_display": ["Sara Varela Amaral", "Teresa Forte", "Jo\u00e3o Ramalho-Santos", "M. Teresa Gir\u00e3o da Cruz"], "article_type": "Research Article", "score": 0.39287743, "title_display": "I Want More and Better Cells! \u2013 An Outreach Project about Stem Cells and Its Impact on the General Population", "publication_date": "2015-07-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0133753"}, {"journal": "PLoS ONE", "abstract": ["\n        Chemistry text mining tools should be interoperable and adaptable regardless of\n                    system-level implementation, installation or even programming issues. We aim to\n                    abstract the functionality of these tools from the underlying implementation via\n                    reconfigurable workflows for automatically identifying chemical names. To\n                    achieve this, we refactored an established named entity recogniser (in the\n                    chemistry domain), OSCAR and studied the impact of each component on the net\n                    performance. We developed two reconfigurable workflows from OSCAR using an\n                    interoperable text mining framework, U-Compare. These workflows can be altered\n                    using the drag-&-drop mechanism of the graphical user\n                    interface of U-Compare. These workflows also provide a platform to study the\n                    relationship between text mining components such as tokenisation and named\n                    entity recognition (using maximum entropy Markov model (MEMM) and pattern\n                    recognition based classifiers). Results indicate that, for chemistry in\n                    particular, eliminating noise generated by tokenisation techniques lead to a\n                    slightly better performance than others, in terms of named entity recognition\n                    (NER) accuracy. Poor tokenisation translates into poorer input to the classifier\n                    components which in turn leads to an increase in Type I or Type II errors, thus,\n                    lowering the overall performance. On the Sciborg corpus, the workflow based\n                    system, which uses a new tokeniser whilst retaining the same MEMM component,\n                    increases the F-score from 82.35% to 84.44%. On the PubMed corpus,\n                    it recorded an F-score of 84.84% as against 84.23% by OSCAR.\n      "], "author_display": ["BalaKrishna Kolluru", "Lezan Hawizy", "Peter Murray-Rust", "Junichi Tsujii", "Sophia Ananiadou"], "article_type": "Research Article", "score": 0.39270508, "title_display": "Using Workflows to Explore and Optimise Named Entity Recognition for\n                    Chemistry", "publication_date": "2011-05-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0020181"}, {"journal": "PLOS ONE", "abstract": ["\nFor a given graph G, \u03b5(v) and deg(v) denote the eccentricity and the degree of the vertex v in G, respectively. The adjacent eccentric distance sum index of a graph G is defined as \u03be s v ( G ) = \u2211 v \u2208 V ( G ) \u03b5 ( v ) D ( v ) d e g ( v ), where D ( v ) = \u2211 u \u2208 V ( G ) d ( u , v ) is the sum of all distances from the vertex v. In this paper we derive some bounds for the adjacent eccentric distance sum index in terms of some graph parameters, such as independence number, covering number, vertex connectivity, chromatic number, diameter and some other graph topological indices.\n"], "author_display": ["Hui Qu", "Shujuan Cao"], "article_type": "Research Article", "score": 0.39247873, "title_display": "On the Adjacent Eccentric Distance Sum Index of Graphs", "publication_date": "2015-06-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0129497"}, {"abstract": ["\n        Protein remote homology detection is one of the most important problems in bioinformatics. Discriminative methods such as support vector machines (SVM) have shown superior performance. However, the performance of SVM-based methods depends on the vector representations of the protein sequences. Prior works have demonstrated that sequence-order effects are relevant for discrimination, but little work has explored how to incorporate the sequence-order information along with the amino acid physicochemical properties into the prediction. In order to incorporate the sequence-order effects into the protein remote homology detection, the physicochemical distance transformation (PDT) method is proposed. Each protein sequence is converted into a series of numbers by using the physicochemical property scores in the amino acid index (AAIndex), and then the sequence is converted into a fixed length vector by PDT. The sequence-order information can be efficiently included into the feature vector with little computational cost by this approach. Finally, the feature vectors are input into a support vector machine classifier to detect the protein remote homologies. Our experiments on a well-known benchmark show the proposed method SVM-PDT achieves superior or comparable performance with current state-of-the-art methods and its computational cost is considerably superior to those of other methods. When the evolutionary information extracted from the frequency profiles is combined with the PDT method, the profile-based PDT approach can improve the performance by 3.4% and 11.4% in terms of ROC score and ROC50 score respectively. The local sequence-order information of the protein can be efficiently captured by the proposed PDT and the physicochemical properties extracted from the amino acid index are incorporated into the prediction. The physicochemical distance transformation provides a general framework, which would be a valuable tool for protein-level study.\n      "], "author_display": ["Bin Liu", "Xiaolong Wang", "Qingcai Chen", "Qiwen Dong", "Xun Lan"], "article_type": "Research Article", "score": 0.39243025, "title_display": "Using Amino Acid Physicochemical Distance Transformation for Fast Protein Remote Homology Detection", "publication_date": "2012-09-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0046633"}, {"journal": "PLoS ONE", "abstract": ["\nCellular networks are highly dynamic in their function, yet evolutionarily conserved in their core network motifs or topologies. Understanding functional tunability and robustness of network motifs to small perturbations in function and structure is vital to our ability to synthesize controllable circuits. In establishing core sets of network motifs, we selected topologies that are overrepresented in mammalian networks, including the linear, feedback, feed-forward, and bifan circuits. Static and dynamic tunability of network motifs were defined as the motif ability to respectively attain steady-state or transient outputs in response to pre-defined input stimuli. Detailed computational analysis suggested that static tunability is insensitive to the circuit topology, since all of the motifs displayed similar ability to attain predefined steady-state outputs in response to constant inputs. Dynamic tunability, in contrast, was tightly dependent on circuit topology, with some motifs performing superiorly in achieving observed time-course outputs. Finally, we mapped dynamic tunability onto motif topologies to determine robustness of motif structures to changes in topology and identify design principles for the rational assembly of robust synthetic networks.\n"], "author_display": ["Sergio Iadevaia", "Luay K. Nakhleh", "Robert Azencott", "Prahlad T. Ram"], "article_type": "Research Article", "score": 0.39233726, "title_display": "Mapping Network Motif Tunability and Robustness in the Design of Synthetic Signaling Circuits", "publication_date": "2014-03-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0091743"}, {"journal": "PLoS Computational Biology", "abstract": ["\nIt is a long-established fact that neuronal plasticity occupies the central role in generating neural function and computation. Nevertheless, no unifying account exists of how neurons in a recurrent cortical network learn to compute on temporally and spatially extended stimuli. However, these stimuli constitute the norm, rather than the exception, of the brain's input. Here, we introduce a geometric theory of learning spatiotemporal computations through neuronal plasticity. To that end, we rigorously formulate the problem of neural representations as a relation in space between stimulus-induced neural activity and the asymptotic dynamics of excitable cortical networks. Backed up by computer simulations and numerical analysis, we show that two canonical and widely spread forms of neuronal plasticity, that is, spike-timing-dependent synaptic plasticity and intrinsic plasticity, are both necessary for creating neural representations, such that these computations become realizable. Interestingly, the effects of these forms of plasticity on the emerging neural code relate to properties necessary for both combating and utilizing noise. The neural dynamics also exhibits features of the most likely stimulus in the network's spontaneous activity. These properties of the spatiotemporal neural code resulting from plasticity, having their grounding in nature, further consolidate the biological relevance of our findings.\nAuthor Summary: The world is not perceived as a chain of segmented sensory still lifes. Instead, it appears that the brain is capable of integrating the temporal dependencies of the incoming sensory stream with the spatial aspects of that input. It then transfers the resulting whole in a useful manner, in order to reach a coherent and causally sound image of our physical surroundings, and to act within it. These spatiotemporal computations are made possible through a cluster of local and coexisting adaptation mechanisms known collectively as neuronal plasticity. While this role is widely known and supported by experimental evidence, no unifying theory of how the brain, through the interaction of plasticity mechanisms, gets to represent spatiotemporal computations in its spatiotemporal activity. In this paper, we aim at such a theory. We develop a rigorous mathematical formalism of spatiotemporal representations within the input-driven dynamics of cortical networks. We demonstrate that the interaction of two of the most common plasticity mechanisms, intrinsic and synaptic plasticity, leads to representations that allow for spatiotemporal computations. We also show that these representations are structured to tolerate noise and to even benefit from it. "], "author_display": ["Hazem Toutounji", "Gordon Pipa"], "article_type": "Research Article", "score": 0.39212298, "title_display": "Spatiotemporal Computations of an Excitable and Plastic Brain: Neuronal Plasticity Leads to Noise-Robust and Noise-Constructive Computations", "publication_date": "2014-03-20T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1003512"}, {"journal": "PLOS ONE", "abstract": ["\nThe recently growing interest in studying flight behaviours of fruit flies, Drosophila melanogaster, has highlighted the need for developing tools that acquire quantitative motion data. Despite recent advance of video tracking systems, acquiring a flying fly\u2019s orientation remains a challenge for these tools. In this paper, we present a novel method for estimating individual flying fly\u2019s orientation using image cues. Thanks to the line reconstruction algorithm in computer vision field, this work can thereby focus on the practical detail of implementation and evaluation of the orientation estimation algorithm. The orientation estimation algorithm can be incorporated into tracking algorithms. We rigorously evaluated the effectiveness and accuracy of the proposed algorithm by running experiments both on simulation data and on real-world data. This work complements methods for studying the fruit fly\u2019s flight behaviours in a three-dimensional environment.\n"], "author_display": ["Xi En Cheng", "Shuo Hong Wang", "Zhi-Ming Qian", "Yan Qiu Chen"], "article_type": "Research Article", "score": 0.39205295, "title_display": "Estimating Orientation of Flying Fruit Flies", "publication_date": "2015-07-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0132101"}, {"journal": "PLOS Computational Biology", "abstract": ["\nWe propose a working hypothesis supported by numerical simulations that brain networks evolve based on the principle of the maximization of their internal information flow capacity. We find that synchronous behavior and capacity of information flow of the evolved networks reproduce well the same behaviors observed in the brain dynamical networks of Caenorhabditis elegans and humans, networks of Hindmarsh-Rose neurons with graphs given by these brain networks. We make a strong case to verify our hypothesis by showing that the neural networks with the closest graph distance to the brain networks of Caenorhabditis elegans and humans are the Hindmarsh-Rose neural networks evolved with coupling strengths that maximize information flow capacity. Surprisingly, we find that global neural synchronization levels decrease during brain evolution, reflecting on an underlying global no Hebbian-like evolution process, which is driven by no Hebbian-like learning behaviors for some of the clusters during evolution, and Hebbian-like learning rules for clusters where neurons increase their synchronization.\nAuthor Summary: The study of the function of the brain is of primordial importance in neuroscience. Several brain models have been studied so far that take into account higher level functions of the external stimulus and the behavioral response attributed to ensembles of neurons of cortical areas. If the brain learns by maximizing the Mutual Information between stimuli and response, or by updating the internal model of probabilities by using Bayesian techniques, or by minimizing the free-energy, it provides little insight about the dynamical mechanisms appearing in brain networks when evolved based on such principles. In our work we propose a working hypothesis supported by numerical simulations that brain dynamical networks evolve based on the principle of the maximization of their internal information flow capacity, i.e. the upper bound for the information transferred per time unit between any two nodes. We make a strong case to verify our hypothesis by showing that the neural networks with the closest spectral graph distance to the brain networks of Caenorhabditis elegans and humans are the Hindmarsh-Rose neural networks evolved with coupling strengths that maximize information flow capacity. We also find that synchronous behavior and capacity of information flow of the evolved neural networks reproduce well the same behaviors observed in the brain dynamical networks of Caenorhabditis elegans and humans. Finally, we find that global neural synchronization levels decrease during brain network evolution. "], "author_display": ["Chris G. Antonopoulos", "Shambhavi Srivastava", "Sandro E. de S. Pinto", "Murilo S. Baptista"], "article_type": "Research Article", "score": 0.39176497, "title_display": "Do Brain Networks Evolve by Maximizing Their Information Flow Capacity?", "publication_date": "2015-08-28T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1004372"}, {"abstract": ["\n        As scientists we like to think that modern societies and their members base their views, opinions and behaviour on scientific facts. This is not necessarily the case, even though we are all (over-) exposed to information flow through various channels of media, i.e. newspapers, television, radio, internet, and web. It is thought that this is mainly due to the conflicting information on the mass media and to the individual attitude (formed by cultural, educational and environmental factors), that is, one external factor and another personal factor. In this paper we will investigate the dynamical development of opinion in a small population of agents by means of a computational model of opinion formation in a co-evolving network of socially linked agents. The personal and external factors are taken into account by assigning an individual attitude parameter to each agent, and by subjecting all to an external but homogeneous field to simulate the effect of the media. We then adjust the field strength in the model by using actual data on scientific perception surveys carried out in two different populations, which allow us to compare two different societies. We interpret the model findings with the aid of simple mean field calculations. Our results suggest that scientifically sound concepts are more difficult to acquire than concepts not validated by science, since opposing individuals organize themselves in close communities that prevent opinion consensus.\n      "], "author_display": ["Gerardo I\u00f1iguez", "Julia Tag\u00fce\u00f1a-Mart\u00ednez", "Kimmo K. Kaski", "Rafael A. Barrio"], "article_type": "Research Article", "score": 0.39160386, "title_display": "Are Opinions Based on Science: Modelling Social Response to Scientific Facts", "publication_date": "2012-08-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0042122"}, {"journal": "PLOS ONE", "abstract": ["\nAccurate and timely detection of plant diseases can help mitigate the worldwide losses experienced by the horticulture and agriculture industries each year. Thermal imaging provides a fast and non-destructive way of scanning plants for diseased regions and has been used by various researchers to study the effect of disease on the thermal profile of a plant. However, thermal image of a plant affected by disease has been known to be affected by environmental conditions which include leaf angles and depth of the canopy areas accessible to the thermal imaging camera. In this paper, we combine thermal and visible light image data with depth information and develop a machine learning system to remotely detect plants infected with the tomato powdery mildew fungus Oidium neolycopersici. We extract a novel feature set from the image data using local and global statistics and show that by combining these with the depth information, we can considerably improve the accuracy of detection of the diseased plants. In addition, we show that our novel feature set is capable of identifying plants which were not originally inoculated with the fungus at the start of the experiment but which subsequently developed disease through natural transmission.\n"], "author_display": ["Shan-e-Ahmed Raza", "Gillian Prince", "John P. Clarkson", "Nasir M. Rajpoot"], "article_type": "Research Article", "score": 0.39139253, "title_display": "Automatic Detection of Diseased Tomato Plants Using Thermal and Stereo Visible Light Images", "publication_date": "2015-04-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0123262"}, {"journal": "PLoS Computational Biology", "abstract": ["\nProgress in science depends on the effective exchange of ideas among scientists. New ideas can be assessed and criticized in a meaningful manner only if they are formulated precisely. This applies to simulation studies as well as to experiments and theories. But after more than 50 years of neuronal network simulations, we still lack a clear and common understanding of the role of computational models in neuroscience as well as established practices for describing network models in publications. This hinders the critical evaluation of network models as well as their re-use.\nWe analyze here 14 research papers proposing neuronal network models of different complexity and find widely varying approaches to model descriptions, with regard to both the means of description and the ordering and placement of material. We further observe great variation in the graphical representation of networks and the notation used in equations. Based on our observations, we propose a good model description practice, composed of guidelines for the organization of publications, a checklist for model descriptions, templates for tables presenting model structure, and guidelines for diagrams of networks. The main purpose of this good practice is to trigger a debate about the communication of neuronal network models in a manner comprehensible to humans, as opposed to machine-readable model description languages.\nWe believe that the good model description practice proposed here, together with a number of other recent initiatives on data-, model-, and software-sharing, may lead to a deeper and more fruitful exchange of ideas among computational neuroscientists in years to come. We further hope that work on standardized ways of describing\u2014and thinking about\u2014complex neuronal networks will lead the scientific community to a clearer understanding of high-level concepts in network dynamics, and will thus lead to deeper insights into the function of the brain.\nAuthor Summary: Scientists make precise, testable statements about their observations and models of nature. Other scientists can then evaluate these statements and attempt to reproduce or extend them. Results that cannot be reproduced will be duly criticized to arrive at better interpretations of experimental results or better models. Over time, this discourse develops our joint scientific knowledge. A crucial condition for this process is that scientists can describe their own models in a manner that is precise and comprehensible to others. We analyze in this paper how well models of neuronal networks are described in the scientific literature and conclude that the wide variety of manners in which network models are described makes it difficult to communicate models successfully. We propose a good model description practice to improve the communication of neuronal network models. "], "author_display": ["Eilen Nordlie", "Marc-Oliver Gewaltig", "Hans Ekkehard Plesser"], "article_type": "Research Article", "score": 0.39137062, "title_display": "Towards Reproducible Descriptions of Neuronal Network Models", "publication_date": "2009-08-07T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1000456"}, {"journal": "PLoS ONE", "abstract": ["Background: We introduced a series of computer-supported workshops in our undergraduate statistics courses, in the hope that it would help students to gain a deeper understanding of statistical concepts. This raised questions about the appropriate design of the Virtual Learning Environment (VLE) in which such an approach had to be implemented. Therefore, we investigated two competing software design models for VLEs. In the first system, all learning features were a function of the classical VLE. The second system was designed from the perspective that learning features should be a function of the course's core content (statistical analyses), which required us to develop a specific\u2013purpose Statistical Learning Environment (SLE) based on Reproducible Computing and newly developed Peer Review (PR) technology. Objectives: The main research question is whether the second VLE design improved learning efficiency as compared to the standard type of VLE design that is commonly used in education. As a secondary objective we provide empirical evidence about the usefulness of PR as a constructivist learning activity which supports non-rote learning. Finally, this paper illustrates that it is possible to introduce a constructivist learning approach in large student populations, based on adequately designed educational technology, without subsuming educational content to technological convenience. Methods: Both VLE systems were tested within a two-year quasi-experiment based on a Reliable Nonequivalent Group Design. This approach allowed us to draw valid conclusions about the treatment effect of the changed VLE design, even though the systems were implemented in successive years. The methodological aspects about the experiment's internal validity are explained extensively. Results: The effect of the design change is shown to have substantially increased the efficiency of constructivist, computer-assisted learning activities for all cohorts of the student population under investigation. The findings demonstrate that a content\u2013based design outperforms the traditional VLE\u2013based design. "], "author_display": ["Patrick Wessa", "Antoon De Rycker", "Ian Edward Holliday"], "article_type": "Research Article", "score": 0.39116946, "title_display": "Content-Based VLE Designs Improve Learning Efficiency in Constructivist Statistics Education", "publication_date": "2011-10-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0025363"}, {"journal": "PLoS ONE", "abstract": ["\nModelled as finite homogeneous Markov chains, probabilistic cellular automata with local transition probabilities in (0, 1) always posses a stationary distribution. This result alone is not very helpful when it comes to predicting the final configuration; one needs also a formula connecting the probabilities in the stationary distribution to some intrinsic feature of the lattice configuration. Previous results on the asynchronous cellular automata have showed that such feature really exists. It is the number of zero-one borders within the automaton's binary configuration. An exponential formula in the number of zero-one borders has been proved for the 1-D, 2-D and 3-D asynchronous automata with neighborhood three, five and seven, respectively. We perform computer experiments on a synchronous cellular automaton to check whether the empirical distribution obeys also that theoretical formula. The numerical results indicate a perfect fit for neighbourhood three and five, which opens the way for a rigorous proof of the formula in this new, synchronous case.\n"], "author_display": ["Alexandru Agapie", "Anca Andreica", "Camelia Chira", "Marius Giuclea"], "article_type": "Research Article", "score": 0.39116514, "title_display": "Predictability in Cellular Automata", "publication_date": "2014-10-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0108177"}, {"journal": "PLoS ONE", "abstract": ["\nHIV-1 protease represents an appealing system for directed enzyme re-design, since it has various different endogenous targets, a relatively simple structure and it is well studied. Recently Chaudhury and Gray (Structure (2009) 17: 1636\u20131648) published a computational algorithm to discern the specificity determining residues of HIV-1 protease. In this paper we present two computational tools aimed at re-designing HIV-1 protease, derived from the algorithm of Chaudhuri and Gray. First, we present an energy-only based methodology to discriminate cleavable and non cleavable peptides for HIV-1 proteases, both wild type and mutant. Secondly, we show an algorithm we developed to predict mutant HIV-1 proteases capable of cleaving a new target substrate peptide, different from the natural targets of HIV-1 protease. The obtained in silico mutant enzymes were analyzed in terms of cleavability and specificity towards the target peptide using the energy-only methodology. We found two mutant proteases as best candidates for specificity and cleavability towards the target sequence.\n"], "author_display": ["Jan H. Jensen", "Martin Willemo\u00ebs", "Jakob R. Winther", "Luca De Vico"], "article_type": "Research Article", "score": 0.39081568, "title_display": "<i>In Silico</i> Prediction of Mutant HIV-1 Proteases Cleaving a Target Sequence", "publication_date": "2014-05-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0095833"}, {"journal": "PLoS ONE", "abstract": ["\nThe formation of collective opinion is a complex phenomenon that results from the combined effects of mass media exposure and social influence between individuals. The present work introduces a model of opinion formation specifically designed to address risk judgments, such as attitudes towards climate change, terrorist threats, or children vaccination. The model assumes that people collect risk information from the media environment and exchange them locally with other individuals. Even though individuals are initially exposed to the same sample of information, the model predicts the emergence of opinion polarization and clustering. In particular, numerical simulations highlight two crucial factors that determine the collective outcome: the propensity of individuals to search for independent information, and the strength of social influence. This work provides a quantitative framework to anticipate and manage how the public responds to a given risk, and could help understanding the systemic amplification of fears and worries, or the underestimation of real dangers.\n"], "author_display": ["Mehdi Moussa\u00efd"], "article_type": "Research Article", "score": 0.39034238, "title_display": "Opinion Formation and the Collective Dynamics of Risk Perception", "publication_date": "2013-12-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0084592"}, {"abstract": ["Background: Protein structure prediction (PSP), which is usually modeled as a computational optimization problem, remains one of the biggest challenges in computational biology. PSP encounters two difficult obstacles: the inaccurate energy function problem and the searching problem. Even if the lowest energy has been luckily found by the searching procedure, the correct protein structures are not guaranteed to obtain. Results: A general parallel metaheuristic approach is presented to tackle the above two problems. Multi-energy functions are employed to simultaneously guide the parallel searching threads. Searching trajectories are in fact controlled by the parameters of heuristic algorithms. The parallel approach allows the parameters to be perturbed during the searching threads are running in parallel, while each thread is searching the lowest energy value determined by an individual energy function. By hybridizing the intelligences of parallel ant colonies and Monte Carlo Metropolis search, this paper demonstrates an implementation of our parallel approach for PSP. 16 classical instances were tested to show that the parallel approach is competitive for solving PSP problem. Conclusions: This parallel approach combines various sources of both searching intelligences and energy functions, and thus predicts protein conformations with good quality jointly determined by all the parallel searching threads and energy functions. It provides a framework to combine different searching intelligence embedded in heuristic algorithms. It also constructs a container to hybridize different not-so-accurate objective functions which are usually derived from the domain expertise. "], "author_display": ["Qiang L\u00fc", "Xiao-Yan Xia", "Rong Chen", "Da-Jun Miao", "Sha-Sha Chen", "Li-Jun Quan", "Hai-Ou Li"], "article_type": "Research Article", "score": 0.39019334, "title_display": "When the Lowest Energy Does Not Induce Native Structures: Parallel Minimization of Multi-Energy Values by Hybridizing Searching Intelligences", "publication_date": "2012-09-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0044967"}, {"journal": "PLoS ONE", "abstract": ["\n        A single glance at your crowded desk is enough to locate your favorite cup. But finding an unfamiliar object requires more effort. This superiority in recognition performance for learned objects has at least two possible sources. For familiar objects observers might: 1) select more informative image locations upon which to fixate their eyes, or 2) extract more information from a given eye fixation. To test these possibilities, we had observers localize fragmented objects embedded in dense displays of random contour fragments. Eight participants searched for objects in 600 images while their eye movements were recorded in three daily sessions. Performance improved as subjects trained with the objects: The number of fixations required to find an object decreased by 64% across the 3 sessions. An ideal observer model that included measures of fragment confusability was used to calculate the information available from a single fixation. Comparing human performance to the model suggested that across sessions information extraction at each eye fixation increased markedly, by an amount roughly equal to the extra information that would be extracted following a 100% increase in functional field of view. Selection of fixation locations, on the other hand, did not improve with practice.\n      "], "author_display": ["Linus Holm", "Stephen Engel", "Paul Schrater"], "article_type": "Research Article", "score": 0.38998166, "title_display": "Object Learning Improves Feature Extraction but Does Not Improve Feature Selection", "publication_date": "2012-12-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0051325"}, {"journal": "PLoS ONE", "abstract": ["\nIn the rodent hippocampus, a phase precession phenomena of place cell firing with the local field potential (LFP) theta is called \u201ctheta phase precession\u201d and is considered to contribute to memory formation with spike time dependent plasticity (STDP). On the other hand, in the primate hippocampus, the existence of theta phase precession is unclear. Our computational studies have demonstrated that theta phase precession dynamics could contribute to primate\u2013hippocampal dependent memory formation, such as object\u2013place association memory. In this paper, we evaluate human theta phase precession by using a theory\u2013experiment combined analysis. Human memory recall of object\u2013place associations was analyzed by an individual hippocampal network simulated by theta phase precession dynamics of human eye movement and EEG data during memory encoding. It was found that the computational recall of the resultant network is significantly correlated with human memory recall performance, while other computational predictors without theta phase precession are not significantly correlated with subsequent memory recall. Moreover the correlation is larger than the correlation between human recall and traditional experimental predictors. These results indicate that theta phase precession dynamics are necessary for the better prediction of human recall performance with eye movement and EEG data. In this analysis, theta phase precession dynamics appear useful for the extraction of memory-dependent components from the spatio\u2013temporal pattern of eye movement and EEG data as an associative network. Theta phase precession may be a common neural dynamic between rodents and humans for the formation of environmental memories.\n"], "author_display": ["Naoyuki Sato", "Yoko Yamaguchi"], "article_type": "Research Article", "score": 0.38995588, "title_display": "A Computational Predictor of Human Episodic Memory Based on a Theta Phase Precession Network", "publication_date": "2009-10-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0007536"}, {"journal": "PLoS ONE", "abstract": ["\n        When one drug influences the level or activity of another drug this is known as a drug-drug interaction (DDI). Knowledge of such interactions is crucial for patient safety. However, the volume and content of published biomedical literature on drug interactions is expanding rapidly, making it increasingly difficult for DDIs database curators to detect and collate DDIs information manually. In this paper, we propose a single kernel-based approach to extract DDIs from biomedical literature. This novel kernel-based approach can effectively make full use of syntactic structural information of the dependency graph. In particular, our approach can efficiently represent both single subgraph topological information and the relation of two subgraphs in the dependency graph. Experimental evaluations showed that our single kernel-based approach can achieve state-of-the-art performance on the publicly available DDI corpus without exploiting multiple kernels or additional domain resources.\n      "], "author_display": ["Yijia Zhang", "Hongfei Lin", "Zhihao Yang", "Jian Wang", "Yanpeng Li"], "article_type": "Research Article", "score": 0.38981327, "title_display": "A Single Kernel-Based Approach to Extract Drug-Drug Interactions from Biomedical Literature", "publication_date": "2012-11-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0048901"}, {"journal": "PLoS ONE", "abstract": ["\n        Identification of gene-disease association is crucial to understanding disease mechanism. A rapid increase in biomedical literatures, led by advances of genome-scale technologies, poses challenge for manually-curated-based annotation databases to characterize gene-disease associations effectively and timely. We propose an automatic method-The Disease Ontology Annotation Framework (DOAF) to provide a comprehensive annotation of the human genome using the computable Disease Ontology (DO), the NCBO Annotator service and NCBI Gene Reference Into Function (GeneRIF). DOAF can keep the resulting knowledgebase current by periodically executing automatic pipeline to re-annotate the human genome using the latest DO and GeneRIF releases at any frequency such as daily or monthly. Further, DOAF provides a computable and programmable environment which enables large-scale and integrative analysis by working with external analytic software or online service platforms. A user-friendly web interface (doa.nubic.northwestern.edu) is implemented to allow users to efficiently query, download, and view disease annotations and the underlying evidences.\n      "], "author_display": ["Wei Xu", "Huisong Wang", "Wenqing Cheng", "Dong Fu", "Tian Xia", "Warren A. Kibbe", "Simon M. Lin"], "article_type": "Research Article", "score": 0.38967234, "title_display": "A Framework for Annotating Human Genome in Disease Context", "publication_date": "2012-12-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0049686"}, {"journal": "PLoS ONE", "abstract": ["\n        Boolean networks have been used as a discrete model for several biological systems, including metabolic and genetic regulatory networks. Due to their simplicity they offer a firm foundation for generic studies of physical systems. In this work we show, using a measure of context-dependent information, set complexity, that prior to reaching an attractor, random Boolean networks pass through a transient state characterized by high complexity. We justify this finding with a use of another measure of complexity, namely, the statistical complexity. We show that the networks can be tuned to the regime of maximal complexity by adding a suitable amount of noise to the deterministic Boolean dynamics. In fact, we show that for networks with Poisson degree distributions, all networks ranging from subcritical to slightly supercritical can be tuned with noise to reach maximal set complexity in their dynamics. For networks with a fixed number of inputs this is true for near-to-critical networks. This increase in complexity is obtained at the expense of disruption in information flow. For a large ensemble of networks showing maximal complexity, there exists a balance between noise and contracting dynamics in the state space. In networks that are close to critical the intrinsic noise required for the tuning is smaller and thus also has the smallest effect in terms of the information processing in the system. Our results suggest that the maximization of complexity near to the state transition might be a more general phenomenon in physical systems, and that noise present in a system may in fact be useful in retaining the system in a state with high information content.\n      "], "author_display": ["Tuomo M\u00e4ki-Marttunen", "Juha Kesseli", "Matti Nykter"], "article_type": "Research Article", "score": 0.38959393, "title_display": "Balance between Noise and Information Flow Maximizes Set Complexity of Network Dynamics", "publication_date": "2013-03-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0056523"}, {"journal": "PLoS ONE", "abstract": ["\n        The identification of disease-causing genes is a fundamental challenge in human health and of great importance in improving medical care, and provides a better understanding of gene functions. Recent computational approaches based on the interactions among human proteins and disease similarities have shown their power in tackling the issue. In this paper, a novel systematic and global method that integrates two heterogeneous networks for prioritizing candidate disease-causing genes is provided, based on the observation that genes causing the same or similar diseases tend to lie close to one another in a network of protein-protein interactions. In this method, the association score function between a query disease and a candidate gene is defined as the weighted sum of all the association scores between similar diseases and neighbouring genes. Moreover, the topological correlation of these two heterogeneous networks can be incorporated into the definition of the score function, and finally an iterative algorithm is designed for this issue. This method was tested with 10-fold cross-validation on all 1,126 diseases that have at least a known causal gene, and it ranked the correct gene as one of the top ten in 622 of all the 1,428 cases, significantly outperforming a state-of-the-art method called PRINCE. The results brought about by this method were applied to study three multi-factorial disorders: breast cancer, Alzheimer disease and diabetes mellitus type 2, and some suggestions of novel causal genes and candidate disease-causing subnetworks were provided for further investigation.\n      "], "author_display": ["Xingli Guo", "Lin Gao", "Chunshui Wei", "Xiaofei Yang", "Yi Zhao", "Anguo Dong"], "article_type": "Research Article", "score": 0.3895643, "title_display": "A Computational Method Based on the Integration of Heterogeneous Networks for Predicting Disease-Gene Associations", "publication_date": "2011-09-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0024171"}, {"journal": "PLoS ONE", "abstract": ["Introduction: Vascular access (VA) surgery, a prerequisite for hemodialysis treatment of end-stage renal-disease (ESRD) patients, is hampered by complication rates, which are frequently related to flow enhancement. To assist in VA surgery planning, a patient-specific computer model for postoperative flow enhancement was developed. The purpose of this study is to assess the benefit of non contrast-enhanced magnetic resonance angiography (NCE-MRA) data as patient-specific geometrical input for the model-based prediction of surgery outcome. Methods: 25 ESRD patients were included in this study. All patients received a NCE-MRA examination of the upper extremity blood vessels in addition to routine ultrasound (US). Local arterial radii were assessed from NCE-MRA and converted to model input using a linear fit per artery. Venous radii were determined with US. The effect of radius measurement uncertainty on model predictions was accounted for by performing Monte-Carlo simulations. The resulting flow prediction interval of the computer model was compared with the postoperative flow obtained from US. Patients with no overlap between model-based prediction and postoperative measurement were further analyzed to determine whether an increase in geometrical detail improved computer model prediction. Results: Overlap between postoperative flows and model-based predictions was obtained for 71% of patients. Detailed inspection of non-overlapping cases revealed that the geometrical details that could be assessed from NCE-MRA explained most of the differences, and moreover, upon addition of these details in the computer model the flow predictions improved. Conclusions: The results demonstrate clearly that NCE-MRA does provide valuable geometrical information for VA surgery planning. Therefore, it is recommended to use this modality, at least for patients at risk for local or global narrowing of the blood vessels as well as for patients for whom an US-based model prediction would not overlap with surgical choice, as the geometrical details are crucial for obtaining accurate flow predictions. "], "author_display": ["Maarten A. G. Merkx", "Wouter Huberts", "E. Mari\u00eblle H. Bosboom", "Aron S. Bode", "Javier Oliv\u00e1n Besc\u00f3s", "Jan H. M. Tordoir", "Marcel Breeuwer", "Frans N. van de Vosse"], "article_type": "Research Article", "score": 0.38940483, "title_display": "The Benefit of Non Contrast-Enhanced Magnetic Resonance Angiography for Predicting Vascular Access Surgery Outcome: A Computer Model Perspective", "publication_date": "2013-02-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0053615"}, {"journal": "PLOS Computational Biology", "abstract": ["\nThe modeling of large biomolecular assemblies relies on an efficient rendering of their hierarchical architecture across a wide range of spatial level of detail. We describe a paradigm shift currently under way in computer graphics towards the use of more realistic global illumination models, and we apply the so-called ambient occlusion approach to our open-source multi-scale modeling program, Sculptor. While there are many other higher quality global illumination approaches going all the way up to full GPU-accelerated ray tracing, they do not provide size-specificity of the features they shade. Ambient occlusion is an aspect of global lighting that offers great visual benefits and powerful user customization. By estimating how other molecular shape features affect the reception of light at some surface point, it effectively simulates indirect shadowing. This effect occurs between molecular surfaces that are close to each other, or in pockets such as protein or ligand binding sites. By adding ambient occlusion, large macromolecular systems look much more natural, and the perception of characteristic surface features is strongly enhanced. In this work, we present a real-time implementation of screen space ambient occlusion that delivers realistic cues about tunable spatial scale characteristics of macromolecular architecture. Heretofore, the visualization of large biomolecular systems, comprising e.g. hundreds of thousands of atoms or Mega-Dalton size electron microscopy maps, did not take into account the length scales of interest or the spatial resolution of the data. Our approach has been uniquely customized with shading that is tuned for pockets and cavities of a user-defined size, making it useful for visualizing molecular features at multiple scales of interest. This is a feature that none of the conventional ambient occlusion approaches provide. Actual Sculptor screen shots illustrate how our implementation supports the size-dependent rendering of molecular surface features.\nAuthor Summary: In this work, we present an implementation of screen space ambient occlusion (SSAO), which supports the visualization and modeling of multi-scale biophysical data, such as atomic structures and 3D density maps, at multiple scales of interest. The ever-growing size of macromolecular assemblies presents a formidable challenge to molecular modeling programs. Ambient occlusion (AO) has recently received a lot of attention in high-quality rendering engines as well as in video games. The technique provides an acceptable real-time approximation of global illumination, by adding realistic cues about the indirect shading of the surface topology. SSAO leverages the computational power and flexibility of graphical processors, which are present on almost all modern video cards. By using SSAO, we put an emphasis on compatibility and speed, while delivering the visual benefits of AO at a user-selected level of detail. Our work is particularly timely because many molecular graphics packages are currently adopting global illumination schemes, but our work is unique in providing size-specificity of the shaded features. "], "author_display": ["Manuel Wahle", "Willy Wriggers"], "article_type": "Research Article", "score": 0.38937357, "title_display": "Multi-scale Visualization of Molecular Architecture Using Real-Time Ambient Occlusion in Sculptor", "publication_date": "2015-10-27T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1004516"}, {"journal": "PLoS ONE", "abstract": ["\nWith the blooming of Web 2.0, Community Question Answering (CQA) services such as Yahoo! Answers (http://answers.yahoo.com), WikiAnswer (http://wiki.answers.com), and Baidu Zhidao (http://zhidao.baidu.com), etc., have emerged as alternatives for knowledge and information acquisition. Over time, a large number of question and answer (Q&A) pairs with high quality devoted by human intelligence have been accumulated as a comprehensive knowledge base. Unlike the search engines, which return long lists of results, searching in the CQA services can obtain the correct answers to the question queries by automatically finding similar questions that have already been answered by other users. Hence, it greatly improves the efficiency of the online information retrieval. However, given a question query, finding the similar and well-answered questions is a non-trivial task. The main challenge is the word mismatch between question query (query) and candidate question for retrieval (question). To investigate this problem, in this study, we capture the word semantic similarity between query and question by introducing the topic modeling approach. We then propose an unsupervised machine-learning approach to finding similar questions on CQA Q&A archives. The experimental results show that our proposed approach significantly outperforms the state-of-the-art methods.\n"], "author_display": ["Wei-Nan Zhang", "Ting Liu", "Yang Yang", "Liujuan Cao", "Yu Zhang", "Rongrong Ji"], "article_type": "Research Article", "score": 0.38887787, "title_display": "A Topic Clustering Approach to Finding Similar Questions from Large Question and Answer Archives", "publication_date": "2014-03-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0071511"}, {"journal": "PLoS ONE", "abstract": ["\n        Visual saliency is the perceptual quality that makes some items in visual scenes stand out from their immediate contexts. Visual saliency plays important roles in natural vision in that saliency can direct eye movements, deploy attention, and facilitate tasks like object detection and scene understanding. A central unsolved issue is: What features should be encoded in the early visual cortex for detecting salient features in natural scenes? To explore this important issue, we propose a hypothesis that visual saliency is based on efficient encoding of the probability distributions (PDs) of visual variables in specific contexts in natural scenes, referred to as context-mediated PDs in natural scenes. In this concept, computational units in the model of the early visual system do not act as feature detectors but rather as estimators of the context-mediated PDs of a full range of visual variables in natural scenes, which directly give rise to a measure of visual saliency of any input stimulus. To test this hypothesis, we developed a model of the context-mediated PDs in natural scenes using a modified algorithm for independent component analysis (ICA) and derived a measure of visual saliency based on these PDs estimated from a set of natural scenes. We demonstrated that visual saliency based on the context-mediated PDs in natural scenes effectively predicts human gaze in free-viewing of both static and dynamic natural scenes. This study suggests that the computation based on the context-mediated PDs of visual variables in natural scenes may underlie the neural mechanism in the early visual cortex for detecting salient features in natural scenes.\n      "], "author_display": ["Jinhua Xu", "Zhiyong Yang", "Joe Z. Tsien"], "article_type": "Research Article", "score": 0.38878646, "title_display": "Emergence of Visual Saliency from Natural Scenes via Context-Mediated Probability Distributions Coding", "publication_date": "2010-12-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0015796"}, {"journal": "PLoS ONE", "abstract": ["\nAs one of the major challenges, cold-start problem plagues nearly all recommender systems. In particular, new items will be overlooked, impeding the development of new products online. Given limited resources, how to utilize the knowledge of recommender systems and design efficient marketing strategy for new items is extremely important. In this paper, we convert this ticklish issue into a clear mathematical problem based on a bipartite network representation. Under the most widely used algorithm in real e-commerce recommender systems, the so-called item-based collaborative filtering, we show that to simply push new items to active users is not a good strategy. Interestingly, experiments on real recommender systems indicate that to connect new items with some less active users will statistically yield better performance, namely, these new items will have more chance to appear in other users' recommendation lists. Further analysis suggests that the disassortative nature of recommender systems contributes to such observation. In a word, getting in-depth understanding on recommender systems could pave the way for the owners to popularize their cold-start products with low costs.\n"], "author_display": ["Jin-Hu Liu", "Tao Zhou", "Zi-Ke Zhang", "Zimo Yang", "Chuang Liu", "Wei-Min Li"], "article_type": "Research Article", "score": 0.38846532, "title_display": "Promoting Cold-Start Items in Recommender Systems", "publication_date": "2014-12-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0113457"}, {"journal": "PLOS ONE", "abstract": ["\nCurrent high-resolution imaging techniques require an intact sample that preserves spatial relationships. We here present a novel approach, \u201cpuzzle imaging,\u201d that allows imaging a spatially scrambled sample. This technique takes many spatially disordered samples, and then pieces them back together using local properties embedded within the sample. We show that puzzle imaging can efficiently produce high-resolution images using dimensionality reduction algorithms. We demonstrate the theoretical capabilities of puzzle imaging in three biological scenarios, showing that (1) relatively precise 3-dimensional brain imaging is possible; (2) the physical structure of a neural network can often be recovered based only on the neural connectivity matrix; and (3) a chemical map could be reproduced using bacteria with chemosensitive DNA and conjugative transfer. The ability to reconstruct scrambled images promises to enable imaging based on DNA sequencing of homogenized tissue samples.\n"], "author_display": ["Joshua I. Glaser", "Bradley M. Zamft", "George M. Church", "Konrad P. Kording"], "article_type": "Research Article", "score": 0.38831073, "title_display": "Puzzle Imaging: Using Large-Scale Dimensionality Reduction Algorithms for Localization", "publication_date": "2015-07-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0131593"}, {"journal": "PLOS Computational Biology", "abstract": ["\nNeural responses are known to be variable. In order to understand how this neural variability constrains behavioral performance, we need to be able to measure the reliability with which a sensory stimulus is encoded in a given population. However, such measures are challenging for two reasons: First, they must take into account noise correlations which can have a large influence on reliability. Second, they need to be as efficient as possible, since the number of trials available in a set of neural recording is usually limited by experimental constraints. Traditionally, cross-validated decoding has been used as a reliability measure, but it only provides a lower bound on reliability and underestimates reliability substantially in small datasets. We show that, if the number of trials per condition is larger than the number of neurons, there is an alternative, direct estimate of reliability which consistently leads to smaller errors and is much faster to compute. The superior performance of the direct estimator is evident both for simulated data and for neuronal population recordings from macaque primary visual cortex. Furthermore we propose generalizations of the direct estimator which measure changes in stimulus encoding across conditions and the impact of correlations on encoding and decoding, typically denoted by Ishuffle and Idiag respectively.\nAuthor Summary: A central problem in systems neuroscience is to understand how the activity of neural populations is mapped onto behavior. Neural responses in sensory areas vary substantially upon repeated presentations of the same stimulus, and this limits the reliability with which two similar stimuli can be discriminated by any read-out of neural activity. Fisher information provides a quantitative measure of the reliability of the sensory representation, and it has been used extensively to analyze neural data. Traditional methods for quantifying Fisher information rely on decoding neural activity; however, optimizing a decoder requires larger amounts of data than available in typical experiments, and as a result decoding-based estimators systematically underestimate information. Here we introduce a novel estimator that can accurately determine information with far less data, and that runs orders of magnitude faster. The estimator is based on analytical calculation, and corrects the bias that arises when estimating information directly from limited data. The analytical guarantee of an unbiased estimator and its computational simplicity will allow experimentalists to compare coding reliability across behavioral conditions and monitor it over time. "], "author_display": ["Ingmar Kanitscheider", "Ruben Coen-Cagli", "Adam Kohn", "Alexandre Pouget"], "article_type": "Research Article", "score": 0.38789153, "title_display": "Measuring Fisher Information Accurately in Correlated Neural Populations", "publication_date": "2015-06-01T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1004218"}, {"journal": "PLoS Medicine", "abstract": ["Background: The Internet is fast gaining recognition as a powerful, low-cost method to deliver health intervention and prevention programs to large numbers of young people across diverse geographic regions. The feasibility and accessibility of Internet-based health interventions in resource-limited settings, where cost-effective interventions are most needed, is unknown. To determine the utility of developing technology-based interventions in resource-limited settings, availability and patterns of usage of the Internet first need to be assessed. Methods and Findings: The Uganda Media and You Survey was a cross-sectional survey of Internet use among adolescents (ages 12\u201318 years) in Mbarara, Uganda, a municipality mainly serving a rural population in sub-Saharan Africa. Participants were randomly selected among eligible students attending one of five participating secondary day and boarding schools in Mbarara, Uganda. Of a total of 538 students selected, 93% (500) participated. Conclusions: Both the desire to use, and the actual use of, the Internet to seek sexual health and HIV/AIDS information is high among secondary school students in Mbarara. The Internet may be a promising strategy to deliver low-cost HIV/AIDS risk reduction interventions in resource-limited settings with expanding Internet access. \n        A survey among 500 adolescent pupils in rural Uganda suggests widespread interest in online information about sexual health and HIV/AIDS. Over one-third of Internet users had already searched for relevant information online, and many of the others said they would like to educate themselves about HIV/AIDS online.\n      Background.: HIV/AIDS is a major health burden in sub-Saharan Africa, including Uganda. Despite a recent reduction of the number of HIV-infected individuals, HIV transmission remains a problem among Ugandan adolescents. Recent surveys suggest that about half of sexually active adolescents do not consistently use condoms, and that young people are less knowledgeable about HIV than they were 15 years ago. Why Was This Study Done?: The Internet has a number of characteristics that make it an attractive tool in health education and HIV prevention, especially for adolescents\u2014including interactivity, privacy, the overlap between education and play, and the ability to individualize information based on an initial assessment of background conditions, interest, and knowledge. It is also thought that despite these advantages, the Internet's potential in resource-poor settings with higher HIV infection rates and limited access to other health care resources has not been explored much. This study was done to gain some initial insights on the desired and actual use of the Internet to seek sexual health and HIV/AIDS information among adolescents in Uganda. What Did the Researchers Do and Find?: They did a survey of 500 adolescent pupils randomly selected from five participating boarding schools in Mbarara, a small town in a rural part of Uganda. They asked three questions: To what extent are the adolescents exposed to computers and the Internet? Are they interested in accessing health information online? Who uses the Internet and how? Almost half of the participants said they had used the Internet at least once, and the majority said they had been online during the previous week. Most Internet users (82%) reported going online at school; 57% said they use Internet cafes, 17% access the Internet at home; and 11% at someone else's house. More than a third of all participants reported having used the Internet or computer to look up health information, and many had been looking for information on sexual health and HIV/AIDS. About two-thirds of the participants said that if Internet use were free, they would search for information on sexual health and HIV/AIDS prevention. The researchers analyzed the responses further to identify the most influential factors in whether one of the Internet users would go online to educate themselves about HIV/AIDS. They found that those participants who used the Internet more often and those who engaged in online activities like chat rooms, games, and e-mail, were more likely to search for HIV/AIDS information. On the other hand, those who went online only at school were less likely to do so. What Do These Findings Mean?: Approximately the same proportion\u2014roughly one-third\u2014of adolescents in a rural setting in Uganda reported having used the Internet to look up health-related information as of young people in the United States. Together with the result that an additional third said that they would go online to educate themselves about HIV/AIDS if Internet use was free, this study suggests that initiatives in Africa to improve online access for adolescents as well as to develop content tailored for young people in specific settings would make a difference. Additional Information.: Please access these Web sites via the online version of this summary at http://dx.doi.org/10.1371/journal.pmed.0030433. "], "author_display": ["Michele L Ybarra", "Julius Kiwanuka", "Nneka Emenyonu", "David R Bangsberg"], "article_type": "Research Article", "score": 0.38785324, "title_display": "Internet Use among Ugandan Adolescents: Implications for HIV Intervention", "publication_date": "2006-11-07T00:00:00Z", "eissn": "1549-1676", "id": "10.1371/journal.pmed.0030433"}, {"journal": "PLoS ONE", "abstract": ["\nWe show how hand-centred visual representations could develop in the primate posterior parietal and premotor cortices during visually guided learning in a self-organizing neural network model. The model incorporates trace learning in the feed-forward synaptic connections between successive neuronal layers. Trace learning encourages neurons to learn to respond to input images that tend to occur close together in time. We assume that sequences of eye movements are performed around individual scenes containing a fixed hand-object configuration. Trace learning will then encourage individual cells to learn to respond to particular hand-object configurations across different retinal locations. The plausibility of this hypothesis is demonstrated in computer simulations.\n"], "author_display": ["Juan M. Galeazzi", "Bedeho M. W. Mender", "Mariana Paredes", "James M. Tromans", "Benjamin D. Evans", "Loredana Minini", "Simon M. Stringer"], "article_type": "Research Article", "score": 0.38768798, "title_display": "A Self-Organizing Model of the Visual Development of Hand-Centred Representations", "publication_date": "2013-06-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0066272"}, {"journal": "PLoS ONE", "abstract": ["\nThis paper presents a computational model to address one prominent psychological behavior of human beings to recognize images. The basic pursuit of our method can be concluded as that differences among multiple images help visual recognition. Generally speaking, we propose a statistical framework to distinguish what kind of image features capture sufficient category information and what kind of image features are common ones shared in multiple classes. Mathematically, the whole formulation is subject to a generative probabilistic model. Meanwhile, a discriminative functionality is incorporated into the model to interpret the differences among all kinds of images. The whole Bayesian formulation is solved in an Expectation-Maximization paradigm. After finding those discriminative patterns among different images, we design an image categorization algorithm to interpret how these differences help visual recognition within the bag-of-feature framework. The proposed method is verified on a variety of image categorization tasks including outdoor scene images, indoor scene images as well as the airborne SAR images from different perspectives.\n"], "author_display": ["Yue Deng", "Yanyu Zhao", "Yebin Liu", "Qionghai Dai"], "article_type": "Research Article", "score": 0.38763243, "title_display": "Differences Help Recognition: A Probabilistic Interpretation", "publication_date": "2013-06-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0063385"}, {"journal": "PLoS ONE", "abstract": ["\n        Recently, sentence comprehension in languages other than European languages has been investigated from a cross-linguistic perspective. In this paper, we examine whether and how animacy-related semantic information is used for real-time sentence comprehension in a SOV word order language (i.e., Japanese). Twenty-three Japanese native speakers participated in this study. They read semantically reversible and non-reversible sentences with canonical word order, and those with scrambled word order. In our results, the second argument position in reversible sentences took longer to read than that in non-reversible sentences, indicating that animacy information is used in second argument processing. In contrast, for the predicate position, there was no difference in reading times, suggesting that animacy information is NOT used in the predicate position. These results are discussed using the sentence comprehension models of an SOV word order language.\n      "], "author_display": ["Satoru Yokoyama", "Kei Takahashi", "Ryuta Kawashima"], "article_type": "Research Article", "score": 0.38761714, "title_display": "Use of Semantic Information to Interpret Thematic Information for Real-Time Sentence Comprehension in an SOV Language", "publication_date": "2013-02-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0056106"}, {"journal": "PLoS ONE", "abstract": ["\nAs a major class of noncoding RNAs, long noncoding RNAs (lncRNAs) have been implicated in various critical biological processes. Accumulating researches have linked dysregulations and mutations of lncRNAs to a variety of human disorders and diseases. However, to date, only a few human lncRNAs have been associated with diseases. Therefore, it is very important to develop a computational method to globally predict potential associated diseases for human lncRNAs. In this paper, we developed a computational framework to accomplish this by combining human lncRNA expression profiles, gene expression profiles, and human disease-associated gene data. Applying this framework to available human long intergenic noncoding RNAs (lincRNAs) expression data, we showed that the framework has reliable accuracy. As a result, for non-tissue-specific lincRNAs, the AUC of our algorithm is 0.7645, and the prediction accuracy is about 89%. This study will be helpful for identifying novel lncRNAs for human diseases, which will help in understanding the roles of lncRNAs in human diseases and facilitate treatment. The corresponding codes for our method and the predicted results are all available at http://asdcd.amss.ac.cn/MingXiLiu/lncRNA-disease.html.\n"], "author_display": ["Ming-Xi Liu", "Xing Chen", "Geng Chen", "Qing-Hua Cui", "Gui-Ying Yan"], "article_type": "Research Article", "score": 0.38759285, "title_display": "A Computational Framework to Infer Human Disease-Associated Long Noncoding RNAs", "publication_date": "2014-01-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0084408"}, {"abstract": ["\n        We present a probabilistic model for natural images that is based on mixtures of Gaussian scale mixtures and a simple multiscale representation. We show that it is able to generate images with interesting higher-order correlations when trained on natural images or samples from an occlusion-based model. More importantly, our multiscale model allows for a principled evaluation. While it is easy to generate visually appealing images, we demonstrate that our model also yields the best performance reported to date when evaluated with respect to the cross-entropy rate, a measure tightly linked to the average log-likelihood. The ability to quantitatively evaluate our model differentiates it from other multiscale models, for which evaluation of these kinds of measures is usually intractable.\n      "], "author_display": ["Lucas Theis", "Reshad Hosseini", "Matthias Bethge"], "article_type": "Research Article", "score": 0.38751146, "title_display": "Mixtures of Conditional Gaussian Scale Mixtures Applied to Multiscale Image Representations", "publication_date": "2012-07-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0039857"}, {"journal": "PLoS ONE", "abstract": ["\nToday, the volume of data and knowledge of processes necessitates more complex models that integrate all available information. This handicap has been solved thanks to the technological advances in both software and hardware. Computational tools available today have allowed developing a new family of models, known as computational models. The description of these models is difficult as they can not be expressed analytically, and it is therefore necessary to create protocols that serve as guidelines for future users. The Population Dynamics P systems models (PDP) are a novel and effective computational tool to model complex problems, are characterized by the ability to work in parallel (simultaneously interrelating different processes), are modular and have a high computational efficiency. However, the difficulty of describing these models therefore requires a protocol to unify the presentation and the steps to follow. We use two case studies to demonstrate the use and implementation of these computational models for population dynamics and ecological process studies, discussing briefly their potential applicability to simulate complex ecosystem dynamics.\n"], "author_display": ["Maria \u00c0ngels Colomer", "Antoni Margalida", "Mario J. P\u00e9rez-Jim\u00e9nez"], "article_type": "Research Article", "score": 0.38691634, "title_display": "Population Dynamics P System (PDP) Models: A Standardized Protocol for Describing and Applying Novel Bio-Inspired Computing Tools", "publication_date": "2013-04-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0060698"}, {"journal": "PLoS ONE", "abstract": ["\nCollections of biological specimens are fundamental to scientific understanding and characterization of natural diversity\u2014past, present and future. This paper presents a system for liberating useful information from physical collections by bringing specimens into the digital domain so they can be more readily shared, analyzed, annotated and compared. It focuses on insects and is strongly motivated by the desire to accelerate and augment current practices in insect taxonomy which predominantly use text, 2D diagrams and images to describe and characterize species. While these traditional kinds of descriptions are informative and useful, they cannot cover insect specimens \u201cfrom all angles\u201d and precious specimens are still exchanged between researchers and collections for this reason. Furthermore, insects can be complex in structure and pose many challenges to computer vision systems. We present a new prototype for a practical, cost-effective system of off-the-shelf components to acquire natural-colour 3D models of insects from around 3 mm to 30 mm in length. (\u201cNatural-colour\u201d is used to contrast with \u201cfalse-colour\u201d, i.e., colour generated from, or applied to, gray-scale data post-acquisition.) Colour images are captured from different angles and focal depths using a digital single lens reflex (DSLR) camera rig and two-axis turntable. These 2D images are processed into 3D reconstructions using software based on a visual hull algorithm. The resulting models are compact (around 10 megabytes), afford excellent optical resolution, and can be readily embedded into documents and web pages, as well as viewed on mobile devices. The system is portable, safe, relatively affordable, and complements the sort of volumetric data that can be acquired by computed tomography. This system provides a new way to augment the description and documentation of insect species holotypes, reducing the need to handle or ship specimens. It opens up new opportunities to collect data for research, education, art, entertainment, biodiversity assessment and biosecurity control.\n"], "author_display": ["Chuong V. Nguyen", "David R. Lovell", "Matt Adcock", "John La Salle"], "article_type": "Research Article", "score": 0.38670015, "title_display": "Capturing Natural-Colour 3D Models of Insects for Species Discovery and Diagnostics", "publication_date": "2014-04-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0094346"}, {"journal": "PLoS ONE", "abstract": ["\n         MrBayes is model-based phylogenetic inference tool using Bayesian statistics. However, model-based assessment of phylogenetic trees adds to the computational burden of tree-searching, and so poses significant computational challenges. Graphics Processing Units (GPUs) have been proposed as high performance, low cost acceleration platforms and several parallelized versions of the Metropolis Coupled Markov Chain Mote Carlo (MC3) algorithm in MrBayes have been presented that can run on GPUs. However, some bottlenecks decrease the efficiency of these implementations. To address these bottlenecks, we propose a tight GPU MC3 (tgMC3) algorithm. tgMC3 implements a different architecture from the one-to-one acceleration architecture employed in previously proposed methods. It merges multiply discrete GPU kernels according to the data dependency and hence decreases the number of kernels launched and the complexity of data transfer. We implemented tgMC3 and made performance comparisons with an earlier proposed algorithm, nMC3, and also with MrBayes MC3 under serial and multiply concurrent CPU processes. All of the methods were benchmarked on the same computing node from DEGIMA. Experiments indicate that the tgMC3 method outstrips nMC3 (v1.0) with speedup factors from 2.1 to 2.7\u00d7. In addition, tgMC3 outperforms the serial MrBayes MC3 by a factor of 6 to 30\u00d7 when using a single GTX480 card, whereas a speedup factor of around 51\u00d7 can be achieved by using two GTX 480 cards on relatively long sequences. Moreover, tgMC3 was compared with MrBayes accelerated by BEAGLE, and achieved speedup factors from 3.7 to 5.7\u00d7. The reported performance improvement of tgMC3 is significant and appears to scale well with increasing dataset sizes. In addition, the strategy proposed in tgMC3 could benefit the acceleration of other Bayesian-based phylogenetic analysis methods using GPUs.\n      "], "author_display": ["Cheng Ling", "Tsuyoshi Hamada", "Jianing Bai", "Xianbin Li", "Douglas Chesters", "Weimin Zheng", "Weifeng Shi"], "article_type": "Research Article", "score": 0.38636348, "title_display": "MrBayes tgMC<sup>3</sup>: A Tight GPU Implementation of MrBayes", "publication_date": "2013-04-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0060667"}, {"journal": "PLOS ONE", "abstract": ["\nWe show that the history of play in a population game contains exploitable information that can be successfully used by sophisticated strategies to defeat memory-one opponents, including zero determinant strategies. The history allows a player to label opponents by their strategies, enabling a player to determine the population distribution and to act differentially based on the opponent\u2019s strategy in each pairwise interaction. For the Prisoner\u2019s Dilemma, these advantages lead to the natural formation of cooperative coalitions among similarly behaving players and eventually to unilateral defection against opposing player types. We show analytically and empirically that optimal play in population games depends strongly on the population distribution. For example, the optimal strategy for a minority player type against a resident TFT population is ALLC, while for a majority player type the optimal strategy versus TFT players is ALLD. Such behaviors are not accessible to memory-one strategies. Drawing inspiration from Sun Tzu\u2019s the Art of War, we implemented a non-memory-one strategy for population games based on techniques from machine learning and statistical inference that can exploit the history of play in this manner. Via simulation we find that this strategy is essentially uninvadable and can successfully invade (significantly more likely than a neutral mutant) essentially all known memory-one strategies for the Prisoner\u2019s Dilemma, including ALLC (always cooperate), ALLD (always defect), tit-for-tat (TFT), win-stay-lose-shift (WSLS), and zero determinant (ZD) strategies, including extortionate and generous strategies.\n"], "author_display": ["Christopher Lee", "Marc Harper", "Dashiell Fryer"], "article_type": "Research Article", "score": 0.3862434, "title_display": "The Art of War: Beyond Memory-one Strategies in Population Games", "publication_date": "2015-03-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0120625"}, {"journal": "PLOS ONE", "abstract": ["\nMinimal hardware implementations able to cope with the processing of large amounts of data in reasonable times are highly desired in our information-driven society. In this work we review the application of stochastic computing to probabilistic-based pattern-recognition analysis of huge database sets. The proposed technique consists in the hardware implementation of a parallel architecture implementing a similarity search of data with respect to different pre-stored categories. We design pulse-based stochastic-logic blocks to obtain an efficient pattern recognition system. The proposed architecture speeds up the screening process of huge databases by a factor of 7 when compared to a conventional digital implementation using the same hardware area.\n"], "author_display": ["Antoni Morro", "Vincent Canals", "Antoni Oliver", "Miquel L. Alomar", "Josep L. Rossello"], "article_type": "Research Article", "score": 0.38617933, "title_display": "Ultra-Fast Data-Mining Hardware Architecture Based on Stochastic Computing", "publication_date": "2015-05-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0124176"}, {"journal": "PLoS ONE", "abstract": ["\nRecent developments in modern computational accelerators like Graphics Processing Units (GPUs) and coprocessors provide great opportunities for making scientific applications run faster than ever before. However, efficient parallelization of scientific code using new programming tools like CUDA requires a high level of expertise that is not available to many scientists. This, plus the fact that parallelized code is usually not portable to different architectures, creates major challenges for exploiting the full capabilities of modern computational accelerators. In this work, we sought to overcome these challenges by studying how to achieve both automated parallelization using OpenACC and enhanced portability using OpenCL. We applied our parallelization schemes using GPUs as well as Intel Many Integrated Core (MIC) coprocessor to reduce the run time of wave propagation simulations. We used a well-established 2D cardiac action potential model as a specific case-study. To the best of our knowledge, we are the first to study auto-parallelization of 2D cardiac wave propagation simulations using OpenACC. Our results identify several approaches that provide substantial speedups. The OpenACC-generated GPU code achieved more than  speedup above the sequential implementation and required the addition of only a few OpenACC pragmas to the code. An OpenCL implementation provided speedups on GPUs of at least  faster than the sequential implementation and  faster than a parallelized OpenMP implementation. An implementation of OpenMP on Intel MIC coprocessor provided speedups of  with only a few code changes to the sequential implementation. We highlight that OpenACC provides an automatic, efficient, and portable approach to achieve parallelization of 2D cardiac wave simulations on GPUs. Our approach of using OpenACC, OpenCL, and OpenMP to parallelize this particular model on modern computational accelerators should be applicable to other computational models of wave propagation in multi-dimensional media.\n"], "author_display": ["Wei Wang", "Lifan Xu", "John Cavazos", "Howie H. Huang", "Matthew Kay"], "article_type": "Research Article", "score": 0.38617024, "title_display": "Fast Acceleration of 2D Wave Propagation Simulations Using Modern Computational Accelerators", "publication_date": "2014-01-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0086484"}, {"journal": "PLoS ONE", "abstract": ["\n        The influence of noise on oscillatory motion is a subject of permanent interest, both for fundamental and practical reasons. Cells respond properly to external stimuli by using noisy systems. We have clarified the effect of intrinsic noise on the dynamics in the human cancer cells following gamma irradiation. It is shown that the large amplification and increasing mutual information with delay are due to coherence resonance. Furthermore, frequency domain analysis is used to study the mechanisms.\n      "], "author_display": ["Bo Liu", "Shiwei Yan", "Xingfa Gao"], "article_type": "Research Article", "score": 0.38583452, "title_display": "Noise Amplification in Human Tumor Suppression following Gamma Irradiation", "publication_date": "2011-08-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0022487"}, {"abstract": ["\n        In many cases, neurons process information carried by the precise timings of spikes. Here we show how neurons can learn to generate specific temporally precise output spikes in response to input patterns of spikes having precise timings, thus processing and memorizing information that is entirely temporally coded, both as input and as output. We introduce two new supervised learning rules for spiking neurons with temporal coding of information (chronotrons), one that provides high memory capacity (E-learning), and one that has a higher biological plausibility (I-learning). With I-learning, the neuron learns to fire the target spike trains through synaptic changes that are proportional to the synaptic currents at the timings of real and target output spikes. We study these learning rules in computer simulations where we train integrate-and-fire neurons. Both learning rules allow neurons to fire at the desired timings, with sub-millisecond precision. We show how chronotrons can learn to classify their inputs, by firing identical, temporally precise spike trains for different inputs belonging to the same class. When the input is noisy, the classification also leads to noise reduction. We compute lower bounds for the memory capacity of chronotrons and explore the influence of various parameters on chronotrons' performance. The chronotrons can model neurons that encode information in the time of the first spike relative to the onset of salient stimuli or neurons in oscillatory networks that encode information in the phases of spikes relative to the background oscillation. Our results show that firing one spike per cycle optimizes memory capacity in neurons encoding information in the phase of firing relative to a background rhythm.\n      "], "author_display": ["R\u0103zvan V. Florian"], "article_type": "Research Article", "score": 0.38574412, "title_display": "The Chronotron: A Neuron That Learns to Fire Temporally Precise Spike Patterns", "publication_date": "2012-08-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0040233"}, {"journal": "PLoS ONE", "abstract": ["\nInformation acquisition, the gathering and interpretation of sensory information, is a basic function of mobile organisms. We describe a new method for measuring this ability in humans, using free-recall responses to sensory stimuli which are scored objectively using a \u201cwisdom of crowds\u201d approach. As an example, we demonstrate this metric using perception of video stimuli. Immediately after viewing a 30 s video clip, subjects responded to a prompt to give a short description of the clip in natural language. These responses were scored automatically by comparison to a dataset of responses to the same clip by normally-sighted viewers (the crowd). In this case, the normative dataset consisted of responses to 200 clips by 60 subjects who were stratified by age (range 22 to 85y) and viewed the clips in the lab, for 2,400 responses, and by 99 crowdsourced participants (age range 20 to 66y) who viewed clips in their Web browser, for 4,000 responses. We compared different algorithms for computing these similarities and found that a simple count of the words in common had the best performance. It correctly matched 75% of the lab-sourced and 95% of crowdsourced responses to their corresponding clips. We validated the measure by showing that when the amount of information in the clip was degraded using defocus lenses, the shared word score decreased across the five predetermined visual-acuity levels, demonstrating a dose-response effect (N\u200a=\u200a15). This approach, of scoring open-ended immediate free recall of the stimulus, is applicable not only to video, but also to other situations where a measure of the information that is successfully acquired is desirable. Information acquired will be affected by stimulus quality, sensory ability, and cognitive processes, so our metric can be used to assess each of these components when the others are controlled.\n"], "author_display": ["Daniel R. Saunders", "Peter J. Bex", "Dylan J. Rose", "Russell L. Woods"], "article_type": "Research Article", "score": 0.38561282, "title_display": "Measuring Information Acquisition from Sensory Input Using Automated Scoring of Natural-Language Descriptions", "publication_date": "2014-04-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0093251"}, {"journal": "PLoS Computational Biology", "abstract": ["DNA signatures are nucleotide sequences that can be used to detect the presence of an organism and to distinguish that organism from all other species. Here we describe Insignia, a new, comprehensive system for the rapid identification of signatures in the genomes of bacteria and viruses. With the availability of hundreds of complete bacterial and viral genome sequences, it is now possible to use computational methods to identify signature sequences in all of these species, and to use these signatures as the basis for diagnostic assays to detect and genotype microbes in both environmental and clinical samples. The success of such assays critically depends on the methods used to identify signatures that properly differentiate between the target genomes and the sample background. We have used Insignia to compute accurate signatures for most bacterial genomes and made them available through our Web site. A sample of these signatures has been successfully tested on a set of 46 Vibrio cholerae strains, and the results indicate that the signatures are highly sensitive for detection as well as specific for discrimination between these strains and their near relatives. Our approach, whereby the entire genomic complement of organisms are compared to identify probe targets, is a promising method for diagnostic assay development, and it provides assay designers with the flexibility to choose probes from the most relevant genes or genomic regions. The Insignia system is freely accessible via a Web interface and has been released as open source software at: http://insignia.cbcb.umd.edu.: Now that the genome sequences of hundreds of bacteria and viruses are known, we can design tests that will rapidly detect the presence of these species based solely on their DNA. Such tests have a wide range of applications, from diagnosing infections to detecting harmful microbes in a water supply. These tests can detect a pathogen in a complex mixture of organic material by recognizing short, distinguishing sequences\u2014called DNA signatures\u2014that occur in the pathogen and not in any other species. We present Insignia, a new computational system that identifies DNA signatures of any length in bacterial and viral genomes. Insignia uses highly efficient algorithms to compare sequenced bacterial and viral genomes against each other and to additional background genomes including plants, animals, and human. These comparisons are stored in a database and used to rapidly compute signatures for any particular target species. To maximize its utility for the community, we have made Insignia available as free, open-source software and as a Web application. We have also validated 50 Insignia-designed assays on a panel of 46 strains of Vibrio cholerae, and our results show that the signatures are both sensitive and specific. "], "author_display": ["Adam M Phillippy", "Jacquline A Mason", "Kunmi Ayanbule", "Daniel D Sommer", "Elisa Taviani", "Anwar Huq", "Rita R Colwell", "Ivor T Knight", "Steven L Salzberg"], "article_type": "Research Article", "score": 0.385363, "title_display": "Comprehensive DNA Signature Discovery and Validation", "publication_date": "2007-05-18T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.0030098"}, {"journal": "PLoS ONE", "abstract": ["\nThis paper presents a method for selecting Regions of Interest (ROI) in brain Magnetic Resonance Imaging (MRI) for diagnostic purposes, using statistical learning and vector quantization techniques. The proposed method models the distribution of GM and WM tissues grouping the voxels belonging to each tissue in ROIs associated to a specific neurological disorder. Tissue distribution of normal and abnormal images is modelled by a Self-Organizing map (SOM), generating a set of representative prototypes, and the receptive field (RF) of each SOM prototype defines a ROI. Moreover, the proposed method computes the relative importance of each ROI by means of its discriminative power. The devised method has been assessed using 818 images from the Alzheimer's disease Neuroimaging Initiative (ADNI) which were previously segmented through Statistical Parametric Mapping (SPM). The proposed algorithm was used over these images to parcel ROIs associated to the Alzheimer's Disease (AD). Additionally, this method can be used to extract a reduced set of discriminative features for classification, since it compresses discriminative information contained in the brain. Voxels marked by ROIs which were computed using the proposed method, yield classification results up to 90% of accuracy for controls (CN) and Alzheimer's disease (AD) patients, and 84% of accuracy for Mild Cognitive Impairment (MCI) and AD patients.\n"], "author_display": ["Andr\u00e9s Ortiz", "Juan M. G\u00f3rriz", "Javier Ram\u00edrez", "Francisco J. Martinez-Murcia", "for the Alzheimer's Disease Neuroimaging Initiative "], "article_type": "Research Article", "score": 0.38520303, "title_display": "Automatic ROI Selection in Structural Brain MRI Using SOM 3D Projection", "publication_date": "2014-04-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0093851"}, {"journal": "PLoS ONE", "abstract": ["\nThe hypothesis of a Hierarchy of the Sciences, first formulated in the 19th century, predicts that, moving from simple and general phenomena (e.g. particle dynamics) to complex and particular (e.g. human behaviour), researchers lose ability to reach theoretical and methodological consensus. This hypothesis places each field of research along a continuum of complexity and \u201csoftness\u201d, with profound implications for our understanding of scientific knowledge. Today, however, the idea is still unproven and philosophically overlooked, too often confused with simplistic dichotomies that contrast natural and social sciences, or science and the humanities. Empirical tests of the hypothesis have usually compared few fields and this, combined with other limitations, makes their results contradictory and inconclusive. We verified whether discipline characteristics reflect a hierarchy, a dichotomy or neither, by sampling nearly 29,000 papers published contemporaneously in 12 disciplines and measuring a set of parameters hypothesised to reflect theoretical and methodological consensus. The biological sciences had in most cases intermediate values between the physical and the social, with bio-molecular disciplines appearing harder than zoology, botany or ecology. In multivariable analyses, most of these parameters were independent predictors of the hierarchy, even when mathematics and the humanities were included. These results support a \u201cgradualist\u201d view of scientific knowledge, suggesting that the Hierarchy of the Sciences provides the best rational framework to understand disciplines' diversity. A deeper grasp of the relationship between subject matter's complexity and consensus could have profound implications for how we interpret, publish, popularize and administer scientific research.\n"], "author_display": ["Daniele Fanelli", "Wolfgang Gl\u00e4nzel"], "article_type": "Research Article", "score": 0.38513038, "title_display": "Bibliometric Evidence for a Hierarchy of the Sciences", "publication_date": "2013-06-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0066938"}, {"journal": "PLoS ONE", "abstract": ["Background: In a recent controversial essay, published by JPA Ioannidis in PLoS Medicine, it has been argued that in some research fields, most of the published findings are false. Based on theoretical reasoning it can be shown that small effect sizes, error-prone tests, low priors of the tested hypotheses and biases in the evaluation and publication of research findings increase the fraction of false positives. These findings raise concerns about the reliability of research. However, they are based on a very simple scenario of scientific research, where single tests are used to evaluate independent hypotheses. Methodology/Principal Findings: In this study, we present computer simulations and experimental approaches for analyzing more realistic scenarios. In these scenarios, research tasks are solved sequentially, i.e. subsequent tests can be chosen depending on previous results. We investigate simple sequential testing and scenarios where only a selected subset of results can be published and used for future rounds of test choice. Results from computer simulations indicate that for the tasks analyzed in this study, the fraction of false among the positive findings declines over several rounds of testing if the most informative tests are performed. Our experiments show that human subjects frequently perform the most informative tests, leading to a decline of false positives as expected from the simulations. Conclusions/Significance: For the research tasks studied here, findings tend to become more reliable over time. We also find that the performance in those experimental settings where not all performed tests could be published turned out to be surprisingly inefficient. Our results may help optimize existing procedures used in the practice of scientific research and provide guidance for the development of novel forms of scholarly communication. "], "author_display": ["Thomas Pfeiffer", "David G. Rand", "Anna Dreber"], "article_type": "Research Article", "score": 0.38500366, "title_display": "Decision-Making in Research Tasks with Sequential Testing", "publication_date": "2009-02-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0004607"}, {"journal": "PLoS Computational Biology", "abstract": ["\n        The cerebellum is a brain structure which has been traditionally devoted to supervised learning. According to this theory, plasticity at the Parallel Fiber (PF) to Purkinje Cell (PC) synapses is guided by the Climbing fibers (CF), which encode an \u2018error signal\u2019. Purkinje cells have thus been modeled as perceptrons, learning input/output binary associations. At maximal capacity, a perceptron with excitatory weights expresses a large fraction of zero-weight synapses, in agreement with experimental findings. However, numerous experiments indicate that the firing rate of Purkinje cells varies in an analog, not binary, manner. In this paper, we study the perceptron with analog inputs and outputs. We show that the optimal input has a sparse binary distribution, in good agreement with the burst firing of the Granule cells. In addition, we show that the weight distribution consists of a large fraction of silent synapses, as in previously studied binary perceptron models, and as seen experimentally.\n      Author Summary: Learning properties of neuronal networks have been extensively studied using methods from statistical physics. However, most of these studies ignore a fundamental constraint in networks of real neurons: synapses are either excitatory or inhibitory, and cannot change sign during learning. Here, we characterize the optimal storage properties of an analog perceptron with excitatory synapses, as a simplified model for cerebellar Purkinje cells. The information storage capacity is shown to be optimized when inputs have a sparse binary distribution, while the weight distribution at maximal capacity consists of a large amount of zero-weight synapses. Both features are in agreement with electrophysiological data. "], "author_display": ["Claudia Clopath", "Nicolas Brunel"], "article_type": "Research Article", "score": 0.38496414, "title_display": "Optimal Properties of Analog Perceptrons with Excitatory Weights", "publication_date": "2013-02-21T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1002919"}, {"journal": "PLoS ONE", "abstract": ["Background: The expending and invasive features of tumor nests could reflect the malignant biological behaviors of breast invasive ductal carcinoma. Useful information on cancer invasiveness hidden within tumor nests could be extracted and analyzed by computer image processing and big data analysis. Methods: Tissue microarrays from invasive ductal carcinoma (n\u200a=\u200a202) were first stained with cytokeratin by immunohistochemical method to clearly demarcate the tumor nests. Then an expert-aided computer analysis system was developed to study the mathematical and geometrical features of the tumor nests. Computer recognition system and imaging analysis software extracted tumor nests information, and mathematical features of tumor nests were calculated. The relationship between tumor nests mathematical parameters and patients' 5-year disease free survival was studied. Results: There were 8 mathematical parameters extracted by expert-aided computer analysis system. Three mathematical parameters (number, circularity and total perimeter) with area under curve >0.5 and 4 mathematical parameters (average area, average perimeter, total area/total perimeter, average (area/perimeter)) with area under curve <0.5 in ROC analysis were combined into integrated parameter 1 and integrated parameter 2, respectively. Multivariate analysis showed that integrated parameter 1 (P\u200a=\u200a0.040) was independent prognostic factor of patients' 5-year disease free survival. The hazard risk ratio of integrated parameter 1 was 1.454 (HR 95% CI [1.017\u20132.078]), higher than that of N stage (HR 1.396, 95% CI [1.125\u20131.733]) and hormone receptor status (HR 0.575, 95% CI [0.353\u20130.936]), but lower than that of histological grading (HR 3.370, 95% CI [1.125\u20135.364]) and T stage (HR 1.610, 95% CI [1.026 \u20132.527]). Conclusions: This study indicated integrated parameter 1 of mathematical features (number, circularity and total perimeter) of tumor nests could be a useful parameter to predict the prognosis of early stage breast invasive ductal carcinoma. "], "author_display": ["Lin-Wei Wang", "Ai-Ping Qu", "Jing-Ping Yuan", "Chuang Chen", "Sheng-Rong Sun", "Ming-Bai Hu", "Juan Liu", "Yan Li"], "article_type": "Research Article", "score": 0.3848782, "title_display": "Computer-Based Image Studies on Tumor Nests Mathematical Features of Breast Cancer and Their Clinical Prognostic Value", "publication_date": "2013-12-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0082314"}, {"journal": "PLoS ONE", "abstract": ["\n        The rejection of unfair offers can be affected by both negative emotions (e.g. anger and moral disgust) and deliberate cognitive processing of behavioral consequences (e.g. concerns of maintaining social fairness and protecting personal reputation). However, whether negative emotions are sufficient to motivate this behavior is still controversial. With modified ultimatum games, a recent study (Yamagishi T, et al. (2009) Proc Natl Acad Sci USA 106\u223611520\u201311523) found that people reject unfair offers even when this behavior increases inequity, and even when they could not communicate to the proposers. Yamagishi suggested that rejection of unfair offers could occurr without people\u2019s concerning of maintaining social fairness, and could be driven by negative emotions. However, as anonymity was not sufficiently guaranteed in Yamagishi\u2019s study, the rejection rates in their experiments may have been influenced by people\u2019s concerns of protecting personal reputation (reputational concerns) in addition to negative emotions; thus, it was unclear whether the rejection was driven by negative emotions, or by reputational concerns, or both. In the present study, with specific methods to ensure anonymity, the effect of reputational concerns was successfully ruled out. We found that in a private situation in which rejection could not be driven by reputational concerns, the rejection rates of unfair offers were significantly larger than zero, and in public situations in which rejection rates could be influenced by both negative emotions and reputational concerns, rejection rates were significantly higher than that in the private situation. These results, together with Yamagishi\u2019s findings, provided more complete evidence suggesting (a) that the rejection of unfair offers can be driven by negative emotions and (b) that deliberate cognitive processing of the consequences of the behavior can increase the rejection rate, which may benefit social cooperation.\n      "], "author_display": ["Ning Ma", "Nan Li", "Xiao-Song He", "De-Lin Sun", "Xiaochu Zhang", "Da-Ren Zhang"], "article_type": "Research Article", "score": 0.38482934, "title_display": "Rejection of Unfair Offers Can Be Driven by Negative Emotions, Evidence from Modified Ultimatum Games with Anonymity", "publication_date": "2012-06-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0039619"}, {"journal": "PLOS ONE", "abstract": ["\nThis paper proposes a modified BFGS formula using a trust region model for solving nonsmooth convex minimizations by using the Moreau-Yosida regularization (smoothing) approach and a new secant equation with a BFGS update formula. Our algorithm uses the function value information and gradient value information to compute the Hessian. The Hessian matrix is updated by the BFGS formula rather than using second-order information of the function, thus decreasing the workload and time involved in the computation. Under suitable conditions, the algorithm converges globally to an optimal solution. Numerical results show that this algorithm can successfully solve nonsmooth unconstrained convex problems.\n"], "author_display": ["Zengru Cui", "Gonglin Yuan", "Zhou Sheng", "Wenjie Liu", "Xiaoliang Wang", "Xiabin Duan"], "article_type": "Research Article", "score": 0.38475865, "title_display": "A Modified BFGS Formula Using a Trust Region Model for Nonsmooth Convex Minimizations", "publication_date": "2015-10-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0140606"}, {"journal": "PLoS ONE", "abstract": ["\n        Electrical stimulation of the nervous system for therapeutic purposes, such as deep brain stimulation in the treatment of Parkinson\u2019s disease, has been used for decades. Recently, increased attention has focused on using microstimulation to restore functions as diverse as somatosensation and memory. However, how microstimulation changes the neural substrate is still not fully understood. Microstimulation may cause cortical changes that could either compete with or complement natural neural processes, and could result in neuroplastic changes rendering the region dysfunctional or even epileptic. As part of our efforts to produce neuroprosthetic devices and to further study the effects of microstimulation on the cortex, we stimulated and recorded from microelectrode arrays in the hand area of the primary somatosensory cortex (area 1) in two awake macaque monkeys. We applied a simple neuroprosthetic microstimulation protocol to a pair of electrodes in the area 1 array, using either random pulses or pulses time-locked to the recorded spiking activity of a reference neuron. This setup was replicated using a computer model of the thalamocortical system, which consisted of 1980 spiking neurons distributed among six cortical layers and two thalamic nuclei. Experimentally, we found that spike-triggered microstimulation induced cortical plasticity, as shown by increased unit-pair mutual information, while random microstimulation did not. In addition, there was an increased response to touch following spike-triggered microstimulation, along with decreased neural variability. The computer model successfully reproduced both qualitative and quantitative aspects of the experimental findings. The physiological findings of this study suggest that even simple microstimulation protocols can be used to increase somatosensory information flow.\n      "], "author_display": ["Weiguo Song", "Cliff C. Kerr", "William W. Lytton", "Joseph T. Francis"], "article_type": "Research Article", "score": 0.38452423, "title_display": "Cortical Plasticity Induced by Spike-Triggered Microstimulation in Primate Somatosensory Cortex", "publication_date": "2013-03-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0057453"}, {"journal": "PLOS ONE", "abstract": ["\nConficker is a computer worm that erupted on the Internet in 2008. It is unique in combining three different spreading strategies: local probing, neighbourhood probing, and global probing. We propose a mathematical model that combines three modes of spreading: local, neighbourhood, and global, to capture the worm\u2019s spreading behaviour. The parameters of the model are inferred directly from network data obtained during the first day of the Conficker epidemic. The model is then used to explore the tradeoff between spreading modes in determining the worm\u2019s effectiveness. Our results show that the Conficker epidemic is an example of a critically hybrid epidemic, in which the different modes of spreading in isolation do not lead to successful epidemics. Such hybrid spreading strategies may be used beneficially to provide the most effective strategies for promulgating information across a large population. When used maliciously, however, they can present a dangerous challenge to current internet security protocols.\n"], "author_display": ["Changwang Zhang", "Shi Zhou", "Benjamin M. Chain"], "article_type": "Research Article", "score": 0.38447186, "title_display": "Hybrid Epidemics\u2014A Case Study on Computer Worm Conficker", "publication_date": "2015-05-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0127478"}, {"journal": "PLoS ONE", "abstract": ["\n        Increasing public interest in science information in a digital and 2.0 science era promotes a dramatically, rapid and deep change in science itself. The emergence and expansion of new technologies and internet-based tools is leading to new means to improve scientific methodology and communication, assessment, promotion and certification. It allows methods of acquisition, manipulation and storage, generating vast quantities of data that can further facilitate the research process. It also improves access to scientific results through information sharing and discussion. Content previously restricted only to specialists is now available to a wider audience. This context requires new management systems to make scientific knowledge more accessible and useable, including new measures to evaluate the reach of scientific information. The new science and research quality measures are strongly related to the new online technologies and services based in social media. Tools such as blogs, social bookmarks and online reference managers, Twitter and others offer alternative, transparent and more comprehensive information about the active interest, usage and reach of scientific publications. Another of these new filters is the Research Blogging platform, which was created in 2007 and now has over 1,230 active blogs, with over 26,960 entries posted about peer-reviewed research on subjects ranging from Anthropology to Zoology. This study takes a closer look at RB, in order to get insights into its contribution to the rapidly changing landscape of scientific communication.\n      "], "author_display": ["Sibele Fausto", "Fabio A. Machado", "Luiz Fernando J. Bento", "Atila Iamarino", "Tatiana R. Nahas", "David S. Munger"], "article_type": "Research Article", "score": 0.3841121, "title_display": "Research Blogging: Indexing and Registering the Change in Science 2.0", "publication_date": "2012-12-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0050109"}, {"journal": "PLOS Computational Biology", "abstract": ["\nQuorum sensing is the regulation of gene expression in response to changes in cell density. To measure their cell density, bacterial populations produce and detect diffusible molecules called autoinducers. Individual bacteria internally represent the external concentration of autoinducers via the level of monitor proteins. In turn, these monitor proteins typically regulate both their own production and the production of autoinducers, thereby establishing internal and external feedbacks. Here, we ask whether feedbacks can increase the information available to cells about their local density. We quantify available information as the mutual information between the abundance of a monitor protein and the local cell density for biologically relevant models of quorum sensing. Using variational methods, we demonstrate that feedbacks can increase information transmission, allowing bacteria to resolve up to two additional ranges of cell density when compared with bistable quorum-sensing systems. Our analysis is relevant to multi-agent systems that track an external driver implicitly via an endogenously generated signal.\nAuthor Summary: Bacteria regulate gene expression in response to changes in cell density in a process called quorum sensing. To synchronize their gene-expression programs, these bacteria need to glean as much information as possible about their cell density. Our study is the first to physically model the flow of information in a quorum-sensing microbial community, wherein the internal regulator of the individuals response tracks the external cell density via an endogenously generated shared signal. Combining information theory and Lagrangian formalism, we find that quorum-sensing systems can improve their information capabilities by tuning circuit feedbacks. Our analysis suggests that achieving information benefit via feedback requires dedicated systems to control gene expression noise, such as sRNA-based regulation. "], "author_display": ["Thibaud Taillefumier", "Ned S. Wingreen"], "article_type": "Research Article", "score": 0.38397184, "title_display": "Optimal Census by Quorum Sensing", "publication_date": "2015-05-12T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1004238"}, {"journal": "PLOS ONE", "abstract": ["\nThe spatial pooling method such as spatial pyramid matching (SPM) is very crucial in the bag of features model used in image classification. SPM partitions the image into a set of regular grids and assumes that the spatial layout of all visual words obey the uniform distribution over these regular grids. However, in practice, we consider that different visual words should obey different spatial layout distributions. To improve SPM, we develop a novel spatial pooling method, namely spatial distribution pooling (SDP). The proposed SDP method uses an extension model of Gauss mixture model to estimate the spatial layout distributions of the visual vocabulary. For each visual word type, SDP can generate a set of flexible grids rather than the regular grids from the traditional SPM. Furthermore, we can compute the grid weights for visual word tokens according to their spatial coordinates. The experimental results demonstrate that SDP outperforms the traditional spatial pooling methods, and is competitive with the state-of-the-art classification accuracy on several challenging image datasets.\n"], "author_display": ["Guangyu Mu", "Ying Liu", "Limin Wang"], "article_type": "Research Article", "score": 0.38396677, "title_display": "Considering the Spatial Layout Information of Bag of Features (BoF) Framework for Image Classification", "publication_date": "2015-06-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0131164"}, {"journal": "PLoS ONE", "abstract": ["\n        Cheminformatics is the application of informatics techniques to solve chemical problems in silico. There are many areas in biology where cheminformatics plays an important role in computational research, including metabolism, proteomics, and systems biology. One critical aspect in the application of cheminformatics in these fields is the accurate exchange of data, which is increasingly accomplished through the use of ontologies. Ontologies are formal representations of objects and their properties using a logic-based ontology language. Many such ontologies are currently being developed to represent objects across all the domains of science. Ontologies enable the definition, classification, and support for querying objects in a particular domain, enabling intelligent computer applications to be built which support the work of scientists both within the domain of interest and across interrelated neighbouring domains. Modern chemical research relies on computational techniques to filter and organise data to maximise research productivity. The objects which are manipulated in these algorithms and procedures, as well as the algorithms and procedures themselves, enjoy a kind of virtual life within computers. We will call these information entities. Here, we describe our work in developing an ontology of chemical information entities, with a primary focus on data-driven research and the integration of calculated properties (descriptors) of chemical entities within a semantic web context. Our ontology distinguishes algorithmic, or procedural information from declarative, or factual information, and renders of particular importance the annotation of provenance to calculated data. The Chemical Information Ontology is being developed as an open collaborative project. More details, together with a downloadable OWL file, are available at http://code.google.com/p/semanticchemistry/ (license: CC-BY-SA).\n      "], "author_display": ["Janna Hastings", "Leonid Chepelev", "Egon Willighagen", "Nico Adams", "Christoph Steinbeck", "Michel Dumontier"], "article_type": "Research Article", "score": 0.38391054, "title_display": "The Chemical Information Ontology: Provenance and Disambiguation for Chemical Data on the Biological Semantic Web", "publication_date": "2011-10-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0025513"}, {"journal": "PLOS Computational Biology", "abstract": ["\nIt is believed that energy efficiency is an important constraint in brain evolution. As synaptic transmission dominates energy consumption, energy can be saved by ensuring that only a few synapses are active. It is therefore likely that the formation of sparse codes and sparse connectivity are fundamental objectives of synaptic plasticity. In this work we study how sparse connectivity can result from a synaptic learning rule of excitatory synapses. Information is maximised when potentiation and depression are balanced according to the mean presynaptic activity level and the resulting fraction of zero-weight synapses is around 50%. However, an imbalance towards depression increases the fraction of zero-weight synapses without significantly affecting performance. We show that imbalanced plasticity corresponds to imposing a regularising constraint on the L1-norm of the synaptic weight vector, a procedure that is well-known to induce sparseness. Imbalanced plasticity is biophysically plausible and leads to more efficient synaptic configurations than a previously suggested approach that prunes synapses after learning. Our framework gives a novel interpretation to the high fraction of silent synapses found in brain regions like the cerebellum.\nAuthor Summary: Recent estimates point out that a large part of the energetic budget of the mammalian cortex is spent in transmitting signals between neurons across synapses. Despite this, studies of learning and memory do not usually take energy efficiency into account. In this work we address the canonical computational problem of storing memories with synaptic plasticity. However, instead of optimising solely for information capacity, we search for energy efficient solutions. This implies that the number of functional synapses needs to be small (sparse connectivity) while maintaining high information. We suggest imbalanced plasticity, a learning regime where net depression is stronger than potentiation, as a simple and plausible means to learn more efficient neural circuits. Our framework gives a novel interpretation to the high fraction of silent synapses found in brain regions like the cerebellum. "], "author_display": ["Jo\u00e3o Sacramento", "Andreas Wichert", "Mark C. W. van Rossum"], "article_type": "Research Article", "score": 0.38355938, "title_display": "Energy Efficient Sparse Connectivity from Imbalanced Synaptic Plasticity Rules", "publication_date": "2015-06-05T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1004265"}, {"journal": "PLoS Computational Biology", "abstract": ["\nIn the mammalian hippocampus, the dentate gyrus (DG) is characterized by sparse and powerful unidirectional projections to CA3 pyramidal cells, the so-called mossy fibers. Mossy fiber synapses appear to duplicate, in terms of the information they convey, what CA3 cells already receive from entorhinal cortex layer II cells, which project both to the dentate gyrus and to CA3. Computational models of episodic memory have hypothesized that the function of the mossy fibers is to enforce a new, well separated pattern of activity onto CA3 cells, to represent a new memory, prevailing over the interference produced by the traces of older memories already stored on CA3 recurrent collateral connections. Can this hypothesis apply also to spatial representations, as described by recent neurophysiological recordings in rats? To address this issue quantitatively, we estimate the amount of information DG can impart on a new CA3 pattern of spatial activity, using both mathematical analysis and computer simulations of a simplified model. We confirm that, also in the spatial case, the observed sparse connectivity and level of activity are most appropriate for driving memory storage \u2013 and not to initiate retrieval. Surprisingly, the model also indicates that even when DG codes just for space, much of the information it passes on to CA3 acquires a non-spatial and episodic character, akin to that of a random number generator. It is suggested that further hippocampal processing is required to make full spatial use of DG inputs.\nAuthor Summary: The CA3 region at the core of the hippocampus, a structure crucial to memory formation, presents one striking anatomical feature. Its neurons receive many thousands of weak inputs from other sources, but only a few tens of very strong inputs from the neurons in the directly preceding region, the dentate gyrus. It had been proposed that such sparse connectivity helps the dentate gyrus to drive CA3 activity during the storage of new memories, but why it needs to be so sparse had remained unclear. Recent recordings of neuronal activity in the dentate gyrus (Leutgeb, et al. 2007) show the firing maps of granule cells of rodents engaged in exploration: the few cells active in a given environment, about 3% of the total, present multiple firing fields. Following these findings, we could now construct a network model that addresses the question quantitatively. Both mathematical analysis and computer simulations of the model show that, while the memory system would function also otherwise, connections as sparse as those observed make it function optimally, in terms of the bits of information new memories contain. Much of this information, we show, is encoded however in a difficult format, suggesting that other regions of the hippocampus, until now with no clear role, may contribute to decode it. "], "author_display": ["Erika Cerasti", "Alessandro Treves"], "article_type": "Research Article", "score": 0.38335282, "title_display": "How Informative Are Spatial CA3 Representations Established by the Dentate Gyrus?", "publication_date": "2010-04-29T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1000759"}, {"journal": "PLoS ONE", "abstract": ["\nInformation-intensive Web services such as price comparison sites have recently been gaining popularity. However, most users including novice shoppers have difficulty in browsing such sites because of the massive amount of information gathered and the uncertainty surrounding Web environments. Even conventional price comparison sites face various problems, which suggests the necessity of a new approach to address these problems. Therefore, for this study, an intelligent product search system was developed that enables price comparisons for online shoppers in a more effective manner. In particular, the developed system adopts linguistic price ratings based on fuzzy logic to accommodate user-defined price ranges, and personalizes product recommendations based on linguistic product clusters, which help online shoppers find desired items in a convenient manner.\n"], "author_display": ["Jun Woo Kim", "Sung Ho Ha"], "article_type": "Research Article", "score": 0.38321185, "title_display": "Price Comparisons on the Internet Based on Computational Intelligence", "publication_date": "2014-09-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0106946"}, {"journal": "PLoS ONE", "abstract": ["Introduction: Starting in June 2010 the Infectious Diseases Institute (IDI) clinic (a large urban HIV out-patient facility) switched to provider-based Electronic Medical Records (EMR) from paper EMR entered in the database by data-entry clerks. Standardized clinics forms were eliminated but providers still fill free text clinical notes in physical patients\u2019 files. The objective of this study was to compare the rate of errors in the database before and after the introduction of the provider-based EMR. Methods and Findings: Data in the database pre and post provider-based EMR was compared with the information in the patients\u2019 files and classified as correct, incorrect, and missing. We calculated the proportion of incorrect, missing and total error for key variables (toxicities, opportunistic infections, reasons for treatment change and interruption). Proportions of total errors were compared using chi-square test. A survey of the users of the EMR was also conducted. We compared data from 2,382 visits (from 100 individuals) of a retrospective validation conducted in 2007 with 34,957 visits (from 10,920 individuals) of a prospective validation conducted in April\u2013August 2011. The total proportion of errors decreased from 66.5% in 2007 to 2.1% in 2011 for opportunistic infections, from 51.9% to 3.5% for ART toxicity, from 82.8% to 12.5% for reasons for ART interruption and from 94.1% to 0.9% for reasons for ART switch (all P<0.0001). The survey showed that 83% of the providers agreed that provider-based EMR led to improvement of clinical care, 80% reported improved access to patients\u2019 records, and 80% appreciated the automation of providers\u2019 tasks. Conclusions: The introduction of provider-based EMR improved the quality of data collected with a significant reduction in missing and incorrect information. The majority of providers and clients expressed satisfaction with the new system. We recommend the use of provider-based EMR in large HIV programs in Sub-Saharan Africa. "], "author_display": ["Barbara Castelnuovo", "Agnes Kiragga", "Victor Afayo", "Malisa Ncube", "Richard Orama", "Stephen Magero", "Peter Okwi", "Yukari C. Manabe", "Andrew Kambugu"], "article_type": "Research Article", "score": 0.38312906, "title_display": "Implementation of Provider-Based Electronic Medical Records and Improvement of the Quality of Data in a Large HIV Program in Sub-Saharan Africa", "publication_date": "2012-12-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0051631"}, {"journal": "PLOS ONE", "abstract": ["\nNext Generation Sequencing (NGS) methods are driving profound changes in biomedical research, with a growing impact on patient care. Many academic medical centers are evaluating potential models to prepare for the rapid increase in NGS information needs. This study sought to investigate (1) how and where sequencing data is generated and analyzed, (2) research objectives and goals for NGS, (3) workforce capacity and unmet needs, (4) storage capacity and unmet needs, (5) available and anticipated funding resources, and (6) future challenges. As a precursor to informed decision making at our institution, we undertook a systematic needs assessment of investigators using survey methods. We recruited 331 investigators from over 60 departments and divisions at the University of Pittsburgh Schools of Health Sciences and had 140 respondents, or a 42% response rate. Results suggest that both sequencing and analysis bottlenecks currently exist. Significant educational needs were identified, including both investigator-focused needs, such as selection of NGS methods suitable for specific research objectives, and program-focused needs, such as support for training an analytic workforce. The absence of centralized infrastructure was identified as an important institutional gap. Key principles for organizations managing this change were formulated based on the survey responses. This needs assessment provides an in-depth case study which may be useful to other academic medical centers as they identify and plan for future needs.\n"], "author_display": ["Albert Geskin", "Elizabeth Legowski", "Anish Chakka", "Uma R Chandran", "M. Michael Barmada", "William A. LaFramboise", "Jeremy Berg", "Rebecca S. Jacobson"], "article_type": "Research Article", "score": 0.3831285, "title_display": "Needs Assessment for Research Use of High-Throughput Sequencing at a Large Academic Medical Center", "publication_date": "2015-06-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0131166"}, {"journal": "PLoS ONE", "abstract": ["\n        Computer simulation techniques for cardiac beating motions potentially have many applications and a broad audience. However, most existing methods require enormous computational costs and often show unstable behavior for extreme parameter sets, which interrupts smooth simulation study and make it difficult to apply them to interactive applications. To address this issue, we present an efficient and robust framework for simulating the cardiac beating motion. The global cardiac motion is generated by the accumulation of local myocardial fiber contractions. We compute such local-to-global deformations using a kinematic approach; we divide a heart mesh model into overlapping local regions, contract them independently according to fiber orientation, and compute a global shape that satisfies contracted shapes of all local regions as much as possible. A comparison between our method and a physics-based method showed that our method can generate motion very close to that of a physics-based simulation. Our kinematic method has high controllability; the simulated ventricle-wall-contraction speed can be easily adjusted to that of a real heart by controlling local contraction timing. We demonstrate that our method achieves a highly realistic beating motion of a whole heart in real time on a consumer-level computer. Our method provides an important step to bridge a gap between cardiac simulations and interactive applications.\n      "], "author_display": ["Takashi Ijiri", "Takashi Ashihara", "Nobuyuki Umetani", "Takeo Igarashi", "Ryo Haraguchi", "Hideo Yokota", "Kazuo Nakazawa"], "article_type": "Research Article", "score": 0.38275725, "title_display": "A Kinematic Approach for Efficient and Robust Simulation of the Cardiac Beating Motion", "publication_date": "2012-05-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0036706"}, {"journal": "PLoS ONE", "abstract": ["\n        Image-based mechanical modeling of the complex micro-structure of human bone has shown promise as a non-invasive method for characterizing bone strength and fracture risk in vivo. In particular, elastic moduli obtained from image-derived micro-finite element (\u03bcFE) simulations have been shown to correlate well with results obtained by mechanical testing of cadaveric bone. However, most existing large-scale finite-element simulation programs require significant computing resources, which hamper their use in common laboratory and clinical environments. In this work, we theoretically derive and computationally evaluate the resources needed to perform such simulations (in terms of computer memory and computation time), which are dependent on the number of finite elements in the image-derived bone model. A detailed description of our approach is provided, which is specifically optimized for \u03bcFE modeling of the complex three-dimensional architecture of trabecular bone. Our implementation includes domain decomposition for parallel computing, a novel stopping criterion, and a system for speeding up convergence by pre-iterating on coarser grids. The performance of the system is demonstrated on a dual quad-core Xeon 3.16 GHz CPUs equipped with 40 GB of RAM. Models of distal tibia derived from 3D in-vivo MR images in a patient comprising 200,000 elements required less than 30 seconds to converge (and 40 MB RAM). To illustrate the system's potential for large-scale \u03bcFE simulations, axial stiffness was estimated from high-resolution micro-CT images of a voxel array of 90 million elements comprising the human proximal femur in seven hours CPU time. In conclusion, the system described should enable image-based finite-element bone simulations in practical computation times on high-end desktop computers with applications to laboratory studies and clinical imaging.\n      "], "author_display": ["Jeremy F. Magland", "Ning Zhang", "Chamith S. Rajapakse", "Felix W. Wehrli"], "article_type": "Research Article", "score": 0.38264742, "title_display": "Computationally-Optimized Bone Mechanical Modeling from High-Resolution Structural Images", "publication_date": "2012-04-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0035525"}, {"journal": "PLoS ONE", "abstract": ["\n        Harnessing community intelligence in knowledge curation bears significant promise in dealing with communication and education in the flood of scientific knowledge. As knowledge is accumulated at ever-faster rates, scientific nomenclature, a particular kind of knowledge, is concurrently generated in all kinds of fields. Since nomenclature is a system of terms used to name things in a particular discipline, accurate translation of scientific nomenclature in different languages is of critical importance, not only for communications and collaborations with English-speaking people, but also for knowledge dissemination among people in the non-English-speaking world, particularly young students and researchers. However, it lacks of accuracy and standardization when translating scientific nomenclature from English to other languages, especially for those languages that do not belong to the same language family as English. To address this issue, here we propose for the first time the application of community intelligence in scientific nomenclature management, namely, harnessing collective intelligence for translation of scientific nomenclature from English to other languages. As community intelligence applied to knowledge curation is primarily aided by wiki and Chinese is the native language for about one-fifth of the world\u2019s population, we put the proposed application into practice, by developing a wiki-based English-to-Chinese Scientific Nomenclature Dictionary (ESND; http://esnd.big.ac.cn). ESND is a wiki-based, publicly editable and open-content platform, exploiting the whole power of the scientific community in collectively and collaboratively managing scientific nomenclature. Based on community curation, ESND is capable of achieving accurate, standard, and comprehensive scientific nomenclature, demonstrating a valuable application of community intelligence in knowledge curation.\n      "], "author_display": ["Lin Dai", "Chao Xu", "Ming Tian", "Jian Sang", "Dong Zou", "Ang Li", "Guocheng Liu", "Fei Chen", "Jiayan Wu", "Jingfa Xiao", "Xumin Wang", "Jun Yu", "Zhang Zhang"], "article_type": "Research Article", "score": 0.38242772, "title_display": "Community Intelligence in Knowledge Curation: An Application to Managing Scientific Nomenclature", "publication_date": "2013-02-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0056961"}, {"journal": "PLoS ONE", "abstract": ["\n        We propose a new technique of measuring user similarity in collaborative filtering using electric circuit analysis. Electric circuit analysis is used to measure the potential differences between nodes on an electric circuit. In this paper, by applying this method to transaction networks comprising users and items, i.e., user\u2013item matrix, and by using the full information about the relationship structure of users in the perspective of item adoption, we overcome the limitations of one-to-one similarity calculation approach, such as the Pearson correlation, Tanimoto coefficient, and Hamming distance, in collaborative filtering. We found that electric circuit analysis can be successfully incorporated into recommender systems and has the potential to significantly enhance predictability, especially when combined with user-based collaborative filtering. We also propose four types of hybrid algorithms that combine the Pearson correlation method and electric circuit analysis. One of the algorithms exceeds the performance of the traditional collaborative filtering by 37.5% at most. This work opens new opportunities for interdisciplinary research between physics and computer science and the development of new recommendation systems\n      "], "author_display": ["Joonhyuk Yang", "Jinwook Kim", "Wonjoon Kim", "Young Hwan Kim"], "article_type": "Research Article", "score": 0.38235897, "title_display": "Measuring User Similarity Using Electric Circuit Analysis: Application to Collaborative Filtering", "publication_date": "2012-11-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0049126"}, {"journal": "PLoS Computational Biology", "abstract": ["\nCones with peak sensitivity to light at long (L), medium (M) and short (S) wavelengths are unequal in number on the human retina: S cones are rare (<10%) while increasing in fraction from center to periphery, and the L/M cone proportions are highly variable between individuals. What optical properties of the eye, and statistical properties of natural scenes, might drive this organization? We found that the spatial-chromatic structure of natural scenes was largely symmetric between the L, M and S sensitivity bands. Given this symmetry, short wavelength attenuation by ocular media gave L/M cones a modest signal-to-noise advantage, which was amplified, especially in the denser central retina, by long-wavelength accommodation of the lens. Meanwhile, total information represented by the cone mosaic remained relatively insensitive to L/M proportions. Thus, the observed cone array design along with a long-wavelength accommodated lens provides a selective advantage: it is maximally informative.\nAuthor Summary: Human color perception arises by comparing the signals from cones with peak sensitivities, at long (L), medium (M) and short (S) wavelengths. In dichromats, a characteristic distribution of S and M cones supports blue-yellow color vision: a few S and mostly M. When L cones are added, allowing red-green color vision, the S proportion remains low, increasing slowly with increasing retinal eccentricity, but the L/M proportion can vary 5-fold without affecting red-green color perception. We offer a unified explanation of these striking facts. First, we find that the spatial-chromatic statistics of natural scenes are largely symmetric between the L, M and S sensitivity bands. Thus, attenuation of blue light in the optical media, and chromatic aberration after long-wavelength accommodation of the lens, can give L/M cones an advantage. Quantitatively, information transmission by the cone array is maximized when the S proportion is low but increasing slowly with retinal eccentricity, accompanied by a lens accommodated to red light. After including blur by the lens, the optimum depends weakly on the red/green ratio, allowing large variations without loss of function. This explains the basic layout of the cone mosaic: for the resources invested, the organization maximizes information. "], "author_display": ["Patrick Garrigan", "Charles P. Ratliff", "Jennifer M. Klein", "Peter Sterling", "David H. Brainard", "Vijay Balasubramanian"], "article_type": "Research Article", "score": 0.382241, "title_display": "Design of a Trichromatic Cone Array", "publication_date": "2010-02-12T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1000677"}, {"journal": "PLOS ONE", "abstract": ["\nInterpretation of human genomes is a major challenge. We present the Scripps Genome ADVISER (SG-ADVISER) suite, which aims to fill the gap between data generation and genome interpretation by performing holistic, in-depth, annotations and functional predictions on all variant types and effects. The SG-ADVISER suite includes a de-identification tool, a variant annotation web-server, and a user interface for inheritance and annotation-based filtration. SG-ADVISER allows users with no bioinformatics expertise to manipulate large volumes of variant data with ease \u2013 without the need to download large reference databases, install software, or use a command line interface. SG-ADVISER is freely available at genomics.scripps.edu/ADVISER.\n"], "author_display": ["Phillip H. Pham", "William J. Shipman", "Galina A. Erikson", "Nicholas J. Schork", "Ali Torkamani"], "article_type": "Research Article", "score": 0.38185725, "title_display": "Scripps Genome ADVISER: Annotation and Distributed Variant Interpretation SERver", "publication_date": "2015-02-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0116815"}, {"journal": "PLoS Computational Biology", "abstract": ["\nThe role of sensory systems is to provide an organism with information about its environment. Because sensory information is noisy and insufficient to uniquely determine the environment, natural perceptual systems have to cope with systematic uncertainty. The extent of that uncertainty is often crucial to the organism: for instance, in judging the potential threat in a stimulus. Inducing uncertainty by using visual noise, we had human observers perform a task where they could improve their performance by choosing the less uncertain among pairs of visual stimuli. Results show that observers had access to a reliable measure of visual uncertainty in their decision-making, showing that subjective uncertainty in this case is connected to objective uncertainty. Based on a Bayesian model of the task, we discuss plausible computational schemes for that ability.\nAuthor Summary: Most work in vision science focuses on the question of why we perceive what we do, and we now have many models explaining what physical properties of a stimulus make us see depth, colour, etc. Here we ask instead what makes us feel confident in our visual perception: in the context of a visual task, what are the physical properties of the stimulus that will make us think we are doing the task well? The mathematical framework of Bayesian statistics provides an elegant way to frame the problem, by assuming that the visual system is trying to estimate physical properties of the world from incomplete, sometimes unreliable visual information. Objective uncertainty will therefore depend on the quality of the information available in the stimulus. In our experiments we compare objective uncertainty\u2014as computed using the Bayesian framework\u2014with subjective uncertainty, the confidence observers report about their visual percepts. To this end, we use a visual task with well-defined statistical properties, discrimination under noise. We report a surprising degree of agreement between objective and subjective uncertainty, and discuss possible computational models that could explain this ability of the visual system. "], "author_display": ["Simon Barthelm\u00e9", "Pascal Mamassian"], "article_type": "Research Article", "score": 0.38152936, "title_display": "Evaluation of Objective Uncertainty in the Visual System", "publication_date": "2009-09-11T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1000504"}, {"journal": "PLoS ONE", "abstract": ["\nProtein-Protein Interaction (PPI) extraction is an important task in the biomedical information extraction. Presently, many machine learning methods for PPI extraction have achieved promising results. However, the performance is still not satisfactory. One reason is that the semantic resources were basically ignored. In this paper, we propose a multiple-kernel learning-based approach to extract PPIs, combining the feature-based kernel, tree kernel and semantic kernel. Particularly, we extend the shortest path-enclosed tree kernel (SPT) by a dynamic extended strategy to retrieve the richer syntactic information. Our semantic kernel calculates the protein-protein pair similarity and the context similarity based on two semantic resources: WordNet and Medical Subject Heading (MeSH). We evaluate our method with Support Vector Machine (SVM) and achieve an F-score of 69.40% and an AUC of 92.00%, which show that our method outperforms most of the state-of-the-art systems by integrating semantic information.\n"], "author_display": ["Lishuang Li", "Panpan Zhang", "Tianfu Zheng", "Hongying Zhang", "Zhenchao Jiang", "Degen Huang"], "article_type": "Research Article", "score": 0.38149586, "title_display": "Integrating Semantic Information into Multiple Kernels for Protein-Protein Interaction Extraction from Biomedical Literatures", "publication_date": "2014-03-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0091898"}, {"journal": "PLoS ONE", "abstract": ["\nParameter identifiability problems can plague biomodelers when they reach the quantification stage of development, even for relatively simple models. Structural identifiability (SI) is the primary question, usually understood as knowing which of P unknown biomodel parameters p1,\u2026, pi,\u2026, pP are-and which are not-quantifiable in principle from particular input-output (I-O) biodata. It is not widely appreciated that the same database also can provide quantitative information about the structurally unidentifiable (not quantifiable) subset, in the form of explicit algebraic relationships among unidentifiable pi. Importantly, this is a first step toward finding what else is needed to quantify particular unidentifiable parameters of interest from new I\u2013O experiments. We further develop, implement and exemplify novel algorithms that address and solve the SI problem for a practical class of ordinary differential equation (ODE) systems biology models, as a user-friendly and universally-accessible web application (app)\u2013COMBOS. Users provide the structural ODE and output measurement models in one of two standard forms to a remote server via their web browser. COMBOS provides a list of uniquely and non-uniquely SI model parameters, and\u2013importantly-the combinations of parameters not individually SI. If non-uniquely SI, it also provides the maximum number of different solutions, with important practical implications. The behind-the-scenes symbolic differential algebra algorithms are based on computing Gr\u00f6bner bases of model attributes established after some algebraic transformations, using the computer-algebra system Maxima. COMBOS was developed for facile instructional and research use as well as modeling. We use it in the classroom to illustrate SI analysis; and have simplified complex models of tumor suppressor p53 and hormone regulation, based on explicit computation of parameter combinations. It\u2019s illustrated and validated here for models of moderate complexity, with and without initial conditions. Built-in examples include unidentifiable 2 to 4-compartment and HIV dynamics models.\n"], "author_display": ["Nicolette Meshkat", "Christine Er-zhen Kuo", "Joseph DiStefano"], "article_type": "Research Article", "score": 0.3813305, "title_display": "On Finding and Using Identifiable Parameter Combinations in Nonlinear Dynamic Systems Biology Models and COMBOS: A Novel Web Implementation", "publication_date": "2014-10-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0110261"}, {"journal": "PLOS ONE", "abstract": ["\nThis paper reports accelerometer and electronic dairy data on typical daily activities of 139 school students from grade six and nine. Recordings covered a typical school day for each student and lasted on average for 23 h. Screen activities (watching television and using the computer) are compared to several other activities performed while sitting (e.g., playing, eating, sitting in school, and doing homework). Body movement was continuously recorded by four accelerometers and transformed into a motion sore. Our results show that extremely low motion scores, as if subjects were freezing, emerge to a greater extent in front of screens compared to other investigated activities. Given the substantial amount of time young people spend in front of screens and the rising obesity epidemic, our data suggest a mechanism for the association of screen time and obesity.\n"], "author_display": ["Judith Streb", "Thomas Kammer", "Manfred Spitzer", "Katrin Hille"], "article_type": "Research Article", "score": 0.38084847, "title_display": "Extremely Reduced Motion in Front of Screens: Investigating Real-World Physical Activity of Adolescents by Accelerometry and Electronic Diary", "publication_date": "2015-05-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0126722"}, {"journal": "PLoS Computational Biology", "abstract": ["\nIn standard attractor neural network models, specific patterns of activity are stored in the synaptic matrix, so that they become fixed point attractors of the network dynamics. The storage capacity of such networks has been quantified in two ways: the maximal number of patterns that can be stored, and the stored information measured in bits per synapse. In this paper, we compute both quantities in fully connected networks of N binary neurons with binary synapses, storing patterns with coding level , in the large  and sparse coding limits (). We also derive finite-size corrections that accurately reproduce the results of simulations in networks of tens of thousands of neurons. These methods are applied to three different scenarios: (1) the classic Willshaw model, (2) networks with stochastic learning in which patterns are shown only once (one shot learning), (3) networks with stochastic learning in which patterns are shown multiple times. The storage capacities are optimized over network parameters, which allows us to compare the performance of the different models. We show that finite-size effects strongly reduce the capacity, even for networks of realistic sizes. We discuss the implications of these results for memory storage in the hippocampus and cerebral cortex.\nAuthor Summary: Two central hypotheses in neuroscience are that long-term memory is sustained by modifications of the connectivity of neural circuits, while short-term memory is sustained by persistent neuronal activity following the presentation of a stimulus. These two hypotheses have been substantiated by several decades of electrophysiological experiments, reporting activity-dependent changes in synaptic connectivity in vitro, and stimulus-selective persistent neuronal activity in delayed response tasks in behaving monkeys. They have been implemented in attractor network models, that store specific patterns of activity using Hebbian plasticity rules, which then allow retrieval of these patterns as attractors of the network dynamics. A long-standing question in the field is how many patterns (or equivalently, how much information) can be stored in such networks? Here, we compute the storage capacity of networks of binary neurons and binary synapses. Synapses store information according to a simple stochastic learning process that consists of transitions between synaptic states conditioned on the states of pre- and post-synaptic neurons. We consider this learning process in two limits: a one shot learning scenario, where each pattern is presented only once, and a slow learning scenario, where noisy versions of a set of patterns are presented multiple times, but transition probabilities are small. The two limits are assumed to represent, in a simplified way, learning in the hippocampus and neocortex, respectively. We show that in both cases, the information stored per synapse remains finite in the large  limit, when the coding is sparse. Furthermore, we characterize the strong finite size effects that exist in such networks. "], "author_display": ["Alexis M. Dubreuil", "Yali Amit", "Nicolas Brunel"], "article_type": "Research Article", "score": 0.38082707, "title_display": "Memory Capacity of Networks with Stochastic Binary Synapses", "publication_date": "2014-08-07T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1003727"}, {"journal": "PLoS Computational Biology", "abstract": ["\nThe mechanisms by which adaptive phenotypes spread within an evolving population after their emergence are understood fairly well. Much less is known about the factors that influence the evolutionary accessibility of such phenotypes, a pre-requisite for their emergence in a population. Here, we investigate the influence of environmental quality on the accessibility of adaptive phenotypes of Escherichia coli's central metabolic network. We used an established flux-balance model of metabolism as the basis for a genotype-phenotype map (GPM). We quantified the effects of seven qualitatively different environments (corresponding to both carbohydrate and gluconeogenic metabolic substrates) on the structure of this GPM. We found that the GPM has a more rugged structure in qualitatively poorer environments, suggesting that adaptive phenotypes could be intrinsically less accessible in such environments. Nevertheless, on average \u223c74% of the genotype can be altered by neutral drift, in the environment where the GPM is most rugged; this could allow evolving populations to circumvent such ruggedness. Furthermore, we found that the normalized mutual information (NMI) of genotype differences relative to phenotype differences, which measures the GPM's capacity to transmit information about phenotype differences, is positively correlated with (simulation-based) estimates of the accessibility of adaptive phenotypes in different environments. These results are consistent with the predictions of a simple analytic theory that makes explicit the relationship between the NMI and the speed of adaptation. The results suggest an intuitive information-theoretic principle for evolutionary adaptation; adaptation could be faster in environments where the GPM has a greater capacity to transmit information about phenotype differences. More generally, our results provide insight into fundamental environment-specific differences in the accessibility of adaptive phenotypes, and they suggest opportunities for research at the interface between information theory and evolutionary biology.\nAuthor Summary: Adaptation involves the discovery by mutation and spread through populations of traits (or \u201cphenotypes\u201d) that have high fitness under prevailing environmental conditions. While the spread of adaptive phenotypes through populations is mediated by natural selection, the likelihood of their discovery by mutation depends primarily on the relationship between genetic information and phenotypes (the genotype-phenotype mapping, or GPM). Elucidating the factors that influence the structure of the GPM is therefore critical to understanding the adaptation process. We investigated the influence of environmental quality on GPM structure for a well-studied model of Escherichia coli's metabolism. Our results suggest that the GPM is more rugged in qualitatively poorer environments and, therefore, the discovery of adaptive phenotypes may be intrinsically less likely in such environments. Nevertheless, we found that the GPM contains large neutral networks in all studied environments, suggesting that populations adapting to these environments could circumvent the frequent \u201chill descents\u201d that would otherwise be required by a rugged GPM. Moreover, we demonstrated that adaptation proceeds faster in environments for which the GPM transmits information about phenotype differences more efficiently, providing a connection between information theory and evolutionary theory. These results have implications for understanding constraints on adaptation in nature. "], "author_display": ["Wilfred Ndifon", "Joshua B. Plotkin", "Jonathan Dushoff"], "article_type": "Research Article", "score": 0.38075128, "title_display": "On the Accessibility of Adaptive Phenotypes of a Bacterial Metabolic Network", "publication_date": "2009-08-21T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1000472"}, {"journal": "PLoS ONE", "abstract": ["\nScientists are increasing their efforts to promote public engagement with their science, but the efficacy of the methods used is often not scientifically evaluated. Here, we designed, installed and evaluated the educational impact of interactive games on touchscreens at two primate research centres based in zoo environments. The games were designed to promote interest in and understanding of primates and comparative psychology, as a scaffold towards interest in science more generally and with the intention of targeting younger individuals (under 16's). We used systematic observational techniques and questionnaires to assess the impact of the games on zoo visitors. The games facilitated increased interest in psychology and science in zoo visitors, and changed the knowledge of visitors, through demonstration of learning about specific scientific findings nested within the games. The impact of such devices was greatest on younger individuals (under 16's) as they were significantly more likely to engage with the games. On the whole, therefore, this study demonstrates that interactive devices can be successful educational tools, and adds to the growing body of evidence that conducting research on public view in zoos can have a tangible impact on public engagement with science.\n"], "author_display": ["Jamie Whitehouse", "Bridget M. Waller", "Mathilde Chanvin", "Emma K. Wallace", "Anne M. Schel", "Kate Peirce", "Heidi Mitchell", "Alaina Macri", "Katie Slocombe"], "article_type": "Research Article", "score": 0.38056314, "title_display": "Evaluation of Public Engagement Activities to Promote Science in a Zoo Environment", "publication_date": "2014-11-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0113395"}, {"journal": "PLOS ONE", "abstract": ["\nIt is important to easily and efficiently obtain high quality species distribution data for predicting the potential distribution of species using species distribution models (SDMs). There is a need for a powerful software tool to automatically or semi-automatically assist in identifying and correcting errors. Here, we use Python to develop a web-based software tool (SDMdata) to easily collect occurrence data from the Global Biodiversity Information Facility (GBIF) and check species names and the accuracy of coordinates (latitude and longitude). It is an open source software (GNU Affero General Public License/AGPL licensed) allowing anyone to access and manipulate the source code. SDMdata is available online free of charge from <http://www.sdmserialsoftware.org/sdmdata/>.\n"], "author_display": ["Xiaoquan Kong", "Minyi Huang", "Renyan Duan"], "article_type": "Research Article", "score": 0.3800101, "title_display": "SDMdata: A Web-Based Software Tool for Collecting Species Occurrence Records", "publication_date": "2015-06-01T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0128295"}, {"journal": "PLoS ONE", "abstract": ["\nInferring regulatory networks from experimental data via probabilistic graphical models is a popular framework to gain insights into biological systems. However, the inherent noise in experimental data coupled with a limited sample size reduces the performance of network reverse engineering. Prior knowledge from existing sources of biological information can address this low signal to noise problem by biasing the network inference towards biologically plausible network structures. Although integrating various sources of information is desirable, their heterogeneous nature makes this task challenging. We propose two computational methods to incorporate various information sources into a probabilistic consensus structure prior to be used in graphical model inference. Our first model, called Latent Factor Model (LFM), assumes a high degree of correlation among external information sources and reconstructs a hidden variable as a common source in a Bayesian manner. The second model, a Noisy-OR, picks up the strongest support for an interaction among information sources in a probabilistic fashion. Our extensive computational studies on KEGG signaling pathways as well as on gene expression data from breast cancer and yeast heat shock response reveal that both approaches can significantly enhance the reconstruction accuracy of Bayesian Networks compared to other competing methods as well as to the situation without any prior. Our framework allows for using diverse information sources, like pathway databases, GO terms and protein domain data, etc. and is flexible enough to integrate new sources, if available.\n"], "author_display": ["Paurush Praveen", "Holger Fr\u00f6hlich"], "article_type": "Research Article", "score": 0.37964055, "title_display": "Boosting Probabilistic Graphical Model Inference by Incorporating Prior Knowledge from Multiple Sources", "publication_date": "2013-06-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0067410"}, {"journal": "PLoS ONE", "abstract": ["\n        Transcriptional networks consist of multiple regulatory layers corresponding to the activity of global regulators, specialized repressors and activators as well as proteins and enzymes shaping the DNA template. Such intrinsic complexity makes uncovering connections difficult and it calls for corresponding methodologies, which are adapted to the available data. Here we present a new computational method that predicts interactions between transcription factors and target genes using compendia of microarray gene expression data and documented interactions between genes and transcription factors. The proposed method, called Kernel Embedding of Regulatory Networks (KEREN), is based on the concept of gene-regulon association, and captures hidden geometric patterns of the network via manifold embedding. We applied KEREN to reconstruct transcription regulatory interactions on a genome-wide scale in the model bacteria Escherichia coli (E. coli). Application of the method not only yielded accurate predictions of verifiable interactions, which outperformed on certain metrics comparable methodologies, but also demonstrated the utility of a geometric approach in the analysis of high-dimensional biological data. We also described possible applications of kernel embedding techniques to other function and network discovery algorithms.\n      "], "author_display": ["Hossein Zare", "Mostafa Kaveh", "Arkady Khodursky"], "article_type": "Research Article", "score": 0.3794177, "title_display": "Inferring a Transcriptional Regulatory Network from Gene Expression Data Using Nonlinear Manifold Embedding", "publication_date": "2011-08-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0021969"}, {"journal": "PLOS ONE", "abstract": ["\nGiven their lack of background knowledge, laypeople require expert help when dealing with scientific information. To decide whose help is dependable, laypeople must judge an expert\u2019s epistemic trustworthiness in terms of competence, adherence to scientific standards, and good intentions. Online, this may be difficult due to the often limited and sometimes unreliable source information available. To measure laypeople\u2019s evaluations of experts (encountered online), we constructed an inventory to assess epistemic trustworthiness on the dimensions expertise, integrity, and benevolence. Exploratory (n = 237) and confirmatory factor analyses (n = 345) showed that the Muenster Epistemic Trustworthiness Inventory (METI) is composed of these three factors. A subsequent experimental study (n = 137) showed that all three dimensions of the METI are sensitive to variation in source characteristics. We propose using this inventory to measure assignments of epistemic trustworthiness, that is, all judgments laypeople make when deciding whether to place epistemic trust in\u2013and defer to\u2013an expert in order to solve a scientific informational problem that is beyond their understanding.\n"], "author_display": ["Friederike Hendriks", "Dorothe Kienhues", "Rainer Bromme"], "article_type": "Research Article", "score": 0.37918618, "title_display": "Measuring Laypeople\u2019s Trust in Experts in a Digital Age: The Muenster Epistemic Trustworthiness Inventory (METI)", "publication_date": "2015-10-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0139309"}, {"journal": "PLoS ONE", "abstract": ["\nThis article explores a new open-source method for developing and manufacturing high-quality scientific equipment suitable for use in virtually any laboratory. A syringe pump was designed using freely available open-source computer aided design (CAD) software and manufactured using an open-source RepRap 3-D printer and readily available parts. The design, bill of materials and assembly instructions are globally available to anyone wishing to use them. Details are provided covering the use of the CAD software and the RepRap 3-D printer. The use of an open-source Rasberry Pi computer as a wireless control device is also illustrated. Performance of the syringe pump was assessed and the methods used for assessment are detailed. The cost of the entire system, including the controller and web-based control interface, is on the order of 5% or less than one would expect to pay for a commercial syringe pump having similar performance. The design should suit the needs of a given research activity requiring a syringe pump including carefully controlled dosing of reagents, pharmaceuticals, and delivery of viscous 3-D printer media among other applications.\n"], "author_display": ["Bas Wijnen", "Emily J. Hunt", "Gerald C. Anzalone", "Joshua M. Pearce"], "article_type": "Research Article", "score": 0.37904733, "title_display": "Open-Source Syringe Pump Library", "publication_date": "2014-09-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0107216"}, {"journal": "PLoS ONE", "abstract": ["\n        The dimension of the population genetics data produced by next-generation sequencing platforms is extremely high. However, the \u201cintrinsic dimensionality\u201d of sequence data, which determines the structure of populations, is much lower. This motivates us to use locally linear embedding (LLE) which projects high dimensional genomic data into low dimensional, neighborhood preserving embedding, as a general framework for population structure and historical inference. To facilitate application of the LLE to population genetic analysis, we systematically investigate several important properties of the LLE and reveal the connection between the LLE and principal component analysis (PCA). Identifying a set of markers and genomic regions which could be used for population structure analysis will provide invaluable information for population genetics and association studies. In addition to identifying the LLE-correlated or PCA-correlated structure informative marker, we have developed a new statistic that integrates genomic information content in a genomic region for collectively studying its association with the population structure and LASSO algorithm to search such regions across the genomes. We applied the developed methodologies to a low coverage pilot dataset in the 1000 Genomes Project and a PHASE III Mexico dataset of the HapMap. We observed that 25.1%, 44.9% and 21.4% of the common variants and 89.2%, 92.4% and 75.1% of the rare variants were the LLE-correlated markers in CEU, YRI and ASI, respectively. This showed that rare variants, which are often private to specific populations, have much higher power to identify population substructure than common variants. The preliminary results demonstrated that next generation sequencing offers a rich resources and LLE provide a powerful tool for population structure analysis.\n      "], "author_display": ["Hoicheong Siu", "Li Jin", "Momiao Xiong"], "article_type": "Research Article", "score": 0.3790343, "title_display": "Manifold Learning for Human Population Structure Studies", "publication_date": "2012-01-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0029901"}, {"journal": "PLoS ONE", "abstract": ["\nThe aim of this study is to evaluate the reliability of a crowd simulation model developed by the authors by reproducing Dyer et al.'s experiments (published in Philosophical Transactions in 2009) on human leadership and consensus decision making in a computer-based environment. The theoretical crowd model of the simulation environment is presented, and its results are compared and analysed against Dyer et al.'s original experiments. It is concluded that the simulation results are largely consistent with the experiments, which demonstrates the reliability of the crowd model. Furthermore, the simulation data also reveals several additional new findings, namely: 1) the phenomena of sacrificing accuracy to reach a quicker consensus decision found in ants colonies was also discovered in the simulation; 2) the ability of reaching consensus in groups has a direct impact on the time and accuracy of arriving at the target position; 3) the positions of the informed individuals or leaders in the crowd could have significant impact on the overall crowd movement; and 4) the simulation also confirmed Dyer et al.'s anecdotal evidence of the proportion of the leadership in large crowds and its effect on crowd movement. The potential applications of these findings are highlighted in the final discussion of this paper.\n"], "author_display": ["Song Wu", "Quanbin Sun"], "article_type": "Research Article", "score": 0.3790079, "title_display": "Computer Simulation of Leadership, Consensus Decision Making and Collective Behaviour in Humans", "publication_date": "2014-01-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0080680"}, {"journal": "PLoS ONE", "abstract": ["\nEstimating the causal interaction between neurons is very important for better understanding the functional connectivity in neuronal networks. We propose a method called normalized permutation transfer entropy (NPTE) to evaluate the temporal causal interaction between spike trains, which quantifies the fraction of ordinal information in a neuron that has presented in another one. The performance of this method is evaluated with the spike trains generated by an Izhikevich\u2019s neuronal model. Results show that the NPTE method can effectively estimate the causal interaction between two neurons without influence of data length. Considering both the precision of time delay estimated and the robustness of information flow estimated against neuronal firing rate, the NPTE method is superior to other information theoretic method including normalized transfer entropy, symbolic transfer entropy and permutation conditional mutual information. To test the performance of NPTE on analyzing simulated biophysically realistic synapses, an Izhikevich\u2019s cortical network that based on the neuronal model is employed. It is found that the NPTE method is able to characterize mutual interactions and identify spurious causality in a network of three neurons exactly. We conclude that the proposed method can obtain more reliable comparison of interactions between different pairs of neurons and is a promising tool to uncover more details on the neural coding.\n"], "author_display": ["Zhaohui Li", "Xiaoli Li"], "article_type": "Research Article", "score": 0.37892643, "title_display": "Estimating Temporal Causal Interaction between Spike Trains with Permutation and Transfer Entropy", "publication_date": "2013-08-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0070894"}, {"journal": "PLoS ONE", "abstract": ["\nIn sample surveys, it is usual to make use of auxiliary information to increase the precision of the estimators. We propose a new chain ratio estimator and regression estimator of a finite population mean using linear combination of two auxiliary variables and obtain the mean squared error (MSE) equations for the proposed estimators. We find theoretical conditions that make proposed estimators more efficient than the traditional multivariate ratio estimator and the regression estimator using information of two auxiliary variables.\n"], "author_display": ["Jingli Lu"], "article_type": "Research Article", "score": 0.37889203, "title_display": "The Chain Ratio Estimator and Regression Estimator with Linear Combination of Two Auxiliary Variables", "publication_date": "2013-11-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0081085"}, {"journal": "PLoS ONE", "abstract": ["\n        Indicators that rank countries according socioeconomic measurements are important tools for regional development and political reform. Those currently in widespread use are sometimes criticized for a lack of reproducibility or the inability to compare values over time, necessitating simple, fast and systematic measures. Here, we applied the \u2018guilt by association\u2019 principle often used in biological networks to the information network within the online encyclopedia Wikipedia to create an indicator quantifying the degree to which pages linked to a country are disputed by contributors. The indicator correlates with metrics of governance, political or economic stability about as well as they correlate with each other, and though faster and simpler, it is remarkably stable over time despite constant changes in the underlying disputes. For some countries, changes over a four year period appear to correlate with world events related to conflicts or economic problems.\n      "], "author_display": ["Gordana Apic", "Matthew J. Betts", "Robert B. Russell"], "article_type": "Research Article", "score": 0.37884247, "title_display": "Content Disputes in Wikipedia Reflect Geopolitical Instability", "publication_date": "2011-06-22T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0020902"}, {"journal": "PLoS ONE", "abstract": ["\n        Maximum parsimony (MP) methods aim to reconstruct the phylogeny of extant species by finding the most parsimonious evolutionary scenario using the species' genome data. MP methods are considered to be accurate, but they are also computationally expensive especially for a large number of species. Several disk-covering methods (DCMs), which decompose the input species to multiple overlapping subgroups (or disks), have been proposed to solve the problem in a divide-and-conquer way.\n        We design a new DCM based on the spectral method and also develop the COGNAC (Comparing Orders of Genes using Novel Algorithms and high-performance Computers) software package. COGNAC uses the new DCM to reduce the phylogenetic tree search space and selects an output tree from the reduced search space based on the MP principle. We test the new DCM using gene order data and inversion distance. The new DCM not only reduces the number of candidate tree topologies but also excludes erroneous tree topologies which can be selected by original MP methods. Initial labeling of internal genomes affects the accuracy of MP methods using gene order data, and the new DCM enables more accurate initial labeling as well. COGNAC demonstrates superior accuracy as a consequence. We compare COGNAC with FastME and the combination of the state of the art DCM (Rec-I-DCM3) and GRAPPA . COGNAC clearly outperforms FastME in accuracy. COGNAC \u2013using the new DCM\u2013also reconstructs a much more accurate tree in significantly shorter time than GRAPPA with Rec-I-DCM3.\n      "], "author_display": ["Seunghwa Kang", "Jijun Tang", "Stephen W. Schaeffer", "David A. Bader"], "article_type": "Research Article", "score": 0.3787189, "title_display": "Rec-DCM-Eigen: Reconstructing a Less Parsimonious but More Accurate Tree in Shorter Time", "publication_date": "2011-08-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0022483"}, {"journal": "PLoS Biology", "abstract": [": A neuroimaging study reveals how coupled brain oscillations at different frequencies align with quasi-rhythmic features of continuous speech such as prosody, syllables, and phonemes. \nCortical oscillations are likely candidates for segmentation and coding of continuous speech. Here, we monitored continuous speech processing with magnetoencephalography (MEG) to unravel the principles of speech segmentation and coding. We demonstrate that speech entrains the phase of low-frequency (delta, theta) and the amplitude of high-frequency (gamma) oscillations in the auditory cortex. Phase entrainment is stronger in the right and amplitude entrainment is stronger in the left auditory cortex. Furthermore, edges in the speech envelope phase reset auditory cortex oscillations thereby enhancing their entrainment to speech. This mechanism adapts to the changing physical features of the speech envelope and enables efficient, stimulus-specific speech sampling. Finally, we show that within the auditory cortex, coupling between delta, theta, and gamma oscillations increases following speech edges. Importantly, all couplings (i.e., brain-speech and also within the cortex) attenuate for backward-presented speech, suggesting top-down control. We conclude that segmentation and coding of speech relies on a nested hierarchy of entrained cortical oscillations.\nAuthor Summary: Continuous speech is organized into a nested hierarchy of quasi-rhythmic components (prosody, syllables, phonemes) with different time scales. Interestingly, neural activity in the human auditory cortex shows rhythmic modulations with frequencies that match these speech rhythms. Here, we use magnetoencephalography and information theory to study brain oscillations in participants as they process continuous speech. We show that auditory brain oscillations at different frequencies align with the rhythmic structure of speech. This alignment is more precise when participants listen to intelligible rather than unintelligible speech. The onset of speech resets brain oscillations and improves their alignment to speech rhythms; it also improves the alignment between the different frequencies of nested brain oscillations in the auditory cortex. Since these brain oscillations reflect rhythmic changes in neural excitability, they are strong candidates for mediating the segmentation of continuous speech at different time scales corresponding to key speech components such as syllables and phonemes. "], "author_display": ["Joachim Gross", "Nienke Hoogenboom", "Gregor Thut", "Philippe Schyns", "Stefano Panzeri", "Pascal Belin", "Simon Garrod"], "article_type": "Research Article", "score": 0.37822327, "title_display": "Speech Rhythms and Multiplexed Oscillatory Sensory Coding in the Human Brain", "publication_date": "2013-12-31T00:00:00Z", "eissn": "1545-7885", "id": "10.1371/journal.pbio.1001752"}, {"journal": "PLoS ONE", "abstract": ["Background: Using the conceptual framework of shared decision-making and evidence-based practice, a web portal was developed to serve as a generic (non disease-specific) tailored intervention to improve the lay public's health literacy skills. Objective: To evaluate the effects of the web portal compared to no intervention in a real-life setting. Methods: A pragmatic randomised controlled parallel trial using simple randomisation of 96 parents who had children aged <4 years. Parents were allocated to receive either access to the portal or no intervention, and assigned three tasks to perform over a three-week period. These included a searching task, a critical appraisal task, and reporting on perceptions about participation. Data were collected from March through June 2011. Results: Use of the web portal was found to improve attitudes towards searching for health information. This variable was identified as the most important predictor of intention to search in both samples. Participants considered the web portal to have good usability, usefulness, and credibility. The intervention group showed slight increases in the use of evidence-based information, critical appraisal skills, and participation compared to the group receiving no intervention, but these differences were not statistically significant. Conclusion: Despite the fact that the study was underpowered, we found that the web portal may have a positive effect on attitudes towards searching for health information. Furthermore, participants considered the web portal to be a relevant tool. It is important to continue experimenting with web-based resources in order to increase user participation in health care decision-making. Trial Registration: ClinicalTrials.gov NCT01266798 "], "author_display": ["Astrid Austvoll-Dahlgren", "Arild Bj\u00f8rndal", "Jan Odgaard-Jensen", "S\u00f8lvi Helseth"], "article_type": "Research Article", "score": 0.37782466, "title_display": "Evaluation of a Web Portal for Improving Public Access to Evidence-Based Health Information and Health Literacy Skills: A Pragmatic Trial", "publication_date": "2012-05-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0037715"}, {"journal": "PLoS ONE", "abstract": ["The regulation of gene expression in a cell relies to a major extent on transcription factors, proteins which recognize and bind the DNA at specific binding sites (response elements) within promoter regions associated with each gene. We present an information theoretic approach to modeling transcriptional regulatory networks, in terms of a simple \u201csequence-matching\u201d rule and the statistics of the occurrence of binding sequences of given specificity in random promoter regions. The crucial biological input is the distribution of the amount of information coded in these cognate response elements and the length distribution of the promoter regions. We provide an analysis of the transcriptional regulatory network of yeast Saccharomyces cerevisiae, which we extract from the available databases, with respect to the degree distributions, clustering coefficient, degree correlations, rich-club coefficient and the k-core structure. We find that these topological features are in remarkable agreement with those predicted by our model, on the basis of the amount of information coded in the interaction between the transcription factors and response elements."], "author_display": ["Duygu Balcan", "Alkan Kabak\u00e7\u0131o\u011flu", "Muhittin Mungan", "Ay\u015fe Erzan"], "article_type": "Research Article", "score": 0.37765902, "title_display": "The Information Coded in the Yeast Response Elements Accounts for Most of the Topological Properties of Its Transcriptional Regulation Network", "publication_date": "2007-06-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0000501"}, {"journal": "PLOS ONE", "abstract": ["\nRecent improvements in online information communication and mobile location-aware technologies have led to the production of large volumes of volunteered geographic information. Widespread, large-scale efforts by volunteers to collect data can inform and drive scientific advances in diverse fields, including ecology and climatology. Traditional workflows to check the quality of such volunteered information can be costly and time consuming as they heavily rely on human interventions. However, identifying factors that can influence data quality, such as inconsistency, is crucial when these data are used in modeling and decision-making frameworks. Recently developed workflows use simple statistical approaches that assume that the majority of the information is consistent. However, this assumption is not generalizable, and ignores underlying geographic and environmental contextual variability that may explain apparent inconsistencies. Here we describe an automated workflow to check inconsistency based on the availability of contextual environmental information for sampling locations. The workflow consists of three steps: (1) dimensionality reduction to facilitate further analysis and interpretation of results, (2) model-based clustering to group observations according to their contextual conditions, and (3) identification of inconsistent observations within each cluster. The workflow was applied to volunteered observations of flowering in common and cloned lilac plants (Syringa vulgaris and Syringa x chinensis) in the United States for the period 1980 to 2013. About 97% of the observations for both common and cloned lilacs were flagged as consistent, indicating that volunteers provided reliable information for this case study. Relative to the original dataset, the exclusion of inconsistent observations changed the apparent rate of change in lilac bloom dates by two days per decade, indicating the importance of inconsistency checking as a key step in data quality assessment for volunteered geographic information. Initiatives that leverage volunteered geographic information can adapt this workflow to improve the quality of their datasets and the robustness of their scientific analyses.\n"], "author_display": ["Hamed Mehdipoor", "Raul Zurita-Milla", "Alyssa Rosemartin", "Katharine L. Gerst", "Jake F. Weltzin"], "article_type": "Research Article", "score": 0.3774498, "title_display": "Developing a Workflow to Identify Inconsistencies in Volunteered Geographic Information: A Phenological Case Study", "publication_date": "2015-10-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0140811"}, {"journal": "PLoS ONE", "abstract": ["Background: In many scientific disciplines the use of a metaphor as an heuristic aid is not uncommon. A well known example in somatic medicine is the \u2018defense army metaphor\u2019 used to characterize the immune system. In fact, probably a large part of the everyday work of doctors consists of \u2018translating\u2019 scientific and clinical information (i.e. causes of disease, percentage of succes versus risk of side-effects) into information tailored to the needs and capacities of the individual patient. The ability to do so in an effective way is at least partly what makes a clinician a good communicator. Schizophrenia is a severe psychiatric disorder which affects approximately 1% of the population. Over the last two decades a large amount of molecular-biological, imaging and genetic data have been accumulated regarding the biological underpinnings of schizophrenia. However, it remains difficult to understand how the characteristic symptoms of schizophrenia such as hallucinations and delusions are related to disturbances on the molecular-biological level. In general, psychiatry seems to lack a conceptual framework with sufficient explanatory power to link the mental- and molecular-biological domains. Methodology/Principal Findings: Here, we present an essay-like study in which we propose to use visualized concepts stemming from the theory on dynamical complex systems as a \u2018visual metaphor\u2019 to bridge the mental- and molecular-biological domains in schizophrenia. We first describe a computer model of neural information processing; we show how the information processing in this model can be visualized, using concepts from the theory on complex systems. We then describe two computer models which have been used to investigate the primary theory on schizophrenia, the neurodevelopmental model, and show how disturbed information processing in these two computer models can be presented in terms of the visual metaphor previously described. Finally, we describe the effects of dopamine neuromodulation, of which disturbances have been frequently described in schizophrenia, in terms of the same visualized metaphor. Conclusions/Significance: The conceptual framework and metaphor described offers a heuristic tool to understand the relationship between the mental- and molecular-biological domains in an intuitive way. The concepts we present may serve to facilitate communication between researchers, clinicians and patients. "], "author_display": ["Nico J. M. van Beveren", "Lieuwe de Haan"], "article_type": "Research Article", "score": 0.37738943, "title_display": "A Visual Metaphor Describing Neural Dynamics in Schizophrenia", "publication_date": "2008-07-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0002577"}, {"journal": "PLOS ONE", "abstract": ["Background: Integrated 18F-fluorodeoxyglucose positron emission tomography/computed tomography (18F-FDG PET/CT) is widely performed for staging solitary pulmonary nodules (SPNs). However, the diagnostic efficacy of SPNs based on PET/CT is not optimal. Here, we propose a method of detection based on PET/CT that can differentiate malignant and benign SPNs with few false-positives. Method: Our proposed method combines the features of positron-emission tomography (PET) and computed tomography (CT). A dynamic threshold segmentation method was used to identify lung parenchyma in CT images and suspicious areas in PET images. Then, an improved watershed method was used to mark suspicious areas on the CT image. Next, the support vector machine (SVM) method was used to classify SPNs based on textural features of CT images and metabolic features of PET images to validate the proposed method. Results: Our proposed method was more efficient than traditional methods and methods based on the CT or PET features alone (sensitivity 95.6%; average of 2.9 false positives per scan). "], "author_display": ["Juanjuan Zhao", "Guohua Ji", "Yan Qiang", "Xiaohong Han", "Bo Pei", "Zhenghao Shi"], "article_type": "Research Article", "score": 0.37738886, "title_display": "A New Method of Detecting Pulmonary Nodules with PET/CT Based on an Improved Watershed Algorithm", "publication_date": "2015-04-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0123694"}, {"journal": "PLoS ONE", "abstract": ["Objective: The objective of the study was to identify the occurrence and outcome of low back ache amongst computer users and their relation to age, gender, occupation and duration of computer use. Materials and Methods: A self reported questionnaire tailored from Occupational Health and Safety Act of the Ministry of Labor, Ontario, Canada was used. Results: 416 participants 55.5% males and 45% females using computers for a minimum of five years with age range 22 to 59 years belonged to different occupational groups. Consecutive hours of computer work was found to be associated with work related backache or discomfort in 27.4% (n\u200a=\u200a114) participants (16.1% male, 11.3% female). Frequent short breaks improved backache (p value <0.001) in 93 (22.4%) participants (13.2% male, 9.2% female). No significant relation was observed with the duration of computer usage or usage per day; between the two genders or occupational groups. Backache had no significance within age groups. Conclusion: Our study identifies the occurrence of low back pain among those who are using computer for consecutive hours without breaks and the results suggest the need to create health awareness especially use of short breaks to minimize the risk and occurrence of low back pain. The result of this study can also be used to improve ergonomic design and standards. "], "author_display": ["Rehana Rehman", "Rakhshaan Khan", "Ambreen Surti", "Hira Khan"], "article_type": "Research Article", "score": 0.37714013, "title_display": "An Ounce of Discretion Is Worth a Pound of Wit \u2014 Ergonomics Is a Healthy Choice", "publication_date": "2013-10-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0071891"}, {"journal": "PLoS ONE", "abstract": ["\nDecision makers in epidemiology and other disciplines are faced with the daunting challenge of designing interventions that will be successful with high probability and robust against a multitude of uncertainties. To facilitate the decision making process in the context of a goal-oriented objective (e.g., eradicate polio by ), stochastic models can be used to map the probability of achieving the goal as a function of parameters. Each run of a stochastic model can be viewed as a Bernoulli trial in which \u201csuccess\u201d is returned if and only if the goal is achieved in simulation. However, each run can take a significant amount of time to complete, and many replicates are required to characterize each point in parameter space, so specialized algorithms are required to locate desirable interventions. To address this need, we present the Separatrix Algorithm, which strategically locates parameter combinations that are expected to achieve the goal with a user-specified probability of success (e.g. 95%). Technically, the algorithm iteratively combines density-corrected binary kernel regression with a novel information-gathering experiment design to produce results that are asymptotically correct and work well in practice. The Separatrix Algorithm is demonstrated on several test problems, and on a detailed individual-based simulation of malaria.\n"], "author_display": ["Daniel J. Klein", "Michael Baym", "Philip Eckhoff"], "article_type": "Research Article", "score": 0.37690988, "title_display": "The Separatrix Algorithm for Synthesis and Analysis of Stochastic Simulations with Applications in Disease Modeling", "publication_date": "2014-07-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0103467"}, {"journal": "PLoS ONE", "abstract": ["\nThe transmission of infectious diseases can be affected by many or even hidden factors, making it difficult to accurately predict when and where outbreaks may emerge. One approach at the moment is to develop and deploy surveillance systems in an effort to detect outbreaks as timely as possible. This enables policy makers to modify and implement strategies for the control of the transmission. The accumulated surveillance data including temporal, spatial, clinical, and demographic information, can provide valuable information with which to infer the underlying epidemic networks. Such networks can be quite informative and insightful as they characterize how infectious diseases transmit from one location to another. The aim of this work is to develop a computational model that allows inferences to be made regarding epidemic network topology in heterogeneous populations. We apply our model on the surveillance data from the 2009 H1N1 pandemic in Hong Kong. The inferred epidemic network displays significant effect on the propagation of infectious diseases.\n"], "author_display": ["Xiang Wan", "Jiming Liu", "William K. Cheung", "Tiejun Tong"], "article_type": "Research Article", "score": 0.37675396, "title_display": "Inferring Epidemic Network Topology from Surveillance Data", "publication_date": "2014-06-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0100661"}, {"journal": "PLOS ONE", "abstract": ["\nWe present, to our knowledge, the first demonstration that a non-invasive brain-to-brain interface (BBI) can be used to allow one human to guess what is on the mind of another human through an interactive question-and-answering paradigm similar to the \u201c20 Questions\u201d game. As in previous non-invasive BBI studies in humans, our interface uses electroencephalography (EEG) to detect specific patterns of brain activity from one participant (the \u201crespondent\u201d), and transcranial magnetic stimulation (TMS) to deliver functionally-relevant information to the brain of a second participant (the \u201cinquirer\u201d). Our results extend previous BBI research by (1) using stimulation of the visual cortex to convey visual stimuli that are privately experienced and consciously perceived by the inquirer; (2) exploiting real-time rather than off-line communication of information from one brain to another; and (3) employing an interactive task, in which the inquirer and respondent must exchange information bi-directionally to collaboratively solve the task. The results demonstrate that using the BBI, ten participants (five inquirer-respondent pairs) can successfully identify a \u201cmystery item\u201d using a true/false question-answering protocol similar to the \u201c20 Questions\u201d game, with high levels of accuracy that are significantly greater than a control condition in which participants were connected through a sham BBI.\n"], "author_display": ["Andrea Stocco", "Chantel S. Prat", "Darby M. Losey", "Jeneva A. Cronin", "Joseph Wu", "Justin A. Abernethy", "Rajesh P. N. Rao"], "article_type": "Research Article", "score": 0.3767283, "title_display": "Playing 20 Questions with the Mind: Collaborative Problem Solving by Humans Using a Brain-to-Brain Interface", "publication_date": "2015-09-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0137303"}, {"journal": "PLoS ONE", "abstract": ["Background: Many areas of biology are open to mathematical and computational modelling. The application of discrete, logical formalisms defines the field of biomedical ontologies. Ontologies have been put to many uses in bioinformatics. The most widespread is for description of entities about which data have been collected, allowing integration and analysis across multiple resources. There are now over 60 ontologies in active use, increasingly developed as large, international collaborations. There are, however, many opinions on how ontologies should be authored; that is, what is appropriate for representation. Recently, a common opinion has been the \u201crealist\u201d approach that places restrictions upon the style of modelling considered to be appropriate. Methodology/Principal Findings: Here, we use a number of case studies for describing the results of biological experiments. We investigate the ways in which these could be represented using both realist and non-realist approaches; we consider the limitations and advantages of each of these models. Conclusions/Significance: From our analysis, we conclude that while realist principles may enable straight-forward modelling for some topics, there are crucial aspects of science and the phenomena it studies that do not fit into this approach; realism appears to be over-simplistic which, perversely, results in overly complex ontological models. We suggest that it is impossible to avoid compromise in modelling ontology; a clearer understanding of these compromises will better enable appropriate modelling, fulfilling the many needs for discrete mathematical models within computational biology. "], "author_display": ["Phillip Lord", "Robert Stevens"], "article_type": "Research Article", "score": 0.3762107, "title_display": "Adding a Little Reality to Building Ontologies for Biology", "publication_date": "2010-09-03T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0012258"}, {"journal": "PLoS Computational Biology", "abstract": ["\nComplex networks of interacting residues and microdomains in the structures of biomolecular systems underlie the reliable propagation of information from an input signal, such as the concentration of a ligand, to sites that generate the appropriate output signal, such as enzymatic activity. This information transduction often carries the signal across relatively large distances at the molecular scale in a form of allostery that is essential for the physiological functions performed by biomolecules. While allosteric behaviors have been documented from experiments and computation, the mechanism of this form of allostery proved difficult to identify at the molecular level. Here, we introduce a novel analysis framework, called N-body Information Theory (NbIT) analysis, which is based on information theory and uses measures of configurational entropy in a biomolecular system to identify microdomains and individual residues that act as (i)-channels for long-distance information sharing between functional sites, and (ii)-coordinators that organize dynamics within functional sites. Application of the new method to molecular dynamics (MD) trajectories of the occluded state of the bacterial leucine transporter LeuT identifies a channel of allosteric coupling between the functionally important intracellular gate and the substrate binding sites known to modulate it. NbIT analysis is shown also to differentiate residues involved primarily in stabilizing the functional sites, from those that contribute to allosteric couplings between sites. NbIT analysis of MD data thus reveals rigorous mechanistic elements of allostery underlying the dynamics of biomolecular systems.\nAuthor Summary: We developed the new information theory-based analysis framework presented here, NbIT analysis, for the study of allosteric mechanisms in biomolecular systems from Molecular Dynamics trajectories. The illustrative application of NbIT to the analysis of the occluded state in the bacterial transporter LeuT, produced a quantitative representation of the allosteric behavior, and identified intramolecular channels that enable the long-distance information transmission. Our findings, identifying the roles of specific residues in the communication of the allosteric information, were validated by the recognition of residues that have been previously shown to play functional roles in this very well studied system. In addition, we show that application of NbIT analysis leads to the discrimination of functional roles by differentiating between residues that are essential to the dynamics within functional sites (e.g., the substrate binding sites), and residues whose role is to communicate between such functional sites. These results demonstrate that the information theoretical analysis presented here is a powerful tool for quantifying complex allosteric behavior in biomolecular systems and for identifying the crucial components underlying those behaviors. "], "author_display": ["Michael V. LeVine", "Harel Weinstein"], "article_type": "Research Article", "score": 0.37614626, "title_display": "NbIT - A New Information Theory-Based Analysis of Allosteric Mechanisms Reveals Residues that Underlie Function in the Leucine Transporter LeuT", "publication_date": "2014-05-01T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1003603"}, {"journal": "PLoS ONE", "abstract": ["\nThis paper presents a novel object detection method using a single instance from the object category. Our method uses biologically inspired global scene context criteria to check whether every individual location of the image can be naturally replaced by the query instance, which indicates whether there is a similar object at this location. Different from the traditional detection methods that only look at individual locations for the desired objects, our method evaluates the consistency of the entire scene. It is therefore robust to large intra-class variations, occlusions, a minor variety of poses, low-revolution conditions, background clutter etc., and there is no off-line training. The experimental results on four datasets and two video sequences clearly show the superior robustness of the proposed method, suggesting that global scene context is important for visual detection/localization.\n"], "author_display": ["Changxin Gao", "Nong Sang", "Rui Huang"], "article_type": "Research Article", "score": 0.37594602, "title_display": "Biologically Inspired Scene Context for Object Detection Using a Single Instance", "publication_date": "2014-05-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0098447"}, {"journal": "PLOS Biology", "abstract": ["\nThe frontal cortex controls behavioral adaptation in environments governed by complex rules. Many studies have established the relevance of firing rate modulation after informative events signaling whether and how to update the behavioral policy. However, whether the spatiotemporal features of these neuronal activities contribute to encoding imminent behavioral updates remains unclear. We investigated this issue in the dorsal anterior cingulate cortex (dACC) of monkeys while they adapted their behavior based on their memory of feedback from past choices. We analyzed spike trains of both single units and pairs of simultaneously recorded neurons using an algorithm that emulates different biologically plausible decoding circuits. This method permits the assessment of the performance of both spike-count and spike-timing sensitive decoders. In response to the feedback, single neurons emitted stereotypical spike trains whose temporal structure identified informative events with higher accuracy than mere spike count. The optimal decoding time scale was in the range of 70\u2013200 ms, which is significantly shorter than the memory time scale required by the behavioral task. Importantly, the temporal spiking patterns of single units were predictive of the monkeys\u2019 behavioral response time. Furthermore, some features of these spiking patterns often varied between jointly recorded neurons. All together, our results suggest that dACC drives behavioral adaptation through complex spatiotemporal spike coding. They also indicate that downstream networks, which decode dACC feedback signals, are unlikely to act as mere neural integrators.\n\nThe decoding of spike trains from the dorsal Anterior Cingulate Cortex reveals that spike timing is relevant for cognitive information transmission and behavioral prediction, and it suggests that downstream areas are unlikely to act as mere neural integrators.\nAuthor Summary: In classical views of how information is processed in the brain, cognitive areas are often thought to encode incoming signals by a simple summation of spikes\u2014action potentials fired by neurons and transmitted along the nerves\u2014elicited by different cues. It is through this summation of spikes that cognitive areas are hypothesized to combine information from different cues and build memories. We investigated whether summation is relevant during the processing of signals emitted by the dorsal anterior cingulate cortex, a brain area which is thought to control behavioral adaptation in response to feedback cues that indicate the animal\u2019s performance during a task. We found that a mere summation of spikes emitted in response to feedback cues actually extracts significantly less information compared to a code that also takes into account the timing of the spikes. Furthermore, we discovered that this temporal structure of the spike discharges predicts well the future behavior of monkeys. Overall, our findings suggest that the brain areas processing the signals emitted by the dorsal anterior cingulate cortex are sensitive to spike times and, thus, are unlikely to implement a mere approximate summation of inputs. "], "author_display": ["Laureline Logiaco", "Ren\u00e9 Quilodran", "Emmanuel Procyk", "Angelo Arleo"], "article_type": "Research Article", "score": 0.37581533, "title_display": "Spatiotemporal Spike Coding of Behavioral Adaptation in the Dorsal Anterior Cingulate Cortex", "publication_date": "2015-08-12T00:00:00Z", "eissn": "1545-7885", "id": "10.1371/journal.pbio.1002222"}, {"journal": "PLoS ONE", "abstract": ["\nMany biomolecules have machine-like functions, and accordingly are discussed in terms of mechanical properties like force and motion. However, the concept of stress, a mechanical property that is of fundamental importance in the study of macroscopic mechanics, is not commonly applied in the biomolecular context. We anticipate that microscopical stress analyses of biomolecules and nanomaterials will provide useful mechanistic insights and help guide molecular design. To enable such applications, we have developed Calculator of Atomistic Mechanical Stress (CAMS), an open-source software package for computing atomic resolution stresses from molecular dynamics (MD) simulations. The software also enables decomposition of stress into contributions from bonded, nonbonded and Generalized Born potential terms. CAMS reads GROMACS topology and trajectory files, which are easily generated from AMBER files as well; and time-varying stresses may be animated and visualized in the VMD viewer. Here, we review relevant theory and present illustrative applications.\n"], "author_display": ["Andrew T. Fenley", "Hari S. Muddana", "Michael K. Gilson"], "article_type": "Research Article", "score": 0.37548453, "title_display": "Calculation and Visualization of Atomistic Mechanical Stresses in Nanomaterials and Biomolecules", "publication_date": "2014-12-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0113119"}, {"journal": "PLoS ONE", "abstract": ["\n        Informed consent is the cornerstone of human research subject protection. Many subjects sign consent documents without understanding the study purpose, procedures, risks, benefits, and their rights. Proof of comprehension is not required and rarely obtained. Understanding might improve by using an interactive system with multiple options for hearing, viewing and reading about the study and the consent form at the subject\u2019s own pace with testing and immediate feedback. This prospective randomized study compared the IRB-approved paper ICF for an actual clinical research study with an interactive presentation of the same study and its associated consent form using an iPad device in two populations: clinical research professionals, and patients drawn from a variety of outpatient practice settings. Of the 90 participants, 69 completed the online test and survey questions the day after the session (maximum 36 hours post-session). Among research professionals (n\u200a=\u200a14), there was a trend (p \u200a=\u200a.07) in the direction of iPad subjects testing better on the online test (mean correct \u200a=\u200a 77%) compared with paper subjects (mean correct \u200a=\u200a 57%). Among patients (n\u200a=\u200a55), iPad subjects had significantly higher test scores than standard paper consent subjects (mean correct \u200a=\u200a 75% vs 58%, p < .001). For all subjects, the total time spent reviewing the paper consent was 13.2 minutes, significantly less than the average of 22.7 minutes total on the three components to be reviewed using the iPad (introductory video, consent form, interactive quiz). Overall satisfaction and overall enjoyment slightly favored the interactive iPad presentation. This study demonstrates that combining an introductory video, standard consent language, and an interactive quiz on a tablet-based system improves comprehension of research study procedures and risks.\n      "], "author_display": ["Michael C. Rowbotham", "John Astin", "Kaitlin Greene", "Steven R. Cummings"], "article_type": "Research Article", "score": 0.3752962, "title_display": "Interactive Informed Consent: Randomized Comparison with Paper Consents", "publication_date": "2013-03-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0058603"}, {"journal": "PLoS ONE", "abstract": ["\nMolecular descriptors have been explored extensively. From these studies, it is known that a large number of descriptors are strongly correlated and capture similar characteristics of molecules. In this paper, we evaluate 919 Dragon-descriptors of 6 different categories by means of clustering. Also, we analyze these different categories of descriptors also find a subset of descriptors which are least correlated among each other and, hence, characterize molecular graphs distinctively.\n"], "author_display": ["Matthias Dehmer", "Frank Emmert-Streib", "Shailesh Tripathi"], "article_type": "Research Article", "score": 0.37514055, "title_display": "Large-Scale Evaluation of Molecular Descriptors by Means of Clustering", "publication_date": "2013-12-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0083956"}, {"journal": "PLoS ONE", "abstract": ["\nThe systematic investigation of susceptibility-induced contrast in MRI is important to better interpret the influence of microvascular and microcellular morphology on DSC-MRI derived perfusion data. Recently, a novel computational approach called the Finite Perturber Method (FPM), which enables the study of susceptibility-induced contrast in MRI arising from arbitrary microvascular morphologies in 3D has been developed. However, the FPM has lower efficiency in simulating water diffusion especially for complex tissues. In this work, an improved computational approach that combines the FPM with a matrix-based finite difference method (FDM), which we call the Finite Perturber the Finite Difference Method (FPFDM), has been developed in order to efficiently investigate the influence of vascular and extravascular morphological features on susceptibility-induced transverse relaxation. The current work provides a framework for better interpreting how DSC-MRI data depend on various phenomena, including contrast agent leakage in cancerous tissues and water diffusion rates. In addition, we illustrate using simulated and micro-CT extracted tissue structures the improved FPFDM along with its potential applications and limitations.\n"], "author_display": ["Natenael B. Semmineh", "Junzhong Xu", "Jerrold L. Boxerman", "Gary W. Delaney", "Paul W. Cleary", "John C. Gore", "C. Chad Quarles"], "article_type": "Research Article", "score": 0.375132, "title_display": "An Efficient Computational Approach to Characterize DSC-MRI Signals Arising from Three-Dimensional Heterogeneous Tissue Structures", "publication_date": "2014-01-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0084764"}, {"journal": "PLoS ONE", "abstract": ["\n        Bayesian network is one of the most successful graph models for representing the reactive oxygen species regulatory pathway. With the increasing number of microarray measurements, it is possible to construct the Bayesian network from microarray data directly. Although large numbers of Bayesian network learning algorithms have been developed, when applying them to learn Bayesian networks from microarray data, the accuracies are low due to that the databases they used to learn Bayesian networks contain too few microarray data. In this paper, we propose a consensus Bayesian network which is constructed by combining Bayesian networks from relevant literatures and Bayesian networks learned from microarray data. It would have a higher accuracy than the Bayesian networks learned from one database. In the experiment, we validated the Bayesian network combination algorithm on several classic machine learning databases and used the consensus Bayesian network to model the 's ROS pathway.\n      "], "author_display": ["Liangdong Hu", "Limin Wang"], "article_type": "Research Article", "score": 0.37451324, "title_display": "Using Consensus Bayesian Network to Model the Reactive Oxygen Species Regulatory Pathway", "publication_date": "2013-02-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0056832"}, {"journal": "PLOS ONE", "abstract": ["\nThe reconstruction of gene regulatory networks (GRNs) from high-throughput experimental data has been considered one of the most important issues in systems biology research. With the development of high-throughput technology and the complexity of biological problems, we need to reconstruct GRNs that contain thousands of genes. However, when many existing algorithms are used to handle these large-scale problems, they will encounter two important issues: low accuracy and high computational cost. To overcome these difficulties, the main goal of this study is to design an effective parallel algorithm to infer large-scale GRNs based on high-performance parallel computing environments. In this study, we proposed a novel asynchronous parallel framework to improve the accuracy and lower the time complexity of large-scale GRN inference by combining splitting technology and ordinary differential equation (ODE)-based optimization. The presented algorithm uses the sparsity and modularity of GRNs to split whole large-scale GRNs into many small-scale modular subnetworks. Through the ODE-based optimization of all subnetworks in parallel and their asynchronous communications, we can easily obtain the parameters of the whole network. To test the performance of the proposed approach, we used well-known benchmark datasets from Dialogue for Reverse Engineering Assessments and Methods challenge (DREAM), experimentally determined GRN of Escherichia coli and one published dataset that contains more than 10 thousand genes to compare the proposed approach with several popular algorithms on the same high-performance computing environments in terms of both accuracy and time complexity. The numerical results demonstrate that our parallel algorithm exhibits obvious superiority in inferring large-scale GRNs.\n"], "author_display": ["Xiangyun Xiao", "Wei Zhang", "Xiufen Zou"], "article_type": "Research Article", "score": 0.37451324, "title_display": "A New Asynchronous Parallel Algorithm for Inferring Large-Scale Gene Regulatory Networks", "publication_date": "2015-03-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0119294"}, {"abstract": ["\n        We analyze simple dynamical network models which describe the limited capacity of nodes to process the input information. For a proper range of their parameters, the information flow pattern in these models is characterized by exponential distribution of the incoming information and a fat-tailed distribution of the outgoing information, as a signature of the law of diminishing marginal returns. We apply this analysis to effective connectivity networks from human EEG signals, obtained by Granger Causality, which has recently been given an interpretation in the framework of information theory. From the distributions of the incoming versus the outgoing values of the information flow it is evident that the incoming information is exponentially distributed whilst the outgoing information shows a fat tail. This suggests that overall brain effective connectivity networks may also be considered in the light of the law of diminishing marginal returns. Interestingly, this pattern is reproduced locally but with a clear modulation: a topographic analysis has also been made considering the distribution of incoming and outgoing values at each electrode, suggesting a functional role for this phenomenon.\n      "], "author_display": ["Daniele Marinazzo", "Guorong Wu", "Mario Pellicoro", "Leonardo Angelini", "Sebastiano Stramaglia"], "article_type": "Research Article", "score": 0.37441564, "title_display": "Information Flow in Networks and the Law of Diminishing Marginal Returns: Evidence from Modeling and Human Electroencephalographic Recordings", "publication_date": "2012-09-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0045026"}, {"journal": "PLoS ONE", "abstract": ["\nIt is a classic topic of social network analysis to evaluate the importance of nodes and identify the node that takes on the role of core or bridge in a network. Because a single indicator is not sufficient to analyze multiple characteristics of a node, it is a natural solution to apply multiple indicators that should be selected carefully. An intuitive idea is to select some indicators with weak correlations to efficiently assess different characteristics of a node. However, this paper shows that it is much better to select the indicators with strong correlations. Because indicator correlation is based on the statistical analysis of a large number of nodes, the particularity of an important node will be outlined if its indicator relationship doesn't comply with the statistical correlation. Therefore, the paper selects the multiple indicators including degree, ego-betweenness centrality and eigenvector centrality to evaluate the importance and the role of a node. The importance of a node is equal to the normalized sum of its three indicators. A candidate for core or bridge is selected from the great degree nodes or the nodes with great ego-betweenness centrality respectively. Then, the role of a candidate is determined according to the difference between its indicators' relationship with the statistical correlation of the overall network. Based on 18 real networks and 3 kinds of model networks, the experimental results show that the proposed methods perform quite well in evaluating the importance of nodes and in identifying the node role.\n"], "author_display": ["Shaobin Huang", "Tianyang Lv", "Xizhe Zhang", "Yange Yang", "Weimin Zheng", "Chao Wen"], "article_type": "Research Article", "score": 0.3742193, "title_display": "Identifying Node Role in Social Network Based on Multiple Indicators", "publication_date": "2014-08-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0103733"}, {"journal": "PLoS Biology", "abstract": ["\n        Plasticity studies suggest that behavioral relevance can change the cortical processing of trained or conditioned sensory stimuli. However, whether this occurs in the context of natural communication, where stimulus significance is acquired through social interaction, has not been well investigated, perhaps because neural responses to species-specific vocalizations can be difficult to interpret within a systematic framework. The ultrasonic communication system between isolated mouse pups and adult females that either do or do not recognize the calls' significance provides an opportunity to explore this issue. We applied an information-based analysis to multi- and single unit data collected from anesthetized mothers and pup-na\u00efve females to quantify how the communicative significance of pup calls affects their encoding in the auditory cortex. The timing and magnitude of information that cortical responses convey (at a 2-ms resolution) for pup call detection and discrimination was significantly improved in mothers compared to na\u00efve females, most likely because of changes in call frequency encoding. This was not the case for a non-natural sound ensemble outside the mouse vocalization repertoire. The results demonstrate that a sensory cortical change in the timing code for communication sounds is correlated with the vocalizations' behavioral relevance, potentially enhancing functional processing by improving its signal to noise ratio.\n      : Like a student in a foreign country immersed in an unfamiliar language or a young mother trying to decipher her baby's cries, we all encounter initially meaningless sounds that in fact carry meaning. As these sounds gain significance, we become better at detecting and discriminating between them. How does this occur? What happens in our brain to facilitate this improvement? We explored these questions in a mouse model by measuring how neurons in the auditory cortex of female mice respond when the ultrasonic calls of mouse pups are played back to the animals. Earlier studies demonstrated that mothers, but not virgin females, recognize these calls as behaviorally significant. Our results indicate that the timing and magnitude of the auditory cortical responses to these communicative sounds differ between these two groups of female mice and that this difference may provide the auditory system in mothers with the capacity for detecting and discriminating pup calls. The results demonstrate that behavioral significance can be correlated with quantifiable functional improvements in the sensory cortical representation of a communication sound. \n        Pup calls produce quicker and larger neural responses, which convey more information for pup call detection and discrimination, in the auditory cortex of mother mice compared with virgin female mice.\n      "], "author_display": ["Robert C Liu", "Christoph E Schreiner"], "article_type": "Research Article", "score": 0.37387016, "title_display": "Auditory Cortical Detection and Discrimination Correlates with Communicative Significance", "publication_date": "2007-06-12T00:00:00Z", "eissn": "1545-7885", "id": "10.1371/journal.pbio.0050173"}, {"abstract": ["Background: It is estimated that 15 to 20 million people are infected with the human T-cell lymphotropic virus type 1 (HTLV-1). At present, there are more than 2,000 unique HTLV-1 isolate sequences published. A central database to aggregate sequence information from a range of epidemiological aspects including HTLV-1 infections, pathogenesis, origins, and evolutionary dynamics would be useful to scientists and physicians worldwide. Described here, we have developed a database that collects and annotates sequence data and can be accessed through a user-friendly search interface. The HTLV-1 Molecular Epidemiology Database website is available at http://htlv1db.bahia.fiocruz.br/. Methodology/Principal Findings: All data was obtained from publications available at GenBank or through contact with the authors. The database was developed using Apache Webserver 2.1.6 and SGBD MySQL. The webpage interfaces were developed in HTML and sever-side scripting written in PHP. The HTLV-1 Molecular Epidemiology Database is hosted on the Gon\u00e7alo Moniz/FIOCRUZ Research Center server. There are currently 2,457 registered sequences with 2,024 (82.37%) of those sequences representing unique isolates. Of these sequences, 803 (39.67%) contain information about clinical status (TSP/HAM, 17.19%; ATL, 7.41%; asymptomatic, 12.89%; other diseases, 2.17%; and no information, 60.32%). Further, 7.26% of sequences contain information on patient gender while 5.23% of sequences provide the age of the patient. Conclusions/Significance: The HTLV-1 Molecular Epidemiology Database retrieves and stores annotated HTLV-1 proviral sequences from clinical, epidemiological, and geographical studies. The collected sequences and related information are now accessible on a publically available and user-friendly website. This open-access database will support clinical research and vaccine development related to viral genotype. "], "author_display": ["Thessika Hialla Almeida Araujo", "Leandro Inacio Souza-Brito", "Pieter Libin", "Koen Deforche", "Dustin Edwards", "Antonio Eduardo de Albuquerque-Junior", "Anne-Mieke Vandamme", "Bernardo Galvao-Castro", "Luiz Carlos Junior Alcantara"], "article_type": "Research Article", "score": 0.3734526, "title_display": "A Public HTLV-1 Molecular Epidemiology Database for Sequence Management and Data Mining", "publication_date": "2012-09-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0042123"}, {"journal": "PLoS ONE", "abstract": ["\nIn sample surveys, it is usual to make use of auxiliary information to increase the precision of estimators. We propose a new exponential ratio-type estimator of a finite population mean using linear combination of two auxiliary variables and obtain mean square error (MSE) equation for proposed estimator. We find theoretical conditions that make proposed estimator more efficient than traditional multivariate ratio estimator using information of two auxiliary variables, the estimator of Bahl and Tuteja and the estimator proposed by Abu-Dayeh et al. In addition, we support these theoretical results with the aid of two numerical examples.\n"], "author_display": ["Jingli Lu", "Zaizai Yan", "Xiuyun Peng"], "article_type": "Research Article", "score": 0.3733592, "title_display": "A New Exponential Ratio-Type Estimator with Linear Combination of Two Auxiliary Variables", "publication_date": "2014-12-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0116124"}, {"journal": "PLoS ONE", "abstract": ["\n        Evaluating other individuals with respect to personality characteristics plays a crucial role in human relations and it is the focus of attention for research in diverse fields such as psychology and interactive computer systems. In psychology, face perception has been recognized as a key component of this evaluation system. Multiple studies suggest that observers use face information to infer personality characteristics. Interactive computer systems are trying to take advantage of these findings and apply them to increase the natural aspect of interaction and to improve the performance of interactive computer systems. Here, we experimentally test whether the automatic prediction of facial trait judgments (e.g. dominance) can be made by using the full appearance information of the face and whether a reduced representation of its structure is sufficient. We evaluate two separate approaches: a holistic representation model using the facial appearance information and a structural model constructed from the relations among facial salient points. State of the art machine learning methods are applied to a) derive a facial trait judgment model from training data and b) predict a facial trait value for any face. Furthermore, we address the issue of whether there are specific structural relations among facial points that predict perception of facial traits. Experimental results over a set of labeled data (9 different trait evaluations) and classification rules (4 rules) suggest that a) prediction of perception of facial traits is learnable by both holistic and structural approaches; b) the most reliable prediction of facial trait judgments is obtained by certain type of holistic descriptions of the face appearance; and c) for some traits such as attractiveness and extroversion, there are relationships between specific structural features and social perceptions.\n      "], "author_display": ["Mario Rojas Q.", "David Masip", "Alexander Todorov", "Jordi Vitria"], "article_type": "Research Article", "score": 0.3728165, "title_display": "Automatic Prediction of Facial Trait Judgments: Appearance vs. Structural Models", "publication_date": "2011-08-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0023323"}, {"journal": "PLOS ONE", "abstract": ["\nEmerging neural theories of consciousness suggest a correlation between a specific type of neural dynamical complexity and the level of consciousness: When awake and aware, causal interactions between brain regions are both integrated (all regions are to a certain extent connected) and differentiated (there is inhomogeneity and variety in the interactions). In support of this, recent work by Casali et al (2013) has shown that Lempel-Ziv complexity correlates strongly with conscious level, when computed on the EEG response to transcranial magnetic stimulation. Here we investigated complexity of spontaneous high-density EEG data during propofol-induced general anaesthesia. We consider three distinct measures: (i) Lempel-Ziv complexity, which is derived from how compressible the data are; (ii) amplitude coalition entropy, which measures the variability in the constitution of the set of active channels; and (iii) the novel synchrony coalition entropy (SCE), which measures the variability in the constitution of the set of synchronous channels. After some simulations on Kuramoto oscillator models which demonstrate that these measures capture distinct \u2018flavours\u2019 of complexity, we show that there is a robustly measurable decrease in the complexity of spontaneous EEG during general anaesthesia.\n"], "author_display": ["Michael Schartner", "Anil Seth", "Quentin Noirhomme", "Melanie Boly", "Marie-Aurelie Bruno", "Steven Laureys", "Adam Barrett"], "article_type": "Research Article", "score": 0.37250936, "title_display": "Complexity of Multi-Dimensional Spontaneous EEG Decreases during Propofol Induced General Anaesthesia", "publication_date": "2015-08-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0133532"}, {"journal": "PLoS ONE", "abstract": ["\n\t\t\t\tThis software package provides an R-based framework to make use of multi-core computers when running analyses in the population genetics program STRUCTURE. It is especially addressed to those users of STRUCTURE dealing with numerous and repeated data analyses, and who could take advantage of an efficient script to automatically distribute STRUCTURE jobs among multiple processors. It also consists of additional functions to divide analyses among combinations of populations within a single data set without the need to manually produce multiple projects, as it is currently the case in STRUCTURE. The package consists of two main functions: MPI_structure() and parallel_structure() as well as an example data file. We compared the performance in computing time for this example data on two computer architectures and showed that the use of the present functions can result in several-fold improvements in terms of computation time. ParallelStructure is freely available at https://r-forge.r-project.org/projects/parallstructure/.\n\t\t\t"], "author_display": ["Francois Besnier", "Kevin A. Glover"], "article_type": "Research Article", "score": 0.3724991, "title_display": "ParallelStructure: A R Package to Distribute Parallel Runs of the Population Genetics Program STRUCTURE on Multi-Core Computers", "publication_date": "2013-07-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0070651"}, {"journal": "PLoS ONE", "abstract": ["\nHuman sensory and motor systems provide the natural means for the exchange of information between individuals, and, hence, the basis for human civilization. The recent development of brain-computer interfaces (BCI) has provided an important element for the creation of brain-to-brain communication systems, and precise brain stimulation techniques are now available for the realization of non-invasive computer-brain interfaces (CBI). These technologies, BCI and CBI, can be combined to realize the vision of non-invasive, computer-mediated brain-to-brain (B2B) communication between subjects (hyperinteraction). Here we demonstrate the conscious transmission of information between human brains through the intact scalp and without intervention of motor or peripheral sensory systems. Pseudo-random binary streams encoding words were transmitted between the minds of emitter and receiver subjects separated by great distances, representing the realization of the first human brain-to-brain interface. In a series of experiments, we established internet-mediated B2B communication by combining a BCI based on voluntary motor imagery-controlled electroencephalographic (EEG) changes with a CBI inducing the conscious perception of phosphenes (light flashes) through neuronavigated, robotized transcranial magnetic stimulation (TMS), with special care taken to block sensory (tactile, visual or auditory) cues. Our results provide a critical proof-of-principle demonstration for the development of conscious B2B communication technologies. More fully developed, related implementations will open new research venues in cognitive, social and clinical neuroscience and the scientific study of consciousness. We envision that hyperinteraction technologies will eventually have a profound impact on the social structure of our civilization and raise important ethical issues.\n"], "author_display": ["Carles Grau", "Romuald Ginhoux", "Alejandro Riera", "Thanh Lam Nguyen", "Hubert Chauvat", "Michel Berg", "Juli\u00e0 L. Amengual", "Alvaro Pascual-Leone", "Giulio Ruffini"], "article_type": "Research Article", "score": 0.37235433, "title_display": "Conscious Brain-to-Brain Communication in Humans Using Non-Invasive Technologies", "publication_date": "2014-08-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0105225"}, {"journal": "PLOS ONE", "abstract": ["\nHow can we enhance the understanding of abstract mathematical principles in elementary school? Different studies found out that nonsymbolic estimation could foster subsequent exact number processing and simple arithmetic. Taking the commutativity principle as a test case, we investigated if the approximate calculation of symbolic commutative quantities can also alter the access to procedural and conceptual knowledge of a more abstract arithmetic principle. Experiment 1 tested first graders who had not been instructed about commutativity in school yet. Approximate calculation with symbolic quantities positively influenced the use of commutativity-based shortcuts in formal arithmetic. We replicated this finding with older first graders (Experiment 2) and third graders (Experiment 3). Despite the positive effect of approximation on the spontaneous application of commutativity-based shortcuts in arithmetic problems, we found no comparable impact on the application of conceptual knowledge of the commutativity principle. Overall, our results show that the usage of a specific arithmetic principle can benefit from approximation. However, the findings also suggest that the correct use of certain procedures does not always imply conceptual understanding. Rather, the conceptual understanding of commutativity seems to lag behind procedural proficiency during elementary school.\n"], "author_display": ["Sonja Maria Hansen", "Hilde Haider", "Alexandra Eichler", "Claudia Godau", "Peter A. Frensch", "Robert Gaschler"], "article_type": "Research Article", "score": 0.37227145, "title_display": "Fostering Formal Commutativity Knowledge with Approximate Arithmetic", "publication_date": "2015-11-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0142551"}, {"journal": "PLoS ONE", "abstract": ["\nThe automatic detection and tracking of human eyes and, in particular, the precise localization of their centers (pupils), is a widely debated topic in the international scientific community. In fact, the extracted information can be effectively used in a large number of applications ranging from advanced interfaces to biometrics and including also the estimation of the gaze direction, the control of human attention and the early screening of neurological pathologies. Independently of the application domain, the detection and tracking of the eye centers are, currently, performed mainly using invasive devices. Cheaper and more versatile systems have been only recently introduced: they make use of image processing techniques working on periocular patches which can be specifically acquired or preliminarily cropped from facial images. In the latter cases the involved algorithms must work even in cases of non-ideal acquiring conditions (e.g in presence of noise, low spatial resolution, non-uniform lighting conditions, etc.) and without user's awareness (thus with possible variations of the eye in scale, rotation and/or translation). Getting satisfying results in pupils' localization in such a challenging operating conditions is still an open scientific topic in Computer Vision. Actually, the most performing solutions in the literature are, unfortunately, based on supervised machine learning algorithms which require initial sessions to set the working parameters and to train the embedded learning models of the eye: this way, experienced operators have to work on the system each time it is moved from an operational context to another. It follows that the use of unsupervised approaches is more and more desirable but, unfortunately, their performances are not still satisfactory and more investigations are required. To this end, this paper proposes a new unsupervised approach to automatically detect the center of the eye: its algorithmic core is a representation of the eye's shape that is obtained through a differential analysis of image intensities and the subsequent combination with the local variability of the appearance represented by self-similarity coefficients. The experimental evidence of the effectiveness of the method was demonstrated on challenging databases containing facial images. Moreover, its capabilities to accurately detect the centers of the eyes were also favourably compared with those of the leading state-of-the-art methods.\n"], "author_display": ["Marco Leo", "Dario Cazzato", "Tommaso De Marco", "Cosimo Distante"], "article_type": "Research Article", "score": 0.37225986, "title_display": "Unsupervised Eye Pupil Localization through Differential Geometry and Local Self-Similarity Matching", "publication_date": "2014-08-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0102829"}, {"journal": "PLoS ONE", "abstract": ["\nCitizen science projects store an enormous amount of information about species distribution, diversity and characteristics. Researchers are now beginning to make use of this rich collection of data. However, access to these databases is not always straightforward. Apart from the largest and international projects, citizen science repositories often lack specific Application Programming Interfaces (APIs) to connect them to the scientific environments. Thus, it is necessary to develop simple routines to allow researchers to take advantage of the information collected by smaller citizen science projects, for instance, programming specific packages to connect them to popular scientific environments (like R). Here, we present rAvis, an R-package to connect R-users with Proyecto AVIS (http://proyectoavis.com), a Spanish citizen science project with more than 82,000 bird observation records. We develop several functions to explore the database, to plot the geographic distribution of the species occurrences, and to generate personal queries to the database about species occurrences (number of individuals, distribution, etc.) and birdwatcher observations (number of species recorded by each collaborator, UTMs visited, etc.). This new R-package will allow scientists to access this database and to exploit the information generated by Spanish birdwatchers over the last 40 years.\n"], "author_display": ["Sara Varela", "Javier Gonz\u00e1lez-Hern\u00e1ndez", "Eduardo Casabella", "Rafael Barrientos"], "article_type": "Research Article", "score": 0.3721344, "title_display": "rAvis: An R-Package for Downloading Information Stored in Proyecto AVIS, a Citizen Science Bird Project", "publication_date": "2014-03-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0091650"}, {"journal": "PLOS Computational Biology", "abstract": ["\nCell signaling dynamics and transcriptional regulatory activities are variable within specific cell types responding to an identical stimulus. In addition to studying the network interactions, there is much interest in utilizing single cell scale data to elucidate the non-random aspects of the variability involved in cellular decision making. Previous studies have considered the information transfer between the signaling and transcriptional domains based on an instantaneous relationship between the molecular activities. These studies predict a limited binary on/off encoding mechanism which underestimates the complexity of biological information processing, and hence the utility of single cell resolution data. Here we pursue a novel strategy that reformulates the information transfer problem as involving dynamic features of signaling rather than molecular abundances. We pursue a computational approach to test if and how the transcriptional regulatory activity patterns can be informative of the temporal history of signaling. Our analysis reveals (1) the dynamic features of signaling that significantly alter transcriptional regulatory patterns (encoding), and (2) the temporal history of signaling that can be inferred from single cell scale snapshots of transcriptional activity (decoding). Immediate early gene expression patterns were informative of signaling peak retention kinetics, whereas transcription factor activity patterns were informative of activation and deactivation kinetics of signaling. Moreover, the information processing aspects varied across the network, with each component encoding a selective subset of the dynamic signaling features. We developed novel sensitivity and information transfer maps to unravel the dynamic multiplexing of signaling features at each of these network components. Unsupervised clustering of the maps revealed two groups that aligned with network motifs distinguished by transcriptional feedforward vs feedback interactions. Our new computational methodology impacts the single cell scale experiments by identifying downstream snapshot measures required for inferring specific dynamical features of upstream signals involved in the regulation of cellular responses.\nAuthor Summary: Single cell studies have shown that differential patterns in the dynamics of signaling proteins, transcription factor activity, gene expression, etc. produce distinct downstream outcomes. The opposite also holds true where particular cellular outcomes have been found to be associated with the dynamical pattern of one or more signaling molecules. Signaling pathways, therefore, serve as signal processing units to inform specific downstream regulation. However, the functional capabilities of the dynamic aspects of signaling are not well understood. To address this issue, we developed a new approach that evaluates information processing between dynamic features in signaling patterns and transcriptional regulatory activity. Our work demonstrates that the information transfer occur through decoding of temporal history of signals rather than only through instantaneous correlations. Moreover, our results identify regulatory network motifs as the critical components in the information processing and filtering of variability in signaling dynamics to produce distinct patterns of downstream transcriptional responses. Our methodology can be broadly applied to single cell scale data on experimentally accessible downstream measures to infer dynamic aspects of upstream signaling. "], "author_display": ["Hirenkumar K. Makadia", "James S. Schwaber", "Rajanikanth Vadigepalli"], "article_type": "Research Article", "score": 0.37207586, "title_display": "Intracellular Information Processing through Encoding and Decoding of Dynamic Signaling Features", "publication_date": "2015-10-22T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1004563"}, {"journal": "PLoS Computational Biology", "abstract": ["\nThe nature of the neural codes for pitch and loudness, two basic auditory attributes, has been a key question in neuroscience for over century. A currently widespread view is that sound intensity (subjectively, loudness) is encoded in spike rates, whereas sound frequency (subjectively, pitch) is encoded in precise spike timing. Here, using information-theoretic analyses, we show that the spike rates of a population of virtual neural units with frequency-tuning and spike-count correlation characteristics similar to those measured in the primary auditory cortex of primates, contain sufficient statistical information to account for the smallest frequency-discrimination thresholds measured in human listeners. The same population, and the same spike-rate code, can also account for the intensity-discrimination thresholds of humans. These results demonstrate the viability of a unified rate-based cortical population code for both sound frequency (pitch) and sound intensity (loudness), and thus suggest a resolution to a long-standing puzzle in auditory neuroscience.\nAuthor Summary: A widely held view among auditory scientists is that the neural code for sound intensity (or loudness) involves temporally coarse spike-rate information, whereas the code for sound frequency (or pitch) requires more fine-grained and precise spike timing information. One problem with this view is that neurons in auditory cortex do not produce precisely time-locked responses to higher frequencies within the pitch range, suggesting that a transformation to a rate code must occur. However, because cortical neurons exhibit relatively broad tuning to frequency and correlated spike counts, it is unclear whether a cortical population code based on spike rates alone can support the remarkably precise pitch-discrimination ability of humans. Here we show that a relatively small population of virtual neurons with frequency-tuning and spike-count correlation characteristics consistent with those of actual neurons in the primary auditory cortex of primates, can account for both the smallest frequency- and intensity-discrimination thresholds measured behaviorally in humans. These results suggest a resolution to a long-standing puzzle in auditory neuroscience. "], "author_display": ["Christophe Micheyl", "Paul R. Schrater", "Andrew J. Oxenham"], "article_type": "Research Article", "score": 0.37184596, "title_display": "Auditory Frequency and Intensity Discrimination Explained Using a Cortical Population Rate Code", "publication_date": "2013-11-14T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1003336"}, {"journal": "PLoS ONE", "abstract": ["\nAdaptation in the retina is thought to optimize the encoding of natural light signals into sequences of spikes sent to the brain. While adaptive changes in retinal processing to the variations of the mean luminance level and second-order stimulus statistics have been documented before, no such measurements have been performed when higher-order moments of the light distribution change. We therefore measured the ganglion cell responses in the tiger salamander retina to controlled changes in the second (contrast), third (skew) and fourth (kurtosis) moments of the light intensity distribution of spatially uniform temporally independent stimuli. The skew and kurtosis of the stimuli were chosen to cover the range observed in natural scenes. We quantified adaptation in ganglion cells by studying linear-nonlinear models that capture well the retinal encoding properties across all stimuli. We found that the encoding properties of retinal ganglion cells change only marginally when higher-order statistics change, compared to the changes observed in response to the variation in contrast. By analyzing optimal coding in LN-type models, we showed that neurons can maintain a high information rate without large dynamic adaptation to changes in skew or kurtosis. This is because, for uncorrelated stimuli, spatio-temporal summation within the receptive field averages away non-gaussian aspects of the light intensity distribution.\n"], "author_display": ["Ga\u0161per Tka\u010dik", "Anandamohan Ghosh", "Elad Schneidman", "Ronen Segev"], "article_type": "Research Article", "score": 0.37182873, "title_display": "Adaptation to Changes in Higher-Order Stimulus Statistics in the Salamander Retina", "publication_date": "2014-01-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0085841"}, {"journal": "PLoS ONE", "abstract": ["Background: Many current works aiming to learn regulatory networks from systems biology data must balance model complexity with respect to data availability and quality. Methods that learn regulatory associations based on unit-less metrics, such as Mutual Information, are attractive in that they scale well and reduce the number of free parameters (model complexity) per interaction to a minimum. In contrast, methods for learning regulatory networks based on explicit dynamical models are more complex and scale less gracefully, but are attractive as they may allow direct prediction of transcriptional dynamics and resolve the directionality of many regulatory interactions. Methodology: We aim to investigate whether scalable information based methods (like the Context Likelihood of Relatedness method) and more explicit dynamical models (like Inferelator 1.0) prove synergistic when combined. We test a pipeline where a novel modification of the Context Likelihood of Relatedness (mixed-CLR, modified to use time series data) is first used to define likely regulatory interactions and then Inferelator 1.0 is used for final model selection and to build an explicit dynamical model. Conclusions/Significance: Our method ranked 2nd out of 22 in the DREAM3 100-gene in silico networks challenge. Mixed-CLR and Inferelator 1.0 are complementary, demonstrating a large performance gain relative to any single tested method, with precision being especially high at low recall values. Partitioning the provided data set into four groups (knock-down, knock-out, time-series, and combined) revealed that using comprehensive knock-out data alone provides optimal performance. Inferelator 1.0 proved particularly powerful at resolving the directionality of regulatory interactions, i.e. \u201cwho regulates who\u201d (approximately  of identified true positives were correctly resolved). Performance drops for high in-degree genes, i.e. as the number of regulators per target gene increases, but not with out-degree, i.e. performance is not affected by the presence of regulatory hubs. "], "author_display": ["Aviv Madar", "Alex Greenfield", "Eric Vanden-Eijnden", "Richard Bonneau"], "article_type": "Research Article", "score": 0.37178415, "title_display": "DREAM3: Network Inference Using Dynamic Context Likelihood of Relatedness and the Inferelator", "publication_date": "2010-03-22T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0009803"}, {"journal": "PLoS ONE", "abstract": ["\n        The type 2 diabetes has increased rapidly in recent years throughout the world. The insulin signal transduction mechanism gets disrupted sometimes and it's known as insulin-resistance. It is one of the primary causes associated with type-2 diabetes. The signaling mechanisms involved several proteins that include 7 major functional proteins such as INS, INSR, IRS1, IRS2, PIK3CA, Akt2, and GLUT4. Using these 7 principal proteins, multiple sequences alignment has been created. The scores between sequences also have been developed. We have constructed a phylogenetic tree and modified it with node and distance. Besides, we have generated sequence logos and ultimately developed the protein-protein interaction network. The small insulin signal transduction protein arrangement shows complex network between the functional proteins.\n      "], "author_display": ["Chiranjib Chakraborty", "Sanjiban S. Roy", "Minna J. Hsu", "Govindasamy Agoramoorthy"], "article_type": "Research Article", "score": 0.37175074, "title_display": "Landscape Mapping of Functional Proteins in Insulin Signal Transduction and Insulin Resistance: A Network-Based Protein-Protein Interaction Analysis", "publication_date": "2011-01-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0016388"}, {"journal": "PLoS ONE", "abstract": ["\nMaltooligosyltrehalose trehalohydrolase (MTHase) catalyzes the release of trehalose by cleaving the \u03b1-1,4-glucosidic linkage next to the \u03b1-1,1-linked terminal disaccharide of maltooligosyltrehalose. Computer simulation using the hydrogen bond analysis, free energy decomposition, and computational alanine scanning were employed to investigate the interaction between maltooligosyltrehalose and the enzyme. The same residues that were chosen for theoretical investigation were also studied by site-directed mutagenesis and enzyme kinetic analysis. The importance of residues determined either experimentally or computed theoretically were in good accord with each other. It was found that residues Y155, D156, and W218 of subsites -2 and -3 of the enzyme might play an important role in interacting with the ligand. The theoretically constructed structure of the enzyme-ligand complex was further validated through an ab initio quantum chemical calculation using the Gaussian09 package. The activation energy computed from this latter study was very similar to those reported in literatures for the same type of hydrolysis reactions.\n"], "author_display": ["Chien-wei Fu", "Yu-Ping Wang", "Tsuei-Yun Fang", "Thy-Hou Lin"], "article_type": "Research Article", "score": 0.37173653, "title_display": "Interaction between Trehalose and MTHase from <i>Sulfolobus solfataricus</i> Studied by Theoretical Computation and Site-Directed Mutagenesis", "publication_date": "2013-07-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0068565"}, {"journal": "PLoS ONE", "abstract": ["Introduction: The global increase in childhood overweight and obesity has been ascribed partly to increases in children's screen time. Parents have a large influence on their children's screen time. Studies investigating parenting and early childhood screen time are limited. In this study, we investigated associations of parenting style and the social and physical home environment on watching TV and using computers or game consoles among 5-year-old children. Methods: This study uses baseline data concerning 5-year-old children (n\u200a=\u200a3067) collected for the \u2018Be active, eat right\u2019 study. Results: Children of parents with a higher score on the parenting style dimension involvement, were more likely to spend >30 min/day on computers or game consoles. Overall, families with an authoritative or authoritarian parenting style had lower percentages of children's screen time compared to families with an indulgent or neglectful style, but no significant difference in OR was found. In families with rules about screen time, children were less likely to watch TV>2 hrs/day and more likely to spend >30 min/day on computers or game consoles. The number of TVs and computers or game consoles in the household was positively associated with screen time, and children with a TV or computer or game console in their bedroom were more likely to watch TV>2 hrs/day or spend >30 min/day on computers or game consoles. Conclusion: The magnitude of the association between parenting style and screen time of 5-year-olds was found to be relatively modest. The associations found between the social and physical environment and children's screen time are independent of parenting style. Interventions to reduce children's screen time might be most effective when they support parents specifically with introducing family rules related to screen time and prevent the presence of a TV or computer or game console in the child's room. "], "author_display": ["Lydian Veldhuis", "Amy van Grieken", "Carry M. Renders", "Remy A. HiraSing", "Hein Raat"], "article_type": "Research Article", "score": 0.370885, "title_display": "Parenting Style, the Home Environment, and Screen Time of 5-Year-Old Children; The \u2018Be Active, Eat Right\u2019 Study", "publication_date": "2014-02-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0088486"}, {"journal": "PLOS ONE", "abstract": ["\nVarious disciplines are trying to solve one of the most noteworthy queries and broadly used concepts in biology, essentiality. Centrality is a primary index and a promising method for identifying essential nodes, particularly in biological networks. The newly created CentiServer is a comprehensive online resource that provides over 110 definitions of different centrality indices, their computational methods, and algorithms in the form of an encyclopedia. In addition, CentiServer allows users to calculate 55 centralities with the help of an interactive web-based application tool and provides a numerical result as a comma separated value (csv) file format or a mapped graphical format as a graph modeling language (GML) file. The standalone version of this application has been developed in the form of an R package. The web-based application (CentiServer) and R package (centiserve) are freely available at http://www.centiserver.org/\n"], "author_display": ["Mahdi Jalili", "Ali Salehzadeh-Yazdi", "Yazdan Asgari", "Seyed Shahriar Arab", "Marjan Yaghmaie", "Ardeshir Ghavamzadeh", "Kamran Alimoghaddam"], "article_type": "Research Article", "score": 0.37072387, "title_display": "CentiServer: A Comprehensive Resource, Web-Based Application and R Package for Centrality Analysis", "publication_date": "2015-11-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0143111"}, {"journal": "PLoS ONE", "abstract": ["\nThe visual system needs to extract the most important elements of the external world from a large flux of information in a short time for survival purposes. It is widely believed that in performing this task, it operates a strong data reduction at an early stage, by creating a compact summary of relevant information that can be handled by further levels of processing. In this work we formulate a model of early vision based on a pattern-filtering architecture, partly inspired by high-speed digital data reduction in experimental high-energy physics (HEP). This allows a much stronger data reduction than models based just on redundancy reduction. We show that optimizing this model for best information preservation under tight constraints on computational resources yields surprisingly specific a-priori predictions for the shape of biologically plausible features, and for experimental observations on fast extraction of salient visual features by human observers. Interestingly, applying the same optimized model to HEP data acquisition systems based on pattern-filtering architectures leads to specific a-priori predictions for the relevant data patterns that these devices extract from their inputs. These results suggest that the limitedness of computing resources can play an important role in shaping the nature of perception, by determining what is perceived as \u201cmeaningful features\u201d in the input data.\n"], "author_display": ["Maria M. Del Viva", "Giovanni Punzi", "Daniele Benedetti"], "article_type": "Research Article", "score": 0.37058136, "title_display": "Information and Perception of Meaningful Patterns", "publication_date": "2013-07-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0069154"}, {"journal": "PLoS Computational Biology", "abstract": ["Finding functional DNA binding sites of transcription factors (TFs) throughout the genome is a crucial step in understanding transcriptional regulation. Unfortunately, these binding sites are typically short and degenerate, posing a significant statistical challenge: many more matches to known TF motifs occur in the genome than are actually functional. However, information about chromatin structure may help to identify the functional sites. In particular, it has been shown that active regulatory regions are usually depleted of nucleosomes, thereby enabling TFs to bind DNA in those regions. Here, we describe a novel motif discovery algorithm that employs an informative prior over DNA sequence positions based on a discriminative view of nucleosome occupancy. When a Gibbs sampling algorithm is applied to yeast sequence-sets identified by ChIP-chip, the correct motif is found in 52% more cases with our informative prior than with the commonly used uniform prior. This is the first demonstration that nucleosome occupancy information can be used to improve motif discovery. The improvement is dramatic, even though we are using only a statistical model to predict nucleosome occupancy; we expect our results to improve further as high-resolution genome-wide experimental nucleosome occupancy data becomes increasingly available.: Identifying transcription factor (TF) binding sites across the genome is an important problem in molecular biology. Large-scale discovery of TF binding sites is usually carried out by searching for short DNA patterns that appear often within promoter regions of genes that are known to be co-bound by a TF. In such problems, promoters have traditionally been treated as strings of nucleotide bases in which TF binding sites are assumed to be equally likely to occur at any position. In vivo, however, TFs localize to DNA binding sites as part of a complicated thermodynamic process of cooperativity and competition, both with one another and, importantly, with DNA packaging proteins called nucleosomes. In particular, TFs are more likely to bind DNA at sites that are not occupied by nucleosomes. In this paper, we show that it is possible to incorporate knowledge of the nucleosome landscape across the genome to aid binding site discovery; indeed, our algorithm incorporating nucleosome occupancy information is significantly more accurate than conventional methods. We use our algorithm to generate a condition-dependent, nucleosome-guided map of binding sites for 55 TFs in yeast. "], "author_display": ["Leelavati Narlikar", "Raluca Gord\u00e2n", "Alexander J Hartemink"], "article_type": "Research Article", "score": 0.3705342, "title_display": "A Nucleosome-Guided Map of Transcription Factor Binding Sites in Yeast", "publication_date": "2007-11-09T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.0030215"}, {"abstract": ["\n        Neurotree is an online database that documents the lineage of academic mentorship in neuroscience. Modeled on the tree format typically used to describe biological genealogies, the Neurotree web site provides a concise summary of the intellectual history of neuroscience and relationships between individuals in the current neuroscience community. The contents of the database are entirely crowd-sourced: any internet user can add information about researchers and the connections between them. As of July 2012, Neurotree has collected information from 10,000 users about 35,000 researchers and 50,000 mentor relationships, and continues to grow. The present report serves to highlight the utility of Neurotree as a resource for academic research and to summarize some basic analysis of its data. The tree structure of the database permits a variety of graphical analyses. We find that the connectivity and graphical distance between researchers entered into Neurotree early has stabilized and thus appears to be mostly complete. The connectivity of more recent entries continues to mature. A ranking of researcher fecundity based on their mentorship reveals a sustained period of influential researchers from 1850\u20131950, with the most influential individuals active at the later end of that period. Finally, a clustering analysis reveals that some subfields of neuroscience are reflected in tightly interconnected mentor-trainee groups.\n      "], "author_display": ["Stephen V. David", "Benjamin Y. Hayden"], "article_type": "Research Article", "score": 0.37046278, "title_display": "Neurotree: A Collaborative, Graphical Database of the Academic Genealogy of Neuroscience", "publication_date": "2012-10-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0046608"}, {"journal": "PLoS ONE", "abstract": ["\n        In this paper, we describe some bounds and inequalities relating -index, -index, -index, and generalized impact factor. We derive the bounds and inequalities relating these indexing parameters from their basic definitions and without assuming any continuous model to be followed by any of them. We verify the theorems using citation data for five Price Medalists. We observe that the lower bound for -index given by Theorem 2, , comes out to be more accurate as compared to Schubert-Glanzel relation  for a proportionality constant of , where  is the number of citations and  is the number of papers referenced. Also, the values of -index obtained using Theorem 2 outperform those obtained using Egghe-Liang-Rousseau power law model for the given citation data of Price Medalists. Further, we computed the values of upper bound on -index given by Theorem 3, , where  denotes the value of -index. We observe that the upper bound on -index given by Theorem 3 is reasonably tight for the given citation record of Price Medalists.\n      "], "author_display": ["Ash Mohammad Abbas"], "article_type": "Research Article", "score": 0.3704427, "title_display": "Bounds and Inequalities Relating <i>h</i>-Index, <i>g</i>-Index, <i>e</i>-Index and Generalized Impact Factor: An Improvement over Existing Models", "publication_date": "2012-04-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0033699"}, {"journal": "PLoS ONE", "abstract": ["\nPosture segmentation plays an essential role in human motion analysis. The state-of-the-art method extracts sufficiently high-dimensional features from 3D depth images for each 3D point and learns an efficient body part classifier. However, high-dimensional features are memory-consuming and difficult to handle on large-scale training dataset. In this paper, we propose an efficient two-stage dimension reduction scheme, termed biview learning, to encode two independent views which are depth-difference features (DDF) and relative position features (RPF). Biview learning explores the complementary property of DDF and RPF, and uses two stages to learn a compact yet comprehensive low-dimensional feature space for posture segmentation. In the first stage, discriminative locality alignment (DLA) is applied to the high-dimensional DDF to learn a discriminative low-dimensional representation. In the second stage, canonical correlation analysis (CCA) is used to explore the complementary property of RPF and the dimensionality reduced DDF. Finally, we train a support vector machine (SVM) over the output of CCA. We carefully validate the effectiveness of DLA and CCA utilized in the two-stage scheme on our 3D human points cloud dataset. Experimental results show that the proposed biview learning scheme significantly outperforms the state-of-the-art method for human posture segmentation.\n"], "author_display": ["Maoying Qiao", "Jun Cheng", "Wei Bian", "Dacheng Tao"], "article_type": "Research Article", "score": 0.370258, "title_display": "Biview Learning for Human Posture Segmentation from 3D Points Cloud", "publication_date": "2014-01-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0085811"}, {"journal": "PLOS ONE", "abstract": ["\nThe problem of finding k-edge-connected components is a fundamental problem in computer science. Given a graph G = (V, E), the problem is to partition the vertex set V into {V1, V2,\u2026, Vh}, where each Vi is maximized, such that for any two vertices x and y in Vi, there are k edge-disjoint paths connecting them. In this paper, we present an algorithm to solve this problem for all k. The algorithm preprocesses the input graph to construct an Auxiliary Graph to store information concerning edge-connectivity among every vertex pair in O(Fn) time, where F is the time complexity to find the maximum flow between two vertices in graph G and n = \u2223V\u2223. For any value of k, the k-edge-connected components can then be determined by traversing the auxiliary graph in O(n) time. The input graph can be a directed or undirected, simple graph or multigraph. Previous works on this problem mainly focus on fixed value of k.\n"], "author_display": ["Tianhao Wang", "Yong Zhang", "Francis Y. L. Chin", "Hing-Fung Ting", "Yung H. Tsin", "Sheung-Hung Poon"], "article_type": "Research Article", "score": 0.37014237, "title_display": "A Simple Algorithm for Finding All <i>k</i>-Edge-Connected Components", "publication_date": "2015-09-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0136264"}, {"journal": "PLoS Computational Biology", "abstract": ["Sensory information about the outside world is encoded by neurons in sequences of discrete, identical pulses termed action potentials or spikes. There is persistent controversy about the extent to which the precise timing of these spikes is relevant to the function of the brain. We revisit this issue, using the motion-sensitive neurons of the fly visual system as a test case. Our experimental methods allow us to deliver more nearly natural visual stimuli, comparable to those which flies encounter in free, acrobatic flight. New mathematical methods allow us to draw more reliable conclusions about the information content of neural responses even when the set of possible responses is very large. We find that significant amounts of visual information are represented by details of the spike train at millisecond and sub-millisecond precision, even though the sensory input has a correlation time of \u223c55 ms; different patterns of spike timing represent distinct motion trajectories, and the absolute timing of spikes points to particular features of these trajectories with high precision. Finally, the efficiency of our entropy estimator makes it possible to uncover features of neural coding relevant for natural visual stimuli: first, the system's information transmission rate varies with natural fluctuations in light intensity, resulting from varying cloud cover, such that marginal increases in information rate thus occur even when the individual photoreceptors are counting on the order of one million photons per second. Secondly, we see that the system exploits the relatively slow dynamics of the stimulus to remove coding redundancy and so generate a more efficient neural code.Author Summary: Neurons communicate by means of stereotyped pulses, called action potentials or spikes, and a central issue in systems neuroscience is to understand this neural coding. Here we study how sensory information is encoded in sequences of spikes, using a combination of novel theoretical and experimental techniques. With motion detection in the blowfly as a model system, we perform experiments in an environment maximally similar to the natural one. We report a number of unexpected, striking observations about the structure of the neural code in this system: First, the timing of spikes is important with a precision roughly two orders of magnitude greater than the temporal dynamics of the stimulus. Second, the fly goes a long way to utilize the redundancy in the stimulus in order to optimize the neural code and encode more refined features than would be possible otherwise. This implies that the neural code, even in low-level vision, may be significantly context dependent. "], "author_display": ["Ilya Nemenman", "Geoffrey D. Lewen", "William Bialek", "Rob R. de Ruyter van Steveninck"], "article_type": "Research Article", "score": 0.3700014, "title_display": "Neural Coding of Natural Stimuli: Information at Sub-Millisecond Resolution", "publication_date": "2008-03-07T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1000025"}, {"journal": "PLoS ONE", "abstract": ["\nOne methodology that has met success to infer gene networks from gene expression data is based upon ordinary differential equations (ODE). However new types of data continue to be produced, so it is worthwhile to investigate how to integrate these new data types into the inference procedure. One such data is physical interactions between transcription factors and the genes they regulate as measured by ChIP-chip or ChIP-seq experiments. These interactions can be incorporated into the gene network inference procedure as a priori network information. In this article, we extend the ODE methodology into a general optimization framework that incorporates existing network information in combination with regularization parameters that encourage network sparsity. We provide theoretical results proving convergence of the estimator for our method and show the corresponding probabilistic interpretation also converges. We demonstrate our method on simulated network data and show that existing network information improves performance, overcomes the lack of observations, and performs well even when some of the existing network information is incorrect. We further apply our method to the core regulatory network of embryonic stem cells utilizing predicted interactions from two studies as existing network information. We show that including the prior network information constructs a more closely representative regulatory network versus when no information is provided.\n"], "author_display": ["Scott Christley", "Qing Nie", "Xiaohui Xie"], "article_type": "Research Article", "score": 0.36993834, "title_display": "Incorporating Existing Network Information into Gene Network Inference", "publication_date": "2009-08-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0006799"}, {"journal": "PLoS ONE", "abstract": ["\nBiotechnological and biomolecular advances have introduced novel uses for DNA such as DNA computing, storage, and encryption. For these applications, DNA sequence design requires maximal desired (and minimal undesired) hybridizations, which are the product of a single new DNA strand from 2 single DNA strands. Here, we propose a novel constraint to design DNA sequences based on thermodynamic properties. Existing constraints for DNA design are based on the Hamming distance, a constraint that does not address the thermodynamic properties of the DNA sequence. Using a unique, improved genetic algorithm, we designed DNA sequence sets which satisfy different distance constraints and employ a free energy gap based on a minimum free energy (MFE) to gauge DNA sequences based on set thermodynamic properties. When compared to the best constraints of the Hamming distance, our method yielded better thermodynamic qualities. We then used our improved genetic algorithm to obtain lower-bound DNA sequence sets. Here, we discuss the effects of novel constraint parameters on the free energy gap.\n"], "author_display": ["Qiang Zhang", "Bin Wang", "Xiaopeng Wei", "Changjun Zhou"], "article_type": "Research Article", "score": 0.36990353, "title_display": "A Novel Constraint for Thermodynamically Designing DNA Sequences", "publication_date": "2013-08-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0072180"}, {"journal": "PLoS ONE", "abstract": ["\nCitizen science programs are increasingly popular for a variety of reasons, from public education to new opportunities for data collection. The literature published in scientific journals resulting from these projects represents a particular perspective on the process. These articles often conclude with recommendations for increasing \u201csuccess\u201d. This study compared these recommendations to those elicited during interviews with program coordinators for programs within the United States. From this comparison, success cannot be unilaterally defined and therefore recommendations vary by perspective on success. Program coordinators tended to have more locally-tailored recommendations specific to particular aspects of their program mission.\n"], "author_display": ["Amy Freitag", "Max J. Pfeffer"], "article_type": "Research Article", "score": 0.3698822, "title_display": "Process, Not Product: Investigating Recommendations for Improving Citizen Science \u201cSuccess\u201d", "publication_date": "2013-05-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0064079"}, {"journal": "PLOS ONE", "abstract": ["\nPosterior leaflet prolapse following chordal elongation or rupture is one of the primary valvular diseases in patients with degenerative mitral valves (MVs). Quadrangular resection followed by ring annuloplasty is a reliable and reproducible surgical repair technique for treatment of posterior leaflet prolapse. Virtual MV repair simulation of leaflet resection in association with patient-specific 3D echocardiographic data can provide quantitative biomechanical and physiologic characteristics of pre- and post-resection MV function. We have developed a solid personalized computational simulation protocol to perform virtual MV repair using standard clinical guidelines of posterior leaflet resection with annuloplasty ring implantation. A virtual MV model was created using 3D echocardiographic data of a patient with posterior chordal rupture and severe mitral regurgitation. A quadrangle-shaped leaflet portion in the prolapsed posterior leaflet was removed, and virtual plication and suturing were performed. An annuloplasty ring of proper size was reconstructed and virtual ring annuloplasty was performed by superimposing the ring and the mitral annulus. Following the quadrangular resection and ring annuloplasty simulations, patient-specific annular motion and physiologic transvalvular pressure gradient were implemented and dynamic finite element simulation of MV function was performed. The pre-resection MV demonstrated a substantial lack of leaflet coaptation which directly correlated with the severe mitral regurgitation. Excessive stress concentration was found along the free marginal edge of the posterior leaflet involving the chordal rupture. Following the virtual resection and ring annuloplasty, the severity of the posterior leaflet prolapse markedly decreased. Excessive stress concentration disappeared over both anterior and posterior leaflets, and complete leaflet coaptation was effectively restored. This novel personalized virtual MV repair strategy has great potential to help with preoperative selection of the patient-specific optimal MV repair techniques, allow innovative surgical planning to expect improved efficacy of MV repair with more predictable outcomes, and ultimately provide more effective medical care for the patient.\n"], "author_display": ["Yonghoon Rim", "Ahnryul Choi", "David D. McPherson", "Hyunggun Kim"], "article_type": "Research Article", "score": 0.36958903, "title_display": "Personalized Computational Modeling of Mitral Valve Prolapse: Virtual Leaflet Resection", "publication_date": "2015-06-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0130906"}, {"journal": "PLOS ONE", "abstract": ["\nCloud computing technology plays a very important role in many areas, such as in the construction and development of the smart city. Meanwhile, numerous cloud services appear on the cloud-based platform. Therefore how to how to select trustworthy cloud services remains a significant problem in such platforms, and extensively investigated owing to the ever-growing needs of users. However, trust relationship in social network has not been taken into account in existing methods of cloud service selection and recommendation. In this paper, we propose a cloud service selection model based on the trust-enhanced similarity. Firstly, the direct, indirect, and hybrid trust degrees are measured based on the interaction frequencies among users. Secondly, we estimate the overall similarity by combining the experience usability measured based on Jaccard\u2019s Coefficient and the numerical distance computed by Pearson Correlation Coefficient. Then through using the trust degree to modify the basic similarity, we obtain a trust-enhanced similarity. Finally, we utilize the trust-enhanced similarity to find similar trusted neighbors and predict the missing QoS values as the basis of cloud service selection and recommendation. The experimental results show that our approach is able to obtain optimal results via adjusting parameters and exhibits high effectiveness. The cloud services ranking by our model also have better QoS properties than other methods in the comparison experiments.\n"], "author_display": ["Yuchen Pan", "Shuai Ding", "Wenjuan Fan", "Jing Li", "Shanlin Yang"], "article_type": "Research Article", "score": 0.36956975, "title_display": "Trust-Enhanced Cloud Service Selection Model Based on QoS Analysis", "publication_date": "2015-11-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0143448"}, {"journal": "PLoS ONE", "abstract": ["\nPhilologists reconstructing ancient texts from variously miscopied manuscripts anticipated information theorists by centuries in conceptualizing information in terms of probability. An example is the editorial principle difficilior lectio potior (DLP): in choosing between otherwise acceptable alternative wordings in different manuscripts, \u201cthe more difficult reading [is] preferable.\u201d As philologists at least as early as Erasmus observed (and as information theory's version of the second law of thermodynamics would predict), scribal errors tend to replace less frequent and hence entropically more information-rich wordings with more frequent ones. Without measurements, it has been unclear how effectively DLP has been used in the reconstruction of texts, and how effectively it could be used. We analyze a case history of acknowledged editorial excellence that mimics an experiment: the reconstruction of Lucretius's De Rerum Natura, beginning with Lachmann's landmark 1850 edition based on the two oldest manuscripts then known. Treating words as characters in a code, and taking the occurrence frequencies of words from a current, more broadly based edition, we calculate the difference in entropy information between Lachmann's 756 pairs of grammatically acceptable alternatives. His choices average 0.26\u00b10.20 bits higher in entropy information (95% confidence interval, P\u200a=\u200a0.005), as against the single bit that determines the outcome of a coin toss, and the average 2.16\u00b10.10 bits (95%) of (predominantly meaningless) entropy information if the rarer word had always been chosen. As a channel width, 0.26\u00b10.20 bits/word corresponds to a 0.790.79+0.09\u22120.15 likelihood of the rarer word being the one accepted in the reference edition, which is consistent with the observed 547/756\u200a=\u200a0.72\u00b10.03 (95%). Statistically informed application of DLP can recover substantial amounts of semantically meaningful entropy information from noise; hence the extension copiosior informatione lectio potior, \u201cthe reading richer in information [is] preferable.\u201d New applications of information theory promise continued refinement in the reconstruction of culturally fundamental texts.\n"], "author_display": ["John L. Cisne", "Robert M. Ziomkowski", "Steven J. Schwager"], "article_type": "Research Article", "score": 0.36950392, "title_display": "Mathematical Philology: Entropy Information in Refining Classical Texts' Reconstruction, and Early Philologists' Anticipation of Information Theory", "publication_date": "2010-01-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0008661"}, {"journal": "PLoS ONE", "abstract": ["Background: Upwards of 1200 miRNA loci have hitherto been annotated in the human genome. The specific features defining a miRNA precursor and deciding its recognition and subsequent processing are not yet exhaustively described and miRNA loci can thus not be computationally identified with sufficient confidence. Results: We rendered pre-miRNA and non-pre-miRNA hairpins as strings of integrated sequence-structure information, and used the software Teiresias to identify sequence-structure motifs (ss-motifs) of variable length in these data sets. Using only ss-motifs as features in a Support Vector Machine (SVM) algorithm for pre-miRNA identification achieved 99.2% specificity and 97.6% sensitivity on a human test data set, which is comparable to previously published algorithms employing combinations of sequence-structure and additional features. Further analysis of the ss-motif information contents revealed strongly significant deviations from those of the respective training sets, revealing important potential clues as to how the sequence and structural information of RNA hairpins are utilized by the miRNA processing apparatus. Conclusion: Integrated sequence-structure motifs of variable length apparently capture nearly all information required to distinguish miRNA precursors from other stem-loop structures. "], "author_display": ["Xiuqin Liu", "Shunmin He", "Geir Skogerb\u00f8", "Fuzhou Gong", "Runsheng Chen"], "article_type": "Research Article", "score": 0.36948827, "title_display": "Integrated Sequence-Structure Motifs Suffice to Identify microRNA Precursors", "publication_date": "2012-03-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0032797"}, {"journal": "PLoS ONE", "abstract": ["\n        Studying illusions provides insight into the way the brain processes information. The M\u00fcller-Lyer Illusion (MLI) is a classical geometrical illusion of size, in which perceived line length is decreased by arrowheads and increased by arrowtails. Many theories have been put forward to explain the MLI, such as misapplied size constancy scaling, the statistics of image-source relationships and the filtering properties of signal processing in primary visual areas. Artificial models of the ventral visual processing stream allow us to isolate factors hypothesised to cause the illusion and test how these affect classification performance. We trained a feed-forward feature hierarchical model, HMAX, to perform a dual category line length judgment task (short versus long) with over 90% accuracy. We then tested the system in its ability to judge relative line lengths for images in a control set versus images that induce the MLI in humans. Results from the computational model show an overall illusory effect similar to that experienced by human subjects. No natural images were used for training, implying that misapplied size constancy and image-source statistics are not necessary factors for generating the illusion. A post-hoc analysis of response weights within a representative trained network ruled out the possibility that the illusion is caused by a reliance on information at low spatial frequencies. Our results suggest that the MLI can be produced using only feed-forward, neurophysiological connections.\n      "], "author_display": ["Astrid Zeman", "Oliver Obst", "Kevin R. Brooks", "Anina N. Rich"], "article_type": "Research Article", "score": 0.36944395, "title_display": "The M\u00fcller-Lyer Illusion in a Computational Model of Biological Object Recognition", "publication_date": "2013-02-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0056126"}, {"journal": "PLoS ONE", "abstract": ["\nThe concept of feature selectivity in sensory signal processing can be formalized as dimensionality reduction: in a stimulus space of very high dimensions, neurons respond only to variations within some smaller, relevant subspace. But if neural responses exhibit invariances, then the relevant subspace typically cannot be reached by a Euclidean projection of the original stimulus. We argue that, in several cases, we can make progress by appealing to the simplest nonlinear construction, identifying the relevant variables as quadratic forms, or \u201cstimulus energies.\u201d Natural examples include non\u2013phase\u2013locked cells in the auditory system, complex cells in the visual cortex, and motion\u2013sensitive neurons in the visual system. Generalizing the idea of maximally informative dimensions, we show that one can search for kernels of the relevant quadratic forms by maximizing the mutual information between the stimulus energy and the arrival times of action potentials. Simple implementations of this idea successfully recover the underlying properties of model neurons even when the number of parameters in the kernel is comparable to the number of action potentials and stimuli are completely natural. We explore several generalizations that allow us to incorporate plausible structure into the kernel and thereby restrict the number of parameters. We hope that this approach will add significantly to the set of tools available for the analysis of neural responses to complex, naturalistic stimuli.\n"], "author_display": ["Kanaka Rajan", "William Bialek"], "article_type": "Research Article", "score": 0.3691334, "title_display": "Maximally Informative \u201cStimulus Energies\u201d in the Analysis of Neural Responses to Natural Signals", "publication_date": "2013-11-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0071959"}, {"journal": "PLoS ONE", "abstract": ["Background: Genetic and genomic data analyses are outputting large sets of genes. Functional comparison of these gene sets is a key part of the analysis, as it identifies their shared functions, and the functions that distinguish each set. The Gene Ontology (GO) initiative provides a unified reference for analyzing the genes molecular functions, biological processes and cellular components. Numerous semantic similarity measures have been developed to systematically quantify the weight of the GO terms shared by two genes. We studied how gene set comparisons can be improved by considering gene set particularity in addition to gene set similarity. Results: We propose a new approach to compute gene set particularities based on the information conveyed by GO terms. A GO term informativeness can be computed using either its information content based on the term frequency in a corpus, or a function of the term's distance to the root. We defined the semantic particularity of a set of GO terms Sg1 compared to another set of GO terms Sg2. We combined our particularity measure with a similarity measure to compare gene sets. We demonstrated that the combination of semantic similarity and semantic particularity measures was able to identify genes with particular functions from among similar genes. This differentiation was not recognized using only a semantic similarity measure. Conclusion: Semantic particularity should be used in conjunction with semantic similarity to perform functional analysis of GO-annotated gene sets. The principle is generalizable to other ontologies. "], "author_display": ["Charles Bettembourg", "Christian Diot", "Olivier Dameron"], "article_type": "Research Article", "score": 0.3691051, "title_display": "Semantic Particularity Measure for Functional Characterization of Gene Sets Using Gene Ontology", "publication_date": "2014-01-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0086525"}, {"journal": "PLoS ONE", "abstract": ["\nWe interact with the world through the assessment of available, but sometimes imperfect, sensory information. However, little is known about how variance in the quality of sensory information affects the regulation of controlled actions. In a series of three experiments, comprising a total of seven behavioral studies, we examined how different types of spatial frequency information affect underlying processes of response inhibition and selection. Participants underwent a stop-signal task, a two choice speed/accuracy balance experiment, and a variant of both these tasks where prior information was given about the nature of stimuli. In all experiments, stimuli were either intact, or contained only high-, or low- spatial frequencies. Overall, drift diffusion model analysis showed a decreased rate of information processing when spatial frequencies were removed, whereas the criterion for information accumulation was lowered. When spatial frequency information was intact, the cost of response inhibition increased (longer SSRT), while a correct response was produced faster (shorter reaction times) and with more certainty (decreased errors). When we manipulated the motivation to respond with a deadline (i.e., be fast or accurate), removal of spatial frequency information slowed response times only when instructions emphasized accuracy. However, the slowing of response times did not improve error rates, when compared to fast instruction trials. These behavioral studies suggest that the removal of spatial frequency information differentially affects the speed of response initiation, inhibition, and the efficiency to balance fast or accurate responses. More generally, the present results indicate a task-independent influence of basic sensory information on strategic adjustments in action control.\n"], "author_display": ["Sara Jahfari", "K. Richard Ridderinkhof", "H. Steven Scholte"], "article_type": "Research Article", "score": 0.36900377, "title_display": "Spatial Frequency Information Modulates Response Inhibition and Decision-Making Processes", "publication_date": "2013-10-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0076467"}, {"journal": "PLOS ONE", "abstract": ["\nAs E-government continues to develop with ever-increasing speed, the requirement to enhance traditional government systems and affairs with electronic methods that are more effective and efficient is becoming critical. As a new product of information technology, E-tendering is becoming an inevitable reality owing to its efficiency, fairness, transparency, and accountability. Thus, developing and promoting government E-tendering (GeT) is imperative. This paper presents a hybrid approach combining genetic algorithm (GA) and Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS) to enable GeT to search for the optimal tenderer efficiently and fairly under circumstances where the attributes of the tenderers are expressed as fuzzy number intuitionistic fuzzy sets (FNIFSs). GA is applied to obtain the optimal weights of evaluation criteria of tenderers automatically. TOPSIS is employed to search for the optimal tenderer. A prototype system is built and validated with an illustrative example from GeT to verify the feasibility and availability of the proposed approach.\n"], "author_display": ["Yan Wang", "Chengyu Xi", "Shuai Zhang", "Wenyu Zhang", "Dejian Yu"], "article_type": "Research Article", "score": 0.36862707, "title_display": "Combined Approach for Government E-Tendering Using GA and TOPSIS with Intuitionistic Fuzzy Information", "publication_date": "2015-07-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0130767"}, {"journal": "PLoS Computational Biology", "abstract": ["\nDiverse ion channels and their dynamics endow single neurons with complex biophysical properties. These properties determine the heterogeneity of cell types that make up the brain, as constituents of neural circuits tuned to perform highly specific computations. How do biophysical properties of single neurons impact network function? We study a set of biophysical properties that emerge in cortical neurons during the first week of development, eventually allowing these neurons to adaptively scale the gain of their response to the amplitude of the fluctuations they encounter. During the same time period, these same neurons participate in large-scale waves of spontaneously generated electrical activity. We investigate the potential role of experimentally observed changes in intrinsic neuronal properties in determining the ability of cortical networks to propagate waves of activity. We show that such changes can strongly affect the ability of multi-layered feedforward networks to represent and transmit information on multiple timescales. With properties modeled on those observed at early stages of development, neurons are relatively insensitive to rapid fluctuations and tend to fire synchronously in response to wave-like events of large amplitude. Following developmental changes in voltage-dependent conductances, these same neurons become efficient encoders of fast input fluctuations over few layers, but lose the ability to transmit slower, population-wide input variations across many layers. Depending on the neurons' intrinsic properties, noise plays different roles in modulating neuronal input-output curves, which can dramatically impact network transmission. The developmental change in intrinsic properties supports a transformation of a networks function from the propagation of network-wide information to one in which computations are scaled to local activity. This work underscores the significance of simple changes in conductance parameters in governing how neurons represent and propagate information, and suggests a role for background synaptic noise in switching the mode of information transmission.\nAuthor Summary: Differences in ion channel composition endow different neuronal types with distinct computational properties. Understanding how these biophysical differences affect network-level computation is an important frontier. We focus on a set of biophysical properties, experimentally observed in developing cortical neurons, that allow these neurons to efficiently encode their inputs despite time-varying changes in the statistical context. Large-scale propagating waves are autonomously generated by the developing brain even before the onset of sensory experience. Using multi-layered feedforward networks, we examine how changes in intrinsic properties can lead to changes in the network's ability to represent and transmit information on multiple timescales. We demonstrate that measured changes in the computational properties of immature single neurons enable the propagation of slow-varying wave-like inputs. In contrast, neurons with more mature properties are more sensitive to fast fluctuations, which modulate the slow-varying information. While slow events are transmitted with high fidelity in initial network layers, noise degrades transmission in downstream network layers. Our results show how short-term adaptation and modulation of the neurons' input-output firing curves by background synaptic noise determine the ability of neural networks to transmit information on multiple timescales. "], "author_display": ["Julijana Gjorgjieva", "Rebecca A. Mease", "William J. Moody", "Adrienne L. Fairhall"], "article_type": "Research Article", "score": 0.3685884, "title_display": "Intrinsic Neuronal Properties Switch the Mode of Information Transmission in Networks", "publication_date": "2014-12-04T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1003962"}, {"journal": "PLoS ONE", "abstract": ["\n        In this paper we propose a new family of cumulative indexes for measuring scientific performance which can be applied to many metrics, including h index and its variants (here we apply it to the h index, h(2) index and Google Scholar's i10 index). These indexes follow the general principle of repeating the index calculation for the same publication set. Using bibliometric data and reviewer scores for accepted and rejected fellowship applicants we examine how valid the cumulative variant is compared to the original variant. These analyses showed that the cumulative indexes result in higher correlations with the reviewer scores than their original variants. Thus, the cumulative indexes better reflect the assessments by peers than the original variants and are useful extensions of the original indexes. In contrast to many other measures of scientific performance proposed up to now, the cumulative indexes seem not only to be effective, but they are also easy to understand and calculate.\n      "], "author_display": ["Marcin Kozak", "Lutz Bornmann"], "article_type": "Research Article", "score": 0.36821842, "title_display": "A New Family of Cumulative Indexes for Measuring Scientific Performance", "publication_date": "2012-10-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0047679"}, {"journal": "PLoS ONE", "abstract": ["\nIn this study, we show the effectiveness of a virtual environment comprising 18 computer games that cover mathematics topics in a playful setting and that can be executed on the Internet with the possibility of player interaction through chat. An arithmetic pre-test contained in the Scholastic Performance Test was administered to 300 children between 7 and 10 years old, including 162 males and 138 females, in the second grade of primary school. Twenty-six children whose scores showed a low level of mathematical knowledge were chosen and randomly divided into the control (CG) and experimental (EG) groups. The EG participated to the virtual environment and the CG participated in reinforcement using traditional teaching methods. Both groups took a post-test in which the Scholastic Performance Test (SPT) was given again. A statistical analysis of the results using the Student's t-test showed a significant learning improvement for the EG and no improvement for the CG (p\u22640.05). The virtual environment allows the students to integrate thought, feeling and action, thus motivating the children to learn and contributing to their intellectual development.\n"], "author_display": ["Marcus Vasconcelos de Castro", "M\u00e1rcia Aparecida Silva Bissaco", "Bruno Marques Panccioni", "Silvia Cristina Martini Rodrigues", "Andreia Miranda Domingues"], "article_type": "Research Article", "score": 0.36820698, "title_display": "Effect of a Virtual Environment on the Development of Mathematical Skills in Children with Dyscalculia", "publication_date": "2014-07-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0103354"}, {"abstract": ["\n        Blueberry growers in Maine attend annual Cooperative Extension presentations given by university faculty members. These presentations cover topics, such as, how to prevent plant disease and monitor for insect pests. In 2012, in order to make the sessions more interactive and promote learning, clicker questions and peer discussion were incorporated into the presentations. Similar to what has been shown at the undergraduate level, after peer discussion, more blueberry growers gave correct answers to multiple-choice questions than when answering independently. Furthermore, because blueberry growers are characterized by diverse levels of education, experience in the field etc., we were able to determine whether demographic factors were associated with changes in performance after peer discussion. Taken together, our results suggest that clicker questions and peer discussion work equally well with adults from a variety of demographic backgrounds without disadvantaging a subset of the population and provide an important learning opportunity to the least formally educated members. Our results also indicate that clicker questions with peer discussion were viewed as a positive addition to university-related informal science education sessions.\n      "], "author_display": ["Michelle K. Smith", "Seanna L. Annis", "Jennifer J. Kaplan", "Frank Drummond"], "article_type": "Research Article", "score": 0.36820227, "title_display": "Using Peer Discussion Facilitated by Clicker Questions in an Informal Education Setting: Enhancing Farmer Learning of Science", "publication_date": "2012-10-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0047564"}, {"journal": "PLoS ONE", "abstract": ["\nPrincipal component analysis (PCA) is routinely used to analyze genome-wide single-nucleotide polymorphism (SNP) data, for detecting population structure and potential outliers. However, the size of SNP datasets has increased immensely in recent years and PCA of large datasets has become a time consuming task. We have developed flashpca, a highly efficient PCA implementation based on randomized algorithms, which delivers identical accuracy in extracting the top principal components compared with existing tools, in substantially less time. We demonstrate the utility of flashpca on both HapMap3 and on a large Immunochip dataset. For the latter, flashpca performed PCA of 15,000 individuals up to 125 times faster than existing tools, with identical results, and PCA of 150,000 individuals using flashpca completed in 4 hours. The increasing size of SNP datasets will make tools such as flashpca essential as traditional approaches will not adequately scale. This approach will also help to scale other applications that leverage PCA or eigen-decomposition to substantially larger datasets.\n"], "author_display": ["Gad Abraham", "Michael Inouye"], "article_type": "Research Article", "score": 0.36791652, "title_display": "Fast Principal Component Analysis of Large-Scale Genome-Wide Data", "publication_date": "2014-04-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0093766"}, {"journal": "PLoS ONE", "abstract": ["\n        Immersive virtual reality (IVR) typically generates the illusion in participants that they are in the displayed virtual scene where they can experience and interact in events as if they were really happening. Teleoperator (TO) systems place people at a remote physical destination embodied as a robotic device, and where typically participants have the sensation of being at the destination, with the ability to interact with entities there. In this paper, we show how to combine IVR and TO to allow a new class of application. The participant in the IVR is represented in the destination by a physical robot (TO) and simultaneously the remote place and entities within it are represented to the participant in the IVR. Hence, the IVR participant has a normal virtual reality experience, but where his or her actions and behaviour control the remote robot and can therefore have physical consequences. Here, we show how such a system can be deployed to allow a human and a rat to operate together, but the human interacting with the rat on a human scale, and the rat interacting with the human on the rat scale. The human is represented in a rat arena by a small robot that is slaved to the human\u2019s movements, whereas the tracked rat is represented to the human in the virtual reality by a humanoid avatar. We describe the system and also a study that was designed to test whether humans can successfully play a game with the rat. The results show that the system functioned well and that the humans were able to interact with the rat to fulfil the tasks of the game. This system opens up the possibility of new applications in the life sciences involving participant observation of and interaction with animals but at human scale.\n      "], "author_display": ["Jean-Marie Normand", "Maria V. Sanchez-Vives", "Christian Waechter", "Elias Giannopoulos", "Bernhard Grosswindhager", "Bernhard Spanlang", "Christoph Guger", "Gudrun Klinker", "Mandayam A. Srinivasan", "Mel Slater"], "article_type": "Research Article", "score": 0.36791116, "title_display": "Beaming into the Rat World: Enabling Real-Time Interaction between Rat and Human Each at Their Own Scale", "publication_date": "2012-10-31T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0048331"}, {"journal": "PLoS Computational Biology", "abstract": ["\nWe describe an innovative experimental and computational approach to control the expression of a protein in a population of yeast cells. We designed a simple control algorithm to automatically regulate the administration of inducer molecules to the cells by comparing the actual protein expression level in the cell population with the desired expression level. We then built an automated platform based on a microfluidic device, a time-lapse microscopy apparatus, and a set of motorized syringes, all controlled by a computer. We tested the platform to force yeast cells to express a desired fixed, or time-varying, amount of a reporter protein over thousands of minutes. The computer automatically switched the type of sugar administered to the cells, its concentration and its duration, according to the control algorithm. Our approach can be used to control expression of any protein, fused to a fluorescent reporter, provided that an external molecule known to (indirectly) affect its promoter activity is available.\nAuthor Summary: A crucial feature of biological systems is their ability to maintain homeostasis in spite of ever-changing conditions. In engineering, this ability can be embedded in devices ranging from the thermostat to the autopilot of a modern plane using control systems which operate via a negative feedback mechanism: the quantity to be controlled is measured then subtracted from the desired reference value, and the resulting error is used to compute the control action to be implemented on the physical system (e.g. switching on or off the heating, changing the position of the rudder). Here, we developed and applied a method to regulate the expression level of a protein, in a growing population of cells over several generations, in a completely automatic fashion. We designed and implemented an integrated platform comprising a microfluidic device, a time-lapse microscopy apparatus, and a set of motorized syringes, all controlled by a computer. We tested the platform to force yeast cells to express a desired time-varying amount of a gene in yeast. Our method can be applied to control a protein of interest in vivo allowing to probe the function of biological systems in unprecedented ways. "], "author_display": ["Filippo Menolascina", "Gianfranco Fiore", "Emanuele Orabona", "Luca De Stefano", "Mike Ferry", "Jeff Hasty", "Mario di Bernardo", "Diego di Bernardo"], "article_type": "Research Article", "score": 0.3678624, "title_display": "<i>In-Vivo</i> Real-Time Control of Protein Expression from Endogenous and Synthetic Gene Networks", "publication_date": "2014-05-15T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1003625"}, {"journal": "PLOS ONE", "abstract": ["Objective: This paper investigated the cost-effectiveness of a computer-assisted Clinical Decision Support System (CDSS) in the identification of maternal complications in Ghana. Methods: A cost-effectiveness analysis was performed in a before- and after-intervention study. Analysis was conducted from the provider\u2019s perspective. The intervention area was the Kassena- Nankana district where computer-assisted CDSS was used by midwives in maternal care in six selected health centres. Six selected health centers in the Builsa district served as the non-intervention group, where the normal Ghana Health Service activities were being carried out. Results: Computer-assisted CDSS increased the detection of pregnancy complications during antenatal care (ANC) in the intervention health centres (before-intervention= 9 /1,000 ANC attendance; after-intervention= 12/1,000 ANC attendance; P-value=0.010). In the intervention health centres, there was a decrease in the number of complications during labour by 1.1%, though the difference was not statistically significant (before-intervention =107/1,000 labour clients; after-intervention= 96/1,000 labour clients; P-value=0.305). Also, at the intervention health centres, the average cost per pregnancy complication detected during ANC (cost \u2013effectiveness ratio) decreased from US$17,017.58 (before-intervention) to US$15,207.5 (after-intervention). Incremental cost \u2013effectiveness ratio (ICER) was estimated at US$1,142. Considering only additional costs (cost of computer-assisted CDSS), cost per pregnancy complication detected was US$285. Conclusions: Computer \u2013assisted CDSS has the potential to identify complications during pregnancy and marginal reduction in labour complications. Implementing computer-assisted CDSS is more costly but more effective in the detection of pregnancy complications compared to routine maternal care, hence making the decision to implement CDSS very complex. Policy makers should however be guided by whether the additional benefit is worth the additional cost. "], "author_display": ["Maxwell Ayindenaba Dalaba", "Patricia Akweongo", "Raymond Akawire Aborigo", "Happiness Pius Saronga", "John Williams", "Antje Blank", "Jens Kaltschmidt", "Rainer Sauerborn", "Svetla Loukanova"], "article_type": "Research Article", "score": 0.3678483, "title_display": "Cost-Effectiveness of Clinical Decision Support System in Improving Maternal Health Care in Ghana", "publication_date": "2015-05-14T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0125920"}, {"journal": "PLoS ONE", "abstract": ["\nMinimum squared error based classification (MSEC) method establishes a unique classification model for all the test samples. However, this classification model may be not optimal for each test sample. This paper proposes an improved MSEC (IMSEC) method, which is tailored for each test sample. The proposed method first roughly identifies the possible classes of the test sample, and then establishes a minimum squared error (MSE) model based on the training samples from these possible classes of the test sample. We apply our method to face recognition. The experimental results on several datasets show that IMSEC outperforms MSEC and the other state-of-the-art methods in terms of accuracy.\n"], "author_display": ["Qi Zhu", "Zhengming Li", "Jinxing Liu", "Zizhu Fan", "Lei Yu", "Yan Chen"], "article_type": "Research Article", "score": 0.3678411, "title_display": "Improved Minimum Squared Error Algorithm with Applications to Face Recognition", "publication_date": "2013-08-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0070370"}, {"journal": "PLoS ONE", "abstract": ["\nIdentification of a small panel of population structure informative markers can reduce genotyping cost and is useful in various applications, such as ancestry inference in association mapping, forensics and evolutionary theory in population genetics. Traditional methods to ascertain ancestral informative markers usually require the prior knowledge of individual ancestry and have difficulty for admixed populations. Recently Principal Components Analysis (PCA) has been employed with success to select SNPs which are highly correlated with top significant principal components (PCs) without use of individual ancestral information. The approach is also applicable to admixed populations. Here we propose a novel approach based on our recent result on summarizing population structure by graph Laplacian eigenfunctions, which differs from PCA in that it is geometric and robust to outliers. Our approach also takes advantage of the priori sparseness of informative markers in the genome. Through simulation of a ring population and the real global population sample HGDP of 650K SNPs genotyped in 940 unrelated individuals, we validate the proposed algorithm at selecting most informative markers, a small fraction of which can recover the similar underlying population structure efficiently. Employing a standard Support Vector Machine (SVM) to predict individuals' continental memberships on HGDP dataset of seven continents, we demonstrate that the selected SNPs by our method are more informative but less redundant than those selected by PCA. Our algorithm is a promising tool in genome-wide association studies and population genetics, facilitating the selection of structure informative markers, efficient detection of population substructure and ancestral inference.\n"], "author_display": ["Jun Zhang"], "article_type": "Research Article", "score": 0.3674918, "title_display": "Ancestral Informative Marker Selection and Population Structure Visualization Using Sparse Laplacian Eigenfunctions", "publication_date": "2010-11-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0013734"}, {"journal": "PLoS ONE", "abstract": ["\n        Computational detection of TF binding patterns has become an indispensable tool in functional genomics research. With the rapid advance of new sequencing technologies, large amounts of protein-DNA interaction data have been produced. Analyzing this data can provide substantial insight into the mechanisms of transcriptional regulation. However, the massive amount of sequence data presents daunting challenges. In our previous work, we have developed a novel algorithm called Hybrid Motif Sampler (HMS) that enables more scalable and accurate motif analysis. Despite much improvement, HMS is still time-consuming due to the requirement to calculate matching probabilities position-by-position. Using the NVIDIA CUDA toolkit, we developed a graphics processing unit (GPU)-accelerated motif analysis program named GPUmotif. We proposed a \u201cfragmentation\" technique to hide data transfer time between memories. Performance comparison studies showed that commonly-used model-based motif scan and de novo motif finding procedures such as HMS can be dramatically accelerated when running GPUmotif on NVIDIA graphics cards. As a result, energy consumption can also be greatly reduced when running motif analysis using GPUmotif. The GPUmotif program is freely available at http://sourceforge.net/projects/gpumotif/\n      "], "author_display": ["Pooya Zandevakili", "Ming Hu", "Zhaohui Qin"], "article_type": "Research Article", "score": 0.36747235, "title_display": "GPUmotif: An Ultra-Fast and Energy-Efficient Motif Analysis Program Using Graphics Processing Units", "publication_date": "2012-05-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0036865"}, {"journal": "PLoS Computational Biology", "abstract": ["\n\t\t\t\tText-mining algorithms make mistakes in extracting facts from natural-language texts. In biomedical applications, which rely on use of text-mined data, it is critical to assess the quality (the probability that the message is correctly extracted) of individual facts\u2014to resolve data conflicts and inconsistencies. Using a large set of almost 100,000 manually produced evaluations (most facts were independently reviewed more than once, producing independent evaluations), we implemented and tested a collection of algorithms that mimic human evaluation of facts provided by an automated information-extraction system. The performance of our best automated classifiers closely approached that of our human evaluators (ROC score close to 0.95). Our hypothesis is that, were we to use a larger number of human experts to evaluate any given sentence, we could implement an artificial-intelligence curator that would perform the classification job at least as accurately as an average individual human evaluator. We illustrated our analysis by visualizing the predicted accuracy of the text-mined relations involving the term cocaine.\n\t\t\tSynopsis: Current automated approaches for extracting biologically important facts from scientific articles are imperfect: while being capable of efficient, fast, and inexpensive analysis of enormous quantities of scientific prose, they make errors. To emulate the human experts evaluating the quality of the automatically extracted facts, we have developed an artificial intelligence program (\u201ca robotic curator\u201d) that closely approaches human experts in the quality of distinguishing the correctly extracted facts from the incorrectly extracted ones. "], "author_display": ["Raul Rodriguez-Esteban", "Ivan Iossifov", "Andrey Rzhetsky"], "article_type": "Research Article", "score": 0.36735705, "title_display": "Imitating Manual Curation of Text-Mined Facts in Biomedicine", "publication_date": "2006-09-08T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.0020118"}, {"journal": "PLOS ONE", "abstract": ["\nOne of the most complex issues in the cloud computing environment is the problem of resource allocation so that, on one hand, the cloud provider expects the most profitability and, on the other hand, users also expect to have the best resources at their disposal considering the budget constraints and time. In most previous work conducted, heuristic and evolutionary approaches have been used to solve this problem. Nevertheless, since the nature of this environment is based on economic methods, using such methods can decrease response time and reducing the complexity of the problem. In this paper, an auction-based method is proposed which determines the auction winner by applying game theory mechanism and holding a repetitive game with incomplete information in a non-cooperative environment. In this method, users calculate suitable price bid with their objective function during several round and repetitions and send it to the auctioneer; and the auctioneer chooses the winning player based the suggested utility function. In the proposed method, the end point of the game is the Nash equilibrium point where players are no longer inclined to alter their bid for that resource and the final bid also satisfies the auctioneer\u2019s utility function. To prove the response space convexity, the Lagrange method is used and the proposed model is simulated in the cloudsim and the results are compared with previous work. At the end, it is concluded that this method converges to a response in a shorter time, provides the lowest service level agreement violations and the most utility to the provider.\n"], "author_display": ["Amin Nezarat", "GH Dastghaibifard"], "article_type": "Research Article", "score": 0.36729884, "title_display": "Efficient Nash Equilibrium Resource Allocation Based on Game Theory Mechanism in Cloud Computing by Using Auction", "publication_date": "2015-10-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0138424"}, {"journal": "PLoS ONE", "abstract": ["\n        This paper examines the proximity of authors to those they cite using degrees of separation in a co-author network, essentially using collaboration networks to expand on the notion of self-citations. While the proportion of direct self-citations (including co-authors of both citing and cited papers) is relatively constant in time and across specialties in the natural sciences (10% of references) and the social sciences (20%), the same cannot be said for citations to authors who are members of the co-author network. Differences between fields and trends over time lie not only in the degree of co-authorship which defines the large-scale topology of the collaboration network, but also in the referencing practices within a given discipline, computed by defining a propensity to cite at a given distance within the collaboration network. Overall, there is little tendency to cite those nearby in the collaboration network, excluding direct self-citations. These results are interpreted in terms of small-scale structure, field-specific citation practices, and the value of local co-author networks for the production of knowledge and for the accumulation of symbolic capital. Given the various levels of integration between co-authors, our findings shed light on the question of the availability of \u2018arm's length\u2019 expert reviewers of grant applications and manuscripts.\n      "], "author_display": ["Matthew L. Wallace", "Vincent Larivi\u00e8re", "Yves Gingras"], "article_type": "Research Article", "score": 0.36729878, "title_display": "A Small World of Citations? The Influence of Collaboration Networks on Citation Practices", "publication_date": "2012-03-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0033339"}, {"journal": "PLoS ONE", "abstract": ["\nWe apply a known algorithm for computing exactly inequalities between Beta distributions to assess whether a given position in a genome is differentially methylated across samples. We discuss the advantages brought by the adoption of this solution with respect to two approximations (Fisher's test and Z score). The same formalism presented here can be applied in a similar way to variant calling.\n"], "author_display": ["Emanuele Raineri", "Marc Dabad", "Simon Heath"], "article_type": "Research Article", "score": 0.36715668, "title_display": "A Note on Exact Differences between Beta Distributions in Genomic (Methylation) Studies", "publication_date": "2014-05-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0097349"}, {"journal": "PLoS ONE", "abstract": ["\n        Experimental studies have provided evidence that the visual processing areas of the primate brain represent facial identity and facial expression within different subpopulations of neurons. For example, in non-human primates there is evidence that cells within the inferior temporal gyrus (TE) respond primarily to facial identity, while cells within the superior temporal sulcus (STS) respond to facial expression. More recently, it has been found that the orbitofrontal cortex (OFC) of non-human primates contains some cells that respond exclusively to changes in facial identity, while other cells respond exclusively to facial expression. How might the primate visual system develop physically separate representations of facial identity and expression given that the visual system is always exposed to simultaneous combinations of facial identity and expression during learning? In this paper, a biologically plausible neural network model, VisNet, of the ventral visual pathway is trained on a set of carefully-designed cartoon faces with different identities and expressions. The VisNet model architecture is composed of a hierarchical series of four Self-Organising Maps (SOMs), with associative learning in the feedforward synaptic connections between successive layers. During learning, the network develops separate clusters of cells that respond exclusively to either facial identity or facial expression. We interpret the performance of the network in terms of the learning properties of SOMs, which are able to exploit the statistical indendependence between facial identity and expression.\n      "], "author_display": ["James Matthew Tromans", "Mitchell Harris", "Simon Maitland Stringer"], "article_type": "Research Article", "score": 0.3671303, "title_display": "A Computational Model of the Development of Separate Representations of Facial Identity and Expression in the Primate Visual System", "publication_date": "2011-10-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0025616"}, {"journal": "PLoS ONE", "abstract": ["\n        STAT3 is a transcription factor that has been found to be constitutively activated in a number of human cancers. Dimerization of STAT3 via its SH2 domain and the subsequent translocation of the dimer to the nucleus leads to transcription of anti-apoptotic genes. Prevention of the dimerization is thus an attractive strategy for inhibiting the activity of STAT3. Phosphotyrosine-based peptidomimetic inhibitors, which mimic pTyr-Xaa-Yaa-Gln motif and have strong to weak binding affinities, have been previously investigated. It is well-known that structures of protein-inhibitor complexes are important for understanding the binding interactions and designing stronger inhibitors. Experimental structures of inhibitors bound to the SH2 domain of STAT3 are, however, unavailable. In this paper we describe a computational study that combined molecular docking and molecular dynamics to model structures of 12 peptidomimetic inhibitors bound to the SH2 domain of STAT3. A detailed analysis of the modeled structures was performed to evaluate the characteristics of the binding interactions. We also estimated the binding affinities of the inhibitors by combining MMPB/GBSA-based energies and entropic cost of binding. The estimated affinities correlate strongly with the experimentally obtained affinities. Modeling results show binding modes that are consistent with limited previous modeling studies on binding interactions involving the SH2 domain and phosphotyrosine(pTyr)-based inhibitors. We also discovered a stable novel binding mode that involves deformation of two loops of the SH2 domain that subsequently bury the C-terminal end of one of the stronger inhibitors. The novel binding mode could prove useful for developing more potent inhibitors aimed at preventing dimerization of cancer target protein STAT3.\n      "], "author_display": ["Ankur Dhanik", "John S. McMurray", "Lydia E. Kavraki"], "article_type": "Research Article", "score": 0.3671107, "title_display": "Binding Modes of Peptidomimetics Designed to Inhibit STAT3", "publication_date": "2012-12-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0051603"}, {"journal": "PLoS ONE", "abstract": ["\nBiological systems perform computations at multiple scales and they do so in a robust way. Engineering metaphors have often been used in order to provide a rationale for modeling cellular and molecular computing networks and as the basis for their synthetic design. However, a major constraint in this mapping between electronic and wet computational circuits is the wiring problem. Although wires are identical within electronic devices, they must be different when using synthetic biology designs. Moreover, in most cases the designed molecular systems cannot be reused for other functions. A new approximation allows us to simplify the problem by using synthetic cellular consortia where the output of the computation is distributed over multiple engineered cells. By evolving circuits in silico, we can obtain the minimal sets of Boolean units required to solve the given problem at the lowest cost using cellular consortia. Our analysis reveals that the basic set of logic units is typically non-standard. Among the most common units, the so called inverted IMPLIES (N-Implies) appears to be one of the most important elements along with the NOT and AND functions. Although NOR and NAND gates are widely used in electronics, evolved circuits based on combinations of these gates are rare, thus suggesting that the strategy of combining the same basic logic gates might be inappropriate in order to easily implement synthetic computational constructs. The implications for future synthetic designs, the general view of synthetic biology as a standard engineering domain, as well as potencial drawbacks are outlined.\n"], "author_display": ["Javier Macia", "Ricard Sole"], "article_type": "Research Article", "score": 0.36707065, "title_display": "How to Make a Synthetic Multicellular Computer", "publication_date": "2014-02-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0081248"}, {"journal": "PLoS ONE", "abstract": ["\n        We analyze the passengers' traffic pattern for 1.58 million taxi trips of Shanghai, China. By employing the non-negative matrix factorization and optimization methods, we find that, people travel on workdays mainly for three purposes: commuting between home and workplace, traveling from workplace to workplace, and others such as leisure activities. Therefore, traffic flow in one area or between any pair of locations can be approximated by a linear combination of three basis flows, corresponding to the three purposes respectively. We name the coefficients in the linear combination as traffic powers, each of which indicates the strength of each basis flow. The traffic powers on different days are typically different even for the same location, due to the uncertainty of the human motion. Therefore, we provide a probability distribution function for the relative deviation of the traffic power. This distribution function is in terms of a series of functions for normalized binomial distributions. It can be well explained by statistical theories and is verified by empirical data. These findings are applicable in predicting the road traffic, tracing the traffic pattern and diagnosing the traffic related abnormal events. These results can also be used to infer land uses of urban area quite parsimoniously.\n      "], "author_display": ["Chengbin Peng", "Xiaogang Jin", "Ka-Chun Wong", "Meixia Shi", "Pietro Li\u00f2"], "article_type": "Research Article", "score": 0.36699462, "title_display": "Collective Human Mobility Pattern from Taxi Trips in Urban Area", "publication_date": "2012-04-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0034487"}, {"journal": "PLOS ONE", "abstract": ["\nComparing DNA or protein sequences plays an important role in the functional analysis of genomes. Despite many methods available for sequences comparison, few methods retain the information content of sequences. We propose a new approach, the Yau-Hausdorff method, which considers all translations and rotations when seeking the best match of graphical curves of DNA or protein sequences. The complexity of this method is lower than that of any other two dimensional minimum Hausdorff algorithm. The Yau-Hausdorff method can be used for measuring the similarity of DNA sequences based on two important tools: the Yau-Hausdorff distance and graphical representation of DNA sequences. The graphical representations of DNA sequences conserve all sequence information and the Yau-Hausdorff distance is mathematically proved as a true metric. Therefore, the proposed distance can preciously measure the similarity of DNA sequences. The phylogenetic analyses of DNA sequences by the Yau-Hausdorff distance show the accuracy and stability of our approach in similarity comparison of DNA or protein sequences. This study demonstrates that Yau-Hausdorff distance is a natural metric for DNA and protein sequences with high level of stability. The approach can be also applied to similarity analysis of protein sequences by graphic representations, as well as general two dimensional shape matching.\n"], "author_display": ["Kun Tian", "Xiaoqian Yang", "Qin Kong", "Changchuan Yin", "Rong L. He", "Stephen S.-T. Yau"], "article_type": "Research Article", "score": 0.3669212, "title_display": "Two Dimensional Yau-Hausdorff Distance with Applications on Comparison of DNA and Protein Sequences", "publication_date": "2015-09-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0136577"}, {"journal": "PLoS ONE", "abstract": ["\n        A key function of the brain is to interpret noisy sensory information. To do so optimally, observers must, in many tasks, take into account knowledge of the precision with which stimuli are encoded. In an orientation change detection task, we find that encoding precision does not only depend on an experimentally controlled reliability parameter (shape), but also exhibits additional variability. In spite of variability in precision, human subjects seem to take into account precision near-optimally on a trial-to-trial and item-to-item basis. Our results offer a new conceptualization of the encoding of sensory information and highlight the brain\u2019s remarkable ability to incorporate knowledge of uncertainty during complex perceptual decision-making.\n      "], "author_display": ["Shaiyan Keshvari", "Ronald van den Berg", "Wei Ji Ma"], "article_type": "Research Article", "score": 0.36681473, "title_display": "Probabilistic Computation in Human Perception under Variability in Encoding Precision", "publication_date": "2012-06-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0040216"}, {"journal": "PLoS ONE", "abstract": ["Background: Selecting an appropriate substitution model and deriving a tree topology for a given sequence set are essential in phylogenetic analysis. However, such time consuming, computationally intensive tasks rely on knowledge of substitution model theories and related expertise to run through all possible combinations of several separate programs. To ensure a thorough and efficient analysis and avert tedious manipulations of various programs, this work presents an intuitive framework, the phylogenetic reconstruction with automatic likelihood model selectors (PALM), with convincing, updated algorithms and a best-fit model selection mechanism for seamless phylogenetic analysis. Methodology: As an integrated framework of ClustalW, PhyML, MODELTEST, ProtTest, and several in-house programs, PALM evaluates the fitness of 56 substitution models for nucleotide sequences and 112 substitution models for protein sequences with scores in various criteria. The input for PALM can be either sequences in FASTA format or a sequence alignment file in PHYLIP format. To accelerate the computing of maximum likelihood and bootstrapping, this work integrates MPICH2/PhyML, PalmMonitor and Palm job controller across several machines with multiple processors and adopts the task parallelism approach. Moreover, an intuitive and interactive web component, PalmTree, is developed for displaying and operating the output tree with options of tree rooting, branches swapping, viewing the branch length values, and viewing bootstrapping score, as well as removing nodes to restart analysis iteratively. Significance: The workflow of PALM is straightforward and coherent. Via a succinct, user-friendly interface, researchers unfamiliar with phylogenetic analysis can easily use this server to submit sequences, retrieve the output, and re-submit a job based on a previous result if some sequences are to be deleted or added for phylogenetic reconstruction. PALM results in an inference of phylogenetic relationship not only by vanquishing the computation difficulty of ML methods but also providing statistic methods for model selection and bootstrapping. The proposed approach can reduce calculation time, which is particularly relevant when querying a large data set. PALM can be accessed online at http://palm.iis.sinica.edu.tw. "], "author_display": ["Shu-Hwa Chen", "Sheng-Yao Su", "Chen-Zen Lo", "Kuei-Hsien Chen", "Teng-Jay Huang", "Bo-Han Kuo", "Chung-Yen Lin"], "article_type": "Research Article", "score": 0.3666889, "title_display": "PALM: A Paralleled and Integrated Framework for Phylogenetic Inference with Automatic Likelihood Model Selectors", "publication_date": "2009-12-07T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0008116"}, {"journal": "PLoS ONE", "abstract": ["Background: The analysis of transcriptome data involves many steps and various programs, along with organization of large amounts of data and results. Without a methodical approach for storage, analysis and query, the resulting ad hoc analysis can lead to human error, loss of data and results, inefficient use of time, and lack of verifiability, repeatability, and extensibility. Methodology: The Transcriptome Computational Workbench (TCW) provides Java graphical interfaces for methodical analysis for both single and comparative transcriptome data without the use of a reference genome (e.g. for non-model organisms). The singleTCW interface steps the user through importing transcript sequences (e.g. Illumina) or assembling long sequences (e.g. Sanger, 454, transcripts), annotating the sequences, and performing differential expression analysis using published statistical programs in R. The data, metadata, and results are stored in a MySQL database. The multiTCW interface builds a comparison database by importing sequence and annotation from one or more single TCW databases, executes the ESTscan program to translate the sequences into proteins, and then incorporates one or more clusterings, where the clustering options are to execute the orthoMCL program, compute transitive closure, or import clusters. Both singleTCW and multiTCW allow extensive query and display of the results, where singleTCW displays the alignment of annotation hits to transcript sequences, and multiTCW displays multiple transcript alignments with MUSCLE or pairwise alignments. The query programs can be executed on the desktop for fastest analysis, or from the web for sharing the results. Conclusion: It is now affordable to buy a multi-processor machine, and easy to install Java and MySQL. By simply downloading the TCW, the user can interactively analyze, query and view their data. The TCW allows in-depth data mining of the results, which can lead to a better understanding of the transcriptome. TCW is freely available from www.agcol.arizona.edu/software/tcw. "], "author_display": ["Carol Soderlund", "William Nelson", "Mark Willer", "David R. Gang"], "article_type": "Research Article", "score": 0.36662066, "title_display": "TCW: Transcriptome Computational Workbench", "publication_date": "2013-07-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0069401"}, {"journal": "PLoS ONE", "abstract": ["\n        Biological named entity recognition, the identification of biological terms in text, is essential for biomedical information extraction. Machine learning-based approaches have been widely applied in this area. However, the recognition performance of current approaches could still be improved. Our novel approach is to combine support vector machines (SVMs) and conditional random fields (CRFs), which can complement and facilitate each other. During the hybrid process, we use SVM to separate biological terms from non-biological terms, before we use CRFs to determine the types of biological terms, which makes full use of the power of SVM as a binary-class classifier and the data-labeling capacity of CRFs. We then merge the results of SVM and CRFs. To remove any inconsistencies that might result from the merging, we develop a useful algorithm and apply two rules. To ensure biological terms with a maximum length are identified, we propose a maximal bidirectional squeezing approach that finds the longest term. We also add a positive gain to rare events to reinforce their probability and avoid bias. Our approach will also gradually extend the context so more contextual information can be included. We examined the performance of four approaches with GENIA corpus and JNLPBA04 data. The combination of SVM and CRFs improved performance. The macro-precision, macro-recall, and macro-F1 of the SVM-CRFs hybrid approach surpassed conventional SVM and CRFs. After applying the new algorithms, the macro-F1 reached 91.67% with the GENIA corpus and 84.04% with the JNLPBA04 data.\n      "], "author_display": ["Fei Zhu", "Bairong Shen"], "article_type": "Research Article", "score": 0.3665659, "title_display": "Combined SVM-CRFs for Biological Named Entity Recognition with Maximal Bidirectional Squeezing", "publication_date": "2012-06-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0039230"}, {"journal": "PLoS ONE", "abstract": ["Introduction: Mortality data provide essential evidence on the health status of populations in crisis-affected and resource-poor settings and to guide and assess relief operations. Retrospective surveys are commonly used to collect mortality data in such populations, but require substantial resources and have important methodological limitations. We evaluated the feasibility of an alternative method for rapidly quantifying mortality (the informant method). The study objective was to assess the economic feasibility of the informant method. Methods: The informant method captures deaths through an exhaustive search for all deaths occurring in a population over a defined and recent recall period, using key community informants and next-of-kin of decedents. Between July and October 2008, we implemented and evaluated the informant method in: Kabul, Afghanistan; Mae La camp for Karen refugees, Thai-Burma border; Chiradzulu District, Malawi; and Lugufu and Mtabila refugee camps, Tanzania. We documented the time and cost inputs for the informant method in each site, and compared these with projections for hypothetical retrospective mortality surveys implemented in the same site with a 6 month recall period and with a 30 day recall period. Findings: The informant method was estimated to require an average of 29% less time inputs and 33% less monetary inputs across all four study sites when compared with retrospective surveys with a 6 month recall period, and 88% less time inputs and 86% less monetary inputs when compared with retrospective surveys with a 1 month recall period. Verbal autopsy questionnaires were feasible and efficient, constituting only 4% of total person-time for the informant method's implementation in Chiradzulu District. Conclusions: The informant method requires fewer resources and incurs less respondent burden. The method's generally impressive feasibility and the near real-time mortality data it provides warrant further work to develop the method given the importance of mortality measurement in such settings. "], "author_display": ["Bayard Roberts", "Oliver W. Morgan", "Mohammed Ghaus Sultani", "Peter Nyasulu", "Sunday Rwebangila", "Egbert Sondorp", "Daniel Chandramohan", "Francesco Checchi"], "article_type": "Research Article", "score": 0.36630774, "title_display": "Economic Feasibility of a New Method to Estimate Mortality in Crisis-Affected and Resource-Poor Settings", "publication_date": "2011-09-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0025175"}, {"journal": "PLoS ONE", "abstract": ["\nThe central resource processed by the sensorimotor system of an organism is information. We propose an information-based quantity that allows one to characterize the efficiency of the perception-action loop of an abstract organism model. It measures the potential of the organism to imprint information on the environment via its actuators in a way that can be recaptured by its sensors, essentially quantifying the options available and visible to the organism. Various scenarios suggest that such a quantity could identify the preferred direction of evolution or adaptation of the sensorimotor loop of organisms.\n"], "author_display": ["Alexander S. Klyubin", "Daniel Polani", "Chrystopher L. Nehaniv"], "article_type": "Research Article", "score": 0.3659696, "title_display": "Keep Your Options Open: An Information-Based Driving Principle for Sensorimotor Systems", "publication_date": "2008-12-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0004018"}, {"journal": "PLoS ONE", "abstract": ["\nWikiPathways is a platform for creating, updating, and sharing biological pathways [1]. Pathways can be edited and downloaded using the wiki-style website. Here we present a SOAP web service that provides programmatic access to WikiPathways that is complementary to the website. We describe the functionality that this web service offers and discuss several use cases in detail. Exposing WikiPathways through a web service opens up new ways of utilizing pathway information and assisting the community curation process.\n"], "author_display": ["Thomas Kelder", "Alexander R. Pico", "Kristina Hanspers", "Martijn P. van Iersel", "Chris Evelo", "Bruce R. Conklin"], "article_type": "Research Article", "score": 0.36568993, "title_display": "Mining Biological Pathways Using WikiPathways Web Services", "publication_date": "2009-07-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0006447"}, {"journal": "PLoS ONE", "abstract": ["\n        Pattern recognition methods have become increasingly popular in fMRI data analysis, which are powerful in discriminating between multi-voxel patterns of brain activities associated with different mental states. However, when they are used in functional brain mapping, the location of discriminative voxels varies significantly, raising difficulties in interpreting the locus of the effect. Here we proposed a hierarchical framework of multivariate approach that maps informative clusters rather than voxels to achieve reliable functional brain mapping without compromising the discriminative power. In particular, we first searched for local homogeneous clusters that consisted of voxels with similar response profiles. Then, a multi-voxel classifier was built for each cluster to extract discriminative information from the multi-voxel patterns. Finally, through multivariate ranking, outputs from the classifiers were served as a multi-cluster pattern to identify informative clusters by examining interactions among clusters. Results from both simulated and real fMRI data demonstrated that this hierarchical approach showed better performance in the robustness of functional brain mapping than traditional voxel-based multivariate methods. In addition, the mapped clusters were highly overlapped for two perceptually equivalent object categories, further confirming the validity of our approach. In short, the hierarchical framework of multivariate approach is suitable for both pattern classification and brain mapping in fMRI studies.\n      "], "author_display": ["Rui Xu", "Zonglei Zhen", "Jia Liu"], "article_type": "Research Article", "score": 0.36565113, "title_display": "Mapping Informative Clusters in a Hierarchial Framework of fMRI Multivariate Analysis", "publication_date": "2010-11-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0015065"}, {"journal": "PLoS ONE", "abstract": ["\n        Fairness considerations are a strong motivational force in social decision-making. Here, we investigated the role of intentionality in response to unfair offers in the Ultimatum Game by manipulating both proposers' degree of control over the selection of offers and the context pertaining to the outcomes of offers proposers can choose from. As a result, the design enabled us to disentangle intention- and context-based decision-making processes. Rejection rates were higher when an unfair offer was intentionally chosen over a fair alternative than when it was chosen by the computer, outside proposers' control. This finding provides direct evidence for intention-based decision-making. Also, rejection rates in general were sensitive to the context in which an offer was made, indicating the involvement of both intention- and context-based processes in social decision-making. Importantly, however, the current study highlights the role of intention-based fairness considerations in basic decision-making situations where outcomes are explicitly stated and thus easy to compare. Based on these results, we propose that fairness can be judged on different, but additive levels of (social-) cognitive processing that might have different developmental trajectories.\n      "], "author_display": ["Sina Radke", "Berna G\u00fcro\u011flu", "Ellen R. A. de Bruijn"], "article_type": "Research Article", "score": 0.36563545, "title_display": "There's Something about a Fair Split: Intentionality Moderates Context-Based Fairness Considerations in Social Decision-Making", "publication_date": "2012-02-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0031491"}, {"journal": "PLoS ONE", "abstract": ["\n        Rapid advancing computational technologies have greatly speeded up the development of computer-aided drug design (CADD). Recently, pharmaceutical companies have increasingly shifted their attentions toward traditional Chinese medicine (TCM) for novel lead compounds. Despite the growing number of studies on TCM, there is no free 3D small molecular structure database of TCM available for virtual screening or molecular simulation. To address this shortcoming, we have constructed TCM Database@Taiwan (http://tcm.cmu.edu.tw/) based on information collected from Chinese medical texts and scientific publications. TCM Database@Taiwan is currently the world's largest non-commercial TCM database. This web-based database contains more than 20,000 pure compounds isolated from 453 TCM ingredients. Both cdx (2D) and Tripos mol2 (3D) formats of each pure compound in the database are available for download and virtual screening. The TCM database includes both simple and advanced web-based query options that can specify search clauses, such as molecular properties, substructures, TCM ingredients, and TCM classification, based on intended drug actions. The TCM database can be easily accessed by all researchers conducting CADD. Over the last eight years, numerous volunteers have devoted their time to analyze TCM ingredients from Chinese medical texts as well as to construct structure files for each isolated compound. We believe that TCM Database@Taiwan will be a milestone on the path towards modernizing traditional Chinese medicine.\n      "], "author_display": ["Calvin Yu-Chian Chen"], "article_type": "Research Article", "score": 0.3655782, "title_display": "TCM Database@Taiwan: The World's Largest Traditional Chinese Medicine Database for Drug Screening <i>In Silico</i>", "publication_date": "2011-01-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0015939"}, {"journal": "PLOS ONE", "abstract": ["\nAutomatic classification of tissue types of region of interest (ROI) plays an important role in computer-aided diagnosis. In the current study, we focus on the classification of three types of brain tumors (i.e., meningioma, glioma, and pituitary tumor) in T1-weighted contrast-enhanced MRI (CE-MRI) images. Spatial pyramid matching (SPM), which splits the image into increasingly fine rectangular subregions and computes histograms of local features from each subregion, exhibits excellent results for natural scene classification. However, this approach is not applicable for brain tumors, because of the great variations in tumor shape and size. In this paper, we propose a method to enhance the classification performance. First, the augmented tumor region via image dilation is used as the ROI instead of the original tumor region because tumor surrounding tissues can also offer important clues for tumor types. Second, the augmented tumor region is split into increasingly fine ring-form subregions. We evaluate the efficacy of the proposed method on a large dataset with three feature extraction methods, namely, intensity histogram, gray level co-occurrence matrix (GLCM), and bag-of-words (BoW) model. Compared with using tumor region as ROI, using augmented tumor region as ROI improves the accuracies to 82.31% from 71.39%, 84.75% from 78.18%, and 88.19% from 83.54% for intensity histogram, GLCM, and BoW model, respectively. In addition to region augmentation, ring-form partition can further improve the accuracies up to 87.54%, 89.72%, and 91.28%. These experimental results demonstrate that the proposed method is feasible and effective for the classification of brain tumors in T1-weighted CE-MRI.\n"], "author_display": ["Jun Cheng", "Wei Huang", "Shuangliang Cao", "Ru Yang", "Wei Yang", "Zhaoqiang Yun", "Zhijian Wang", "Qianjin Feng"], "article_type": "Research Article", "score": 0.36531106, "title_display": "Enhanced Performance of Brain Tumor Classification via Tumor Region Augmentation and Partition", "publication_date": "2015-10-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0140381"}, {"abstract": ["\n        Integration is a fundamental working memory operation, requiring the insertion of information from one task into the execution of another concurrent task. Previous neuroimaging studies have suggested the involvement of left anterior prefrontal cortex (L-aPFC) in relation to working memory integration demands, increasing during presentation of information to be integrated (loading), throughout its maintenance during a secondary task, up to the integration step, and then decreasing afterward (unloading). Here we used short bursts of 5 Hz repetitive Transcranic Magnetic Stimulation (rTMS) to modulate L-aPFC activity and to assess its causal role in integration. During experimental blocks, rTMS was applied (N\u200a=\u200a10) over L-aPFC or vertex (control site) at different time-points of a task involving integration of a preloaded digit into a sequence of arithmetical steps, and contrasted with a closely matched task without integration demand (segregation). When rTMS was applied during the loading phase, reaction times during secondary task were faster, without significant changes in error rates. RTMS instead worsened performance when applied during information unloading. In contrast, no effects were observed when rTMS was applied during the other phases of integration, or during the segregation condition. These results confirm the hypothesis that L-aPFC is causally and selectively involved in the integration of information in working memory. They additionally suggest that pre-integration loading and post-integration unloading of information involving this area may be active and resource-consuming processes.\n      "], "author_display": ["Nicola De Pisapia", "Marco Sandrini", "Todd S. Braver", "Luigi Cattaneo"], "article_type": "Research Article", "score": 0.3652929, "title_display": "Integration in Working Memory: A Magnetic Stimulation Study on the Role of Left Anterior Prefrontal Cortex", "publication_date": "2012-08-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0043731"}, {"journal": "PLoS ONE", "abstract": ["\n        The field of volume visualization has undergone rapid development during the past years, both due to advances in suitable computing hardware and due to the increasing availability of large volume datasets. Recent work has focused on increasing the visual realism in Direct Volume Rendering (DVR) by integrating a number of visually plausible but often effect-specific rendering techniques, for instance modeling of light occlusion and depth of field. Besides yielding more attractive renderings, especially the more realistic lighting has a positive effect on perceptual tasks. Although these new rendering techniques yield impressive results, they exhibit limitations in terms of their exibility and their performance. Monte Carlo ray tracing (MCRT), coupled with physically based light transport, is the de-facto standard for synthesizing highly realistic images in the graphics domain, although usually not from volumetric data. Due to the stochastic sampling of MCRT algorithms, numerous effects can be achieved in a relatively straight-forward fashion. For this reason, we have developed a practical framework that applies MCRT techniques also to direct volume rendering (DVR). With this work, we demonstrate that a host of realistic effects, including physically based lighting, can be simulated in a generic and flexible fashion, leading to interactive DVR with improved realism. In the hope that this improved approach to DVR will see more use in practice, we have made available our framework under a permissive open source license.\n      "], "author_display": ["Thomas Kroes", "Frits H. Post", "Charl P. Botha"], "article_type": "Research Article", "score": 0.36523774, "title_display": "Exposure Render: An Interactive Photo-Realistic Volume Rendering Framework", "publication_date": "2012-07-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0038586"}, {"journal": "PLoS ONE", "abstract": ["\n        Quantitative predictions in computational life sciences are often based on regression models. The advent of machine learning has led to highly accurate regression models that have gained widespread acceptance. While there are statistical methods available to estimate the global performance of regression models on a test or training dataset, it is often not clear how well this performance transfers to other datasets or how reliable an individual prediction is\u2013a fact that often reduces a user\u2019s trust into a computational method. In analogy to the concept of an experimental error, we sketch how estimators for individual prediction errors can be used to provide confidence intervals for individual predictions. Two novel statistical methods, named CONFINE and CONFIVE, can estimate the reliability of an individual prediction based on the local properties of nearby training data. The methods can be applied equally to linear and non-linear regression methods with very little computational overhead. We compare our confidence estimators with other existing confidence and applicability domain estimators on two biologically relevant problems (MHC\u2013peptide binding prediction and quantitative structure-activity relationship (QSAR)). Our results suggest that the proposed confidence estimators perform comparable to or better than previously proposed estimation methods. Given a sufficient amount of training data, the estimators exhibit error estimates of high quality. In addition, we observed that the quality of estimated confidence intervals is predictable. We discuss how confidence estimation is influenced by noise, the number of features, and the dataset size. Estimating the confidence in individual prediction in terms of error intervals represents an important step from plain, non-informative predictions towards transparent and interpretable predictions that will help to improve the acceptance of computational methods in the biological community.\n      "], "author_display": ["Sebastian Briesemeister", "J\u00f6rg Rahnenf\u00fchrer", "Oliver Kohlbacher"], "article_type": "Research Article", "score": 0.36515942, "title_display": "No Longer Confidential: Estimating the Confidence of Individual Regression Predictions", "publication_date": "2012-11-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0048723"}, {"journal": "PLoS ONE", "abstract": [": The brain is able to maintain a stable perception although the visual stimuli vary substantially on the retina due to geometric transformations and lighting variations in the environment. This paper presents a theory for achieving basic invariance properties already at the level of receptive fields. Specifically, the presented framework comprises (i) local scaling transformations caused by objects of different size and at different distances to the observer, (ii) locally linearized image deformations caused by variations in the viewing direction in relation to the object, (iii) locally linearized relative motions between the object and the observer and (iv) local multiplicative intensity transformations caused by illumination variations. The receptive field model can be derived by necessity from symmetry properties of the environment and leads to predictions about receptive field profiles in good agreement with receptive field profiles measured by cell recordings in mammalian vision. Indeed, the receptive field profiles in the retina, LGN and V1 are close to ideal to what is motivated by the idealized requirements. By complementing receptive field measurements with selection mechanisms over the parameters in the receptive field families, it is shown how true invariance of receptive field responses can be obtained under scaling transformations, affine transformations and Galilean transformations. Thereby, the framework provides a mathematically well-founded and biologically plausible model for how basic invariance properties can be achieved already at the level of receptive fields and support invariant recognition of objects and events under variations in viewpoint, retinal size, object motion and illumination. The theory can explain the different shapes of receptive field profiles found in biological vision, which are tuned to different sizes and orientations in the image domain as well as to different image velocities in space-time, from a requirement that the visual system should be invariant to the natural types of image transformations that occur in its environment. "], "author_display": ["Tony Lindeberg"], "article_type": "Research Article", "score": 0.3650976, "title_display": "Invariance of visual operations at the level of receptive fields", "publication_date": "2013-07-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0066990"}, {"journal": "PLOS ONE", "abstract": ["Motivation: Reproducing the results from a scientific paper can be challenging due to the absence of data and the computational tools required for their analysis. In addition, details relating to the procedures used to obtain the published results can be difficult to discern due to the use of natural language when reporting how experiments have been performed. The Investigation/Study/Assay (ISA), Nanopublications (NP), and Research Objects (RO) models are conceptual data modelling frameworks that can structure such information from scientific papers. Computational workflow platforms can also be used to reproduce analyses of data in a principled manner. We assessed the extent by which ISA, NP, and RO models, together with the Galaxy workflow system, can capture the experimental processes and reproduce the findings of a previously published paper reporting on the development of SOAPdenovo2, a de novo genome assembler. Results: Executable workflows were developed using Galaxy, which reproduced results that were consistent with the published findings. A structured representation of the information in the SOAPdenovo2 paper was produced by combining the use of ISA, NP, and RO models. By structuring the information in the published paper using these data and scientific workflow modelling frameworks, it was possible to explicitly declare elements of experimental design, variables, and findings. The models served as guides in the curation of scientific information and this led to the identification of inconsistencies in the original published paper, thereby allowing its authors to publish corrections in the form of an errata. Availability: SOAPdenovo2 scripts, data, and results are available through the GigaScience Database: http://dx.doi.org/10.5524/100044; the workflows are available from GigaGalaxy: http://galaxy.cbiit.cuhk.edu.hk; and the representations using the ISA, NP, and RO models are available through the SOAPdenovo2 case study website http://isa-tools.github.io/soapdenovo2/. Contact: philippe.rocca-serra@oerc.ox.ac.uk and susanna-assunta.sansone@oerc.ox.ac.uk. "], "author_display": ["Alejandra Gonz\u00e1lez-Beltr\u00e1n", "Peter Li", "Jun Zhao", "Maria Susana Avila-Garcia", "Marco Roos", "Mark Thompson", "Eelke van der Horst", "Rajaram Kaliyaperumal", "Ruibang Luo", "Tin-Lap Lee", "Tak-wah Lam", "Scott C. Edmunds", "Susanna-Assunta Sansone", "Philippe Rocca-Serra"], "article_type": "Research Article", "score": 0.36498255, "title_display": "From Peer-Reviewed to Peer-Reproduced in Scholarly Publishing: The Complementary Roles of Data Models and Workflows in Bioinformatics", "publication_date": "2015-07-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0127612"}, {"abstract": ["\n        Neurodegeneration is a major cause of human disease. Within the cerebellum, neuronal degeneration and/or dysfunction has been associated with many diseases, including several forms of cerebellar ataxia, since normal cerebellar function is paramount for proper motor coordination, balance, and motor learning. The cerebellum represents a well-established neural circuit. Determining the effects of neuronal loss is of great importance for understanding the fundamental workings of the cerebellum and disease-associated dysfunctions. This paper presents computational modeling of cerebellar function in relation to neurodegeneration either affecting a specific cerebellar cell type, such as granule cells or Purkinje cells, or more generally affecting cerebellar cells and the implications on effects in relation to performance degradation throughout the progression of cell death. The results of the models show that the overall number of cells, as a percentage of the total cell number in the model, of a particular type and, primarily, their proximity to the circuit output, and not the neuronal convergence due to the relative number of cells of a particular type, is the main indicator of the gravity of the functional deficit caused by the degradation of that cell type. Specifically, the greater the percentage loss of neurons of a specific type and the closer proximity of those cells to the deep cerebellar neurons, the greater the deficit caused by the neuronal cell loss. These findings contribute to the understanding of the functional consequences of neurodegeneration and the functional importance of specific connectivity within a neuronal circuit.\n      "], "author_display": ["Robert A. Nawrocki", "Majid Shaalan", "Sean E. Shaheen", "Nancy M. Lorenzon"], "article_type": "Research Article", "score": 0.3648516, "title_display": "Monitoring Performance Degradation of Cerebellar Functions Using Computational Neuroscience Methods: Implications on Neurological Diseases", "publication_date": "2012-09-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0045581"}, {"journal": "PLoS ONE", "abstract": ["The ability to manage the constantly growing clinically relevant information in genetics available on the internet is becoming crucial in medical practice. Therefore, training students in teaching environments that develop bioinformatics skills is a particular challenge to medical schools. We present here an instructional approach that potentiates learning of hormone/vitamin mechanisms of action in gene regulation with the acquisition and practice of bioinformatics skills. The activity is integrated within the study of the Endocrine System module. Given a nucleotide sequence of a hormone or vitamin-response element, students use internet databases and tools to find the gene to which it belongs. Subsequently, students search how the corresponding hormone/vitamin influences the expression of that particular gene and how a dysfunctional interaction might cause disease. This activity was presented for four consecutive years to cohorts of 50\u201360 students/year enrolled in the 2nd year of the medical degree. 90% of the students developed a better understanding of the usefulness of bioinformatics and 98% intend to use web-based resources in the future. Since hormones and vitamins regulate genes of all body organ systems, this activity successfully integrates the whole body physiology of the medical curriculum."], "author_display": ["Jo\u00e3o Carlos Sousa", "Manuel Jo\u00e3o Costa", "Joana Almeida Palha"], "article_type": "Research Article", "score": 0.36442605, "title_display": "Hormone-Mediated Gene Regulation and Bioinformatics: Learning One from the Other", "publication_date": "2007-05-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0000481"}, {"journal": "PLoS ONE", "abstract": ["\nA coupled model containing two neurons and one astrocyte is constructed by integrating Hodgkin-Huxley neuronal model and Li-Rinzel calcium model. Based on this hybrid model, information transmission between neurons is studied numerically. Our results show that when the successive spikes are produced in neuron 1 (N1), the bursting-like spikes (BLSs) occur in two neurons simultaneously during the spikes being transferred to neuron 2 (N2). The existence of the astrocyte and a higher expression level of mGluRs facilitate the occurrence of BLSs, but the rate of occurrence is not sensitive to the parameters. Furthermore, time delay \u03c4 occurs during the information transmission, and \u03c4 is almost independent of the effect of the astrocyte. Additionally, we found that low coupling strength may result in the distortion of the information, and this distortion is also proven to be almost independent of the astrocyte.\n"], "author_display": ["Jun Tang", "Jin-Ming Luo", "Jun Ma"], "article_type": "Research Article", "score": 0.36442596, "title_display": "Information Transmission in a Neuron-Astrocyte Coupled Model", "publication_date": "2013-11-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0080324"}, {"journal": "PLoS ONE", "abstract": ["\n        Tracking the volume of keywords in Internet searches, message boards, or Tweets has provided an alternative for following or predicting associations between popular interest or disease incidences. Here, we extend that research by examining the role of e-communications among day traders and their collective understanding of the market. Our study introduces a general method that focuses on bundles of words that behave differently from daily communication routines, and uses original data covering the content of instant messages among all day traders at a trading firm over a 40-month period. Analyses show that two word bundles convey traders' understanding of same day market events and potential next day market events. We find that when market volatility is high, traders' communications are dominated by same day events, and when volatility is low, communications are dominated by next day events. We show that the stronger the traders' attention to either same day or next day events, the higher their collective trading performance. We conclude that e-communication among traders is a product of mass collaboration over diverse viewpoints that embodies unique information about their weak or strong understanding of the market.\n      "], "author_display": ["Serguei Saavedra", "Jordi Duch", "Brian Uzzi"], "article_type": "Research Article", "score": 0.36430776, "title_display": "Tracking Traders' Understanding of the Market Using e-Communication Data", "publication_date": "2011-10-25T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0026705"}, {"abstract": ["\n        This paper proposes a set of web-based indicators for quantifying and ranking the relevance of terms related to key-issues in Ecology and Sustainability Science. Search engines that operate in different contexts (e.g. global, social, scientific) are considered as web information carriers (WICs) and are able to analyse; (i) relevance on different levels: global web, individual/personal sphere, on-line news, and culture/science; (ii) time trends of relevance; (iii) relevance of keywords for environmental governance. For the purposes of this study, several indicators and specific indices (relational indices and dynamic indices) were applied to a test-set of 24 keywords. Outputs consistently show that traditional study topics in environmental sciences such as water and air have remained the most quantitatively relevant keywords, while interest in systemic issues (i.e. ecosystem and landscape) has grown over the last 20 years. Nowadays, the relevance of new concepts such as resilience and ecosystem services is increasing, but the actual ability of these concepts to influence environmental governance needs to be further studied and understood. The proposed approach, which is based on intuitive and easily replicable procedures, can support the decision-making processes related to environmental governance.\n      "], "author_display": ["Sergio Malcevschi", "Agnese Marchini", "Dario Savini", "Tullio Facchinetti"], "article_type": "Research Article", "score": 0.36412728, "title_display": "Opportunities for Web-Based Indicators in Environmental Sciences", "publication_date": "2012-08-15T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0042128"}, {"journal": "PLoS ONE", "abstract": ["\n        Neurons in sensory systems can represent information not only by their firing rate, but also by the precise timing of individual spikes. For example, certain retinal ganglion cells, first identified in the salamander, encode the spatial structure of a new image by their first-spike latencies. Here we explore how this temporal code can be used by downstream neural circuits for computing complex features of the image that are not available from the signals of individual ganglion cells. To this end, we feed the experimentally observed spike trains from a population of retinal ganglion cells to an integrate-and-fire model of post-synaptic integration. The synaptic weights of this integration are tuned according to the recently introduced tempotron learning rule. We find that this model neuron can perform complex visual detection tasks in a single synaptic stage that would require multiple stages for neurons operating instead on neural spike counts. Furthermore, the model computes rapidly, using only a single spike per afferent, and can signal its decision in turn by just a single spike. Extending these analyses to large ensembles of simulated retinal signals, we show that the model can detect the orientation of a visual pattern independent of its phase, an operation thought to be one of the primitives in early visual processing. We analyze how these computations work and compare the performance of this model to other schemes for reading out spike-timing information. These results demonstrate that the retina formats spatial information into temporal spike sequences in a way that favors computation in the time domain. Moreover, complex image analysis can be achieved already by a simple integrate-and-fire model neuron, emphasizing the power and plausibility of rapid neural computing with spike times.\n      "], "author_display": ["Robert G\u00fctig", "Tim Gollisch", "Haim Sompolinsky", "Markus Meister"], "article_type": "Research Article", "score": 0.3640371, "title_display": "Computing Complex Visual Features with Retinal Spike Times", "publication_date": "2013-01-02T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0053063"}, {"journal": "PLoS ONE", "abstract": ["Background: Poor self-rated health (SRH) is socially patterned with health communication inequalities, arguably, serving as one mechanisms. This study investigated the effects of health information seeking on SRH, and its mediation effects on disparities in SRH. Methods: We conducted probability-based telephone surveys administered over telephone in 2009, 2010/11 and 2012 to monitor health information use among 4553 Chinese adults in Hong Kong. Frequency of information seeking from television, radio, newspapers/magazines and Internet was dichotomised as <1 time/month and \u22651 time/month. Adjusted odds ratios (aOR) for poor SRH were calculated for health information seeking from different sources and socioeconomic status (education and income). Mediation effects of health information seeking on the association between SES and poor SRH was estimated. Results: Poor SRH was associated with lower socioeconomic status (P for trend <0.001), and less than monthly health information seeking from newspapers/magazines (aOR\u200a=\u200a1.23, 95% CI 1.07\u20131.42) and Internet (aOR\u200a=\u200a1.13, 95% CI 0.98\u20131.31). Increasing combined frequency of health information seeking from newspapers/magazines and Internet was linearly associated with better SRH (P for trend <0.01). Health information seeking from these two sources contributed 9.2% and 7.9% of the total mediation effects of education and household income on poor SRH, respectively. Conclusions: Poor SRH was associated with lower socioeconomic status, and infrequent health information seeking from newspapers/magazines and Internet among Hong Kong Chinese. Disparities in SRH may be partially mediated by health information seeking from newspapers/magazines and Internet. "], "author_display": ["Man Ping Wang", "Xin Wang", "Tai Hing Lam", "Kasisomayajula Viswanath", "Sophia S. Chan"], "article_type": "Research Article", "score": 0.36403358, "title_display": "Health Information Seeking Partially Mediated the Association between Socioeconomic Status and Self-Rated Health among Hong Kong Chinese", "publication_date": "2013-12-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0082720"}, {"journal": "PLoS ONE", "abstract": ["\n        The manner in which information is encoded in neural signals is a major issue in Neuroscience. A common distinction is between rate codes, where information in neural responses is encoded as the number of spikes within a specified time frame (encoding window), and temporal codes, where the position of spikes within the encoding window carries some or all of the information about the stimulus. One test for the existence of a temporal code in neural responses is to add artificial time jitter to each spike in the response, and then assess whether or not information in the response has been degraded. If so, temporal encoding might be inferred, on the assumption that the jitter is small enough to alter the position, but not the number, of spikes within the encoding window. Here, the effects of artificial jitter on various spike train and information metrics were derived analytically, and this theory was validated using data from afferent neurons of the turtle vestibular and paddlefish electrosensory systems, and from model neurons. We demonstrate that the jitter procedure will degrade information content even when coding is known to be entirely by rate. For this and additional reasons, we conclude that the jitter procedure by itself is not sufficient to establish the presence of a temporal code.\n      "], "author_display": ["Alexander B. Neiman", "David F. Russell", "Michael H. Rowe"], "article_type": "Research Article", "score": 0.36355156, "title_display": "Identifying Temporal Codes in Spontaneously Active Sensory Neurons", "publication_date": "2011-11-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0027380"}, {"abstract": ["\n        We show that the Confusion Entropy, a measure of performance in multiclass problems has a strong (monotone) relation with the multiclass generalization of a classical metric, the Matthews Correlation Coefficient. Analytical results are provided for the limit cases of general no-information (n-face dice rolling) of the binary classification. Computational evidence supports the claim in the general case.\n      "], "author_display": ["Giuseppe Jurman", "Samantha Riccadonna", "Cesare Furlanello"], "article_type": "Research Article", "score": 0.3635151, "title_display": "A Comparison of MCC and CEN Error Measures in Multi-Class Prediction", "publication_date": "2012-08-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0041882"}, {"journal": "PLoS ONE", "abstract": ["\n        In recent years, the American Society of Nephrology (ASN) has increased its efforts to use its annual conference to inform and educate the public about kidney disease. Social media, including Twitter, has been one method used by the Society to accomplish this goal. Twitter is a popular microblogging service that serves as a potent tool for disseminating information. It allows for short messages (140 characters) to be composed by any author and distributes those messages globally and quickly. The dissemination of information is necessary if Twitter is to be considered a tool that can increase public awareness of kidney disease. We hypothesized that content, citation, and sentiment analyses of tweets generated from Kidney Week 2011 would reveal a large number of educational tweets that were disseminated to the public. An ideal tweet for accomplishing this goal would include three key features: 1) informative content, 2) internal citations, and 3) positive sentiment score. Informative content was found in 29% of messages, greater than that found in a similarly sized medical conference (2011 ADA Conference, 16%). Informative tweets were more likely to be internally, rather than externally, cited (38% versus 22%, p<0.0001), thereby amplifying the original information to an even larger audience. Informative tweets had more negative sentiment scores than uninformative tweets (means \u22120.162 versus 0.199 respectively, p<0.0001), therefore amplifying a tweet whose content had a negative tone. Our investigation highlights significant areas of promise and improvement in using Twitter to disseminate medical information in nephrology from a scientific conference. This goal is pertinent to many nephrology-focused conferences that wish to increase public awareness of kidney disease.\n      "], "author_display": ["Tejas Desai", "Afreen Shariff", "Aabid Shariff", "Mark Kats", "Xiangming Fang", "Cynthia Christiano", "Maria Ferris"], "article_type": "Research Article", "score": 0.3633922, "title_display": "Tweeting the Meeting: An In-Depth Analysis of Twitter Activity at Kidney Week 2011", "publication_date": "2012-07-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0040253"}, {"journal": "PLoS ONE", "abstract": ["\nPeople need to rely on cooperation with other individuals in many aspects of everyday life, such as teamwork and economic exchange in anonymous markets. We study whether and how the ability to make or break links in social networks fosters cooperate, paying particular attention to whether information on an individual's actions is freely available to potential partners. Studying the role of information is relevant as information on other people's actions is often not available for free: a recruiting firm may need to call a job candidate's references, a bank may need to find out about the credit history of a new client, etc. We find that people cooperate almost fully when information on their actions is freely available to their potential partners. Cooperation is less likely, however, if people have to pay about half of what they gain from cooperating with a cooperator. Cooperation declines even further if people have to pay a cost that is almost equivalent to the gain from cooperating with a cooperator. Thus, costly information on potential neighbors' actions can undermine the incentive to cooperate in fluid networks.\n"], "author_display": ["Alberto Antonioni", "Maria Paula Cacault", "Rafael Lalive", "Marco Tomassini"], "article_type": "Research Article", "score": 0.3633922, "title_display": "Know Thy Neighbor: Costly Information Can Hurt Cooperation in Dynamic Networks", "publication_date": "2014-10-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0110788"}, {"journal": "PLoS Computational Biology", "abstract": ["\nNeuronal population codes are increasingly being investigated with multivariate pattern-information analyses. A key challenge is to use measured brain-activity patterns to test computational models of brain information processing. One approach to this problem is representational similarity analysis (RSA), which characterizes a representation in a brain or computational model by the distance matrix of the response patterns elicited by a set of stimuli. The representational distance matrix encapsulates what distinctions between stimuli are emphasized and what distinctions are de-emphasized in the representation. A model is tested by comparing the representational distance matrix it predicts to that of a measured brain region. RSA also enables us to compare representations between stages of processing within a given brain or model, between brain and behavioral data, and between individuals and species. Here, we introduce a Matlab toolbox for RSA. The toolbox supports an analysis approach that is simultaneously data- and hypothesis-driven. It is designed to help integrate a wide range of computational models into the analysis of multichannel brain-activity measurements as provided by modern functional imaging and neuronal recording techniques. Tools for visualization and inference enable the user to relate sets of models to sets of brain regions and to statistically test and compare the models using nonparametric inference methods. The toolbox supports searchlight-based RSA, to continuously map a measured brain volume in search of a neuronal population code with a specific geometry. Finally, we introduce the linear-discriminant t value as a measure of representational discriminability that bridges the gap between linear decoding analyses and RSA. In order to demonstrate the capabilities of the toolbox, we apply it to both simulated and real fMRI data. The key functions are equally applicable to other modalities of brain-activity measurement. The toolbox is freely available to the community under an open-source license agreement (http://www.mrc-cbu.cam.ac.uk/methods-and-resources/toolboxes/license/).\n"], "author_display": ["Hamed Nili", "Cai Wingfield", "Alexander Walther", "Li Su", "William Marslen-Wilson", "Nikolaus Kriegeskorte"], "article_type": "Research Article", "score": 0.36333054, "title_display": "A Toolbox for Representational Similarity Analysis", "publication_date": "2014-04-17T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1003553"}, {"journal": "PLOS Computational Biology", "abstract": ["\nRecent genome-wide experiments in different eukaryotic genomes provide an unprecedented view of transcription factor (TF) binding locations and of nucleosome occupancy. These experiments revealed that a large fraction of TF binding events occur in regions where only a small number of specific TF binding sites (TFBSs) have been detected. Furthermore, in vitro protein-DNA binding measurements performed for hundreds of TFs indicate that TFs are bound with wide range of affinities to different DNA sequences that lack known consensus motifs. These observations have thus challenged the classical picture of specific protein-DNA binding and strongly suggest the existence of additional recognition mechanisms that affect protein-DNA binding preferences. We have previously demonstrated that repetitive DNA sequence elements characterized by certain symmetries statistically affect protein-DNA binding preferences. We call this binding mechanism nonconsensus protein-DNA binding in order to emphasize the point that specific consensus TFBSs do not contribute to this effect. In this paper, using the simple statistical mechanics model developed previously, we calculate the nonconsensus protein-DNA binding free energy for the entire C. elegans and D. melanogaster genomes. Using the available chromatin immunoprecipitation followed by sequencing (ChIP-seq) results on TF-DNA binding preferences for ~100 TFs, we show that DNA sequences characterized by low predicted free energy of nonconsensus binding have statistically higher experimental TF occupancy and lower nucleosome occupancy than sequences characterized by high free energy of nonconsensus binding. This is in agreement with our previous analysis performed for the yeast genome. We suggest therefore that nonconsensus protein-DNA binding assists the formation of nucleosome-free regions, as TFs outcompete nucleosomes at genomic locations with enhanced nonconsensus binding. In addition, here we perform a new, large-scale analysis using in vitro TF-DNA preferences obtained from the universal protein binding microarrays (PBM) for ~90 eukaryotic TFs belonging to 22 different DNA-binding domain types. As a result of this new analysis, we conclude that nonconsensus protein-DNA binding is a widespread phenomenon that significantly affects protein-DNA binding preferences and need not require the presence of consensus (specific) TFBSs in order to achieve genome-wide TF-DNA binding specificity.\nAuthor Summary: Interactions between proteins and DNA trigger many important biological processes. Therefore, to fully understand how the information encoded on the DNA transcribes into RNA, which in turn translates into proteins in the cell, we need to unravel the molecular design principles of protein-DNA interactions. It is known that many interactions occur when a protein is attracted to a specific short segment on the DNA called a specific protein-DNA binding motif. Strikingly, recent experiments revealed that many regulatory proteins reproducibly bind to different regions on the DNA lacking such specific motifs. This suggests that fundamental molecular mechanisms responsible for protein-DNA recognition specificity are not fully understood. Here, using high-throughput protein-DNA binding data obtained by two entirely different methods for ~100 TFs in each case, we show that DNA regions possessing certain repetitive sequence elements exert the statistical attractive potential on DNA-binding proteins, and as a result, such DNA regions are enriched in bound proteins. This is in agreement with our previous analysis performed for the yeast genome. We use the term nonconsensus protein-DNA binding in order to describe protein-DNA interactions that occur in the absence of specific protein-DNA binding motifs. Here we demonstrate that the identified nonconsensus effect is highly significant for a variety of organismal genomes and it affects protein-DNA binding preferences and nucleosome occupancy at the genome-wide level. "], "author_display": ["Ariel Afek", "Hila Cohen", "Shiran Barber-Zucker", "Raluca Gord\u00e2n", "David B. Lukatsky"], "article_type": "Research Article", "score": 0.3632733, "title_display": "Nonconsensus Protein Binding to Repetitive DNA Sequence Elements Significantly Affects Eukaryotic Genomes", "publication_date": "2015-08-18T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1004429"}, {"journal": "PLoS ONE", "abstract": ["\n        Leaking of confidential material is a major threat to information security within organizations and to society as a whole. This insight has gained traction in the political realm since the activities of Wikileaks, which hopes to attack \u2018unjust\u2019 systems or \u2018conspiracies\u2019. Eventually, such threats to information security rely on a biologistic argument on the benefits and drawbacks that uncontrolled leaking might pose for \u2018just\u2019 and \u2018unjust\u2019 entities. Such biological metaphors are almost exclusively based on the economic advantage of participants. Here, I introduce a mathematical model of the complex dynamics implied by leaking. The complex interactions of adversaries are modeled by coupled logistic equations including network effects of econo-communication networks. The modeling shows, that there might arise situations where the leaking envisioned and encouraged by Wikileaks and the like can strengthen the defending entity (the \u2018conspiracy\u2019). In particular, the only severe impact leaking can have on an organization seems to originate in the exploitation of leaks by another entity the organization competes with. Therefore, the model suggests that leaks can be used as a `tactical mean\u2019 in direct adversary relations, but do not necessarily increase public benefit and societal immunization to \u2018conspiracies\u2019. Furthermore, within the model the exploitation of the (open) competition between entities seems to be a more promising approach to control malicious organizations : divide-et-impera policies triumph here.\n      "], "author_display": ["Kay Hamacher"], "article_type": "Research Article", "score": 0.36319822, "title_display": "Resilience to Leaking \u2014 Dynamic Systems Modeling of Information Security", "publication_date": "2012-12-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0049804"}, {"journal": "PLoS ONE", "abstract": ["\nWearable health tech provides doctors with the ability to remotely supervise their patients' wellness. It also makes it much easier to authorize someone else to take appropriate actions to ensure the person's wellness than ever before. Information Technology may soon change the way medicine is practiced, improving the performance, while reducing the price of healthcare. We analyzed the secrecy demands of wearable devices, including Smartphone, smart watch and their computing techniques, that can soon change the way healthcare is provided. However, before this is adopted in practice, all devices must be equipped with sufficient privacy capabilities related to healthcare service. In this paper, we formulated a new improved conceptual framework for wearable healthcare systems. This framework consists of ten principles and nine checklists, capable of providing complete privacy protection package to wearable device owners. We constructed this framework based on the analysis of existing mobile technology, the results of which are combined with the existing security standards. The approach also incorporates the market share percentage level of every app and its respective OS. This framework is evaluated based on the stringent CIA and HIPAA principles for information security. This evaluation is followed by testing the capability to revoke rights of subjects to access objects and ability to determine the set of available permissions for a particular subject for all models Finally, as the last step, we examine the complexity of the required initial setup.\n"], "author_display": ["Seyedmostafa Safavi", "Zarina Shukur"], "article_type": "Research Article", "score": 0.36307243, "title_display": "Conceptual Privacy Framework for Health Information on Wearable Device", "publication_date": "2014-12-05T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0114306"}, {"journal": "PLoS ONE", "abstract": ["\nDespite current enthusiasm for investigation of gene-gene interactions and gene-environment interactions, the essential issue of how to define and detect gene-environment interactions remains unresolved. In this report, we define gene-environment interactions as a stochastic dependence in the context of the effects of the genetic and environmental risk factors on the cause of phenotypic variation among individuals. We use mutual information that is widely used in communication and complex system analysis to measure gene-environment interactions. We investigate how gene-environment interactions generate the large difference in the information measure of gene-environment interactions between the general population and a diseased population, which motives us to develop mutual information-based statistics for testing gene-environment interactions. We validated the null distribution and calculated the type 1 error rates for the mutual information-based statistics to test gene-environment interactions using extensive simulation studies. We found that the new test statistics were more powerful than the traditional logistic regression under several disease models. Finally, in order to further evaluate the performance of our new method, we applied the mutual information-based statistics to three real examples. Our results showed that P-values for the mutual information-based statistics were much smaller than that obtained by other approaches including logistic regression models.\n"], "author_display": ["Xuesen Wu", "Li Jin", "Momiao Xiong"], "article_type": "Research Article", "score": 0.36303514, "title_display": "Mutual Information for Testing Gene-Environment Interaction", "publication_date": "2009-02-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0004578"}, {"journal": "PLOS ONE", "abstract": ["\nIt is well known that the visual cortex efficiently processes high-dimensional spatial information by using a hierarchical structure. Recently, computational models that were inspired by the spatial hierarchy of the visual cortex have shown remarkable performance in image recognition. Up to now, however, most biological and computational modeling studies have mainly focused on the spatial domain and do not discuss temporal domain processing of the visual cortex. Several studies on the visual cortex and other brain areas associated with motor control support that the brain also uses its hierarchical structure as a processing mechanism for temporal information. Based on the success of previous computational models using spatial hierarchy and temporal hierarchy observed in the brain, the current report introduces a novel neural network model for the recognition of dynamic visual image patterns based solely on the learning of exemplars. This model is characterized by the application of both spatial and temporal constraints on local neural activities, resulting in the self-organization of a spatio-temporal hierarchy necessary for the recognition of complex dynamic visual image patterns. The evaluation with the Weizmann dataset in recognition of a set of prototypical human movement patterns showed that the proposed model is significantly robust in recognizing dynamically occluded visual patterns compared to other baseline models. Furthermore, an evaluation test for the recognition of concatenated sequences of those prototypical movement patterns indicated that the model is endowed with a remarkable capability for the contextual recognition of long-range dynamic visual image patterns.\n"], "author_display": ["Minju Jung", "Jungsik Hwang", "Jun Tani"], "article_type": "Research Article", "score": 0.36291876, "title_display": "Self-Organization of Spatio-Temporal Hierarchy via Learning of Dynamic Visual Image Patterns on Action Sequences", "publication_date": "2015-07-06T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0131214"}, {"journal": "PLoS ONE", "abstract": ["\nOnline users nowadays are facing serious information overload problem. In recent years, recommender systems have been widely studied to help people find relevant information. Adaptive social recommendation is one of these systems in which the connections in the online social networks are optimized for the information propagation so that users can receive interesting news or stories from their leaders. Validation of such adaptive social recommendation methods in the literature assumes uniform distribution of users' activity frequency. In this paper, our empirical analysis shows that the distribution of online users' activity is actually heterogenous. Accordingly, we propose a more realistic multi-agent model in which users' activity frequency are drawn from a power-law distribution. We find that previous social recommendation methods lead to serious delay of information propagation since many users are connected to inactive leaders. To solve this problem, we design a new similarity measure which takes into account users' activity frequencies. With this similarity measure, the average delay is significantly shortened and the recommendation accuracy is largely improved.\n"], "author_display": ["Duan-Bing Chen", "Guan-Nan Wang", "An Zeng", "Yan Fu", "Yi-Cheng Zhang"], "article_type": "Research Article", "score": 0.36275715, "title_display": "Optimizing Online Social Networks for Information Propagation", "publication_date": "2014-05-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0096614"}, {"journal": "PLoS ONE", "abstract": ["\nRecent research has focused on the monitoring of global\u2013scale online data for improved detection of epidemics, mood patterns, movements in the stock market political revolutions, box-office revenues, consumer behaviour and many other important phenomena. However, privacy considerations and the sheer scale of data available online are quickly making global monitoring infeasible, and existing methods do not take full advantage of local network structure to identify key nodes for monitoring. Here, we develop a model of the contagious spread of information in a global-scale, publicly-articulated social network and show that a simple method can yield not just early detection, but advance warning of contagious outbreaks. In this method, we randomly choose a small fraction of nodes in the network and then we randomly choose a friend of each node to include in a group for local monitoring. Using six months of data from most of the full Twittersphere, we show that this friend group is more central in the network and it helps us to detect viral outbreaks of the use of novel hashtags about 7 days earlier than we could with an equal-sized randomly chosen group. Moreover, the method actually works better than expected due to network structure alone because highly central actors are both more active and exhibit increased diversity in the information they transmit to others. These results suggest that local monitoring is not just more efficient, but also more effective, and it may be applied to monitor contagious processes in global\u2013scale networks.\n"], "author_display": ["Manuel Garcia-Herranz", "Esteban Moro", "Manuel Cebrian", "Nicholas A. Christakis", "James H. Fowler"], "article_type": "Research Article", "score": 0.3627409, "title_display": "Using Friends as Sensors to Detect Global-Scale Contagious Outbreaks", "publication_date": "2014-04-09T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0092413"}, {"journal": "PLoS ONE", "abstract": ["Purpose: The ICECAP-A and EQ-5D-5L are two index measures appropriate for use in health research.  Assessment of content validity allows understanding of whether a measure captures the most relevant and important aspects of a concept.  This paper reports a qualitative assessment of the content validity and appropriateness for use of the eq-5D-5L and ICECAP-A measures, using novel methodology. Methods: In-depth semi-structured interviews were conducted with research professionals in the UK and Australia.  Informants were purposively sampled based on their professional role.  Data were analysed in an iterative, thematic and constant comparative manner.  A two stage investigation - the comparative direct approach - was developed to address the methodological challenges of the content validity research and allow rigorous assessment. Results: Informants viewed the ICECAP-A as an assessment of the broader determinants of quality of life, but lacking in assessment of health-related determinants.  The eq-5D-5L was viewed as offering good coverage of health determinants, but as lacking in assessment of these broader determinants.  Informants held some concerns about the content or wording of the Self-care, Pain/Discomfort and Anxiety/Depression items (EQ-5D-5L) and the Enjoyment, Achievement and attachment items (ICECAP-A).   Conclusion: Using rigorous qualitative methodology the results suggest that the ICECAP-A and EQ-5D-5L hold acceptable levels of content validity and are appropriate for use in health research.  This work adds expert opinion to the emerging body of research using patients and public to validate these measures.   "], "author_display": ["Thomas Keeley", "Hareth Al-Janabi", "Paula Lorgelly", "Joanna Coast"], "article_type": "Research Article", "score": 0.36248797, "title_display": "A Qualitative Assessment of the Content Validity of the ICECAP-A and EQ-5D-5L and Their Appropriateness for Use in Health Research", "publication_date": "2013-12-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0085287"}, {"abstract": ["\n        The visual world is complex and continuously changing. Yet, our brain transforms patterns of light falling on our retina into a coherent percept within a few hundred milliseconds. Possibly, low-level neural responses already carry substantial information to facilitate rapid characterization of the visual input. Here, we computationally estimated low-level contrast responses to computer-generated naturalistic images, and tested whether spatial pooling of these responses could predict image similarity at the neural and behavioral level. Using EEG, we show that statistics derived from pooled responses explain a large amount of variance between single-image evoked potentials (ERPs) in individual subjects. Dissimilarity analysis on multi-electrode ERPs demonstrated that large differences between images in pooled response statistics are predictive of more dissimilar patterns of evoked activity, whereas images with little difference in statistics give rise to highly similar evoked activity patterns. In a separate behavioral experiment, images with large differences in statistics were judged as different categories, whereas images with little differences were confused. These findings suggest that statistics derived from low-level contrast responses can be extracted in early visual processing and can be relevant for rapid judgment of visual similarity. We compared our results with two other, well- known contrast statistics: Fourier power spectra and higher-order properties of contrast distributions (skewness and kurtosis). Interestingly, whereas these statistics allow for accurate image categorization, they do not predict ERP response patterns or behavioral categorization confusions. These converging computational, neural and behavioral results suggest that statistics of pooled contrast responses contain information that corresponds with perceived visual similarity in a rapid, low-level categorization task.\n      Author Summary: Humans excel in rapid and accurate processing of visual scenes. However, it is unclear which computations allow the visual system to convert light hitting the retina into a coherent representation of visual input in a rapid and efficient way. Here we used simple, computer-generated image categories with similar low-level structure as natural scenes to test whether a model of early integration of low-level information can predict perceived category similarity. Specifically, we show that summarized (spatially pooled) responses of model neurons covering the entire visual field (the population response) to low-level properties of visual input (contrasts) can already be informative about differences in early visual evoked activity as well as behavioral confusions of these categories. These results suggest that low-level population responses can carry relevant information to estimate similarity of controlled images, and put forward the exciting hypothesis that the visual system may exploit these responses to rapidly process real natural scenes. We propose that the spatial pooling that allows for the extraction of this information may be a plausible first step in extracting scene gist to form a rapid impression of the visual input. "], "author_display": ["Iris I. A. Groen", "Sennay Ghebreab", "Victor A. F. Lamme", "H. Steven Scholte"], "article_type": "Research Article", "score": 0.36242104, "title_display": "Spatially Pooled Contrast Responses Predict Neural and Perceptual Similarity of Naturalistic Image Categories", "publication_date": "2012-10-18T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1002726"}, {"journal": "PLoS ONE", "abstract": ["\nOnline social media is widespread, easily accessible and attracts a global audience with a widening demographic. As a large proportion of adults now seek health information online and through social media applications, communication about health has become increasingly interactive and dynamic. Online health information has the potential to significantly impact public health, especially as the population gets older and the prevalence of dementia increases. However, little is known about how information pertaining to age-associated diseases is disseminated on popular social media platforms. To fill this knowledge gap, we examined empirically: (i) who is using social media to share information about dementia, (ii) what sources of information about dementia are promoted, and (iii) which dementia themes dominate the discussion. We data-mined the microblogging platform Twitter for content containing dementia-related keywords for a period of 24 hours and retrieved over 9,200 tweets. A coding guide was developed and content analysis conducted on a random sample (10%), and on a subsample from top users\u2019 tweets to assess impact. We found that a majority of tweets contained a link to a third party site rather than personal information, and these links redirected mainly to news sites and health information sites. As well, a large number of tweets discussed recent research findings related to the prediction and risk management of Alzheimer\u2019s disease. The results highlight the need for the dementia research community to harness the reach of this medium and its potential as a tool for multidirectional engagement.\n"], "author_display": ["Julie M. Robillard", "Thomas W. Johnson", "Craig Hennessey", "B. Lynn Beattie", "Judy Illes"], "article_type": "Research Article", "score": 0.36224863, "title_display": "Aging 2.0: Health Information about Dementia on Twitter", "publication_date": "2013-07-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0069861"}, {"journal": "PLoS ONE", "abstract": ["\n            Reliable and comprehensive maps of molecular pathways are indispensable for guiding complex biomedical experiments. Such maps are typically assembled from myriads of disparate research reports and are replete with inconsistencies due to variations in experimental conditions and/or errors. It is often an intractable task to manually verify internal consistency over a large collection of experimental statements. To automate large-scale reconciliation efforts, we propose a random-arcs-and-nodes model where both nodes (tissue-specific states of biological molecules) and arcs (interactions between them) are represented with random variables. We show how to obtain a non-contradictory model of a molecular network by computing the joint distribution for arc and node variables, and then apply our methodology to a realistic network, generating a set of experimentally testable hypotheses. This network, derived from an automated analysis of over 3,000 full-text research articles, includes genes that have been hypothetically linked to four neurological disorders: Alzheimer's disease, autism, bipolar disorder, and schizophrenia. We estimated that approximately 10% of the published molecular interactions are logically incompatible. Our approach can be directly applied to an array of diverse problems including those encountered in molecular biology, ecology, economics, politics, and sociology.\n         "], "author_display": ["Andrey Rzhetsky", "Tian Zheng", "Chani Weinreb"], "article_type": "Research Article", "score": 0.3622286, "title_display": "Self-Correcting Maps of Molecular Pathways", "publication_date": "2006-12-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0000061"}, {"journal": "PLoS ONE", "abstract": ["\nBased on the necessity for enclosure protection of temperature and relative humidity sensors installed in a hostile environment, a wind tunnel was used to quantify the time that the sensors take to reach equilibrium in the environmental conditions to which they are exposed. Two treatments were used: (1) sensors with polyvinyl chloride (PVC) enclosure protection, and (2) sensors with no enclosure protection. The primary objective of this study was to develop and validate a 3-D computational fluid dynamics (CFD) model for analyzing the temperature and relative humidity distribution in a wind tunnel using sensors with PVC enclosure protection and sensors with no enclosure protection. A CFD simulation model was developed to describe the temperature distribution and the physics of mass transfer related to the airflow relative humidity. The first results demonstrate the applicability of the simulation. For verification, a sensor device was successfully assembled and tested in an environment that was optimized to ensure fast change conditions. The quantification setup presented in this paper is thus considered to be adequate for testing different materials and morphologies for enclosure protection. The results show that the boundary layer flow regime has a significant impact on the heat flux distribution. The results indicate that the CFD technique is a powerful tool which provides a detailed description of the flow and temperature fields as well as the time that the relative humidity takes to reach equilibrium with the environment in which the sensors are inserted.\n"], "author_display": ["Keller Sullivan Oliveira Rocha", "Jos\u00e9 Helvecio Martins", "Marcio Ar\u00eades Martins", "Ilda de F\u00e1tima Ferreira Tin\u00f4co", "Jairo Alexander Osorio Saraz", "Ad\u00edlio Flauzino Lacerda Filho", "Luiz Henrique Martins Fernandes"], "article_type": "Research Article", "score": 0.36212343, "title_display": "Modeling and Simulation of the Transient Response of Temperature and Relative Humidity Sensors with and without Protective Housing", "publication_date": "2014-05-22T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0095874"}, {"journal": "PLoS ONE", "abstract": ["\n        The large amount of information contained in bibliographic databases has recently boosted the use of citations, and other indicators based on citation numbers, as tools for the quantitative assessment of scientific research. Citations counts are often interpreted as proxies for the scientific influence of papers, journals, scholars, and institutions. However, a rigorous and scientifically grounded methodology for a correct use of citation counts is still missing. In particular, cross-disciplinary comparisons in terms of raw citation counts systematically favors scientific disciplines with higher citation and publication rates. Here we perform an exhaustive study of the citation patterns of millions of papers, and derive a simple transformation of citation counts able to suppress the disproportionate citation counts among scientific domains. We find that the transformation is well described by a power-law function, and that the parameter values of the transformation are typical features of each scientific discipline. Universal properties of citation patterns descend therefore from the fact that citation distributions for papers in a specific field are all part of the same family of univariate distributions.\n      "], "author_display": ["Filippo Radicchi", "Claudio Castellano"], "article_type": "Research Article", "score": 0.3621183, "title_display": "A Reverse Engineering Approach to the Suppression of Citation Biases Reveals Universal Properties of Citation Distributions", "publication_date": "2012-03-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0033833"}, {"journal": "PLoS ONE", "abstract": ["\n        Polymers can be modeled as open polygonal paths and their closure generates knots. Knotted proteins detection is currently achieved via high-throughput methods based on a common framework insensitive to the handedness of knots. Here we propose a topological framework for the computation of the HOMFLY polynomial, an handedness-sensitive invariant. Our approach couples a multi-component reduction scheme with the polynomial computation. After validation on tabulated knots and links the framework was applied to the entire Protein Data Bank along with a set of selected topological checks that allowed to discard artificially entangled structures. This led to an up-to-date table of knotted proteins that also includes two newly detected right-handed trefoil knots in recently deposited protein structures. The application range of our framework is not limited to proteins and it can be extended to the topological analysis of biological and synthetic polymers and more generally to arbitrary polygonal paths.\n      "], "author_display": ["Federico Comoglio", "Maurizio Rinaldi"], "article_type": "Research Article", "score": 0.3620903, "title_display": "A Topological Framework for the Computation of the HOMFLY Polynomial and Its Application to Proteins", "publication_date": "2011-04-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0018693"}, {"journal": "PLoS ONE", "abstract": ["\n        Schools of fish and flocks of birds are examples of self-organized animal groups that arise through social interactions among individuals. We numerically study two individual-based models, which recent empirical studies have suggested to explain self-organized group animal behavior: (i) a zone-based model where the group communication topology is determined by finite interacting zones of repulsion, attraction, and orientation among individuals; and (ii) a model where the communication topology is described by Delaunay triangulation, which is defined by each individual's Voronoi neighbors. The models include a tunable parameter that controls an individual's relative weighting of attraction and alignment. We perform computational experiments to investigate how effectively simulated groups transfer information in the form of velocity when an individual is perturbed. A cross-correlation function is used to measure the sensitivity of groups to sudden perturbations in the heading of individual members. The results show how relative weighting of attraction and alignment, location of the perturbed individual, population size, and the communication topology affect group structure and response to perturbation. We find that in the Delaunay-based model an individual who is perturbed is capable of triggering a cascade of responses, ultimately leading to the group changing direction. This phenomenon has been seen in self-organized animal groups in both experiments and nature.\n      "], "author_display": ["Allison Kolpas", "Michael Busch", "Hong Li", "Iain D. Couzin", "Linda Petzold", "Jeff Moehlis"], "article_type": "Research Article", "score": 0.36207294, "title_display": "How the Spatial Position of Individuals Affects Their Influence on Swarms: A Numerical Comparison of Two Popular Swarm Dynamics Models", "publication_date": "2013-03-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0058525"}, {"journal": "PLoS ONE", "abstract": ["Background: Although being an important source of science news information to the public, print news media have often been criticized in their credibility. Health-related content of press media articles has been examined by many studies underlining that information about benefits, risks and costs are often incomplete or inadequate and financial conflicts of interest are rarely reported. However, these studies have focused their analysis on very selected science articles. The present research aimed at adopting a wider explorative approach, by analysing all types of health science information appearing on the Italian national press in one-week period. Moreover, we attempted to score the balance of the articles. Methodology/Principal Findings: We collected 146 health science communication articles defined as articles aiming at improving the reader's knowledge on health from a scientific perspective. Articles were evaluated by 3 independent physicians with respect to different divulgation parameters: benefits, costs, risks, sources of information, disclosure of financial conflicts of interest and balance. Balance was evaluated with regard to exaggerated or non correct claims. The selected articles appeared on 41 Italian national daily newspapers and 41 weekly magazines, representing 89% of national circulation copies: 97 articles (66%) covered common medical treatments or basic scientific research and 49 (34%) were about new medical treatments, procedures, tests or products. We found that only 6/49 (12%) articles on new treatments, procedures, tests or products mentioned costs or risks to patients. Moreover, benefits were always maximized and in 16/49 cases (33%) they were presented in relative rather than absolute terms. The majority of stories (133/146, 91%) did not report any financial conflict of interest. Among these, 15 were shown to underreport them (15/146, 9.5%), as we demonstrated that conflicts of interest did actually exist. Unbalanced articles were 27/146 (18%). Specifically, the probability of unbalanced reporting was significantly increased in stories about a new treatment, procedure, test or product (22/49, 45%), compared to stories covering common treatments or basic scientific research (5/97, 5%) (risk ratio, 8.72). Conclusions/Significance: Consistent with prior research on health science communication in other countries, we report undisclosed costs and risks, emphasized benefits, unrevealed financial conflicts of interest and exaggerated claims in Italian print media. In addition, we show that the risk for a story about a new medical approach to be unbalanced is almost 9 times higher with respect to stories about any other kind of health science-related topics. These findings raise again the fundamental issue whether popular media is detrimental rather than useful to public health. "], "author_display": ["Luca Iaboli", "Luana Caselli", "Angelina Filice", "Gianpaolo Russi", "Eleonora Belletti"], "article_type": "Research Article", "score": 0.36180437, "title_display": "The Unbearable Lightness of Health Science Reporting: A Week Examining Italian Print Media", "publication_date": "2010-03-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0009829"}, {"journal": "PLoS ONE", "abstract": ["\n        In this paper we introduce Armadillo v1.1, a novel workflow platform dedicated to designing and conducting phylogenetic studies, including comprehensive simulations. A number of important phylogenetic and general bioinformatics tools have been included in the first software release. As Armadillo is an open-source project, it allows scientists to develop their own modules as well as to integrate existing computer applications. Using our workflow platform, different complex phylogenetic tasks can be modeled and presented in a single workflow without any prior knowledge of programming techniques. The first version of Armadillo was successfully used by professors of bioinformatics at Universit\u00e9 du Quebec \u00e0 Montreal during graduate computational biology courses taught in 2010\u201311. The program and its source code are freely available at: <http://www.bioinfo.uqam.ca/armadillo>.\n      "], "author_display": ["Etienne Lord", "Mickael Leclercq", "Alix Boc", "Abdoulaye Banir\u00e9 Diallo", "Vladimir Makarenkov"], "article_type": "Research Article", "score": 0.36168355, "title_display": "Armadillo 1.1: An Original Workflow Platform for Designing and Conducting Phylogenetic Analysis and Simulations", "publication_date": "2012-01-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0029903"}, {"journal": "PLoS ONE", "abstract": ["\nWe apply our recently developed information-theoretic measures for the characterisation and comparison of protein\u2013protein interaction networks. These measures are used to quantify topological network features via macroscopic statistical properties. Network differences are assessed based on these macroscopic properties as opposed to microscopic overlap, homology information or motif occurrences. We present the results of a large\u2013scale analysis of protein\u2013protein interaction networks. Precise null models are used in our analyses, allowing for reliable interpretation of the results. By quantifying the methodological biases of the experimental data, we can define an information threshold above which networks may be deemed to comprise consistent macroscopic topological properties, despite their small microscopic overlaps. Based on this rationale, data from yeast\u2013two\u2013hybrid methods are sufficiently consistent to allow for intra\u2013species comparisons (between different experiments) and inter\u2013species comparisons, while data from affinity\u2013purification mass\u2013spectrometry methods show large differences even within intra\u2013species comparisons.\n"], "author_display": ["Luis P. Fernandes", "Alessia Annibale", "Jens Kleinjung", "Anthony C. C. Coolen", "Franca Fraternali"], "article_type": "Research Article", "score": 0.36168158, "title_display": "Protein Networks Reveal Detection Bias and Species Consistency When Analysed by Information-Theoretic Methods", "publication_date": "2010-08-18T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0012083"}, {"journal": "PLoS ONE", "abstract": ["\n        The assembly of molecular machines and transient signaling complexes does not typically occur under circumstances in which the appropriate proteins are isolated from all others present in the cell. Rather, assembly must proceed in the context of large-scale protein-protein interaction (PPI) networks that are characterized both by conflict and combinatorial complexity. Conflict refers to the fact that protein interfaces can often bind many different partners in a mutually exclusive way, while combinatorial complexity refers to the explosion in the number of distinct complexes that can be formed by a network of binding possibilities. Using computational models, we explore the consequences of these characteristics for the global dynamics of a PPI network based on highly curated yeast two-hybrid data. The limited molecular context represented in this data-type translates formally into an assumption of independent binding sites for each protein. The challenge of avoiding the explicit enumeration of the astronomically many possibilities for complex formation is met by a rule-based approach to kinetic modeling. Despite imposing global biophysical constraints, we find that initially identical simulations rapidly diverge in the space of molecular possibilities, eventually sampling disjoint sets of large complexes. We refer to this phenomenon as \u201ccompositional drift\u201d. Since interaction data in PPI networks lack detailed information about geometric and biological constraints, our study does not represent a quantitative description of cellular dynamics. Rather, our work brings to light a fundamental problem (the control of compositional drift) that must be solved by mechanisms of assembly in the context of large networks. In cases where drift is not (or cannot be) completely controlled by the cell, this phenomenon could constitute a novel source of phenotypic heterogeneity in cell populations.\n      "], "author_display": ["Eric J. Deeds", "Jean Krivine", "J\u00e9r\u00f4me Feret", "Vincent Danos", "Walter Fontana"], "article_type": "Research Article", "score": 0.36165118, "title_display": "Combinatorial Complexity and Compositional Drift in Protein Interaction Networks", "publication_date": "2012-03-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0032032"}, {"journal": "PLoS ONE", "abstract": ["Background: A minority of scientific journals publishes the majority of scientific papers and receives the majority of citations. The extent of concentration of the most influential articles is less well known. Methods/Principal Findings: The 100 most-cited papers in the last decade in each of 21 scientific fields were analyzed; fields were considered as ecosystems and their \u201cspecies\u201d (journal) diversity was evaluated. Only 9% of journals in Journal Citation Reports had published at least one such paper. Among this 9%, half of them had published only one such paper. The number of journals that had published a larger number of most-cited papers decreased exponentially according to a Lotka law. Except for three scientific fields, six journals accounted for 53 to 94 of the 100 most-cited papers in their field. With increasing average number of citations per paper (citation density) in a scientific field, concentration of the most-cited papers in a few journals became even more prominent (p<0.001). Concentration was unrelated to the number of papers published or number of journals available in a scientific field. Multidisciplinary journals accounted for 24% of all most-cited papers, with large variability across fields. The concentration of most-cited papers in multidisciplinary journals was most prominent in fields with high citation density (correlation coefficient 0.70, p<0.001). Multidisciplinary journals had published fewer than eight of the 100 most-cited papers in eight scientific fields (none in two fields). Journals concentrating most-cited original articles often differed from those concentrating most-cited reviews. The concentration of the most-influential papers was stronger than the already prominent concentration of papers published and citations received. Conclusions: Despite a plethora of available journals, the most influential papers are extremely concentrated in few journals, especially in fields with high citation density. Existing multidisciplinary journals publish selectively most-cited papers from fields with high citation density. "], "author_display": ["John P. A. Ioannidis"], "article_type": "Research Article", "score": 0.36162078, "title_display": "Concentration of the Most-Cited Papers in the Scientific Literature: Analysis of Journal Ecosystems", "publication_date": "2006-12-20T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0000005"}, {"journal": "PLoS Computational Biology", "abstract": ["\nG-protein coupled receptors, the largest family of proteins in the human genome, are involved in many complex signal transduction pathways, typically activated by orthosteric ligand binding and subject to allosteric modulation. Dopaminergic receptors, belonging to the class A family of G-protein coupled receptors, are known to be modulated by sodium ions from an allosteric binding site, although the details of sodium effects on the receptor have not yet been described. In an effort to understand these effects, we performed microsecond scale all-atom molecular dynamics simulations on the dopaminergic D2 receptor, finding that sodium ions enter the receptor from the extracellular side and bind at a deep allosteric site (Asp2.50). Remarkably, the presence of a sodium ion at this allosteric site induces a conformational change of the rotamer toggle switch Trp6.48 which locks in a conformation identical to the one found in the partially inactive state of the crystallized human \u03b22 adrenergic receptor. This study provides detailed quantitative information about binding of sodium ions in the D2 receptor and reports a possibly important sodium-induced conformational change for modulation of D2 receptor function.\nAuthor Summary: G-protein coupled receptors represent more than 50% of the current drug targets, hence playing a crucial role in drug discovery today. A deeper understanding of G-protein coupled receptor functioning and modulation will help in the development of new drugs that are able to interact with such systems in a more subtle way than simple agonists or antagonists. In the present work, we studied the energetics of sodium ions, which have been described to act as an allosteric regulator within the D2 receptor using long-time molecular dynamics simulations, in order to gain insight into the molecular mechanism by which they exert this effect. In our simulations, we observed how sodium ions are able to induce a conformational change of the Trp6.48, a molecular rotamer switch which is implicated in the activation mechanism of G-protein coupled receptors. This observation, never reported before, has interesting implications for the design of drugs able to interact in a proper way with D2 receptor in particular and GPCR in general. "], "author_display": ["Jana Selent", "Ferran Sanz", "Manuel Pastor", "Gianni De Fabritiis"], "article_type": "Research Article", "score": 0.3615041, "title_display": "Induced Effects of Sodium Ions on Dopaminergic G-Protein Coupled Receptors", "publication_date": "2010-08-12T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1000884"}, {"abstract": ["\n        Human and many other animals can detect, recognize, and classify natural actions in a very short time. How this is achieved by the visual system and how to make machines understand natural actions have been the focus of neurobiological studies and computational modeling in the last several decades. A key issue is what spatial-temporal features should be encoded and what the characteristics of their occurrences are in natural actions. Current global encoding schemes depend heavily on segmenting while local encoding schemes lack descriptive power. Here, we propose natural action structures, i.e., multi-size, multi-scale, spatial-temporal concatenations of local features, as the basic features for representing natural actions. In this concept, any action is a spatial-temporal concatenation of a set of natural action structures, which convey a full range of information about natural actions. We took several steps to extract these structures. First, we sampled a large number of sequences of patches at multiple spatial-temporal scales. Second, we performed independent component analysis on the patch sequences and classified the independent components into clusters. Finally, we compiled a large set of natural action structures, with each corresponding to a unique combination of the clusters at the selected spatial-temporal scales. To classify human actions, we used a set of informative natural action structures as inputs to two widely used models. We found that the natural action structures obtained here achieved a significantly better recognition performance than low-level features and that the performance was better than or comparable to the best current models. We also found that the classification performance with natural action structures as features was slightly affected by changes of scale and artificially added noise. We concluded that the natural action structures proposed here can be used as the basic encoding units of actions and may hold the key to natural action understanding.\n      "], "author_display": ["Xiaoyuan Zhu", "Meng Li", "Xiaojian Li", "Zhiyong Yang", "Joe Z. Tsien"], "article_type": "Research Article", "score": 0.3613612, "title_display": "Robust Action Recognition Using Multi-Scale Spatial-Temporal Concatenations of Local Features as Natural Action Structures", "publication_date": "2012-10-04T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0046686"}, {"journal": "PLoS ONE", "abstract": ["Background: microRNAs are generally understood to regulate gene expression through binding to target sequences within 3\u2032-UTRs of mRNAs. Therefore, computational prediction of target sites is usually restricted to these gene regions. Recent experimental studies though have suggested that microRNAs may alternatively modulate gene expression by interacting with promoters. A database of potential microRNA target sites in promoters would stimulate research in this field leading to more understanding of complex microRNA regulatory mechanism. Methodology: We developed a database hosting predicted microRNA target sites located within human promoter sequences and their associated genomic features, called microPIR (microRNA-Promoter Interaction Resource). microRNA seed sequences were used to identify perfect complementary matching sequences in the human promoters and the potential target sites were predicted using the RNAhybrid program. >15 million target sites were identified which are located within 5000 bp upstream of all human genes, on both sense and antisense strands. The experimentally confirmed argonaute (AGO) binding sites and EST expression data including the sequence conservation across vertebrate species of each predicted target are presented for researchers to appraise the quality of predicted target sites. The microPIR database integrates various annotated genomic sequence databases, e.g. repetitive elements, transcription factor binding sites, CpG islands, and SNPs, offering users the facility to extensively explore relationships among target sites and other genomic features. Furthermore, functional information of target genes including gene ontologies, KEGG pathways, and OMIM associations are provided. The built-in genome browser of microPIR provides a comprehensive view of multidimensional genomic data. Finally, microPIR incorporates a PCR primer design module to facilitate experimental validation. Conclusions: The proposed microPIR database is a useful integrated resource of microRNA-promoter target interactions for experimental microRNA researchers and computational biologists to study the microRNA regulation through gene promoter. The database can be freely accessed from: http://www4a.biotec.or.th/micropir. "], "author_display": ["Jittima Piriyapongsa", "Chaiwat Bootchai", "Chumpol Ngamphiw", "Sissades Tongsima"], "article_type": "Research Article", "score": 0.36124346, "title_display": "microPIR: An Integrated Database of MicroRNA Target Sites within Human Promoter Sequences", "publication_date": "2012-03-16T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0033888"}, {"journal": "PLoS ONE", "abstract": ["\nInferring parameters for models of biological processes is a current challenge in systems biology, as is the related problem of comparing competing models that explain the data. In this work we apply Skilling's nested sampling to address both of these problems. Nested sampling is a Bayesian method for exploring parameter space that transforms a multi-dimensional integral to a 1D integration over likelihood space. This approach focusses on the computation of the marginal likelihood or evidence. The ratio of evidences of different models leads to the Bayes factor, which can be used for model comparison. We demonstrate how nested sampling can be used to reverse-engineer a system's behaviour whilst accounting for the uncertainty in the results. The effect of missing initial conditions of the variables as well as unknown parameters is investigated. We show how the evidence and the model ranking can change as a function of the available data. Furthermore, the addition of data from extra variables of the system can deliver more information for model comparison than increasing the data from one variable, thus providing a basis for experimental design.\n"], "author_display": ["Nick Pullen", "Richard J. Morris"], "article_type": "Research Article", "score": 0.3611775, "title_display": "Bayesian Model Comparison and Parameter Inference in Systems Biology Using Nested Sampling", "publication_date": "2014-02-11T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0088419"}, {"journal": "PLoS ONE", "abstract": ["\nIn this paper, we develop a novel semi-supervised learning algorithm called active hybrid deep belief networks (AHD), to address the semi-supervised sentiment classification problem with deep learning. First, we construct the previous several hidden layers using restricted Boltzmann machines (RBM), which can reduce the dimension and abstract the information of the reviews quickly. Second, we construct the following hidden layers using convolutional restricted Boltzmann machines (CRBM), which can abstract the information of reviews effectively. Third, the constructed deep architecture is fine-tuned by gradient-descent based supervised learning with an exponential loss function. Finally, active learning method is combined based on the proposed deep architecture. We did several experiments on five sentiment classification datasets, and show that AHD is competitive with previous semi-supervised learning algorithm. Experiments are also conducted to verify the effectiveness of our proposed method with different number of labeled reviews and unlabeled reviews respectively.\n"], "author_display": ["Shusen Zhou", "Qingcai Chen", "Xiaolong Wang"], "article_type": "Research Article", "score": 0.36092457, "title_display": "Active Semi-Supervised Learning Method with Hybrid Deep Belief Networks", "publication_date": "2014-09-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0107122"}, {"journal": "PLOS Biology", "abstract": ["\nDistributed neural processing likely entails the capability of networks to reconfigure dynamically the directionality and strength of their functional connections. Yet, the neural mechanisms that may allow such dynamic routing of the information flow are not yet fully understood. We investigated the role of gamma band (50\u201380 Hz) oscillations in transient modulations of communication among neural populations by using measures of direction-specific causal information transfer. We found that the local phase of gamma-band rhythmic activity exerted a stimulus-modulated and spatially-asymmetric directed effect on the firing rate of spatially separated populations within the primary visual cortex. The relationships between gamma phases at different sites (phase shifts) could be described as a stimulus-modulated gamma-band wave propagating along the spatial directions with the largest information transfer. We observed transient stimulus-related changes in the spatial configuration of phases (compatible with changes in direction of gamma wave propagation) accompanied by a relative increase of the amount of information flowing along the instantaneous direction of the gamma wave. These effects were specific to the gamma-band and suggest that the time-varying relationships between gamma phases at different locations mark, and possibly causally mediate, the dynamic reconfiguration of functional connections.\n\nThis study reveals the presence of gamma-frequency travelling waves, which appear to dynamically route the flow of information within the visual cortex.\nAuthor Summary: Complex and flexible behavior likely results from the ability of groups of neurons in the brain to reconfigure dynamically the information flow across different brain areas, depending on what the brain is engaged in (processing a stimulus or carrying out a task). Here, we investigate how oscillations of cortical activity in the gamma frequency range (50\u201380 Hz) may influence dynamically the direction and strength of information flow across different groups of neurons. By recording neural activity and measuring information flow between multiple locations in visual cortex during the presentation of Hollywood movies, we found that the arrangement of the phase of gamma oscillations at different locations indicated the presence of waves propagating along the cortical tissue. These waves were observed to propagate along the direction with the maximal flow of information transmitted between neural populations. The gamma waves changed direction during presentation of different movie scenes, and when this happened, the strength of information flow in the direction of the gamma wave propagation was transiently reinforced. These findings suggest that the propagation of gamma oscillations may reconfigure dynamically the directional flow of cortical information during sensory processing. "], "author_display": ["Michel Besserve", "Scott C. Lowe", "Nikos K. Logothetis", "Bernhard Sch\u00f6lkopf", "Stefano Panzeri"], "article_type": "Research Article", "score": 0.36092097, "title_display": "Shifts of Gamma Phase across Primary Visual Cortical Sites Reflect Dynamic Stimulus-Modulated Information Transfer", "publication_date": "2015-09-22T00:00:00Z", "eissn": "1545-7885", "id": "10.1371/journal.pbio.1002257"}, {"journal": "PLoS ONE", "abstract": ["\nRevocation functionality is necessary and crucial to identity-based cryptosystems. Revocable identity-based encryption (RIBE) has attracted a lot of attention in recent years, many RIBE schemes have been proposed in the literature but shown to be either insecure or inefficient. In this paper, we propose a new scalable RIBE scheme with decryption key exposure resilience by combining Lewko and Waters\u2019 identity-based encryption scheme and complete subtree method, and prove our RIBE scheme to be semantically secure using dual system encryption methodology. Compared to existing scalable and semantically secure RIBE schemes, our proposed RIBE scheme is more efficient in term of ciphertext size, public parameters size and decryption cost at price of a little looser security reduction. To the best of our knowledge, this is the first construction of scalable and semantically secure RIBE scheme with constant size public system parameters.\n"], "author_display": ["Changji Wang", "Yuan Li", "Xiaonan Xia", "Kangjia Zheng"], "article_type": "Research Article", "score": 0.36087534, "title_display": "An Efficient and Provable Secure Revocable Identity-Based Encryption Scheme", "publication_date": "2014-09-19T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0106925"}, {"journal": "PLoS ONE", "abstract": ["Background: For years, emerging infectious diseases have appeared worldwide and threatened the health of people. The emergence and spread of an infectious-disease outbreak are usually unforeseen, and have the features of suddenness and uncertainty. Timely understanding of basic information in the field, and the collection and analysis of epidemiological information, is helpful in making rapid decisions and responding to an infectious-disease emergency. Therefore, it is necessary to have an unobstructed channel and convenient tool for the collection and analysis of epidemiologic information in the field. Methodology/Principal Findings: Baseline information for each county in mainland China was collected and a database was established by geo-coding information on a digital map of county boundaries throughout the country. Google Maps was used to display geographic information and to conduct calculations related to maps, and the 3G wireless network was used to transmit information collected in the field to the server. This study established a decision support system for the response to infectious-disease emergencies based on WebGIS and mobile services (DSSRIDE). The DSSRIDE provides functions including data collection, communication and analyses in real time, epidemiological detection, the provision of customized epidemiological questionnaires and guides for handling infectious disease emergencies, and the querying of professional knowledge in the field. These functions of the DSSRIDE could be helpful for epidemiological investigations in the field and the handling of infectious-disease emergencies. Conclusions/Significance: The DSSRIDE provides a geographic information platform based on the Google Maps application programming interface to display information of infectious disease emergencies, and transfers information between workers in the field and decision makers through wireless transmission based on personal computers, mobile phones and personal digital assistants. After a 2-year practice and application in infectious disease emergencies, the DSSRIDE is becoming a useful platform and is a useful tool for investigations in the field carried out by response sections and individuals. The system is suitable for use in developing countries and low-income districts. "], "author_display": ["Ya-pin Li", "Li-qun Fang", "Su-qing Gao", "Zhen Wang", "Hong-wei Gao", "Peng Liu", "Ze-rui Wang", "Yan-li Li", "Xu-guang Zhu", "Xin-lou Li", "Bo Xu", "Yin-jun Li", "Hong Yang", "Sake J. de Vlas", "Tao-xing Shi", "Wu-chun Cao"], "article_type": "Research Article", "score": 0.36085272, "title_display": "Decision Support System for the Response to Infectious Disease Emergencies Based on WebGIS and Mobile Services in China", "publication_date": "2013-01-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0054842"}, {"journal": "PLOS ONE", "abstract": ["\nConformational entropy for atomic-level, three dimensional biomolecules is known experimentally to play an important role in protein-ligand discrimination, yet reliable computation of entropy remains a difficult problem. Here we describe the first two accurate and efficient algorithms to compute the conformational entropy for RNA secondary structures, with respect to the Turner energy model, where free energy parameters are determined from UV absorption experiments. An algorithm to compute the derivational entropy for RNA secondary structures had previously been introduced, using stochastic context free grammars (SCFGs). However, the numerical value of derivational entropy depends heavily on the chosen context free grammar and on the training set used to estimate rule probabilities. Using data from the Rfam database, we determine that both of our thermodynamic methods, which agree in numerical value, are substantially faster than the SCFG method. Thermodynamic structural entropy is much smaller than derivational entropy, and the correlation between length-normalized thermodynamic entropy and derivational entropy is moderately weak to poor. In applications, we plot the structural entropy as a function of temperature for known thermoswitches, such as the repression of heat shock gene expression (ROSE) element, we determine that the correlation between hammerhead ribozyme cleavage activity and total free energy is improved by including an additional free energy term arising from conformational entropy, and we plot the structural entropy of windows of the HIV-1 genome. Our software RNAentropy can compute structural entropy for any user-specified temperature, and supports both the Turner\u201999 and Turner\u201904 energy parameters. It follows that RNAentropy is state-of-the-art software to compute RNA secondary structure conformational entropy. Source code is available at https://github.com/clotelab/RNAentropy/; a full web server is available at http://bioinformatics.bc.edu/clotelab/RNAentropy, including source code and ancillary programs.\n"], "author_display": ["Juan Antonio Garcia-Martin", "Peter Clote"], "article_type": "Research Article", "score": 0.36080882, "title_display": "RNA Thermodynamic Structural Entropy", "publication_date": "2015-11-10T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0137859"}, {"journal": "PLOS ONE", "abstract": ["\nGenetic data, in digital format, is used in different biological phenomena such as DNA translation, mRNA transcription and protein synthesis. The accuracy of these biological phenomena depend on genetic codes and all subsequent processes. To computerize the biological procedures, different domain experts are provided with the authorized access of the genetic codes; as a consequence, the ownership protection of such data is inevitable. For this purpose, watermarks serve as the proof of ownership of data. While protecting data, embedded hidden messages (watermarks) influence the genetic data; therefore, the accurate execution of the relevant processes and the overall result becomes questionable. Most of the DNA based watermarking techniques modify the genetic data and are therefore vulnerable to information loss. Distortion-free techniques make sure that no modifications occur during watermarking; however, they are fragile to malicious attacks and therefore cannot be used for ownership protection (particularly, in presence of a threat model). Therefore, there is a need for a technique that must be robust and should also prevent unwanted modifications. In this spirit, a watermarking technique with aforementioned characteristics has been proposed in this paper. The proposed technique makes sure that: (i) the ownership rights are protected by means of a robust watermark; and (ii) the integrity of genetic data is preserved. The proposed technique\u2014GenInfoGuard\u2014ensures its robustness through the \u201cwatermark encoding\u201d in permuted values, and exhibits high decoding accuracy against various malicious attacks.\n"], "author_display": ["Saman Iftikhar", "Sharifullah Khan", "Zahid Anwar", "Muhammad Kamran"], "article_type": "Research Article", "score": 0.3607488, "title_display": "GenInfoGuard\u2014A Robust and Distortion-Free Watermarking Technique for Genetic Data", "publication_date": "2015-02-17T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0117717"}, {"abstract": ["\n        Pattern recognition techniques have been used to automatically recognize the objects, personal identities, predict the function of protein, the category of the cancer, identify lesion, perform product inspection, and so on. In this paper we propose a novel quaternion-based discriminant method. This method represents and classifies color images in a simple and mathematically tractable way. The proposed method is suitable for a large variety of real-world applications such as color face recognition and classification of the ground target shown in multispectrum remote images. This method first uses the quaternion number to denote the pixel in the color image and exploits a quaternion vector to represent the color image. This method then uses the linear discriminant analysis algorithm to transform the quaternion vector into a lower-dimensional quaternion vector and classifies it in this space. The experimental results show that the proposed method can obtain a very high accuracy for color face recognition.\n      "], "author_display": ["Yong Xu"], "article_type": "Research Article", "score": 0.3607341, "title_display": "Quaternion-Based Discriminant Analysis Method for Color Face Recognition", "publication_date": "2012-08-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0043493"}, {"journal": "PLoS ONE", "abstract": ["\n        Accurate clinical assessment of a patient's response to treatment for glioblastoma multiforme (GBM), the most malignant type of primary brain tumor, is undermined by the wide patient-to-patient variability in GBM dynamics and responsiveness to therapy. Using computational models that account for the unique geometry and kinetics of individual patients' tumors, we developed a method for assessing treatment response that discriminates progression-free and overall survival following therapy for GBM. Applying these models as untreated virtual controls, we generate a patient-specific \u201cDays Gained\u201d response metric that estimates the number of days a therapy delayed imageable tumor progression. We assessed treatment response in terms of Days Gained scores for 33 patients at the time of their first MRI scan following first-line radiation therapy. Based on Kaplan-Meier analyses, patients with Days Gained scores of 100 or more had improved progression-free survival, and patients with scores of 117 or more had improved overall survival. Our results demonstrate that the Days Gained response metric calculated at the routinely acquired first post-radiation treatment time point provides prognostic information regarding progression and survival outcomes. Applied prospectively, our model-based approach has the potential to improve GBM treatment by accounting for patient-to-patient heterogeneity in GBM dynamics and responses to therapy.\n      "], "author_display": ["Maxwell Lewis Neal", "Andrew D. Trister", "Tyler Cloke", "Rita Sodt", "Sunyoung Ahn", "Anne L. Baldock", "Carly A. Bridge", "Albert Lai", "Timothy F. Cloughesy", "Maciej M. Mrugala", "Jason K. Rockhill", "Russell C. Rockne", "Kristin R. Swanson"], "article_type": "Research Article", "score": 0.36064076, "title_display": "Discriminating Survival Outcomes in Patients with Glioblastoma Using a Simulation-Based, Patient-Specific Response Metric", "publication_date": "2013-01-23T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0051951"}, {"journal": "PLOS ONE", "abstract": ["\nUntrained, \u201cflower-na\u00efve\u201d bumblebees display behavioural preferences when presented with visual properties such as colour, symmetry, spatial frequency and others. Two unsupervised neural networks were implemented to understand the extent to which these models capture elements of bumblebees\u2019 unlearned visual preferences towards flower-like visual properties. The computational models, which are variants of Independent Component Analysis and Feature-Extracting Bidirectional Associative Memory, use images of test-patterns that are identical to ones used in behavioural studies. Each model works by decomposing images of floral patterns into meaningful underlying factors. We reconstruct the original floral image using the components and compare the quality of the reconstructed image to the original image. Independent Component Analysis matches behavioural results substantially better across several visual properties. These results are interpreted to support a hypothesis that the temporal and energetic costs of information processing by pollinators served as a selective pressure on floral displays: flowers adapted to pollinators\u2019 cognitive constraints.\n"], "author_display": ["Levente L. Orb\u00e1n", "Sylvain Chartier"], "article_type": "Research Article", "score": 0.3605913, "title_display": "Unsupervised Neural Network Quantifies the Cost of Visual Information Processing", "publication_date": "2015-07-22T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0132218"}, {"journal": "PLOS Computational Biology", "abstract": ["\nHumans stand out from other animals in that they are able to explicitly report on the reliability of their internal operations. This ability, which is known as metacognition, is typically studied by asking people to report their confidence in the correctness of some decision. However, the computations underlying confidence reports remain unclear. In this paper, we present a fully Bayesian method for directly comparing models of confidence. Using a visual two-interval forced-choice task, we tested whether confidence reports reflect heuristic computations (e.g. the magnitude of sensory data) or Bayes optimal ones (i.e. how likely a decision is to be correct given the sensory data). In a standard design in which subjects were first asked to make a decision, and only then gave their confidence, subjects were mostly Bayes optimal. In contrast, in a less-commonly used design in which subjects indicated their confidence and decision simultaneously, they were roughly equally likely to use the Bayes optimal strategy or to use a heuristic but suboptimal strategy. Our results suggest that, while people\u2019s confidence reports can reflect Bayes optimal computations, even a small unusual twist or additional element of complexity can prevent optimality.\nAuthor Summary: Confidence plays a key role in group interactions: when people express an opinion, they almost always communicate\u2014either implicitly or explicitly\u2014their confidence, and the degree of confidence has a strong effect on listeners. Understanding both how confidence is generated and how it is interpreted are therefore critical for understanding group interactions. Here we ask: how do people generate their confidence? A priori, they could use a heuristic strategy (e.g. their confidence could scale more or less with the magnitude of the sensory data) or what we take to be an optimal strategy (i.e. their confidence is a function of the probability that their opinion is correct). We found, using Bayesian model selection, that confidence reports reflect probability correct, at least in more standard experimental designs. If this result extends to other domains, it would provide a relatively simple interpretation of confidence, and thus greatly extend our understanding of group interactions. "], "author_display": ["Laurence Aitchison", "Dan Bang", "Bahador Bahrami", "Peter E. Latham"], "article_type": "Research Article", "score": 0.3602925, "title_display": "Doubly Bayesian Analysis of Confidence in Perceptual Decision-Making", "publication_date": "2015-10-30T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.1004519"}, {"journal": "PLoS Computational Biology", "abstract": ["Local neocortical circuits are characterized by stereotypical physiological and structural features that subserve generic computational operations. These basic computations of the cortical microcircuit emerge through the interplay of neuronal connectivity, cellular intrinsic properties, and synaptic plasticity dynamics. How these interacting mechanisms generate specific computational operations in the cortical circuit remains largely unknown. Here, we identify the neurophysiological basis of both the rate of change and anticipation computations on synaptic inputs in a cortical circuit. Through biophysically realistic computer simulations and neuronal recordings, we show that the rate-of-change computation is operated robustly in cortical networks through the combination of two ubiquitous brain mechanisms: short-term synaptic depression and spike-frequency adaptation. We then show how this rate-of-change circuit can be embedded in a convergently connected network to anticipate temporally incoming synaptic inputs, in quantitative agreement with experimental findings on anticipatory responses to moving stimuli in the primary visual cortex. Given the robustness of the mechanism and the widespread nature of the physiological machinery involved, we suggest that rate-of-change computation and temporal anticipation are principal, hard-wired functions of neural information processing in the cortical microcircuit.: The cerebral cortex is the region of the brain whose intricate connectivity and physiology is thought to subserve most computations required for effective action in mammals. Through biophysically realistic computer simulation and experimental recordings in brain tissue, the authors show how a specific combination of physiological mechanisms often found in neurons of the cortex transforms an input signal into another signal that represents the rate of change of the slower components of the input. This is the first report of a neurobiological implementation of an approximate mathematical derivative in the cortex. Further, such a signal integrates naturally into a neurobiologically simple network that is able to generate a linear prediction of its inputs. Anticipation of information is a primary concern of spatially extended organisms which are subject to neural delays, and it has been demonstrated at various different levels: from the retina to sensori-motor integration. We present here a simple and general mechanism for anticipation that can operate incrementally within local circuits of the cortex, to compensate for time-consuming computations and conduction delays and thus contribute to effective real-time action. "], "author_display": ["Gabriel D Puccini", "Maria V Sanchez-Vives", "Albert Compte"], "article_type": "Research Article", "score": 0.36022675, "title_display": "Integrated Mechanisms of Anticipation and Rate-of-Change Computations in Cortical Circuits", "publication_date": "2007-05-11T00:00:00Z", "eissn": "1553-7358", "id": "10.1371/journal.pcbi.0030082"}, {"journal": "PLoS ONE", "abstract": ["\nMachine learning is a popular method for mining and analyzing large collections of medical data. We focus on a particular problem from medical research, supervised multiple sclerosis (MS) lesion segmentation in structural magnetic resonance imaging (MRI). We examine the extent to which the choice of machine learning or classification algorithm and feature extraction function impacts the performance of lesion segmentation methods. As quantitative measures derived from structural MRI are important clinical tools for research into the pathophysiology and natural history of MS, the development of automated lesion segmentation methods is an active research field. Yet, little is known about what drives performance of these methods. We evaluate the performance of automated MS lesion segmentation methods, which consist of a supervised classification algorithm composed with a feature extraction function. These feature extraction functions act on the observed T1-weighted (T1-w), T2-weighted (T2-w) and fluid-attenuated inversion recovery (FLAIR) MRI voxel intensities. Each MRI study has a manual lesion segmentation that we use to train and validate the supervised classification algorithms. Our main finding is that the differences in predictive performance are due more to differences in the feature vectors, rather than the machine learning or classification algorithms. Features that incorporate information from neighboring voxels in the brain were found to increase performance substantially. For lesion segmentation, we conclude that it is better to use simple, interpretable, and fast algorithms, such as logistic regression, linear discriminant analysis, and quadratic discriminant analysis, and to develop the features to improve performance.\n"], "author_display": ["Elizabeth M. Sweeney", "Joshua T. Vogelstein", "Jennifer L. Cuzzocreo", "Peter A. Calabresi", "Daniel S. Reich", "Ciprian M. Crainiceanu", "Russell T. Shinohara"], "article_type": "Research Article", "score": 0.3600853, "title_display": "A Comparison of Supervised Machine Learning Algorithms and Feature Vectors for MS Lesion Segmentation Using Multimodal Structural MRI", "publication_date": "2014-04-29T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0095753"}, {"journal": "PLoS ONE", "abstract": ["\nTo address the vulnerability of geographic routing to multiple security threats such as false routing information, selective forwarding and the Sybil attack in wireless sensor networks, this paper proposes a trust-based defending model against above-mentioned multiple attacks. Considering the characteristics of resource-constrained sensor nodes, trust values of neighboring nodes on the routing path can be calculated through the Dirichlet distribution function, which is based on data packets' acknowledgements in a certain period instead of energy-consuming monitoring. Trust is combined with the cost of geographic and energy aware routing for selecting the next hop of routing. At the same time, the initial trust is dynamically determined, service requests are restricted for malicious nodes in accordance with trust values, and the impact of node mobility is weakened by the trust evolution. The simulation results and analysis show that the proposed model under multiple attacks has advantages in packet delivery ratio and network lifetime over the existing models.\n"], "author_display": ["Guanghua Zhang", "Yuqing Zhang", "Zhenguo Chen"], "article_type": "Research Article", "score": 0.3599268, "title_display": "Using Trust to Secure Geographic and Energy Aware Routing against Multiple Attacks", "publication_date": "2013-10-21T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0077488"}, {"journal": "PLoS ONE", "abstract": ["\nScientific productivity of middle income countries correlates stronger with present and future wealth than indices reflecting its financial, social, economic or technological sophistication. We identify the contribution of the relative productivity of different scientific disciplines in predicting the future economic growth of a nation. Results show that rich and poor countries differ in the relative proportion of their scientific output in the different disciplines: countries with higher relative productivity in basic sciences such as physics and chemistry had the highest economic growth in the following five years compared to countries with a higher relative productivity in applied sciences such as medicine and pharmacy. Results suggest that the economies of middle income countries that focus their academic efforts in selected areas of applied knowledge grow slower than countries which invest in general basic sciences.\n"], "author_display": ["Klaus Jaffe", "Mario Caicedo", "Marcos Manzanares", "Mario Gil", "Alfredo Rios", "Astrid Florez", "Claudia Montoreano", "Vicente Davila"], "article_type": "Research Article", "score": 0.3598846, "title_display": "Productivity in Physical and Chemical Science Predicts the Future Economic Growth of Developing Countries Better than Other Popular Indices", "publication_date": "2013-06-12T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0066239"}, {"journal": "PLoS ONE", "abstract": ["\n        The vertex coloring problem is a classical problem in combinatorial optimization that consists of assigning a color to each vertex of a graph such that no adjacent vertices share the same color, minimizing the number of colors used. Despite the various practical applications that exist for this problem, its NP-hardness still represents a computational challenge. Some of the best computational results obtained for this problem are consequences of hybridizing the various known heuristics. Automatically revising the space constituted by combining these techniques to find the most adequate combination has received less attention. In this paper, we propose exploring the heuristics space for the vertex coloring problem using evolutionary algorithms. We automatically generate three new algorithms by combining elementary heuristics. To evaluate the new algorithms, a computational experiment was performed that allowed comparing them numerically with existing heuristics. The obtained algorithms present an average 29.97% relative error, while four other heuristics selected from the literature present a 59.73% error, considering 29 of the more difficult instances in the DIMACS benchmark.\n      "], "author_display": ["Carlos Contreras Bolton", "Gustavo Gatica", "V\u00edctor Parada"], "article_type": "Research Article", "score": 0.3598746, "title_display": "Automatically Generated Algorithms for the Vertex Coloring Problem", "publication_date": "2013-03-13T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0058551"}, {"journal": "PLOS ONE", "abstract": ["Background: Analyzing high throughput genomics data is a complex and compute intensive task, generally requiring numerous software tools and large reference data sets, tied together in successive stages of data transformation and visualisation. A computational platform enabling best practice genomics analysis ideally meets a number of requirements, including: a wide range of analysis and visualisation tools, closely linked to large user and reference data sets; workflow platform(s) enabling accessible, reproducible, portable analyses, through a flexible set of interfaces; highly available, scalable computational resources; and flexibility and versatility in the use of these resources to meet demands and expertise of a variety of users. Access to an appropriate computational platform can be a significant barrier to researchers, as establishing such a platform requires a large upfront investment in hardware, experience, and expertise. Results: We designed and implemented the Genomics Virtual Laboratory (GVL) as a middleware layer of machine images, cloud management tools, and online services that enable researchers to build arbitrarily sized compute clusters on demand, pre-populated with fully configured bioinformatics tools, reference datasets and workflow and visualisation options. The platform is flexible in that users can conduct analyses through web-based (Galaxy, RStudio, IPython Notebook) or command-line interfaces, and add/remove compute nodes and data resources as required. Best-practice tutorials and protocols provide a path from introductory training to practice. The GVL is available on the OpenStack-based Australian Research Cloud (http://nectar.org.au) and the Amazon Web Services cloud. The principles, implementation and build process are designed to be cloud-agnostic. Conclusions: This paper provides a blueprint for the design and implementation of a cloud-based Genomics Virtual Laboratory. We discuss scope, design considerations and technical and logistical constraints, and explore the value added to the research community through the suite of services and resources provided by our implementation. "], "author_display": ["Enis Afgan", "Clare Sloggett", "Nuwan Goonasekera", "Igor Makunin", "Derek Benson", "Mark Crowe", "Simon Gladman", "Yousef Kowsar", "Michael Pheasant", "Ron Horst", "Andrew Lonie"], "article_type": "Research Article", "score": 0.35986143, "title_display": "Genomics Virtual Laboratory: A Practical Bioinformatics Workbench for the Cloud", "publication_date": "2015-10-26T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0140829"}, {"journal": "PLoS ONE", "abstract": ["\nBrain-Computer Interfaces (BCIs) strive to decode brain signals into control commands for severely handicapped people with no means of muscular control. These potential users of noninvasive BCIs display a large range of physical and mental conditions. Prior studies have shown the general applicability of BCI with patients, with the conflict of either using many training sessions or studying only moderately restricted patients. We present a BCI system designed to establish external control for severely motor-impaired patients within a very short time. Within only six experimental sessions, three out of four patients were able to gain significant control over the BCI, which was based on motor imagery or attempted execution. For the most affected patient, we found evidence that the BCI could outperform the best assistive technology (AT) of the patient in terms of control accuracy, reaction time and information transfer rate. We credit this success to the applied user-centered design approach and to a highly flexible technical setup. State-of-the art machine learning methods allowed the exploitation and combination of multiple relevant features contained in the EEG, which rapidly enabled the patients to gain substantial BCI control. Thus, we could show the feasibility of a flexible and tailorable BCI application in severely disabled users. This can be considered a significant success for two reasons: Firstly, the results were obtained within a short period of time, matching the tight clinical requirements. Secondly, the participating patients showed, compared to most other studies, very severe communication deficits. They were dependent on everyday use of AT and two patients were in a locked-in state. For the most affected patient a reliable communication was rarely possible with existing AT.\n"], "author_display": ["Johannes H\u00f6hne", "Elisa Holz", "Pit Staiger-S\u00e4lzer", "Klaus-Robert M\u00fcller", "Andrea K\u00fcbler", "Michael Tangermann"], "article_type": "Research Article", "score": 0.35931176, "title_display": "Motor Imagery for Severely Motor-Impaired Patients: Evidence for Brain-Computer Interfacing as Superior Control Solution", "publication_date": "2014-08-27T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0104854"}, {"journal": "PLOS ONE", "abstract": ["Purpose: The exciting prospect of Spectral CT (SCT) using photon-counting detectors (PCD) will lead to new techniques in computed tomography (CT) that take advantage of the additional spectral information provided. We introduce a method to reduce metal artifact in X-ray tomography by incorporating knowledge obtained from SCT into a statistical iterative reconstruction scheme. We call our method Spectral-driven Iterative Reconstruction (SPIR). Method: The proposed algorithm consists of two main components: material decomposition and penalized maximum likelihood iterative reconstruction. In this study, the spectral data acquisitions with an energy-resolving PCD were simulated using a Monte-Carlo simulator based on EGSnrc C++ class library. A jaw phantom with a dental implant made of gold was used as an object in this study. A total of three dental implant shapes were simulated separately to test the influence of prior knowledge on the overall performance of the algorithm. The generated projection data was first decomposed into three basis functions: photoelectric absorption, Compton scattering and attenuation of gold. A pseudo-monochromatic sinogram was calculated and used as input in the reconstruction, while the spatial information of the gold implant was used as a prior. The results from the algorithm were assessed and benchmarked with state-of-the-art reconstruction methods. Results: Decomposition results illustrate that gold implant of any shape can be distinguished from other components of the phantom. Additionally, the result from the penalized maximum likelihood iterative reconstruction shows that artifacts are significantly reduced in SPIR reconstructed slices in comparison to other known techniques, while at the same time details around the implant are preserved. Quantitatively, the SPIR algorithm best reflects the true attenuation value in comparison to other algorithms. Conclusion: It is demonstrated that the combination of the additional information from Spectral CT and statistical reconstruction can significantly improve image quality, especially streaking artifacts caused by the presence of materials with high atomic numbers. "], "author_display": ["Radin A. Nasirudin", "Kai Mei", "Petar Panchev", "Andreas Fehringer", "Franz Pfeiffer", "Ernst J. Rummeny", "Martin Fiebich", "Peter B. No\u00ebl"], "article_type": "Research Article", "score": 0.359108, "title_display": "Reduction of Metal Artifact in Single Photon-Counting Computed Tomography by Spectral-Driven Iterative Reconstruction Technique", "publication_date": "2015-05-08T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0124831"}, {"journal": "PLoS ONE", "abstract": ["\n        The EC numbers represent enzymes and enzyme genes (genomic information), but they are also utilized as identifiers of enzymatic reactions (chemical information). In the present work (ECAssigner), our newly proposed reaction difference fingerprints (RDF) are applied to assign EC numbers to enzymatic reactions. The fingerprints of reactant molecules minus the fingerprints of product molecules will generate reaction difference fingerprints, which are then used to calculate reaction Euclidean distance, a reaction similarity measurement, of two reactions. The EC number of the most similar training reaction will be assigned to an input reaction. For 5120 balanced enzymatic reactions, the RDF with a fingerprint length at 3 obtained at the sub-subclass, subclass, and main class level with cross-validation accuracies of 83.1%, 86.7%, and 92.6% respectively. Compared with three published methods, ECAssigner is the first fully automatic server for EC number assignment. The EC assignment system (ECAssigner) is freely available via: http://cadd.whu.edu.cn/ecassigner/.\n      "], "author_display": ["Qian-Nan Hu", "Hui Zhu", "Xiaobing Li", "Manman Zhang", "Zhe Deng", "Xiaoyan Yang", "Zixin Deng"], "article_type": "Research Article", "score": 0.3590707, "title_display": "Assignment of EC Numbers to Enzymatic Reactions with Reaction Difference Fingerprints", "publication_date": "2012-12-28T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0052901"}, {"journal": "PLoS ONE", "abstract": ["\n        Mammals rely on vision, audition, and olfaction to remotely sense stimuli in their environment. Determining how the mammalian brain uses this sensory information to recognize objects has been one of the major goals of psychology and neuroscience. Likewise, researchers in computer vision, machine audition, and machine olfaction have endeavored to discover good algorithms for stimulus classification. Almost 50 years ago, the neuroscientist Jerzy Konorski proposed a theoretical model in his final monograph in which competing sets of \u201cgnostic\u201d neurons sitting atop sensory processing hierarchies enabled stimuli to be robustly categorized, despite variations in their presentation. Much of what Konorski hypothesized has been remarkably accurate, and neurons with gnostic-like properties have been discovered in visual, aural, and olfactory brain regions. Surprisingly, there have not been any attempts to directly transform his theoretical model into a computational one. Here, I describe the first computational implementation of Konorski's theory. The model is not domain specific, and it surpasses the best machine learning algorithms on challenging image, music, and olfactory classification tasks, while also being simpler. My results suggest that criticisms of exemplar-based models of object recognition as being computationally intractable due to limited neural resources are unfounded.\n      "], "author_display": ["Christopher Kanan"], "article_type": "Research Article", "score": 0.35900283, "title_display": "Recognizing Sights, Smells, and Sounds with Gnostic Fields", "publication_date": "2013-01-24T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0054088"}, {"journal": "PLoS ONE", "abstract": ["\nThe inference of a genetic network is a problem in which mutual interactions among genes are inferred from time-series of gene expression levels. While a number of models have been proposed to describe genetic networks, this study focuses on a mathematical model proposed by Vohradsk\u00fd. Because of its advantageous features, several researchers have proposed the inference methods based on Vohradsk\u00fd's model. When trying to analyze large-scale networks consisting of dozens of genes, however, these methods must solve high-dimensional non-linear function optimization problems. In order to resolve the difficulty of estimating the parameters of the Vohradsk\u00fd's model, this study proposes a new method that defines the problem as several two-dimensional function optimization problems. Through numerical experiments on artificial genetic network inference problems, we showed that, although the computation time of the proposed method is not the shortest, the method has the ability to estimate parameters of Vohradsk\u00fd's models more effectively with sufficiently short computation times. This study then applied the proposed method to an actual inference problem of the bacterial SOS DNA repair system, and succeeded in finding several reasonable regulations.\n"], "author_display": ["Shuhei Kimura", "Masanao Sato", "Mariko Okada-Hatakeyama"], "article_type": "Research Article", "score": 0.35895962, "title_display": "Inference of Vohradsk\u00fd's Models of Genetic Networks by Solving Two-Dimensional Function Optimization Problems", "publication_date": "2013-12-30T00:00:00Z", "eissn": "1932-6203", "id": "10.1371/journal.pone.0083308"}], "response": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}